bioRxiv preprint doi: https://doi.org/10.1101/852475; this version posted November 23, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available
under aCC-BY-ND 4.0 International license.

1

Title

2

Estimating animal density in three dimensions using capture-frequency data from remote detectors

3

Authors and affiliations

4

Juan S. Vargas Soto1,2,*, Rowshyra A. Casta√±eda1,2, Nicholas E. Mandrak1,2 and P√©ter K. Moln√°r1,2

5

1

6

Ontario, Canada M5S 3B2

7

2

8

Scarborough, Ontario, Canada M1C 1A4

9

* Corresponding author: juan.vargassoto@mail.utoronto.ca

10

Department of Ecology and Evolutionary Biology, University of Toronto, 25 Willcocks Street, Toronto,

Department of Biological Sciences, University of Toronto Scarborough, 1265 Military Trail,

Running headline: Estimating 3D animal density from remote detectors

1

bioRxiv preprint doi: https://doi.org/10.1101/852475; this version posted November 23, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available
under aCC-BY-ND 4.0 International license.

11

Abstract

12

1.

Remote detectors are being used increasingly often to study aquatic and aerial species,

13

for which movement is significantly different from terrestrial species. While terrestrial camera-trapping

14

studies have shown that capture frequency, along with the species‚Äô movement speed and detector

15

specifications can be used to estimate absolute densities, the approach has not yet been adapted to

16

cases where movement occurs in three dimensions. Frameworks based on animal movement patterns

17

allow estimating population density from camera-trapping data when animals are not individually

18

distinguishable.

19

2.

Here we adapt one such framework to three-dimensional movement to characterize the

20

relationship between population density, animal speed, characteristics of a remote sensor‚Äôs detection

21

zone, and detection frequency. The derivation involves defining the detection zone mathematically and

22

calculating the mean area of the profile it presents to approaching individuals.

23

3.

We developed two variants of the model ‚Äì one assuming random movement of all

24

individuals, and one allowing for different probabilities for each approach direction (e.g. that animals

25

more often swim/fly horizontally than vertically). We used computer simulations to evaluate model

26

performance for a wide range of animal and detector densities. Simulations show that in ideal

27

conditions the method approximates true density well, and that estimates become increasingly accurate

28

using more detectors, or sampling for longer. Moreover, the method is robust to invalidation of

29

assumptions, accuracy is decreased only in extreme cases where all detectors are facing the same way.

30

4.

We provide equations for estimating population density from detection frequency and

31

outline how to estimate the necessary parameters. We discuss how environmental variables and

32

species-specific characteristics affect parameter estimates and how to account for these differences in

33

density estimations.

2

bioRxiv preprint doi: https://doi.org/10.1101/852475; this version posted November 23, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available
under aCC-BY-ND 4.0 International license.

34

5.

Our method can be applied to common remote detection methods (cameras and

35

acoustic detectors), which are currently being used to study a diversity of species and environments.

36

Therefore, our work may significantly expand the number and diversity of species for which density can

37

be estimated.

38

Keywords

39

3D movement, ideal gas model, population density, population surveys, random encounter model,

40

remote detectors

41

3

bioRxiv preprint doi: https://doi.org/10.1101/852475; this version posted November 23, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available
under aCC-BY-ND 4.0 International license.

42

Introduction

43

Remote detection methods like camera-traps originated for studying surface-dwelling species

44

but are increasingly being used to study aquatic, arboreal, and airborne species. These technologies

45

require considerably less effort and resources than alternatives like transects, particularly for long-term

46

studies, and are less invasive. In aquatic studies, remote underwater video has allowed more efficient,

47

extensive, and less biased sampling than diver-based surveys (King, George, Buckle, Novak, & Fulton,

48

2018; Mallet & Pelletier, 2014). Similarly, acoustic detectors are becoming a more accessible and

49

popular tool for studying and monitoring a variety of taxa such as birds (Blumstein et al., 2011; Celis-

50

Murillo, Deppe, & Allen, 2009), echolocating bats (Marques et al., 2013), and even fishes that produce

51

species-specific sounds, especially with the aid of automatic identification algorithms (Lindseth, Lobel,

52

Lindseth, & Lobel, 2018). Camera-traps in turn have been useful to study the demographics of elusive

53

species or to learn about biodiversity across different spatial scales (Ahumada et al., 2011; Barea-Azc√≥n,

54

Virg√≥s, Ballesteros-Duper√≥n, Mole√≥n, & Chirosa, 2007; Karanth, Nichols, Kumar, & Hines, 2006). In

55

every case, the technologies, software and analysis methods are constantly evolving (Burton 2015).

56

Two decades of theoretical developments have created a vast literature of methods for

57

estimating the density of wild populations from camera-trapping data, and some of these methods can

58

be adapted to other types of detectors. Mark-recapture methods, for example, have been used to study

59

aquatic species with individually recognizable spots like whale sharks (Rhincodon typus) and eagle rays

60

(Myliobatidae) (Gonz√°lez-Ramos, Santos-Moreno, Rosas-Alquicira, & Fuentes-Mascorro, 2017; Meekan

61

et al., 2006). However, most species lack individual markings and are therefore not amenable to these

62

approaches (e.g., Karanth et al. 2006). For these species, existing frameworks for estimating density

63

either provide relative estimates only [e.g. detection frequency, minimum number of detected

64

individuals (Sherman, Chin, Heupel, & Simpfendorfer, 2018), maximum number of conspecifics in a

65

single frame (Willis, Millar, & Babcock, 2000)], or make assumptions about how animals move for
4

bioRxiv preprint doi: https://doi.org/10.1101/852475; this version posted November 23, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available
under aCC-BY-ND 4.0 International license.

66

estimating absolute density [e.g. movement around a home range centre (Campos-Candela, Palmer,

67

Balle, & Al√≥s, 2018), random walks (Nakashima, Fukasawa, & Samejima, 2018), ideal gas movement

68

(Rowcliffe, Field, Turvey, & Carbone, 2008)]. These movement-based frameworks need to be adapted

69

for key differences between terrestrial and aquatic or aerial species that perceive the world and move in

70

three dimensions.

71

One approach for estimating population density when individuals cannot be distinguished is the

72

Random Encounter Model (REM), formalized for encounters between animals and camera-traps by

73

Rowcliffe et al. (2008). The model assumes that animals move like ideal gases (in straight lines, in

74

random directions, with constant speed) and as a result the frequency at which a species is

75

photographed by camera-traps (henceforth, ‚Äúdetection frequency‚Äù) scales positively with the number of

76

individuals present in an area (i.e. population density), the species‚Äô mean speed, and the size of the

77

camera‚Äôs detection zone. This relationship can therefore be used to estimate density from the detection

78

frequency. While this method requires more information about a study species than relative abundance

79

measures, it considerably expands the number of species for which density can be estimated using

80

camera-traps. The REM framework can be adapted to species that move in three dimensions rather than

81

two, considering the three-dimensional shape and size of the detection zone.

82

Here, we present such an adaptation to estimate absolute density for aquatic species, using

83

underwater cameras, and for birds and echolocating species, using acoustic sensors. Our adaptation

84

substantially expands possibilities for estimating density from remote detection methods when species

85

can be identified but individuals cannot. This method only requires the detection frequency of a species,

86

information about the sensor‚Äôs detection zone, and an estimate of the species‚Äô speed. We provide two

87

alternatives; the first assumes completely random movement (within the specifications of the ideal gas

88

framework, cf. below), while the second allows accounting for biases in movement direction (e.g. that

89

fish more often swim parallel, rather than perpendicular, to the bottom). First, we will explain the
5

bioRxiv preprint doi: https://doi.org/10.1101/852475; this version posted November 23, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available
under aCC-BY-ND 4.0 International license.

90

framework in detail, highlighting the importance of an accurate mathematical description of the

91

detection zone. We will then develop the formulae for density for two types of detectors and test our

92

estimator‚Äôs performance using computer simulations.

93

Materials and Methods

94

1.

95
96

Model Derivation
1.1. The Random Encounter Model framework

The REM method for estimating density of unmarked animals from camera-trapping data is

97

based on the idea that the encounter rate between a stationary detector and animals of a given species

98

scales linearly with the species‚Äô density (see Hutchinson & Waser, 2007), with a scaling factor that

99

depends on the species‚Äô movement characteristics and the detector‚Äôs ability to record animals that are

100

111

passing by at various distances (Rowcliffe et al., 2008). This relationship is summarized as:

ùê∑=

ùëì
ùëùÃÇ ùë£ÃÖ

(1)

101

where D is animal density, f is detection frequency, and vÃÑ is mean animal movement speed (See Table 1

102

for description of parameters). The scaling constant ùëùÃÇ can be thought of as the mean distance (in 2D) or

103

area (in 3D) sampled instantaneously by the detector over all possible directions, and therefore depends

104

on the sensor‚Äôs technical specifications (opening angles and maximum detection distance) and on

105

environmental variables (e.g. water clarity, foliage or clutter). Here, we will derive an equation for ùëùÃÇ for

106

the three-dimensional case. The main steps are 1) determining the shape of the detection zone, 2)

107

describing it mathematically, and 3) calculating the mean area of its two-dimensional projection. The

108

result is used directly in equation 1 to estimate density from capture frequency. We explain in more

109

detail the projection of the detection zone in Section 2 and derive the density estimator in Section 3. We

110

then test the performance of our estimator with computer-simulated capture data.

6

bioRxiv preprint doi: https://doi.org/10.1101/852475; this version posted November 23, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available
under aCC-BY-ND 4.0 International license.

112
113

Table 1. Parameters used in the derivation of the 3D random encounter method for estimating
animal densities.
Parameter
ùëùÃÇ
S, Si
s
ùúô
Œ∫, Œª, Œ≥
Œº, ŒΩ
œâ, Œ∏
D
ùë£ÃÖ
f

Description
Mean area of the 2D projection of the 3D detection zone.
Surface area of the detection zone, and of its components i.
Maximum distance from the detector at which animals can be
detected
Opening angle of the detector, corresponding to the diagonal field of
vision in a camera.
Horizontal, vertical and diagonal fields of vision of a cropped (camera)
detector.
Opening angles of the disc sectors that make up the sides of a
camera‚Äôs detection zone
Angles describing the direction of an animal approaching the detection
zone, relative to the detector‚Äôs direction.
Population density [animals per unit volume]
Mean movement speed of the study species
Detection frequency: the number of independent recordings of a
species by a detector, divided by deployment time

114
115
116

1.2. Profile of the detection zone, extension from 2D to 3D
For terrestrial camera-traps, where animals move in two dimensions only, ùëùÃÇ in equation 1 is the

117

mean profile width of the camera‚Äôs detection zone, as presented to approaching animals. Given the

118

shape of the detection zone, the profile width depends on the direction of approach (Fig. 1). While not

119

explicitly stated by Rowcliffe et al. (2008), the ‚Äúprofile‚Äù is the projection of the detection zone onto a

120

line perpendicular to the direction of approach. One can therefore use Cauchy‚Äôs surface area theorem

121

(Cauchy, 1841) to calculate the mean profile width. This theorem states that the average projected area

122

of a convex body is proportional to its surface area. The mean profile width of the two-dimensional

123

detection zone is its perimeter P multiplied by a constant:

124

ùëùÃÇ =

1
ùëÉ
ùúã

7

(2)

bioRxiv preprint doi: https://doi.org/10.1101/852475; this version posted November 23, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available
under aCC-BY-ND 4.0 International license.

125

which is the same as equation 2 in Rowcliffe et al. (2008). The theorem can readily be applied to

126

determine the mean profile of a three-dimensional detection zone.

127
128

Figure 1. The different profiles presented by the detection zone of a remote detector to an approaching animal

129

in two (left) or three dimensions (center). In 2D, profiles are lines, while in 3D they are surfaces. The silhouettes

130

A and B (right) show the profiles presented to individuals approaching from directions A and B, respectively.

131
132
133

In the case of a three-dimensional detection zone, equation (1) still holds, with the difference

134

that ùëùÃÇ corresponds to an area instead of a length. This area is nonetheless the projection of the

135

detection zone onto a lower-dimensional object, i.e. onto a plane. According to Cauchy‚Äôs theorem, the

136

mean projected area of a three-dimensional object is obtained from the surface area S as:

137

1
ùëùÃÇ = ùëÜ
4

138

(Cauchy, 1841; Vouk, 1948). To calculate S, we will define the detection zone for two types of sensors: a

139

generic sensor with a conical detection zone (acoustic detectors), and the special case of a sensor with a

140

cropped image (i.e. cameras). For acoustic detectors we also determined how to calculate density when
8

(3)

bioRxiv preprint doi: https://doi.org/10.1101/852475; this version posted November 23, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available
under aCC-BY-ND 4.0 International license.

141

there is bias in directions of approach relative to the direction of the detector, for example bats flying

142

most frequently horizontally into a detection zone facing up.

143
144

2.

Definition of the detection zone

145

A remote detector‚Äôs detection zone is generally determined by two parameters: detection angle

146

and maximum distance from which a signal is detectable. Geometrically, this translates into a cone with

147

a convex base. Acoustic detectors report signals no matter where in this zone they occur, but for

148

cameras some near-boundary signals are lost when images are cropped to a rectangular frame. As such,

149

acoustic detectors have the full ‚Äúconical-with-hat-shaped‚Äù detection zone (Fig. 2A), whereas cameras

150

have a subset of this region defined by the horizontal and vertical angles of view (Fig. 2D). In Section 3.1,

151

we derive expressions to calculate the surface area for both detector types and use Cauchy‚Äôs theorem to

152

calculate the respective mean profile area. This method applies if we assume that all directions of

153

approach are equally likely. In section 3.2, we relax this assumption and consider potential biases in the

154

direction of approach.

9

bioRxiv preprint doi: https://doi.org/10.1101/852475; this version posted November 23, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available
under aCC-BY-ND 4.0 International license.

155
156

Figure 2. Perspective of the detection zone of (A) an acoustic detector and (D) a camera, where the image is

157

cropped into a rectangle. We decomposed both types of detection zone into bottom and top sections to

158

calculate their surface areas: for acoustic detectors, we considered the lateral surface area ùë∫ùë∫ of a cone (B), and

159

the area ùë∫ùë™ of a spherical cap (C); for cameras, we considered the surface area ùë∫ùë≥ of four disc segments (E) and

160

the area ùë∫ùëπ of a spherical rectangle (F). Parameters are œÜ: opening angle of the detector, s: slant height of cone,

161

b: basal radius of the cone, h: height of the spherical cap, Œº and ŒΩ: angles of the disc segments that make up the

162

sides of the cropped detection zone, Œ∫ and Œª: horizontal and vertical fields of vision of the camera

163
164
165

2.1. Uniformly distributed approach directions
To calculate the surface area of an acoustic sensor‚Äôs detection zone, we divide the detection

166

zone into two components: a conic base (Fig. 2B) and a spherical cap (Fig. 2C). The lateral surface area of

167

the cone is calculated from the slant height, s (i.e. the maximum detection distance), and the cone‚Äôs

168

basal radius, b, as:

10

bioRxiv preprint doi: https://doi.org/10.1101/852475; this version posted November 23, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available
under aCC-BY-ND 4.0 International license.

ùëÜùê∂ = ùúãùëèùë†
169

ùëÜùê∂ = ùúãùë† 2 sin

ùúô
2

(4)

170

where ùúô is the detector‚Äôs opening angle. The spherical cap‚Äôs surface area is ùëÜùëÜ = 2ùúãùë†‚Ñé (Kern & Bland,

171

1938), where h is the cap‚Äôs height, given by ‚Ñé = ùë†(1 ‚àí cos 2 ) (Appendix A1), so:

ùúô

172

ùúô
ùëÜùëÜ = 2ùúãùë† 2 (1 ‚àí cos )
2

173

Combining equations (4) ‚Äì (5), we obtain the mean profile area of an acoustic detector‚Äôs detection zone:

174

1
ùëùÃÇ
ùëéùëë = (ùëÜùê∂ + ùëÜùëÜ )
4

175

1
ùúô
ùúô
2
2
ùëùÃÇ
ùëéùëë = (ùúãùë† sin + 2ùúãùë† (1 ‚àí cos ))
4
2
2

176

1 2
ùúô
ùúô
ùëùÃÇ
ùëéùëë = ùúãùë† (2 ‚àí 2 cos + sin )
4
2
2

(5)

(6)

177

Cauchy‚Äôs theorem is only valid for convex bodies, so equation 6 only applies when the detection

178

angle is smaller than œÄ (180¬∞) or equal to 2œÄ (360¬∞). The latter occurs when signals can be detected from

179

any direction, as is the case with omnidirectional microphones. Consider for example an acoustic

180

detector with a detection angle of 90¬∞ (œÄ/2 rad) that can detect signals from 10 meters away.

181

Substituting s and œï in eq. 6 we obtain:

182

1
1 ùúã
1 ùúã
2
ùëùÃÇ
ùëéùëë = ùúã √ó 10 (2 ‚àí 2 cos ( √ó ) + sin ( √ó ))
4
2 2
2 2

183

2
ùëùÃÇ
ùëéùëë = 101.5 ùëö

184

To calculate the surface area of a camera‚Äôs detection zone, we note that image cropping creates

185

a detection zone bounded by a spherical rectangle cap sitting atop two pairs of disc sectors of radius s

186

and delimited by angles Œº and ŒΩ (Fig. 2D). These angles are related to the vertical (Œª), horizontal (Œ∫), and

187

diagonal (Œ≥) fields of vision of the camera, of which at least one is normally provided by the
11

bioRxiv preprint doi: https://doi.org/10.1101/852475; this version posted November 23, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available
under aCC-BY-ND 4.0 International license.

188

manufacturer (see Appendix A3 for how to calculate the unknown angles). As the surface areas of the

189

disc sectors are given by ùëÜùúà = ùúãùë† 2 2ùúã or ùëÜùúá = ùúãùë† 2 2ùúã, respectively, the total lateral surface area of the

190

camera‚Äôs detection zone, SL (Fig. 1E), becomes

ùúà

ùúá

ùëÜùêø = 2ùëÜùë£ + 2ùëÜùúá = ùë† 2 (ùúá + ùúà)

191

(7)

192

Furthermore, the surface area of the spherical rectangle cap, SR (Fig. 1F), is obtained by multiplying the

193

surface area of a sphere of radius s by the proportion of the sphere occupied by the spherical rectangle:

194

ùëÜùëÖ = 4ùúãùë† 2

Œ©
4ùúã

= s2Œ©

195

(8)

196

where Œ© is the solid angle delimited by the horizontal and vertical fields of vision, Œ∫ and Œª, given by

197

(Appendix A2):
ùúÖ
ùúÜ
tan 2 tan 2

Œ© = 4 arcsin

198

‚àö1 +

199

ùúÖ
tan2 ‚àö1 + tan2
2

ùúÜ
2

(9)

The mean profile area for a camera is therefore:
1
ùëùÃÇ
ùëêùëéùëö = (ùëÜùêø + ùëÜùëÖ )
4
1 2
ùëùÃÇ
ùë† (ùúá + ùúà) + 4ùë† 2 arcsin
ùëêùëéùëö =
4

200

(
1 2
ùëùÃÇ
ùúá + ùúà + 4 arcsin
ùëêùëéùëö = ùë†
4
(

201
202

ùúÖ
ùúÜ
tan 2 tan 2
ùúÖ
ùúÜ
‚àö1 + tan2 ‚àö1 + tan2
2
2)

ùúÖ
ùúÜ
tan 2 tan 2
ùúÖ
ùúÜ
‚àö1 + tan2 ‚àö1 + tan2
2
2)

(10)

Consider for example an underwater camera with horizontal and vertical fields of vision (FOV) of
122.6¬∞ (2.1 rad) and 94.4¬∞ (1.6 rad), respectively (these correspond to a GoPro Hero 7 using a wide 4:3
12

bioRxiv preprint doi: https://doi.org/10.1101/852475; this version posted November 23, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available
under aCC-BY-ND 4.0 International license.

203

aspect ratio). Using eq. S3.2 we obtain a diagonal FOV of 154.5¬∞ (2.7 rad) and using eq. S3.4 we obtain

204

the lateral angles Œº = 142.1¬∞ = 2.5 rad and ŒΩ = 125.2¬∞ = 2.2 rad. Substituting these values in eq. 10,

205

assuming again a detection distance of 10 m, we obtain the mean profile area for this camera:

206

2.1
1.6
tan 2 tan 2
1
2
ùëùÃÇ
2.5 + 2.2 + 4 arcsin
ùëêùëéùëö = √ó 10
4
‚àö1 + tan2 2.1 ‚àö1 + tan2 1.6
(
2
2)

207

2
ùëùÃÇ
ùëêùëéùëö = 186.6 ùëö

208

2.2. Bias in direction of movement

209

The method described above for calculating the detection zone‚Äôs mean profile area ùëùÃÇ assumes

210

that every direction of approach is equally likely. However, some angles of approach could occur more

211

frequently than others depending both on the study species and the placement and orientation of the

212

detector. Picture, for example, fish moving in a stream in the direction of the current. The profile

213

presented to all of them by a camera is the same and the effective mean profile area is greater or lower

214

than the expected mean profile area, depending on which way the detector is facing. To account for

215

these biases, we derive formulae to calculate the detection zone‚Äôs projected area ùëù for any direction of

216

approach and then weight these according to the probability of approach directions. As these

217

calculations quickly become lengthy and complicated, we here summarize the approach for acoustic

218

detectors and provide detailed calculations in Appendix A4.

219

220

The mean profile area is given by:

ùëùÃÇ =

1 ùúã
‚à´ ùëÉ(ùúî)ùëù(ùúî) ùëëùúî
œÄ 0

(11)

221

where P(œâ) is the probability that an individual approaches the detector at angle œâ, and p(œâ) gives the

222

profile area corresponding to that direction. Depending on œâ ‚Äì the angle relative to the direction of the

13

bioRxiv preprint doi: https://doi.org/10.1101/852475; this version posted November 23, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available
under aCC-BY-ND 4.0 International license.

223

detector ‚Äì the different components of the detection zone may be visible or hidden. We obtain

224

therefore four different formulae to calculate p(œâ):
ùúô
2
ùúô
ùëéùëí + ùëéùëë ,
sin ùúî ‚â§ sin
2
ùëù(ùúî) =
ùúô
ùëéùëí + ùëé‚Ñé ,
sin ùúî > sin
2
ùúô
{ùëéùëí + ùëéùëë + ùëé‚Ñé , sin ùúî > sin 2
ùëéùëí ,

225

sin ùúî ‚â§ sin

ùúô
2
ùúô
ùëéùëõùëë sin ùúî > cos
2
ùúô
ùëéùëõùëë sin ùúî ‚â§ cos
2
ùúô
ùëéùëõùëë sin ùúî > cos
2
ùëéùëõùëë sin ùúî ‚â§ cos

(12)

226

The first element, ùëéùëí , corresponds to the area of the projection encompassed by the base of the cone

227

(area I in Fig. 3), while the areas ùëé‚Ñé and ùëéùëë are the projections of the visible parts of the spherical cap

228

(area II) and the cone (area III). The derivation of ae, ah, ad, and the rationale behind them are given in

229

Appendix A4.

230
231

Figure 3. Perspective of a conic detection zone (left), showing the angles œâ and Œ∏ that define the direction of

232

approach of an individual. Different angles œâ result in different profiles (right). The labels indicate the different

233

areas that need to be calculated in each case; I is the area of the projection encompassed by the circle at the

234

base of the cone, II is the projection of the spherical cap, III is the projection of the cone‚Äôs sides.

235

14

bioRxiv preprint doi: https://doi.org/10.1101/852475; this version posted November 23, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available
under aCC-BY-ND 4.0 International license.

236
237

3.

Simulation tests

We tested the formula for estimating density from detection frequency using computer

238

simulations. Firstly, these serve to confirm that the method performs well under ideal conditions (i.e.

239

when all model assumptions are met and perfect information about the species‚Äô movement is available)

240

and secondly to test the robustness to invalidation of assumptions. The assumptions regarding animal

241

movement are those of the ideal gas model: individuals moving in a straight line, in random directions,

242

and at a constant speed. We assumed perfect detection and an exact knowledge of the detection

243

distance and opening angle and evaluated the method‚Äôs performance for a range of animal densities

244

and detector numbers. Furthermore, to determine the robustness of our method, we evaluated its

245

performance for the following scenarios: (i) allowing variation in speed by randomly selecting different

246

individual speeds; (ii) allowing a non-random distribution of approach directions by randomly selecting

247

individual ‚Äòtilt‚Äô angles, combined with realistic scenarios of detector placement and orientation.

248

We set up the simulation as follows: Individuals were distributed at random locations within a

249

cube of side 10, and each one was assigned a random direction (x, y, z vector components drawn from a

250

uniform distribution from -1 to 1). All individuals moved at the same speed and bounced back into the

251

cube if they reached the reflective boundaries. We tested the density estimator for a range of densities

252

between 0.1 and 10 ind.uv-1 (individuals per unit volume), i.e. between 100 and 10000 individuals.

253

We placed between 5 and 25 detectors facing in random directions at random locations within

254

the ‚Äòsampling zone‚Äô, a cube of side 4 situated at the centre of the larger cube. We set a detection

255

distance of 0.5 ud (unit distance) and a detection angle of 45¬∞, which yields a mean profile area of 0.105

256

ua (unit area) (equation 6).

257
258

We set movement speed equal to one length of the detection radius per time step and ran each
simulation for 40 steps. We counted an encounter whenever an individual entered a detection zone.
15

bioRxiv preprint doi: https://doi.org/10.1101/852475; this version posted November 23, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available
under aCC-BY-ND 4.0 International license.

259

Thus, we obtained for each step and each detector the total number of detections up to that time point.

260

We calculated the detection frequency f by dividing this cumulative count by the number of steps and

261

then divided f by the mean profile area and the movement speed to estimate density (eq. 1). We

262

recorded the mean estimated density across all detectors at every time step. We also determined how

263

performance changed with effort both in terms of the number of detectors and sampling time.

264

To test how variability in speed among individuals affected density estimates, we ran

265

simulations assigning each individual a speed drawn at random from a normal distribution with mean

266

0.5 ud/step, and with standard deviation between 0 and 0.1 ud/step. Similarly, to determine the effect

267

of having biased movement directions, we drew the vertical (z) component of the individual direction

268

vectors from a truncated normal distribution centred around 0 and bounded between -1 and 1, so that

269

most individuals would move near horizontally. We assessed the effect of this bias on the performance

270

of the estimator for several scenarios of detector distribution (regular spacing of detectors in 2D on a

271

plane, regular spacing in 3D throughout the sampling cube, random distribution in 3D) and orientation

272

(all horizontal, all vertical, random). These simulations were run with ten detectors.

273

We iterated each scenario 100 times, and assessed model performance by calculating the bias,

274

precision and accuracy of the density estimate relative to the real density at the end of each simulation.

275

We used the scaled mean error (SME), the coefficient of variation (CV) and the scaled mean square error

276

(SMSE) as indicators of bias, precision, and accuracy, respectively:
ùëõ

277

1
ùëÜùëÄùê∏ =
‚àë(ùê∏ùëó ‚àí ùê¥)
ùê¥ùëõ
ùëó=1

ùëÜùê∑
ùê∏ÃÖ

278

ùê∂ùëâ =

279

1
2
ùëÜùëÄùëÜùê∏ = 2 ‚àë(ùê∏ùëó ‚àí ùê¥)
ùê¥ ùëõ

ùëõ

ùëó=1

16

bioRxiv preprint doi: https://doi.org/10.1101/852475; this version posted November 23, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available
under aCC-BY-ND 4.0 International license.

280

where ƒí is the mean density estimate, Ej is the density estimate of the jth iteration, SD is the standard

281

deviation of density estimates across iterations, A is the real density, and n is the number of iterations

282

(Walther & Moore, 2005). The bias metric SME indicates whether the true value is over- or

283

underestimated, the precision metric CV indicates the variability among estimates, and the accuracy

284

metric SMSE measures both bias and precision in a single index. The closer to zero each index is, the

285

better the performance of the estimator.

286
287

Results

288

Simulations show that, under ideal conditions, the estimated density closely approximates the

289

real density. Regardless of the number of detectors used, the estimate density was within 5% of the real

290

value at the end of each simulation (Fig. 4A), and the standard deviation was no larger than 30% of the

291

mean estimate (Fig. 4B). The overall performance of the method nonetheless depended on sampling

292

effort, both in terms of the number of detectors and the sampling time. The lowest number of detectors

293

yielded the greatest bias and the lowest precision and accuracy, and all indices improved substantially

294

with the deployment of additional detectors (Fig. 4A-C). Moreover, bias also decreases, and precision

295

and accuracy increase, as sampling time increases (Fig. 4D-F), meaning that in real-life applications a low

296

availability of detectors could at least be partially compensated for by longer sampling times.

17

bioRxiv preprint doi: https://doi.org/10.1101/852475; this version posted November 23, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available
under aCC-BY-ND 4.0 International license.

B

Precision (CV)

Bias (SME)

0.00

-0.02

-0.04

0.3

C

Accuracy (SMSE)

A

0.2

0.1

0.0
5

10

15

20

25

5

10

15

20

F

0.1
0.0

1.5
1.0
0.5
0.0

-0.1

297

Accuracy (SMSE)

Precision (CV)

Bias (SME)

0.2

Time (steps)

5

40

10

15

20

25

Detectors

E

30

0.02

25

0.3

20

0.04

Detectors

D

10

0.06

0.00

Detectors

0

0.08

4

Detectors
5

3

10
15

2

20
25

1
0

0

10

20

30

Time (steps)

40

0

10

20

30

40

Time (steps)

298

Figure 4. Performance of the 3D density estimation method for different levels of effort. The top row shows the

299

mean bias, precision, and accuracy metric values at the end of the simulations as a function of the number of

300

detectors deployed. The bottom row shows the change in these metrics as time progresses in the simulation.

301
302

The real density did not seem to bias the estimator, except at extremely low densities (Fig. 5A). Precision

303

(Fig. 5B), and thus overall accuracy (Fig. 5C), however, depended strongly on the population‚Äôs density,

304

with the estimator‚Äôs CV decreasing by a factor of more than 9 between the lowest (0.1 ind.uv-1) and

18

bioRxiv preprint doi: https://doi.org/10.1101/852475; this version posted November 23, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available
under aCC-BY-ND 4.0 International license.

highest (10 ind.uv-1) densities considered.
B

Bias (SME)

0.15
0.10
0.05
0.00

C

Accuracy (SMSE)

A

Precision (CV)

305

0.6
0.4
0.2
0.0

0.0

2.5

5.0

7.5 10.0

7.5 10.0

E

0.000
-0.025
-0.050

306

5.0

0.0

2.5

5.0

7.5 10.0
1

Real density (uv )
F

Accuracy (SMSE)

Bias (SME)

0.025

2.5

Real density (uv )

Precision (CV)

0.050

0.2

1

Real density (uv )
D

0.4

0.0
0.0

1

0.6

0.20
0.15
0.10
0.05
0.00

0.04

0.02

0.00

0 0.025 0.05 0.075 0.1

0 0.025 0.05 0.075 0.1

0 0.025 0.05 0.075 0.1

Speed SD

Speed SD

Speed SD

307

Figure 5. Effects of population density (top row), and among-individual variability in movement speed (bottom

308

row) on the performance of the 3D density estimation method. All parameters were as described in the text,

309

using in particular a constant mean speed of 0.5 ud/step in the top row, and varying the speed among

310

individuals by drawing from a normal distribution with standard deviation between 0 and 0.1 ud in the bottom

311

row.

312
313

The simplifying assumption of equal movement speeds among individuals appears to be robust,

314

as introducing among-individual variance in speed did not noticeably affect the estimator‚Äôs bias,

315

precision, or accuracy (Figs. 5D-F).

316

Having biases in the direction of movement, however, did decrease performance in some scenarios,

317

particularly when all individuals were moving horizontally, i.e. with a standard deviation of zero around
19

bioRxiv preprint doi: https://doi.org/10.1101/852475; this version posted November 23, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available
under aCC-BY-ND 4.0 International license.

318

the mean direction. In this case, precision and accuracy decreased when detectors were placed on a

319

single plane and oriented vertically (Fig. 6D, G) or horizontally (Fig. 6E, H). Conversely, detectors

320

distributed regularly or randomly across the 3D space yielded similar accuracy and precision estimates

321

for all cases of movement direction bias. Nevertheless, there was virtually equal performance regardless

322

of detector placement when detectors faced in random directions (Fig. 6F, I). Moreover, the random

323

orientation of detectors also generally lowered the estimator‚Äôs bias compared to the scenarios where all

324

detectors were facing vertically or horizontally (contrast Figs. 6A-C).

Bias (SME)

Vertical

Horizontal

Random

A

B

C

D

E

F

G

H

I

0.000
-0.025
-0.050

Accuracy (SMSE) Precision (CV)

-0.075
0.3
0.2
0.1
0.0
0.100
0.075
0.050
0.025
0.000
0

325
326
327

1

2

3

4

5 0

1

2

3

4

5 0

1

2

3

4

5

Direction bias (SD)
Figure 6. Effects of sampling strategies and animal movement bias on the performance of the 3D
density estimation method. Columns represent the orientation of detectors: all vertical (left), all horizontal

20

bioRxiv preprint doi: https://doi.org/10.1101/852475; this version posted November 23, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available
under aCC-BY-ND 4.0 International license.

328

(middle) or all random (right). Symbols show the distribution of detectors: regular spacing on a 2D plane

329

(circles), regular spacing in 3D throughout the sampling cube (triangles), and random distribution in 3D

330

(squares). Parameters are as described in the text, using in particular a vertical direction component drawn at

331

random from a truncated normal distribution centred at 0 with standard deviations between 0 and 5 to show

332

direction bias.

333
334

Discussion

335

We have outlined a method to use remote detectors such as underwater cameras or acoustic

336

sensors to estimate population density from an encounter rate. The underlying random encounter

337

model was originally proposed and tested as a density estimator for species moving in a two-

338

dimensional terrestrial environment by Rowcliffe et al. (2008), and our calculations now allow its

339

adaptation to species that move in three dimensions such as fishes, birds and bats. The basic

340

requirements regarding detector specifications and information on movement speed remain the same

341

as for the two-dimensional case.

342

Simulations show good performance of the estimator, low levels of bias, and a high degree of

343

precision. Such consistent performance was expected when all assumptions were met, which may not

344

be the case in real-life applications. We showed, however, that the method is robust to violations of

345

assumptions. There was very little effect of increased variance in speed among individuals, or of bias in

346

direction. Furthermore, our simulation results suggest that any effect could be limited or altogether

347

eliminated simply by orienting detectors in different directions, even when detectors are placed on a

348

single plane (on the ground, for example).

349
350

Performance is significantly influenced by sampling effort as it relates to the real density, in
terms of both time and number of detectors. At low densities especially, insufficient effort could result
21

bioRxiv preprint doi: https://doi.org/10.1101/852475; this version posted November 23, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available
under aCC-BY-ND 4.0 International license.

351

in an over- or underestimation of density. An advantage of using a method based on movement models

352

is that researchers can use the same framework to calculate the effort needed beforehand. Rearranging

353

eq. 1 and substituting density and speed with prior information allows calculating an expected capture

354

frequency. We saw no bias at higher densities, but we expect that in practice extremely high densities

355

(e.g. in fish schools) may prevent properly counting individuals, resulting in underestimated densities. In

356

these cases, researchers could first estimate a density of groups and obtain overall density by

357

multiplying this estimate by an independently calculated mean group size (Rowcliffe et al. 2008).

358

Our method can be applied in cases where a lack of individual markings impedes the use of

359

mark-recapture techniques. It is also an improvement over indices of relative abundance such as the

360

maximum or mean number of conspecifics in a single frame (Schobernd, Bacheler, & Conn, 2014;

361

Sherman et al., 2018). These metrics are commonly used to analyse footage from baited-remote-

362

underwater-video-stations (BRUVS), but can underestimate true abundance (Cappo, Harvey, Malcolm, &

363

Speare, 2003; Stobart et al., 2015). We know of no similar tools to estimate abundance from acoustic

364

detectors. These sensors allow to identify species and count passages through the detection zone, so

365

our method is also applicable with these technologies, provided the detection zone and mean species

366

speed can be accurately measured.

367

For many species, mean speed will not be immediately available in the literature but could be

368

approximated with additional measurements. For example, using two cameras in a stereo arrangement,

369

the footage from both could be used to estimate speed (Somerton, Williams, & Campbell, 2017;

370

Williams, Rooper, & Towler, 2010) and ‚Äì using only one of the two cameras ‚Äì population density in the

371

same study.. Movement If detectors are deployed for several days the estimation of mean speed must

372

include periods of inactivity (see Carbone, Cowlishaw, Isaac, & Rowcliffe, 2005). Ideally, surveys should

373

be conducted at the same time of day when working in different sites, during the species‚Äô daily activity

374

peak.
22

bioRxiv preprint doi: https://doi.org/10.1101/852475; this version posted November 23, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available
under aCC-BY-ND 4.0 International license.

375

Additional to speed, an exact characterization of the detection zone is required to estimate

376

density accurately. This zone is determined first by the opening angle (for acoustic detectors), or the

377

horizontal and vertical fields of vision (for cameras), usually given by the manufacturer. However, action

378

cameras commonly used in remote underwater surveys have wide-angle lenses, which distort the

379

image. Because of this, the diagonal field of vision will not correspond to the angle calculated assuming

380

a rectilinear lens (see Appendix A3). The additional area in the projection due to the distortion should be

381

small, so we suggest assuming a rectilinear lens for consistency.

382

The second element needed to characterize the detection zone is the detection radius. Unlike

383

the opening angles the detection radius is influenced by environmental variables. For example, for

384

underwater cameras, detection distance depends on visibility (i.e. turbidity), which should be

385

considered when comparing densities across sites. Similarly, for acoustic detectors, atmospheric

386

conditions like temperature and humidity affect how far an acoustic signal travels, effectively influencing

387

the detection distance for birds and bats (Lawrence & Simmons, 1982; Snell-Rood, 2012). In both

388

aquatic and aerial surveys, physical obstacles such as vegetation will also limit detectability; for instance,

389

detection of bats with low-frequency calls (25 kHz) is significantly hindered by habitat structure

390

(Patriquin, Hogberg, Chruszcz, Barclay, & Barclay, 2003; Weller & Zabel, 2002).

391

Environmental variables also interact with species-specific traits, generating different detection

392

distances even under comparable environmental conditions. For example, cameras can detect larger

393

species further than smaller species, and acoustic detectors will detect species with lower frequency or

394

more intense calls at greater distances from than species with high-frequency calls (Jakobsen, Brinkl√∏v,

395

& Surlykke, 2013; Lawrence & Simmons, 1982; Snell-Rood, 2012; Surlykke & Kalko, 2008). Given the

396

multiple factors that influence detection distance, we suggest an ad-hoc calculation for every system, for

397

example placing a model of the study species progressively further from a camera until it is no longer

398

recognizable. With acoustic detectors the same can be done using a speaker at a range of distances.
23

bioRxiv preprint doi: https://doi.org/10.1101/852475; this version posted November 23, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available
under aCC-BY-ND 4.0 International license.

399

Our calculations are performed for individual detectors, but, as our simulations show, the best

400

results are obtained when averaging across multiple detectors. Improved performance using more

401

detectors is to be expected, as averaging across detectors minimizes possible sampling errors (Rowcliffe

402

et al., 2008). Using more detectors also reduced variability across trials, implying that less effort is

403

required to obtain accurate density estimates. Sampling designs should seek to maximize the number of

404

encounters with a target species. using a number and spacing of detectors that capture the movement

405

of the species of interest, while maintaining independence across detectors (Keiter et al., 2017).

406

Finally, we note that detection zones can also be affected by placement. Cameras placed in

407

shallow streams, for example, could have their detection zone cut at the top by the surface and at the

408

bottom by the substrate. In these cases, the resulting video frames can be cropped so that neither the

409

ground nor the surface are visible, and the fields of vision and capture frequency recalculated

410

accordingly (this would be equivalent to having a camera with narrower field of vision). If topography

411

permits, one can prevent the issue of an incomplete detection zone by placing the camera in mid-water

412

such that neither the water surface nor the bottom are visible. This would be a departure from current

413

designs that set cameras on the ground but would avoid the issue of cropping the detection zone. For

414

benthic species or very shallow streams none of these solutions might be feasible, and we would

415

recommend using a 2D approach.

416

In summary, we have proposed a method for estimating density in three dimensions using data

417

from remote detectors, which can be used in ecological and conservation research and as a monitoring

418

tool. The description of detection zones provided will be useful in translating other density estimation

419

methods that are also based on the ideal gas model (Campos-Candela et al., 2018; Moeller, Lukacs, &

420

Horne, 2018). There is an extensive and growing literature characterizing sampling requirements for

421

camera-traps in two dimensions, but such considerations are practically non-existent for species moving

422

in three dimensions (Burton et al., 2015; O‚ÄôConnell, Nichols, & Karanth, 2010). Our analyses allow
24

bioRxiv preprint doi: https://doi.org/10.1101/852475; this version posted November 23, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available
under aCC-BY-ND 4.0 International license.

423

extending these considerations to three dimensions, and it is our hope that our work will prompt new

424

study designs and applications of remote detection methods to study a broader range of species and

425

environments. We lay out the theoretical foundation of the method but recognize that it will require

426

empirical validation. The sampling of aquatic, airborne, and arboreal species each comes with intrinsic

427

challenges and field trials must be conducted to confirm that the method performs in real conditions as

428

well as predicted by simulations. Given the existing camera-trapping literature and the use of

429

underwater cameras to estimate abundance, we believe the application of our method will be more

430

straightforward in underwater censuses. The application to acoustic detectors will require further work

431

to characterize the detection zone, as currently there is no quantitative way to measure the detection

432

distance under different environmental conditions. Thus, the requirements of our method open new

433

avenues of research in remote detection.

25

bioRxiv preprint doi: https://doi.org/10.1101/852475; this version posted November 23, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available
under aCC-BY-ND 4.0 International license.

434

Acknowledgments

435

PKM is grateful for support from an NSERC (Natural Sciences and Engineering Research Council

436

of Canada) Discovery Grant, CFI (Canada Foundation for Innovation) John R. Evans Leader Funds, and

437

MRIS Ontario Research Funds. Funding by FRQNT (Fonds de recherche nature et technologie) to RAC

438

and an NSERC Discovery Grant to NEM is gratefully acknowledged.

439

Authors‚Äô contributions

440

RAC, PKM and JSVS conceived the original concept. JSVS and PKM derived the equations. JSVS

441

conducted simulations and led the writing of the manuscript. All authors contributed critically to the

442

drafts and gave final approval for publication.

443

Data accessibility

444

The code for the simulations and to reproduce figures is available on github in the following

445

repository: juansvs/3D_DensityEstimation

446

26

bioRxiv preprint doi: https://doi.org/10.1101/852475; this version posted November 23, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available
under aCC-BY-ND 4.0 International license.

447

References

448

Ahumada, J. A., Silva, C. E. F., Gajapersad, K., Hallam, C., Hurtado, J., Martin, E., ‚Ä¶ Andelman, S. J. (2011).

449

Community structure and diversity of tropical forest mammals: data from a global camera trap

450

network. Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences,

451

366(1578), 2703‚Äì11. doi:10.1098/rstb.2011.0115

452

Barea-Azc√≥n, J. M., Virg√≥s, E., Ballesteros-Duper√≥n, E., Mole√≥n, M., & Chirosa, M. (2007). Surveying

453

carnivores at large spatial scales: a comparison of four broad-applied methods. Biodiversity and

454

Conservation, 16(4), 1213‚Äì1230. doi:10.1007/s10531-006-9114-x

455

Blumstein, D. T., Mennill, D. J., Clemins, P., Girod, L., Yao, K., Patricelli, G., ‚Ä¶ Kirschel, A. N. G. (2011).

456

Acoustic monitoring in terrestrial environments using microphone arrays: applications,

457

technological considerations and prospectus. Journal of Applied Ecology, 48, 758‚Äì767.

458

doi:10.1111/j.1365-2664.2011.01993.x

459

Burton, A. C., Neilson, E., Moreira, D., Ladle, A., Steenweg, R., Fisher, J. T., ‚Ä¶ Boutin, S. (2015). REVIEW:

460

Wildlife camera trapping: a review and recommendations for linking surveys to ecological

461

processes. Journal of Applied Ecology, 52(3), 675‚Äì685. doi:10.1111/1365-2664.12432

462

Campos-Candela, A., Palmer, M., Balle, S., & Al√≥s, J. (2018). A camera-based method for estimating

463

absolute density in animals displaying home range behaviour. Journal of Animal Ecology, 87(3),

464

825‚Äì837. doi:10.1111/1365-2656.12787

465

Cappo, M., Harvey, E., Malcolm, H., & Speare, P. (2003). Potential of video techniques to monitor

466

diversity, abundance and size of fish in studies of marine protected areas. University of

467

Queensland.

468

Carbone, C., Cowlishaw, G., Isaac, N. J. B., & Rowcliffe, J. M. (2005). How Far Do Animals Go?
27

bioRxiv preprint doi: https://doi.org/10.1101/852475; this version posted November 23, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available
under aCC-BY-ND 4.0 International license.

469

Determinants of Day Range in Mammals. The American Naturalist, 165(2), 290‚Äì297.

470

doi:10.1086/426790

471
472
473

Cauchy, A. (1841). Note sur divers th√©or√®mes relatifs √† la rectification des courbes et √† la quadrature des
surfaces. Comptes Rendus de l‚ÄôAcad√©mie de Sciences, 13, 1060‚Äì1065.
Celis-Murillo, A., Deppe, J. L., & Allen, M. F. (2009). Using soundscape recordings to estimate bird

474

species abundance, richness, and composition. Journal of Field Ornithology, 80(1), 64‚Äì78.

475

doi:10.1111/j.1557-9263.2009.00206.x

476

Gonz√°lez-Ramos, M. S., Santos-Moreno, A., Rosas-Alquicira, E. F., & Fuentes-Mascorro, G. (2017).

477

Validation of photo-identification as a mark-recapture method in the spotted eagle ray Aetobatus

478

narinari. Journal of Fish Biology, 90(3), 1021‚Äì1030. doi:10.1111/jfb.13215

479
480
481
482

Hutchinson, J. M. C., & Waser, P. M. (2007). Use, misuse and extensions of ‚Äúideal gas‚Äù models of animal
encounter. Biological Reviews, 82(3), 335‚Äì359. doi:10.1111/j.1469-185X.2007.00014.x
Jakobsen, L., Brinkl√∏v, S., & Surlykke, A. (2013). Intensity and directionality of bat echolocation signals.
Frontiers in Physiology, 4, 89. doi:10.3389/fphys.2013.00089

483

Karanth, K. U., Nichols, J. D., Kumar, S., & Hines, J. E. (2006). Assessing Tiger Population Dynamics Using

484

Photographic Capture ‚Äì Recapture Sampling. Ecology, 87(11), 2925‚Äì2937. doi:10.1890/0012-

485

9623(2006)87[323:ATPD]2.0.CO;2

486

Keiter, D. A., Davis, A. J., Rhodes, O. E., Cunningham, F. L., Kilgo, J. C., Pepin, K. M., & Beasley, J. C.

487

(2017). Effects of scale of movement, detection probability, and true population density on

488

common methods of estimating population density. Scientific Reports, 7(1), 9446.

489

doi:10.1038/s41598-017-09746-5

490

Kern, W. F., & Bland, J. R. (1938). Solid mensuration with proofs (2nd ed.). London: Chapman & Hall.
28

bioRxiv preprint doi: https://doi.org/10.1101/852475; this version posted November 23, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available
under aCC-BY-ND 4.0 International license.

491

King, A. J., George, A., Buckle, D. J., Novak, P. A., & Fulton, C. J. (2018). Efficacy of remote underwater

492

video cameras for monitoring tropical wetland fishes. Hydrobiologia, 807(1), 145‚Äì164.

493

doi:10.1007/s10750-017-3390-1

494

Lawrence, B. D., & Simmons, J. A. (1982). Measurements of atmospheric attenuation at ultrasonic

495

frequencies and the significance for echolocation by bats. The Journal of the Acoustical Society of

496

America, 71(3), 585‚Äì590. doi:10.1121/1.387529

497
498
499

Lindseth, A., Lobel, P., Lindseth, A. V., & Lobel, P. S. (2018). Underwater Soundscape Monitoring and Fish
Bioacoustics: A Review. Fishes, 3(3), 36. doi:10.3390/fishes3030036
Mallet, D., & Pelletier, D. (2014). Underwater video techniques for observing coastal marine biodiversity:

500

A review of sixty years of publications (1952‚Äì2012). Fisheries Research, 154, 44‚Äì62.

501

doi:10.1016/j.fishres.2014.01.019

502

Marques, T. A., Thomas, L., Martin, S. W., Mellinger, D. K., Ward, J. A., Moretti, D. J., ‚Ä¶ Tyack, P. L.

503

(2013). Estimating animal population density using passive acoustics. Biological Reviews, 88(2),

504

287‚Äì309. doi:10.1111/brv.12001

505

Meekan, M. G., Bradshaw, C. J. A., Press, M., McLean, C., Richards, A., & Quasnichka, S. (2006).

506

Population size and structure of whale sharks Rhincodon typus at Ningaloo Reef, Western Australia

507

. Marine Ecology Progress Series, 319, 275‚Äì285.

508
509
510

Moeller, A. K., Lukacs, P. M., & Horne, J. S. (2018). Three novel methods to estimate abundance of
unmarked animals using remote cameras. Ecosphere, 9(8), e02331. doi:10.1002/ecs2.2331
Nakashima, Y., Fukasawa, K., & Samejima, H. (2018). Estimating animal density without individual

511

recognition using information derivable exclusively from camera traps. Journal of Applied Ecology,

512

55(2), 735‚Äì744. doi:10.1111/1365-2664.13059
29

bioRxiv preprint doi: https://doi.org/10.1101/852475; this version posted November 23, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available
under aCC-BY-ND 4.0 International license.

513

O‚ÄôConnell, A., Nichols, J., & Karanth, K. (2010). Camera traps in animal ecology: methods and analyses.

514

Patriquin, K. J., Hogberg, L. K., Chruszcz, B. J., Barclay, R. M. R., & Barclay, R. M. B. (2003). The influence

515

of habitat structure on the ability to detect ultrasound using bat detectors. Wildlife Society Bulletin,

516

31(2), 475‚Äì481.

517

Rowcliffe, J. M., Field, J., Turvey, S. T., & Carbone, C. (2008). Estimating animal density using camera

518

traps without the need for individual recognition. Journal of Applied Ecology, 45(4), 1228‚Äì1236.

519

doi:10.1111/j.1365-2664.2008.01473.x

520

Schobernd, Z. H., Bacheler, N. M., & Conn, P. B. (2014). Examining the utility of alternative video

521

monitoring metrics for indexing reef fish abundance. Canadian Journal of Fisheries and Aquatic

522

Sciences, 71(3), 464‚Äì471. doi:10.1139/cjfas-2013-0086

523

Sherman, C. S., Chin, A., Heupel, M. R., & Simpfendorfer, C. A. (2018). Are we underestimating

524

elasmobranch abundances on baited remote underwater video systems (BRUVS) using traditional

525

metrics? Journal of Experimental Marine Biology and Ecology, 503, 80‚Äì85.

526

doi:10.1016/J.JEMBE.2018.03.002

527

Snell-Rood, E. C. (2012). The effect of climate on acoustic signals: Does atmospheric sound absorption

528

matter for bird song and bat echolocation? The Journal of the Acoustical Society of America,

529

131(2), 1650‚Äì1658. doi:10.1121/1.3672695

530

Somerton, D. A., Williams, K., & Campbell, M. D. (2017). Quantifying the behavior of fish in response to a

531

moving camera vehicle by using benthic stereo cameras and target tracking. Fishery Bulletin,

532

115(3), 343‚Äì354. doi:10.7755/FB.115.3.5

533
534

Stobart, B., D√≠az, D., √Ålvarez, F., Alonso, C., Mallol, S., & Go√±i, R. (2015). Performance of Baited
Underwater Video: Does It Underestimate Abundance at High Population Densities? PLOS ONE,
30

bioRxiv preprint doi: https://doi.org/10.1101/852475; this version posted November 23, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available
under aCC-BY-ND 4.0 International license.

535
536
537

10(5), e0127559. doi:10.1371/journal.pone.0127559
Surlykke, A., & Kalko, E. K. V. (2008). Echolocating Bats Cry Out Loud to Detect Their Prey. PLoS ONE,
3(4), e2036. doi:10.1371/journal.pone.0002036

538

Vouk, V. (1948). Projected area of convex bodies. Nature, 162, 330‚Äì331. doi:10.1038/162330a0

539

Walther, B. A., & Moore, J. L. (2005). The concepts of bias, precision and accuracy, and their use in

540

testing the performance of species richness estimators, with a literature review of estimator

541

performance. Ecography, 28(6), 815‚Äì829. doi:10.1111/j.2005.0906-7590.04112.x

542
543

Weller, T. J., & Zabel, C. J. (2002). Variation in bat detections due to detector orientation in a forest.
Wildlife Society Bulletin, 30(3), 922‚Äì930.

544

Williams, K., Rooper, C. N., & Towler, R. (2010). Use of stereo camera systems for assessment of rockfish

545

abundancein untrawlable areas and for recording pollock behavior during midwater trawls. Fishery

546

Bulletin, 108(3), 352‚Äì362.

547

Willis, T., Millar, R., & Babcock, R. (2000). Detection of spatial variability in relative density of fishes:

548

comparison of visual census, angling, and baited underwater video. Marine Ecology Progress

549

Series, 198, 249‚Äì260.

550

31

