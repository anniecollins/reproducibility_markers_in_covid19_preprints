bioRxiv preprint doi: https://doi.org/10.1101/616367; this version posted April 23, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission.

Graph-Based Method for Anomaly Detection in Functional
Brain Network using Variational Autoencoder
Jalal Mirakhorli1
Mojgan Mirakhorli2
1
2

jalalmiry@aut.ac.ir
genomic66@gmail.com

Department of Electrical Engineering, Amirkabir University of Technology
Genetic Centre, Iranian Comprehensive Hemophilia Care Center (ICHCC)

Abstract:
Functional neuroimaging techniques using resting-state functional MRI (rs-fMRI)
have accelerated progress in brain disorders and dysfunction studies. Since, there are
the slight differences between healthy and disorder brains, investigation in the
complex topology of human brain functional networks is difficult and complicated
task with the growth of evaluation criteria. Recently, graph theory and deep learning
applications have spread widely to understanding human cognitive functions that are
linked to gene expression and related distributed spatial patterns. Irregular graph
analysis has been widely applied in many brain recognition domains, these
applications might involve both node-centric and graph-centric tasks. In this paper,
we discuss about individual Variational Autoencoder and Graph Convolutional
Network (GCN) for the region of interest identification areas of brain which do not
have normal connection when apply certain tasks. Here, we identified a framework
of Graph Auto-Encoder (GAE) with hyper sphere distributer for functional data
analysis in brain imaging studies that is underlying non-Euclidean structure, in
learning of strong rigid graphs among large scale data. In addition, we distinguish
the possible mode correlations in abnormal brain connections.
Keywords:
Functional brain networks, Graph theory, Generative Models, resting-state fMRI, VAE&GAN.

Introduction:
The human brain has a complex connection of various parts which dynamically shift
during its operation. Therefore, the model and cost of each part can change according

bioRxiv preprint doi: https://doi.org/10.1101/616367; this version posted April 23, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission.

to type of its operation in carried out or rest state. the fMRI data exhibits nonstationary properties in the context of task-based studies [1, 2]. Therefore, the
analysis of these sections are outmost value and is able to predict the connection
factors for each independent profile. Here in, we present a theoretical model based
on VAE and graph theory to learn probability distributed of graph that can to extract
the data model of tasks from brain regions with a semi-unknown prior knowledge
method. We used each tasks-base functional connectivity matrix that were collected
in an rs-fMRI experiment, using rs-fMRI data from Alzheimer's Disease
Neuroimaging Initiative (ADNI). Functional connectome analysis is able to reveal
biomarkers of individual psychological or clinical traits and describes the pairwise
statistical dependencies which exist between brain regions [3]. In this article, we
present brain as a graph using functional connectome structures, which allow us to
probing and inference about how dynamic changes progress of improvement degree
of brain disorder or predict the disease as well as the term brain abnormalities. This
paper propose to introduce a framework for feature extraction of the brain graphs
which provide across many subjects, for prediction of ambiguous parts of brain. In
this method a VAE is developed to make the graph and experiment a Bayesian Von
Misesâ€“Fisher (VMF) [4] mixture model as a latent distribution that can place mass
on the surface of the unit hypersphere [5] and stable the VAE. Our experiments
demonstrate that this method significantly outperform other methods and is a large
step forward to inference brain structure. It is capable to handle both homogeneous
and heterogeneous graph. Recent studies have shown, geometric deep learning
methods have been successfully applied to data residing on graphs and manifolds in
terms of various tasks[6,7], for example brain function prediction and its graph
expression analysis address the multifaceted challenges arising in diagnosis of brain
diseases. Here in, we present a novel method using a graph model in revealing of the
relationship between the parts of brain and recover missing parts or no properly
function parts.

Related works:
As our approach focuses on completing graph and prediction defective parts of graph
with obtained feature of network embedding, we consider some of related fields. In
additional, we used a combination of graph convolution VAE to address both
recovery and learning problems which can be performed in spectral [8, 9] or spatial
domain [10]. D. Xu and et al in [11] construct a graph from a set of object proposals,

bioRxiv preprint doi: https://doi.org/10.1101/616367; this version posted April 23, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission.

provide initial embedding to each node and edge while used message passing to
obtain a consistent prediction. Simonovsky and Komodakis in [12] used a generative
model to produces a probabilistic graph from a single opaque vector without
specifying the number of nodes or the structure explicitly. Pan and et al in [13]
proposed an adversarial training scheme to regularize and enforce the latent code to
match a prior distribution with a graph convolutional Autoencoder. Makhzani in [14]
showed an adversarial Autoencoder to learn the latent embedding by merging the
adversarial mechanism into Autoencoder for general data but Dai and et al [15]
applied the adversarial procedure for the graph embedding. Also in [12] used an
encoder with edge condition convolution (ECC) [16] and condition both encoder and
decoder which associated with each of the input graph, this method is useful only
for generation small graphs.

Approach
In spite of individual alteration, human brains performed common patterns among
different subjects. Therefore, algorithms base on graph are essential tool to capture
and model complicated relationship between functional connectivity.in this work,
we used a model of graph embedding to convert graph data into a low dimensional
and compaction continuous feature space that is able to detect abnormal parts of
input graphs [17] which is involved with graph matching and partial graph
completion problems. To develop this algorithms need to present a generative model
that construct from a Graph Varational Autoencoder with hypersphere distribution
[18,19,20]. Partial abnormality can be appear by features train in latent space,
considering both first-order proximity, the local pairwise proximity between the
vertices in the network, and second-order proximity. This refers to vertices sharing
many connections to other vertices that are similar to each other. The work flow of
the algorithm, in more details, is showed in figure 1.
Brain network as a graph: As shown in figure 1, using rs-fMRI data of subjects
acquired by preprocessing ADNI dataset to provide an adjacency matrix that
encodes similarities between nodes and a feature matrix representing a nodeâ€™s
connectivity profile, to define the input data as an undirected, connected graph G =
(V, E, W), which consists of a finite set of vertices V with |V| = n, a set of edges E,
and a weighted adjacency matrix W. If there is an edge e = ( i, j) connecting vertices
i and j, the entry Wij or aij represents the weight of the edge aij>0 , otherwise aij = 0.
For each of n subjects make a data matrix Xn Îµ R dn*dy , where dy is the dimensionally

bioRxiv preprint doi: https://doi.org/10.1101/616367; this version posted April 23, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission.

of the nodeâ€™s feature vector. This structure of fMRI data will be merge to the graph
defined. We will show that graph base algorithm on brain connectivity is useful to
analyze brain information processing.
Graph Convolutional Neural Network (G-CNN): for apply convolution-like operators
over irregular local supports, as graphs where nodes can have a varying number of
neighbors which can be used as layers in deep networks, for node classification or
recommendation, link prediction and etc. in this process we involved with three
challenges, a) defining translation structure on graphs to allow parameters sharing,
b) designing compactly supported filters on graphs, c) aggregating multi-scale
information, the proposed strategies broadly fall into two domains, there is one
spatial operation directly perform the convolution by aggregating the neighbor
nodesâ€™ information in a certain batch of the graph, where weights can be easily
shared across different structures[21,22] and other one is spectral operation relies
on the Eigen-decomposition of the Laplacian matrix that apply in whole graph at
the same time [23,24,25,26], spectral-based decomposition is often unstable making
the generalization across different graphs difficult[10], that cannot preserve both the
local and global network structures also require large memory and computation. On
the other hand, local filtering approaches [27] rely on possibly suboptimal hardcoded local pseudo-coordinates over graph to define filters. The third approach rely
on point-cloud representation [28] that cannot leverage surface information encoded
in meshes or need ad-hoc transformation of mesh data to map it to the unit sphere
[29].overall, spectral approach has the limitation of graph structure being same for
all samples i.e. homogeneous structure, this is a hard constraint, as most of sample
graphs in the learning phase has same structures and size for different structures i.e.
heterogeneous structures. Then here, we apply the spatial approach that is not
obligatory homogeneous graph structure, in turn requires preprocessing of graph to
enable learning on it. Therefore we used a method that propose a graph embed
pooling. In [30] Graph convolution transforms only the vertex values whereas graph
pooling transforms both the vertex values and adjacency matrix. Convolution of
vertices V with filter H only require matrix multiplication of the form, Ï…out=HÏ…in
where Ï…in , Ï…out Îµ RN*N. the filter H is defined as the k-th degree polynomial of the
graph adjacency matrix A;
H=h0I+h1A+h2A2+â€¦+hnAk , H Îµ RN*N .
We used the first two taps of H for any given filter.

(1)

bioRxiv preprint doi: https://doi.org/10.1101/616367; this version posted April 23, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission.

Graph Autoencoder (GAE): GAE is inherently an unsupervised generative model,
our model is closely base on framework of Varational Autoencoder by [20,31]. In
follow we briefly describe GAE and introduce our method with objectives. For
learning both encoder, decoder in the figure 1 to map between the space of graph
and their continuous embedding Z Îµ RC, stochastic graph encoder qÎ¦(Z|G) embed
the graph into continuous representation Z. given a point in the latent space Z, the
graph decoder pÎ¸ (G|Z) outputs a probabilistic fully-connected graph Ä on
predefined nodes, where Î¦,Î¸ are learned parameters. Reconstruction ability of GAE
is facilitated by approximate graph matching for aligning G with Ä, as well as a prior
distribution P(Z) imposed on the latent code representation as a regularization and
train GAE via optimization of the marginal likelihood, P(G)= âˆ« ğ‘ƒğœƒ (ğ‘)ğ‘ƒ(ğº|ğ‘) ğ‘‘ğ‘§,
then the marginal log likelihood can be written;
log pÎ¸ (G) =Æ˜Ä¹(qÎ¦(Z|G) || pÎ¸ (Z|G)) + â‚¤(Î¸,Î¦;G).

(2)

Where Kullbackâ€“Leibler (Æ˜Ä¹) and are a divergence term in loss function that
encourages the Varational posterior and a Varational approximation to the posterior
p (Z|G), respectively. Here, we used a hyperspherical latent structure for
parameterization of both prior posterior, because one of important limitation using
Gaussian mixture is that Æ˜Ä¹ term may encourage the posterior distribution of the
latent variable to collapse in prior or tends to pull the model toward the prior, during
approximation the prior, whereas in the VMF case is not such pressure toward a
single distribution convergence. Therefore a VMF [32,33] distribution is more
suitable for capturing data[20], VMF distribution defines a probability density over
points on a unit-sphere also The consequences of ignoring the underlying spherical
manifold are rarely analyzed in parts due to computational challenges imposed by
directional statistics.

Geometric deep learning: for graph generation, we employed the GAE to graph G Îµ
Rn*m under an unsupervised learning method, our goal is to learn an implicit
generative mode that can predict abnormal sections in the graph, of course here we
are not sure that close links have similar features to detect unseen deformable and
hidden angle of graphs. Our method inspired by [34, 35], and combination from the
GAE and generative adversarial network (GAN) that decoder of GAE and generator
of GAN have been supportive role. Following the above mentioned items, we used
the uniform distribution VMF(0,Ò  =0) for our prior and approximate pÎ¸ (Z | Ä) with
variational posterior qÎ¦(Z|G) = VMF(Z; Î¼, Ò ), where Î¼ is mean parameter and Ò  is

bioRxiv preprint doi: https://doi.org/10.1101/616367; this version posted April 23, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission.

a constant, the variational distribution is associated with a prior distribution over the
latent variables, our GAE loss combines the graph reconstruction Ä¹r=|| Ä â€“ G ||2
encouraging concatenation both the encoder-decoder to be a nearly identity
transformation, a regularization prior loss measured by the Æ˜Ä¹ divergence, Ä¹p=D
Æ˜Ä¹(q(z|G)|| P(Z)) and a cross entropy loss Ä¹2D-GAN for GAN, Ä¹ G-GAN=log D(G)+ log
(1-D(G(z))), where D is discriminator as a confidence D(G) of the whether a input
graph G is real or synthetic[24]. The total GAE+GAN loss is computed as Ä¹= Ä¹r
+Î»1Ä¹p+Î»2Ä¹G-GAN where Î»1 and Î»2 are weights of Æ˜Ä¹ divergence loss and
reconstruction loss. As discussed in above, our desire to focus on graph completion
for deformable object classes in brain connectome therefore we used dynamic
weight of filtering in each convolutional layer.

Partial graph completion: once our model GAE-GAN has been trained, the encoder
the element of GAN are discarded away, so that the role of the decoder is only as a
graph generator that a probabilistic latent space z act as a base for finding the target
graphs the same graph prior. At inference, for each space of the latent vector z may
represent a few complete graph correspondence a latent vector, then partial graph or
deformation graph in the input of system makes a few complete graph in the output,
the higher deformation rate in input, the more of graph is generated. Each partial
graph represent with the partial adjacency matrix Î´ that apply it to any graph Ä
generated by our model and explore similarity between their , for finding best
compatibility or a latent vector z* which can minimize differences between input
and output graph, to provide more geometric insight on the problem. Process to
measure similarities among elements of graphs with probing combination of
dependences similar unary, pairwise or high-order [36, 37, 38] as well as there are
Potentials between reference graph and their counterparts similar to [39,40] That
follow a function is used for finding dissimilarity or deformation with a convex
optimization problem over the set of doubly stochastic matrices.

Graph recovery plan: As mentioned above, our goal is the optimal choice a latent
vector z* so that minimize dissimilarity of between the partial graph related to a
disease brain, G, and the generated graph, Ä =dec(z);min (Ä, Î¶G)
Where Î¶ denote a rigid transformation, this procedure is performed over z, non-rigid
deformation and Î¶ crosswise. Similar to [39] minimizing the following function is
our goal as an objective function;

bioRxiv preprint doi: https://doi.org/10.1101/616367; this version posted April 23, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission.

min j(p, Î¶) = âˆ‘ğ‘–,ğ‘— ğ‘ğ‘–ğ‘— ||ğ‘£Ä âˆ’ Î¶(ğ‘£ğº )||^2 + Î³(Î¶)

(3)

Where Î³ is a regularization term of geometric transformation Î¶: Ä G, p is a map
for measuring the difference of graph attributes in similarity transformation domain,
in each step of optimization a weight matrix measure the degree of deformation on
the radial basis function method [39], graph recovery is ill-posed problem that has
multiple plausible solution while in this paper we limit the prediction space to only
several structures of the graphs.

Connection matrix
encoder

Å½

Ä
Adjacency matrix

G
Graph pool

G
Multiple Ä

Wishart
distribution

DI
SC
RE
MI
NA
TO
R1

Anatomical or functional
imaging

loss

Discriminator

Graph
matching

Best graph

sampler

Fig. 1. The data flow of the proposed network architecture

Materials and Discussion
In the present study, we employed an rs-FMRI data to construct graphs via adjacency
matrix and feed the graphs into our model to exploring functional brain network
alteration in patients with Alzheimerâ€™s disease (AD).GAE was used to extracts
salience alteration of brain connection of AD in diagnosis as well as in order in order
to detect changes in the abnormal convergence of brain which might occur in brain
disorders, we generate structure-correlated attributes on graphs.
Our model architecture is comprising of multiple layer graph-CNN networks as
cascades of spatial graph convolution
bath normalization
Relu for both
encoder-decoder and the discriminator block. We directly trained this model on 96
subjects from preprocess ADNI dataset, including 48 AD and 48 normal control
(NC), which describe above and used ADAM [41] optimizer with learning rate set
to 10-4, momentum of 0.09 during update, mini-batch size of 200 sample that train
for 3*1015 iterations.

bioRxiv preprint doi: https://doi.org/10.1101/616367; this version posted April 23, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission.

To verify our model, we get two datasets, a real rs-FMRI data from the benchmark
ADNI and the other synthetic dataset to develop the illusion correlation matrices to
prediction abnormal segmentations.
The synthetic dataset made to further investigation and predict possible construction.
This dataset is generated by sampling the Wishart distribution [42] over the average
of all ADNI subjects matrices [43] of two AD and NC groups by computing the
measure in term of log-likelihood from the classes estimated by the Wishart
distribution to show that our model can distinguishes salient nodes or different
connection between AD and NC groups, as well as by the synthetic dataset, we
detected abnormal area and identified possible state for each defective of brain
regions.
Figure 2 depicts the generated graphs by the decoder that fed with given partial graph
in the input and several possible solutions on the output. Possible modes of
significant correlation between different regions brain from an incomplete graph is
produced via synthetic AD dataset. One of the advantages of our approach is to find
close convergence between brain regions and predict fitting connections or
reconstructing where only partial functional connectivity data is available.
Eventually, during a competition to minimize the equation 3, the most appropriate
graph is chosen.
SFGmed.L
POCG.L
MTG.
L

MOG.L

PHG.L
PCU N.L

PoCG.R
PCU N.R

TPOmid.L
TPomid.R
PreCG.L
PAL.L
MoG.R
PoCG.L
PUT.L
IOG.R
CAL.L
LIN G.L

CAL.R

SFGdor.R
PoCG.L

IOG.L

PreCG.R

LIN G.R

MOG.L
CAL.L

SPG.R
CUN.R PreCG.R

SFGdor.L
MFG.L
ORBsup.L
OLF.L

PreCG.L
ORBmid.R
MFG.R MOG.R
OLF.R
SOG.R

SMA.R
RoL.R

MOG.L

MCGL.R

MOG.R

CUN.R

Fig. 2. Possible solutions to complete the input partial graph that are revealed by the model. Abbreviation: L,left & R,right;PreCG,
Precentral gyrus; SFGmed, Super frontal gyrus, medial;PCUN, Precuneus; CUN, Cuneus; CAL, Calcarine; LING, Lingual gyrus;
IOG, Inferior occipital gyrus; MTG, middle temporal gryus; MOG, middle occipital gyrus; ROL, Rolandic opercular; SMA,
Supplementary motor area; PoCG, Postcentral gyrus; PHG, Parahippocampal gyrus; SPG, Super parietal gyrus; SFGdor,
Dorsolateral of the superior the frontal gyrus; ORBsup, orbital part of the superior frontal gyrus ;OLF, Olfactory cortex;SOG,
Superior occipital gyrus; TPOmid, Temporal pole, middle part; PAL, lenticular nucleus, pallidum;PUT, lenticular nucleus,
putamen; MFG, Middle frontal gyrus, orbital.

To mitigate the excessive cost involved in computing and converging faster, into
significate connection of the AD group, we consider larger weights for unusual
connection to highlight correlation of these relationships. However, the use of
synthetic dataset will cause the results in the open-world assumption drive, while it
is desirable here for unexpected prediction on the partial connections.

bioRxiv preprint doi: https://doi.org/10.1101/616367; this version posted April 23, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission.

In our future study, based on the analysis above and focus on these topological
attributes to extend this work, we will extract important information on the higherorder functional of brain network via semantic functional rich-club organization.

References:
[1] Hutchison, R. M. et al. â€œDynamic functional connectivity: promise, issues, and
interpretationsâ€. Neuroimage 80, 360â€“378 (2013).
[2] Calhoun, V. D., Miller, R., Pearlson, G., and Adali, T. â€œThe chronnectome : time-varying
connectivity networks as the next frontier in fMRI data discoveryâ€ .Neuron 84, 262â€“274. doi:
10.1016/j.neuron.2014.10.01. (2014).
[3] Friston, k. J. â€œFunctional and effective connectivityâ€ a review. Brain connectivity 1(1), 13â€“36
(2011).
[4] Mardia, K. V. and El-Atoum, S. â€œBayesian inference for the von Mises-Fisher distributionâ€,
Biom, 63, 203â€“206, 1976.
[5] A. Banerjee,I. S. Dhillon,J. Ghosh,S. Sra, â€œClustering on the unit hypersphere using von MisesFisher distributionsâ€, Journal of Machine Learning Research, 6 (2005): 1345-1382.
[6] M. M. Bronstein, J. Bruna, Y. LeCun, A. Szlam, P. Vandergheynst, "Geometric deep learning:
Going beyond Euclidean data", IEEE Signal Process. Mag., vol. 34, no. 4, pp. 8-42, Jul. 2017.
[7] J.Mirakhorli, H.Amindavar, â€œSemi-Supervised Hierarchical Semantic Object Parsingâ€ IEEE
Conference on Signal Processing and Intelligent Systems, (ICSPIS 2017).
[8] M. Defferrard, X. Bresson, and P. Vandergheynst. â€œConvolutional neural networks on graphs
with fast localized spectral ï¬lteringâ€. In NIPS, 2016.
[9] R. Levie, F. Monti, X. Bresson, and M. M. Bronstein, â€œCayleynets: Graph convolutional neural
networks with complex rational spectral ï¬ltersâ€. IEEE Transactions on Signal Processing, vol. 67,
no. 1, pp. 97â€“109, Jan 2019.
[10] F. Monti, D. Boscaini, J. Masci, E. Rodola, J. Svoboda, and M. M. Bronstein. â€œGeometric
deep learning on graphs and manifolds using mixture model cnnsâ€. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pages 5115â€“5124, 2017.
[11] D. Xu, Y. Zhu, C. B. Choy, and L. Fei-Fei. â€œScene Graph Generation by Iterative Message
Passingâ€. arXiv:1701.02426 [cs], Jan. 2017. arXiv: 1701.02426.
[12] M. Simonovsky and N. Komodakis. â€œGraphVAE: Towards generation of small graphs using
variational autoencodersâ€. arXiv preprint arXiv:1802.03480, 2018.

bioRxiv preprint doi: https://doi.org/10.1101/616367; this version posted April 23, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission.

[13] S. Pan et al., â€˜â€˜Adversarially regularized graph autoencoder for graph embeddingâ€™â€™, in Proc.
27th Int. Joint Conf. Artif. Intell. pp. 2609â€“2615,(2018).
[14] A. Makhzani, J. Shlens, N. Jaitly, I. Goodfellow, and B. Frey, â€œAdversarial autoencoders,â€
CoRR, vol. abs/1511.05644, (2015).
[15] Q. Dai, Q. Li, J. Tang, and et.al, â€œAdversarial network embedding,â€arXiv: 1711.07838, (
2017).
[16] Daniel D Johnson. â€œLearning graphical state transitionsâ€. ICLR, 2017.
[17] N. Verma, E. Boyer, and J. Verbeek. â€œFeaStNet: Feature-Steered Graph Convolutions for 3D
Shape Analysisâ€. In CVPR, (2018).
[18] Davidson, T. R., et al. â€œHyperspherical Variational Auto-Encodersâ€. 34th Conference on
Uncertainty in Artificial Intelligence (UAI-18).
[19] D. P. Kingma and M. Welling. â€œAuto-Encoding Variational Bayesâ€. In ICLR, (2014).
[20] T. N. Kipf and M. Welling, â€œVariational graph auto-encoders,â€ NIPS, (2016).
[21] M. Niepert, M. Ahmed, and K. Kutzkov. â€œLearning Convolutional Neural Networks for
Graphsâ€. In ICML. 2014â€“2023(2016).
[22] H. Gao, Z. Wang, and S. Ji, â€œLarge-scale learnable graph convolutional networks,â€ in
Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery & Data
Mining. ACM, pp. 1416â€“1424, (2018).
[23] Henaff M, Bruna J and LeCun Y. â€œDeep convolutional networks on graph-structured dataâ€,
https://arxiv.org/abs/1506.05163, (2015).
[24] R. Levie, F. Monti, X. Bresson, and M. M. Bronstein, â€œCayleyNets: Graph Convolutional
Neural Networks. With Complex Rational Spectral Filters,â€ IEEE Transactions on Signal
Processing, vol. 67, no. 1, pp. 97â€“109, Jan (2019).
[25] Bruna, J., Zaremba, W., Szlam, A., and LeCun, Y. â€œSpectral networks and locally connected
networks on graphsâ€. In ICLR (2014).
[26] Thomas N Kipf and Max Welling. â€œSemi-supervised classiï¬cation with graph convolutional
networksâ€. ICLR, (2017).
[27] D. Boscaini, J. Masci, E. Rodol` a, and M. Bronstein. â€œLearning shape correspondence with
anisotropic convolutional neural networksâ€. In Advances in Neural Information Processing
Systems, pages 3189â€“3197, (2016).
[28] R. Klokov and V. Lempitsky. â€œEscape from cells: deep Kd-Networks for the recognition of
3D point cloud modelsâ€. In IEEE Int. Conf. on Computer Vision (ICCV), (2017).
[29] Sinha, A., Bai, J., Ramani, K.: â€œDeep learning 3D shape surfaces using geometry imagesâ€.
In: European Conference on Computer Vision. pp. 223â€“240 (2016).

bioRxiv preprint doi: https://doi.org/10.1101/616367; this version posted April 23, 2019. The copyright holder for this preprint (which was not
certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission.

[30] FP Such, et al. â€œRobust spatial filtering with graph convolutional neural networksâ€.
arXiv:1703.00792 (2017).
[31] Xu, Jiacheng and Durrett, Greg. â€œSpherical Latent Spaces for Stable Variational
Autoencodersâ€. arxiv:1808.10805.(2018).
[32] Sir Ronald Fisher, F.R.S. â€œDispersion on a sphereâ€. Proc. R. Soc. Lond. Ser. A Math. Phys.
Sci. 1953, 217, 295â€“305.(1953).
[33] Kanti V Mardia and SAM El-Atoum. â€œBayesian inference for the von mises-ï¬sher
distributionâ€. Biometrika, 63(1):203â€“206, (1976).
[34] Anders Boesen Lindbo Larsen, SÃ¸ren Kaae SÃ¸nderby, Hugo Larochelle, and Ole Winther.
â€œAutoencoding beyond pixels using a learned similarity metricâ€. arXiv:1512.09300, (2015).
[35] Wu, J., Zhang, C., Xue, T., Freeman, B., Tenenbaum, J.: â€œLearning a probabilistic latent space
of object shapes via 3D generative-adversarial modelingâ€. In: Advances in Neural Information
Processing Systems. pp. 82â€“90 (2016).
[36] D. Khue Le-Huu, Nikos Paragios, "Alternating direction graph matching", IEEE CVPR, pp.
6253-6261, (2017).
[37] J. Yan, C. Li, Y. Li, and G. Cao, â€˜â€˜Adaptive discrete hypergraph matching,â€™â€™ IEEE Trans.
Cybern., vol. 48, no. 2, pp. 765â€“779, Feb. (2018).
[38] Yu, J.G., Xia, G.S., Samal, A., Tian, J.: â€œGlobally consistent correspondence of multiple
feature sets using proximal gausseidel relaxationâ€. Pattern Recognition 51, 255 â€“ 267 (2016)
[39] Fu-Dong Wang. et al â€œA Functional Representation for Graph mtchingâ€arXiv:1901.05179.(2019).
[40] Wang F., Xue N., Zhang Y., Bai X., Xia GS. â€œAdaptively Transforming Graph Matchingâ€.
Computer Vision â€“ ECCV .(2018).
[41] Diederik P. Kingma and Jimmy Lei Ba. â€œAdam: A method for stochastic optimizationâ€.
arXiv:1412.6980v9(2014).
[42] Shiwei Lan et al. â€œFlexible Bayesian Dynamic Modeling of Covariance and Correlation
Matricesâ€ https://arxiv.org/abs/1711.02869v5. (2019).
[43] Christof Seiler,Susan Holmes.â€Multivariate Heteroscedasticity Models for Functional Brain
Connectivityâ€. bioRxiv .(2017). Frontiers in Neuroscience, DOI: 10.3389/fnins.2017.00696, (2017).

