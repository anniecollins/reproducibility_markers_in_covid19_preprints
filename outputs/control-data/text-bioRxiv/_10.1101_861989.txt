bioRxiv preprint doi: https://doi.org/10.1101/861989; this version posted December 2, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available
under aCC-BY 4.0 International license.

Neural Encoding for Human Visual Cortex with Deep Neural
Networks Learning ‚ÄúWhat‚Äù and ‚ÄúWhere‚Äù
Haibao Wang1,2 , Lijie Huang1 , Changde Du1,2 , Dan Li1,2 , Bo Wang1,2 , Huiguang He1,2,3* ,
1 Research Center for Brain-Inspired Intelligence and National Laboratory of Pattern
Recognition, Institute of Automation, Chinese Academy of Sciences (CAS), Beijing 100190,
China
2 University of Chinese Academy of Sciences, Beijing 100049, China
3 Center for Excellence in Brain Science and Intelligence Technology, CAS, Shanghai, 200031,
China
*huiguang.he@ia.ac.cn

Abstract
Neural encoding, a crucial aspect to understand human brain information processing system,
aims to establish a quantitative relationship between the stimuli and the evoked brain activities.
In the field of visual neuroscience, with the ability to explain how neurons in primary visual
cortex work, population receptive field (pRF) models have enjoyed high popularity and made
reliable progress in recent years. However, existing models rely on either the inflexible prior
assumptions about pRF or the clumsy parameter estimation methods, severely limiting the
expressiveness and interpretability. In this paper, we propose a novel neural encoding framework
by learning ‚Äúwhat‚Äù and ‚Äúwhere‚Äù with deep neural networks. The modeling approach involves
two separate aspects: the spatial characteristic (‚Äúwhere‚Äù) and feature selection (‚Äúwhat‚Äù) of
neuron populations in visual cortex. Specifically, we use the receptive field estimation and
multiple features regression to learn these two aspects respectively, which are implemented
simultaneously in a deep neural network. The two forms of regularizations: sparsity and
smoothness, are also adopted in our modeling approach, so that the receptive field can be
estimated automatically without prior assumptions about shapes. Furthermore, an attempt is
made to extend the voxel-wise modeling approach to multi-voxel joint encoding models, and we
show that it is conducive to rescuing voxels with poor signal-to-noise characteristics. Extensive
empirical results demonstrate that the method developed herein provides an effective strategy to
establish neural encoding for human visual cortex, with the weaker prior constraints but the
higher encoding performance.

Author summary
Characterizing the quantitative relationship between the stimuli and the evoked brain
activities usually involves learning the spatial characteristic (‚Äúwhere‚Äù) and feature selection
(‚Äúwhat‚Äù) of neuron populations. As an effective strategy, we propose a novel end-to-end ‚Äúwhat‚Äù
and ‚Äúwhere‚Äù architecture to perform neural encoding. The proposed modeling approach consists
of receptive field estimation and multiple features regression, which learns ‚Äúwhere‚Äù and ‚Äúwhat‚Äù
simultaneously in a deep neural network. Different from previous methods, we use the sparsity
and smoothness regularizations in the deep neural network to guide the receptive field
estimation, so that the receptive field for each voxel can be estimated automatically. Moreover,
in consideration of computational similarities between adjacent voxels, we made an attempt to

November 27, 2019

1/22

bioRxiv preprint doi: https://doi.org/10.1101/861989; this version posted December 2, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available
under aCC-BY 4.0 International license.

extend the proposed modeling approach to multi-voxel joint encoding models, improving the
encoding performance of voxels with poor signal-to-noise characteristics. Empirical evaluations
show that the proposed method outperforms other baselines to achieve the state-of-the-art
performance.

Introduction

1

A great mystery in computational neuroscience is understanding how the brain effortlessly
performs information perception and processing given sensory input. Uncovering this internal
mechanism is of great scientific importance, not only for neuroscience researches, but also for
artificial intelligence researches. In the field of visual neuroscience, one common method for
insights into visual information processing is to establish neural encoding models with
functional magnetic resonance imaging(fMRI) [1, 2]. This modeling approach (Fig 1) of neural
encoding links fMRI signals at the millimeter scale to neural response at the micron scale,
providing a non-invasive approach to revealing the nonlinear relationship between the external
stimuli and the evoked brain activities [3‚Äì5].

2
3
4
5
6
7
8
9
10

(a)
Stimuli

BOLD responses

(b)

Voxel
activity

Stimuli
Feature 1

N images

Feature 2

Nonlinear
feature mapping

Linear
feature mapping

N responses

Fig 1. The general approach of neural encoding models. (a) Voxel activity evoked by
experimental stimuli (such as natural images) is collected. (b) Encoding models are specified by
a nonlinear mapping (curvy arrow) of the stimuli into an abstract feature space, as well as a
linear mapping (straight arrow) from feature space to voxel activity.
Numerous studies have built neural encoding models from diverse perspectives for human
visual cortex, including, but not limited to, Gabor wavelet pyramid model [4], luminance
contrast models [6‚Äì10], the motion energy model [11], semantic models [12], deepnet
models [13], and other machine learning methods [14, 15]. The estimated characteristics of
visual cortex derived from these models can be roughly summarized in two aspects: ‚Äúwhat‚Äù and
‚Äúwhere‚Äù. ‚ÄúWhere‚Äù characterizes the spatial characteristic of neuron populations in visual cortex,
i.e. the location and extent of pooling over visual features, whereas ‚Äúwhat‚Äù characterizes the
feature selection property of neuron populations in visual cortex.
In general, ‚Äúwhere‚Äù is based on the classical receptive fields. In the population receptive
field (pRF) model [6], the visual feature is a binary map of the pixels occupied by a
high-contrast stimulus (e.g., bar, ring, wedge). For each voxel, the model is constructed by an
isotropic Gaussian area, the pRF, that pooled the visual feature map within a spatially localized
area. This seminal approach quantitatively measured the population receptive field properties in
human visual areas for the first time and was extended to some similar pRF models [7‚Äì10].

November 27, 2019

2/22

11
12
13
14
15
16
17
18
19
20
21
22
23
24

bioRxiv preprint doi: https://doi.org/10.1101/861989; this version posted December 2, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available
under aCC-BY 4.0 International license.

‚ÄúWhat‚Äù focuses on feature selection and feature tuning functions. In the semantic model [12],
several object category features (e.g. the presence of an animal, or a car.) are encoded as the
vector of binary variables. Then, every object category was assigned a tuning parameter for each
voxel to construct the model. Similarly, different visual features are further studied in
subsequent models [4, 11, 14‚Äì16].
Recently, deep learning with neural networks [17‚Äì19] has been widely used to perform
feature learning from scratch with promising performance, which sparks interest in using deep
learning methods for understanding information processing in visual cortex [5, 20‚Äì22]. Based on
deep neural networks, [13] proposed a new approach to encoding visual features named
feature-weighted receptive field (fwRF) [13]. It starts with a natural image, obtains feature maps
in a pre-trained convolutional neural network, and computes a weighted sum within the spatial
extent of an 2D Gaussian receptive field. Finally, it regresses all the feature maps onto brain
activity simultaneously, which yielded the state-of-the-art prediction accuracy. However, while
previous work demonstrated promising results of processing in visual cortex, neural encoding
models still lack adequate examination and require plenty of effort to improve.
There are two main challenges getting in the way of development of effective models. On
one hand, conventional approaches [6, 7] are endowed with inflexible prior assumptions on the
spatial characteristics of receptive fields, which limit the effectiveness of models to a large
extent. For example, in the population receptive model [6], it assumed that the pRF has an
isotropic Gaussian topography while the potentially suppressive surround is neglected. There
have been subsequent models [7, 9, 10, 13] which have adopted the same principles with different
pRF topographies. In general, one assumption about receptive field structure puts one prior
constraint on the ability to extract the receptive field topography of the model. Specifically,
inaccurate assumptions about receptive field topography may lead to erroneous estimation of the
receptive field. Hence, it is meaningful to propose a new method that can extract receptive field
topography without inflexible prior assumptions. On the other hand, previous
approaches [6, 7, 9, 10, 13, 15] to obtaining receptive fields are based on grid search, which set
search parameters according to experience. Accordingly, they are prone to pRF center
mislocalization and size miscalculation. In the population receptive models [6, 7, 13], the pRF
topography parameters were set to certain parameters, which can be obtained by minimizing the
residual between the observed fMRI signal and the predicted signal. In this case, according to
different shape parameters (i.e. center and radius), these models will inevitably generate large
quantities of candidate pRF. That is, the grid fitting requires searching over quite large
model-parameter spaces. Consequently, their encoding performances depend on the amount and
parameter interval of candidate receptive fileds, which are set artificially. Obviously, more often
than not, it is not optimal and requires lots of manual effort. It would therefore be significant to
obtain the receptive fileds automatically in a more reasonable way.
Existing methods are prone to suffer from one or both of these issues and yield
dissatisfaction. Attaching great importance to these bottlenecks, we proposed a novel ‚Äúwhat‚Äù
and ‚Äúwhere‚Äù neural encoding architecture via deep neural networks. The proposed method first
extract hierarchical features from the DNN driven for image recognition. Then, the original
features are refined via channel attention and spatial receptive field, and finally are regressed
simultaneously onto voxel activity. Different from previous methods, we use the sparsity and
smoothness regularizations in the deep neural network to guide the receptive field estimation.
This modelling approach can estimate the receptive field for each voxel automatically and
maintain powerful expressiveness. In consideration of computational similarities, we extend the
voxel-wise modeling approach to multi-voxel joint encoding models, which is beneficial to
rescuing voxels with poor signal-to-noise characteristics. Our main contributions can be
summarized as follows.
‚Ä¢ We provide a new perspective on the deep-learning-based neural encoding models,
performing receptive field estimation and features regression simultaneously in a deep
neural network. This modeling approach can yield explicit receptive fields (‚Äúwhere‚Äù) and

November 27, 2019

3/22

25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73

74
75
76

bioRxiv preprint doi: https://doi.org/10.1101/861989; this version posted December 2, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available
under aCC-BY 4.0 International license.

feature turning functions (‚Äúwhat‚Äù) automatically, which is rich in interpretability.

77

‚Ä¢ The estimation of receptive fields is endowed with weaker constraints. Instead of strong
prior assumptions on the shape of receptive fields, L1 regularization and Laplacian
smoothing are adopted in our modeling approach, which can be regarded as weak prior
assumptions about receptive fields.
‚Ä¢ We made an attempt in the extension of the modeling approach. In consideration of the
computational similarities between voxels, the voxel-wise modeling approach is extended
to multi-voxel joint encoding models, suggesting a new approach to rescuing voxels with
poor signal-to-noise characteristics more effectively.
‚Ä¢ Extensive empirical evaluations on the publicly available fMRI dataset demonstrate that
our modeling approach achieves superior performance compared with other neural
encoding models.

Methods

78
79
80
81

82
83
84
85

86
87
88

89

In the neural encoding dataset, we assume X = [x1 , ..., xN ]T ‚àà RN √óM and
Y = [y1 , ..., yN ]T ‚àà RN √óD denote the matrices of visual images and the evoked fMRI
activities, respectively. Here, N denotes the size of the training set. M and D denote the
dimensions of visual image and fMRI activity pattern, respectively. Given an image xi , its
hierarchical visual features can be obstained from a pretrained deep neural network (e.g.,
AlexNet [18]). Here, H = [h1 , ..., hN ]T ‚àà RN √óK donotes the intermediate DNN features,
where K denotes the number of feature maps. For modeling the statistical relationship between
the visual images and the evoked voxel activities, we put forward a novel neural encoding
framework by learning ‚Äúwhat‚Äù and ‚Äúwhere‚Äù based on deep neural networks. The simplified
illustration of the proposed modeling approach is shown in Fig 2. Formally, it consists of three
cascaded stages : 1) nonlinear feature extraction: extracting hierarchical visual features through
a pretrained DNN model. 2) nonlinear feature refinement: converting original features into
refined features with channel attention module and spatial receptive field (RF) module. 3)
voxel-wise linear mapping: regressing refined features simultaneously onto voxel activities. In
the following, we present the proposed approach in detail.
DNN
feature maps

Images
Feature
extraction

Convolutional
neural network

Refined
feature matrix

90
91
92
93
94
95
96
97
98
99
100
101
102
103

Voxel activities

Feature
refinement

Linear
mapping

Channel attention
and spatial RF

Voxel-wise
regression

Fig 2. An overview of the proposed neural encoding framework. It consists of three cascaded
stages : 1) nonlinear feature extraction: extracting hierarchical visual features through a
pretrained DNN model. 2) nonlinear feature refinement: converting original features into refined
features with channel attention module and spatial receptive field (RF) module. 3) voxel-wise
linear mapping: regressing refined features simultaneously onto voxel activities.
104

November 27, 2019

4/22

bioRxiv preprint doi: https://doi.org/10.1101/861989; this version posted December 2, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available
under aCC-BY 4.0 International license.

Nonlinear feature extraction

105

The neural encoding models take visual stimuli as the input, and output the evoked brain
activities. Normally, it includs two sequential stpdf. The first step is a nonlinear feature
mapping, converting the visual input to its feature representations; the second step is a
voxel-wise linear mapping, projecting the feature representations onto activities at each
voxel [4, 23‚Äì27]. In the present study, The nonlinear feature mapping consists of two parts:
nonlinear feature extraction and nonlinear feature refinement. The nonlinear feature extraction is
introduced in this sub-section, while nonlinear feature refinement and voxel-wise linear mapping
are described in the next several sub-sections.
Recent studies [13, 28] has demonstrated that Alexnet [18], a specific version of DNNs, is
capable of predicting voxel activities with statistical significance and high accuracies throughout
the visual cortex. In line with previous work [13], a deep neural network (a specific
implementation referred as the AlexNet) is adopted to extract nonlinear features in the present
study. Briefly, AlexNet has been pre-trained to achieve the best preformance in Large Scale
Visual Recognition Challenge 2012 [18]. It consists of eight layers of computational units: the
first five layers were convolutional layers, while the rest layers were fully connected. The image
input was fed into the first layer; the output from one layer served as the input to its next layer.
Each convolutional layer involves plenty of units and a set of filters that extracts filtered outputs
from different locations of the input. The resolution (square root of the number of pixels in each
feature map) and depth (number of feature maps) for each convolutional layer was (55, 96), (27,
256), (13, 384), (13, 384), (13, 256) respectively. The fully-connected layers contained 4096,
4096 and 1000 units respectively. All feature maps from all convolutional layers, as well as up to
1024 units from the fully-connected layers , are used to result in hierarchical nonlinear features
in the present study. Once trained, the CNN is able to undergo feature extraction by a simple
feedforward pass of an input image. Herein, we refer to hi as the intermediate DNN features
given image xi . In consideration of feature maps from multiple layers may be adopted in the
model, we index each layer by l, and let hli denotes the features from the l-th layer.
A question arises whether multiple feature maps included in the model are meaningful. In
reality, we do not konw what features can better explain activity in the visual cortex under many
circumstances. In this way, the total feature maps hi are able to contain adequate features to
capture the reasonable hypotheses about what is encoded in the visual cortex. Based on these
original feature maps, the proposed method can infer which features and locations are important
for explaining the activity in the voxel. In the nonlinear feature refinement, the feature map
pixels are focused on within the spatial RF, as concentrating on important locations and
suppressing unnecessary ones are conducive to improving the encoding performance. In this
way, spatial RF is the ‚Äúwhere‚Äù parameters of the neural encoding model. In the voxel-wise linear
mapping, each feature map will be assigned an associated feature weight, which indicates the
importance of the feature map for predicting the activity of each voxel. In this sense, all the
feature map weights are ‚Äúwhat‚Äù parameters of the neural encoding model. Noting that while
original features maps are same for each voxel, but the feature refinement and feature weights
vary across voxels.

Nonlinear feature refinement

107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145

146

The crucial part of the proposed method is the nonlinear feature refinement, which
contributes to encoding performance promotion and interpretability enhancement. In this
subsection, we sequentially employ the channel attention module and spatial RF module
whereby the proposed method can learn ‚Äúwhere‚Äù to attend in visual information processing. The
illustration of this feature refinement is shown in Fig 3.

November 27, 2019

106

5/22

147
148
149
150
151

bioRxiv preprint doi: https://doi.org/10.1101/861989; this version posted December 2, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available
under aCC-BY 4.0 International license.

Original
feature maps

ùê¶ùíçùíä

Channel-wise weighted
feature maps

RF

ùê¨ùíäùíç

ùê°ùíçùíä

p-th channel

ùê©ùíç

Channel Attention

q-th position

Refined
features

rùíäùíç
Spatial RF

Fig 3. The feature refinement in the specific layer (the l-th layer). In the channel attention
module, the original feature maps hli are initially used to obtain the channel attention weights
mli . In the next phase, the original feature maps are element-wise multiplied by the channel
attention weights to obtain the channel-wise weighted feature maps sli . In the spatial RF module,
the receptive field p is reshaped to the corresponding receptive field pl according to the size of
channel-wise weighted feature maps sli . The Hadamard product of pl and sli finally produce the
refined features rli .
Channel attention module

152

It is acknowledged that attention plays an crucial role in human perception [29‚Äì31]. One
important property of the human visual system is that one does not attempt to process a whole
scene at once. Instead, humans exploit a sequence of partial glimpses and selectively focus on
salient features in order to capture visual structure better [32‚Äì34]. In the present work, we pay
more attention to the meaningful feature maps rather than considering each feature map equally.
Since a channel-wise feature map is a detector response map of the corresponding filter in
essence, channel attention can be regarded as the process of selecting inter-layer feature
attributes and reducing redundant information (strengthening important ones and weakening
unimportant ones).
Here, the channel attention module is built up based on the hierarchical features hi . Without
loss of generality, we discard the image subscript i and layer-wise superscrip l. In each specific
convolutional layer, the original feature maps h (hli ) are reshaped to
h = [h1 , h2 , ..., hC ] ‚àà RS√óC , where hk donotes the k-th channel of the feature maps. C and S
are the channel dimension and spatial dimension in this layer, respectively. Given the input h,
global average pooling is applied to each channel to obtain the mean vector
hÃÑ = [hÃÑ1 , hÃÑ2 , ..., hÃÑC ] ‚àà R1√óC , where hÃÑk is the channel mean of hk . On the basis of the mean
vector hÃÑ, the channel attention module Œ¶(¬∑) can be further formulated as follows:
m = œÉ(hÃÑWa + ba )
s=m‚äóh

154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169

(1)

where Wa ‚àà RC√óC and ba ‚àà R1√óC are the transformation matrix and bias term respectively;
œÉ denotes the sigmoid function and ‚äó denotes element-wise product; m ‚àà R1√óC denotes the
attention weight while s ‚àà RS√óC stands for attentioned feature maps. During element-wise
multiplication, the channel attention weight is broadcasted (copied) along the spatial dimension.
In this way, the original feature maps h are converted into attentioned feature maps s.

172

Spatial RF module

175

In visual areas, population activity at each voxel in the cortical sheet encodes visual features
within a spatially localized region of visual field [6‚Äì9]. In view of this, the attentioned feature
maps are pooled within a limited and contiguous area in this module, which is the spatial
receptive field (RF). In order to overcome the drawbacks brought by the inflexible prior
assumptions or the clumsy parameter estimation methods, the sparsity regularization and
smoothness regularization are adopted in this module. Under the guidance of both

November 27, 2019

153

6/22

170
171

173
174

176
177
178
179
180
181

bioRxiv preprint doi: https://doi.org/10.1101/861989; this version posted December 2, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available
under aCC-BY 4.0 International license.

regularizations, the proposed method yields explicit receptive field automatically and encodes
features within a contiguous region of the visual field.
Mathematically, the receptive field is denoted as p given each specific voxel, and randomly
initialized with the same spatial size of natural images (227 √ó 227). In consideration of different
sizes of feature maps, the receptive field is adapted to different convolutional layers by the
means of reshaping. Taking the first convolutional layer as an example, the corresponding
receptive field size is 55 √ó 55. In this way, there are five sizes of receptive fields converted from
the original receptive field, with one-to-one correspondence to the five convolutional layers. As
a direct way to combine attentioned feature maps with receptive fields, element-wise product is
more natural and general than specially designed operations. Herein, the receptive field is
broadcasted (copied) along the channel dimension in the beginning, matching with the
dimensions of attentioned feature maps. Given the feature maps s = [s1 , s2 , ..., sC ] ‚àà RS√óC ,
the spatial RF module Œ®(¬∑) can be formulated as follows:
rk = (sk p) k = 1, 2, ..., C
r = [r1 , r2 , ..., rC ]

184
185
186
187
188
189
190
191
192
193
194

195
196
197
198
199
200
201
202

(3)

To ensure the receptive field focuses on an localized area as effectively as possible, we use an L2
penalty on the Laplacian of the receptive field with the strength Œªl :
Ô£Æ
Ô£π
0 ‚àí1 0
Llaplace = ||p ~ L||F , L = Ô£∞‚àí1 4 ‚àí1Ô£ª
(4)
0 ‚àí1 0
Here, || ¬∑ ||F and || ¬∑ ||1 denote Frobenius norm and L1 norm (‚Äúentriwise‚Äù norm) of a matrix
respectively, and ~ is the convolutional operation. In this way, sparsity and smoothness
regularizations make the ordinary mask transform into the meaningful receptive field. In
contrast to Gaussian assumption, these two forms of regularization can be viewed as weak prior
assumptions about receptive field , which is more conducive to the expressiveness and flexibility
of neural encoding models.

Voxel-wise linear mapping

203
204

205
206
207
208
209
210

211

The original intention of neural encoding models is to account for the responses of different
visual processing stages and reveal the information processing mechanism of neurons in visual
cortex. Voxel-wise linear mapping sets up such a computational path to relate visual features to
the evoked response at each voxel, bridging the gap between feature selection property of neuron
populations in visual cortex and hierarchical feature maps in deep neural networks.
In some previous studies [21, 28, 35], the voxel-wise encoding models regress feature maps
in each layer onto brain activity independently, which is proved highly effective. However, it
reduces the model scale at the expense of model expressiveness. In reality, increasingly abstract
and complex visual features are encoded in the deep neural networks (AlexNet). The
convolutional layer encoded location and orientation-selective features, whereas the

November 27, 2019

183

(2)

where sk ‚àà RS√ó1 is the c-th channel of attentioned feature maps and p ‚àà RS√ó1 is the
voxel-wise receptive field. is the Hadamard product of matrices. Hence, spatial RF module
Œ®(¬∑) outputs the refined feature vector r ‚àà R1√óC .
As a matter of fact, if there is no any other operation applied to the receptive field, it is just
an ordinary mask. Inspired by the physiological structural characteristics of receptive field, our
model adopted two forms of regularization: sparsity and smoothness. Specifically, for each
specific voxel, since we expect its receptive field to be highly sparse, the receptive field was
regularized by L1 penalty with strength Œªs :
Lsparsity = ||p||1

182

7/22

212
213
214
215
216
217
218
219
220
221

bioRxiv preprint doi: https://doi.org/10.1101/861989; this version posted December 2, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available
under aCC-BY 4.0 International license.

(a)

(b)
Receptive field constrains
Feature maps

Images

L1

DNN

Sparsity
loss

Transformation
Matrix

Predicted Measured
activity
activity

0 -1 0
-1 4 -1
0 -1 0

Smoothness
L2
loss

Random initialized
receptive field

Laplacian

Guide

Reshape

MSE loss

Feedback
The evolution of receptive field

Fig 4. The deatails of the proposed modeling approach. (a) The image data are converted into
the feature maps (or attentioned feature maps), and the receptived field is randomly initialized
and reshaped to same size as feature maps. In the next phase, the Hadamard product ( ) of the
feature maps and the receptive field produce the refined features. Finally, the refined features
simultaneously regressed onto voxel-wise activity by a transformation matrix. (b) The receptive
field is regularized by the two forms of regularizations: sparsity and smoothness. These two
regularizations are able to guide the evolution of receptive field.
fully-connected layer encoded semantic features. In the light of feature diversity, we use feature
maps from all layers to predict each voxel‚Äôs response rather than assume an one-to-one
correspondence between a voxel and a DNN layer. It is a reasonable way that each feature map
is assigned an appropriate weight, which can be learned from training data directly by the
appropriate optimization algorithm.
For each specific image xi , its original multi-layer feature maps hi turn into the refined
multi-layer feature vector ri through the nonlinear feature refinement module. The predicted
response at the specific voxel is modeled as a linear combination of multi-layer features. Let us
denote the weight of multi-layer features as w. In order to predict the response of the specific
voxel to the natural image, the weight w is element-wise multiplied by the refined feature vector
ri . Formally:
yÃÇi = ri
1√óK

w

where ri ‚àà R
and w ‚àà R
, K donotes the number of total feature maps. There is tend
to be a voxel-wise bias b in practice, and we omit it for notational simplicity, as it does not play a
part in the validation accuracy. Finally, the mean-squared error can be formulated as below:
1 X
(yi ‚àí yÃÇi )2
(6)
Lmse =
B i
where yi is the measured voxel-wise activity in response to image i, yÃÇi is the predicted activity
of the model, B indicates the minibatch size.

The objective function of the model

224
225
226
227
228
229
230
231
232

233
234
235

236
237

238

The deatails of the whole modeling approach are shown in Fig 4. The final objective
function for each specific voxel is defined as follows:

November 27, 2019

223

(5)

1√óK

T = Lmse + Œªs Lsparsity + Œªl Llaplace

222

239
240

(7)

8/22

bioRxiv preprint doi: https://doi.org/10.1101/861989; this version posted December 2, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available
under aCC-BY 4.0 International license.

where Œªl and Œªs are hyper-parameters. The first term Lmse is the MSE loss. Intuitively,
minimizing this loss, which is equivalent to making predicted values approximate true values,
can result in more accurate predictions. It is obvious that the L1 regularization term Lsparsity
plays an sparse role. The RF is supposed to be such a highly sparse area that optimizing the
second term contributes to revealing the localized structure of RF, which is beneficial to the
interpretability of our model. The third term Llaplace is the L2 penalty on the Laplacian of the
RF, which is used to make the RF smooth. More Specifically, minimizing this term, the pixels in
the particular area of RF tend to be numerically consistent. Therefore, this constraint ensures
that RF encodes features within a contiguous region.
When optimizing the objective function, the final goal is to infer RF area and weights of
feature maps that lead to more accurate predictions of the voxel‚Äôs response. Herein, the objective
function can be minimized by Adam Optimizer [36] based on gradient descent method.

242
243
244
245
246
247
248
249
250
251
252

Materials

253

Data description

254

The data uesd in the present study are the public fMRI dataset vim-1 (Data are available at
https://crcns.org/data-sets/vc/vim-1.), which are described in detail in [4]. In summary,
functional BOLD activity was measured in the occipital lobe with 4T INOVA MR scanner
(Varian, Inc.) at a spatial resolution of 2mm√ó2mm√ó2.5mm and a temporal resolution of 1 Hz.
During the acquisition, subjects viewed sequences of 20o √ó 20o greyscale natural photographs
while fixating on a central white square. Photographs were presented for 1s with a delay of 3s
between successive photographs.
The data are partitioned into distinct training and testing sets. The training set consists of
estimated voxel activity in response to 1750 photographs while the testing set consists of
estimated voxel activity in response to 120 photographs. In present study, the fMRI data from
visual area V1, V2, V3, V4, LO, V3a, V3b are used for the analysis, and the original training set
is further partitioned into training set and validation set. It divides the original data into 5-Fold,
and validates each subset (consists of 20% of the original training set) separately, whereas the
remaining 4 subsets are used as training set.

Ethics statement

255
256
257
258
259
260
261
262
263
264
265
266
267
268

269

Two healthy subjects with normal or corrected-to-normal vision participated in the
experiments: Subject 1 (male, age 33) and Subject 2 (male, age 25). All subjects provided
written informed consent for participation in the experiments, in accordance with the
Declaration of Helsinki, and the sharing of vim-1 dataset has been approved by the UC Berkeley
Office for Protection of Human Subjects.

Compared methods

270
271
272
273
274

275

The following models are considered as compared methods:

276

a) Compressive spatial summation model (CSS): The CSS model takes a contrast image
(i.e. an image representing the location of contrast in the visual field) as the input,
computes a weighted sum of the contrast image using an isotropic 2D Gaussian, and
applies a static power-law nonlinearity [9]. It is an effective way to encode contrast
images, while the input pattern limits the flexibility and generalization of the SOC model.
b) Second-order contrast model (SOC): The SOC model starts with a grayscale image
(luminance values), applies Gabor filters as a way of computing local contrast, computes
second-order contrast within the spatial extent of an isotropic 2D Gaussian, and applies a

November 27, 2019

241

9/22

277
278
279
280
281

282
283
284

bioRxiv preprint doi: https://doi.org/10.1101/861989; this version posted December 2, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available
under aCC-BY 4.0 International license.

static power-law nonlinearity [15]. Whereas the CSS model explains only how the
location and size of the stimulus relate to the response, the SOC model is more general,
explaining the relation of an arbitrary grayscale image to the response. However, its
relatively shallow feature representations limits encoding performance, and the progress
hits a bottleneck at the natural images.
c) Feature weighted receptive field (fwRF): A latest neural encoding method based on
convolutional neural network and multivariate linear regression [13]. It starts with a
natural image, obtains feature maps in a pre-trained convolutional neural network, and
computes a weighted sum within the spatial extent of an 2D Gaussian receptive field.
Finally, it regresses all the feature maps onto brain activity simultaneously and outputs
accurate predictions, which outperforms other comparable encoding models to achieve the
state-of-the-art performance. However, fwRF only considers the fusion of diverse features
from DNNs and still neglects the drawbacks resulted from strong prior assumptions
(Gaussian assumption), like previous methods.

Voxel selection

285
286
287
288
289

290
291
292
293
294
295
296
297
298

299

Voxel selection is a crucial component to fMRI brain encoding, as plenty of voxels may not
respond to the visual stimuli in reality. A common approach is to choose those voxels to which
the model provided better predictability (encoding performance) during the training process.
The goodness of fit between model predictions and measured voxel activities is quantified by the
Pearson‚Äôs correlation coefficient (PCC). For each voxel, the PCC is computed on the validation
set, and is finally an average of 5 runs with different data splits in our experiments. We select
voxels with positive PCC for further analyses, and the details of the selected data are
summarized in Table 1 (The details of the selected data from subject 2 are shown in S1 Table).
Figures in this study refer to data from subject 1 (except Fig 6).

300
301
302
303
304
305
306
307

Table 1. The details of the selected data from subject 1 in our experiments.
ROIs

#Instances

#Pixels

#Voxels

#Training

#Validation

#Testing

V1
V2
V3
V4
LO
V3a
V3b

1870
1870
1870
1870
1870
1870
1870

227√ó227
227√ó227
227√ó227
227√ó227
227√ó227
227√ó227
227√ó227

992
1529
1202
1015
621
311
210

1400
1400
1400
1400
1400
1400
1400

350
350
350
350
350
350
350

120
120
120
120
120
120
120
308

Model fitting

309

The AlexNet [18] architecture pre-trained on ImageNet dataset is exploited to initial both the
convolutional and fully-connected layers, and other parameters of the model are randomly
initialized. The hyper-parameters of the proposed model are set to (Œªs , Œªl ) = (1, 1), while
five-fold cross-validation is carried out to choose better regularization parameters from [0.01,
0.1, 0.5, 1, 5, 10]. In our experiments, the minibatch size B is set to 20. The Adam
Optimizer [36] with an initial learning rate of 0.00005 and early stopping is adopted.
Specifically, we monitor the validation loss every iteration of totally 200 iterations and early stop
when the validation loss have not decreased for 5 consecutive times.

Model Evaluation

311
312
313
314
315
316
317

318

To evaluate the encoding performance quantitatively, we use several standard similarity
metrics, including mean squared error (MSE), Pearson‚Äôs correlation coefficient (PCC), and
coefficient of determination (COD, i.e. R2 ). These metrics focus on different properties for

November 27, 2019

310

10/22

319
320
321

bioRxiv preprint doi: https://doi.org/10.1101/861989; this version posted December 2, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available
under aCC-BY 4.0 International license.

the encoding performance. MSE is a common way to evaluate prediction performance in
machine learning, which focuses on the point-to-point prediction accuracy. Note that MSE is not
highly indicative of predictions, whereas PCC and FEV can take variable texture and goodness
of fit into account, which are more significant in neuroscience. We also performed the statistical
significance test (SST) of model prediction accuracy. For each voxel and each model type, the
Pearson‚Äôs correlation coefficient between the model prediction and measured response above
0.27 is significant p < 0.001 relative to its null hypothesis distribution [4, 13]. In the present
study, we use PCC to denotes the prediction accuracy, if there is no special instruction.

Feature map contribution to the prediction

322
323
324
325
326
327
328
329

330

According to voxel-wise mapping module, all the feature maps contribute linearly to the
model prediction. It is a natural way to determine the relative importance of each feature map in
terms of the regression weights. In practice, it is difficult to make comparisons across
multi-layer feature maps, as the regression weights are dependent on the typical values of each
feature map. Hence, in consideration of the linearity of the proposed model, we calculate the
Pearson correlation coefficient œÅl = cov(yÃÇ, y)/ sqrt(var(yÃÇ)var(y)) over a subset of feature
maps hl ‚àà h instead of focusing on regression weights. All the disjoint subsets Œ£l hl cover all
K feature maps, and they follows Œ£l œÅl = œÅ where œÅ is the cumulative PCC between predicted
voxel-wise activity and true voxel-wise activity. Here, each œÅl thereby denotes the contributions
of the subset of feature maps to the model prediction.

331
332
333
334
335
336
337
338
339
340

Results

341

Relationship between CNN layers and brain ROIs

342

 / D \ H U  F R Q W U L E X W L R Q  W R  S U H G L F W L R Q  D F F X U D F \
   D Y H U D J H G  R Y H U  Y R [ H O V  L Q  5 2 , 

     
     
     
     
     
     
     

   

 / D \ H U  F R Q W U L E X W L R Q  W R  S U H G L F W L R Q  D F F X U D F \
   D Y H U D J H G  R Y H U  Y R [ H O V  L Q  5 2 , 

 / 2
 9 
 9 
 9 
 9 
 9  D
 9  E

     

 F R Q Y 
 F R Q Y 
 F R Q Y 
 F R Q Y 
 F R Q Y 
 I F 
 I F 
 I F 

   

   

   

   

     
 F R Q Y 

 F R Q Y 

 F R Q Y 

 F R Q Y 

 F R Q Y 

 / D \ H U

(a)

 I F 

 I F 

 I F 

   

 / 2

 9 

 9 

 9 

 5 2 ,

 9 

 9  D

 9  E

(b)

Fig 5. Contributions of the DNN layers to predictions of ROIs. (a) Each column shows the
distribution of the single DNN layer contributions to the prediction accuracy of each ROI. The
prediction accuracy are averaged over all voxels in each ROI. (b) Each column shows the
distribution of each DNN layer contributions to the prediction accuracy for a single ROI.
Colored bars within each column indicate the contribution to the prediction accuracy averaged
over all voxels in that ROI.
Previous neuroscience studies [21, 24] have shown that the ventral and dorsal visual streams
are hierarchically organized, with early visual areas processing low-level visual features (such as
edges) and downstream visual areas processing increasingly complex visual features (such as
shapes). Does the hierarchical features of CNN have anything to do with the hierarchical visual
areas of brain? To answer this question, we analyzed the contributions of different CNN layers
to activity prediction in different brain regions-of-interest (ROI). As shown in Fig 5 (a) , the

November 27, 2019

11/22

343
344
345
346
347
348

bioRxiv preprint doi: https://doi.org/10.1101/861989; this version posted December 2, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available
under aCC-BY 4.0 International license.

 3 U H G L F W L R Q  D F F X U D F \  I U R P  6 X E M H F W 

 F R Q Y 

 F R Q Y 

 F R Q Y 

 F R Q Y 

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

 F R Q Y 

   

   

   

   

 I F 

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

 I F 

   

   

   

 I F 

   

   

   

   

   

 3 U H G L F W L R Q  D F F X U D F \  I U R P  6 X E M H F W 

Fig 6. Encoding accuracies of individual feature maps or units (AlexNet) from two subjects of
the Vim-1 dataset. Each dot denotes the encoding accuracy of each feature map or unit estimated
from Subject1 and Subject2. The color of each dot indicates the density of the plotted dots.
contribution of the early visual areas in ventral (V1, V2) and dorsal (V3, V3a, V3b) streams
exhibit a clear counter-gradient organization. Contributions of downstream visual areas (V4,
LO) are also graded, but are much more uniformly distributed across the CNN layers. Whlie in
Fig 5 (b), the contribution of the lowest (conv1) and highest (fc8) layers exhibit a clear
counter-gradient organization. Contributions of intermediate DNN layers are also graded, but
are much more uniformly distributed across ROIs.
More specifically, the most contributable CNN features for the prediction of the early visual
areas come from shallow CNN features, whereas most contributable voxels for the prediction of
the downstream visual areas come from the deep CNN features. These results demonstrate a
homology between computer and human vision, providing a new opportunity to make use of the
hierarchical information from CNN features.

Individual differences between subjects

350
351
352
353
354
355
356
357
358
359

360

To assess the degree of consistency of encodability across subjects, we evaluated the feature
map-by-map or unit-by-unit similarity of the prediction accuracy between two subjects of Vim-1
dataset. Fig 6 shows the scatter plots of feature encoding accuracies between Subject1 and
Subject2. The prediction accuracies of individual maps or units from the two subjects densely
distribute along the diagonal axis for most layers, showing positive correlations between the two
subjects. The positive PCCs for all layers of the DNN architectures suggest that the DNN-based
neural encoding was highly consistent across subjects even at the feature map or unit level.

The visualization and convergence of receptive fields

361
362
363
364
365
366
367

368

To verify the capacity to estimate receptive fileds of our modeling approach, we intuitively
visualized the receptive field with increasing iterations across different ROIs. The results are
illustrated in Fig 7, which are the representative voxels from V1, V2, V3, V4, LO. On one hand,
it is easy to find that the receptive fields are smooth and localized for the particular voxels. The
receptive field shapes may not be regular for all voxels, whereas the main shapes can be clearly
distinguished. On the other hand, the preliminary outlines can be formed within 30 iterations

November 27, 2019

349

12/22

369
370
371
372
373
374

bioRxiv preprint doi: https://doi.org/10.1101/861989; this version posted December 2, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available
under aCC-BY 4.0 International license.

0

0
0.012
0.010
50
0.008
0.006 100
0.004
150
0.002
0.000 200
0.002

50
100
150
200
0

50

100 150 200

0.04
0.03

(a) V1#257:10

50

0

0
0.010
50
0.008
0.006 100
0.004
150
0.002
0.000 200

100
150
200
50

200
0

50

100 150 200

0

150
200
0

50

100 150 200

0

100
150
200
0

50

100 150 200

0

0.04

50

50

0.01

100 150 200

0
0.0025
50
0.0020
0.0015 100
0.0010
0.0005 150
0.0000 200
0.0005

(u) LO#600:10

50

100
150
200
0

50

100 150 200

0
0.03
0.02

50
100

0.01 150
0.00 200
0

50

100 150 200

0

50

100 150 200

0

0.02
0.01
0.00
50

50
100
150
200

100 150 200

0

(q) V4#278:20

0

50

100 150 200

100

0.05

150

0.00

200
0

50

100 150 200

0
0.10
0.08
50
0.06
100
0.04
0.02 150
0.00
200
0.02

0
0.14
0.12
50
0.10
0.08 100
0.06
0.04 150
0.02
0.00 200
0.02

0
0.12
0.10
50
0.08
0.06 100
0.04
0.02 150
0.00
0.02 200
0.04

(v) LO#600:20

100 150 200

0.00
0

0.08

0

0.06

50

0.02

50

0.20

0

0.15

50

0.05 150
0.00 200
0

50

0.05

100 150 200

(e) V1#257:50

100 150 200

0

50

100 150 200

0.30
0.25
0.20
0.15
0.10
0.05
0.00
0.05

(j) V2#384:50
0.20
0.15

0

0.30
0.25
0.20
0.15
0.10
0.05
0.00

50

0.10 100
0.05 150
0.00 200
0

50

100 150 200

0

50

100 150 200

(o) V3#1256:50
0

0.15
0.10
0.05
0.00

50
100
150

0.05 200
0

50

100 150 200

0

(s) V4#278:40

0.00
100 150 200

0.05

200

(n) V3#1256:40

0.02

50

0.10

150

(i) V2#384:40

0.04

0

0.15

100

0.10 100

(r) V4#278:30
0
0.030
0.025
50
0.020
0.015 100
0.010
0.005 150
0.000
200
0.005

50

0.20

50

(d) V1#257:40

(m) V3#1256:30
0.03

0

0.02

100 150 200

0.10

(h) V2#384:30
0.04

(p) V4#278:10

50

0

(l) V3#1256:20

0
0.012
0.010
50
0.008
0.006 100
0.004
0.002 150
0.000
200
0.002

100

0.00

(g) V2#384:20

(k) V3#1256:10

50

0.02

200

0.05

0.00

0
0.006
0.005
50
0.004
0.003 100
0.002
0.001 150
0.000
0.001 200
0.002

150

0.04

0
0.15

(c) V1#257:30

0.01

(f) V2#384:10

100

150

0.02

0

0

0.01

0.03

100 150 200

50

50

(b) V1#257:20

50

0

0

0.06

100

0.01

100 150 200

0.08

0.02
0.00
0

0
50

0.10
0.05

150

0.00

200

100 150 200

(t) V4#278:50
0.15

100

50

0

0.20

50

0.15
0.10

100

0.05

150

0.00
0.05

0.05 200
0

(w) LO#600:30

50

100 150 200

(x) LO#600:40

0.20
0.15
0.10
0.05
0.00
0.05
0.10

0

50

100 150 200

(y) LO#600:50

Fig 7. The estimated receptive fileds of the representative voxels with different iterations in
ROIs. For example, (a) V1#257:10 shows the results of the 257th voxel in V1 after the 10
iterations. For the particular voxels, the receptive fields are smooth and localized. The
preliminary outlines can be formed within 30 iterations while optimization procedure converges
within roughly 50 iterations.
while optimization procedure converges within roughly 50 iterations. In practice, for arbitrary
voxel, the receptive field can be optimized automatically in this way. The results on the rest of
voxels are similar, and we omit them due to space limitations. It can be inferred that, owing to
the regularizations, the receptive field in our proposed method is able to capture the reasonable
location and extent of pooling over visual features.

Quantitative analysis of encoding performance

376
377
378
379

380

The encoding performce distribution [37] of the proposed method is shown in Fig 8. Voxels
located in early visual cortex (V1, V2) are more accurately predicted than those located in higher
visual cortex (V4, LO), whileas the difference of encoding performance between most of the

November 27, 2019

375

13/22

381
382
383

   

   

   

   

 0 H D Q  V T X D U H G  S U H G L F W L R Q  H U U R U

 3 U H G L F W L R Q  D F F X U D F \

bioRxiv preprint doi: https://doi.org/10.1101/861989; this version posted December 2, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available
under aCC-BY 4.0 International license.

   

   

   

   

 / 2

 9 

 9 

 9 

 5 2 ,

 9 

 9  D

   

   

   

   

 9  E

 / 2

 9 

(a) PCC

 9 

 9 

 5 2 ,

 9 

 9  D

 9  E

(b) MSE

0.0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

(c) PCC Distribution

Fig 8. Encoding accuracy for each ROI. (a) Distributions of the encoding accuracy of individual
voxels in terms of PCC. Green bars denote median prediction accuracies while red triangles
denote mean prediction accuracies averaged across all voxels. (b) Distributions of the encoding
accuracy of individual voxels in terms of MSE. Blue bars denote mean prediction accuracies
averaged across all voxels. (c) Prediction accuracy of voxels plotted on a digitally flattened map
of visual cortex.
visual areas is not very obvious, especially for the MSE metric. It verifies the feasibility of our
method in the visual areas. We further compare the proposed method with other baseline
methods in terms of three metrics, and the quantitative performance comparisons are shown in
Fig 9 and Fig 10 (the details are shown in Table 2). From them, we can find that our method
outperforms the baselines in most brain ROIs. Compared with CSS and SOC, the consistently
encouraging result shows that the proposed method with a DNN model for visual images is more
able to extract nonlinear features from visual images, which may contribute to encoding
performance in the primary visual cortex. Furthermore, our method shows obvious better
performance than fwRF. In spite of the same feature representation network, it is maybe caused
by the fact that fwRF is endowed with the two-dimensional Gaussian assumption and manual
parameter space, which may not obtain the global optimal solution of model parameters. In
summary, for the current dataset, the substantial superior performance (Consistent results are
obtained for subject 2, as shown in S2 Table) of the proposed model verifies that estimating
receptive field automatically with weak prior assumptions about the spatial characteristics is
beneficial to enhancing the encoding performance.

November 27, 2019

14/22

384
385
386
387
388
389
390
391
392
393
394
395
396
397
398

bioRxiv preprint doi: https://doi.org/10.1101/861989; this version posted December 2, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available
under aCC-BY 4.0 International license.

 3 U H G L F W L R Q  D F F X U D F \  I U R P  R X U  P R G H O   3 & & 

PCCour < 0.27  D Q G PCCfwRF < 0.27

PCCour > 0.27  D Q G PCCour > PCCfwRF

PCCfwRF > 0.27  D Q G PCCfwRF > PCCour

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

 9 

   

   

   

   

   

   

   

 9 

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

 9 

   

   

   

   

   

   

   

 / 2

   

   

   

   

   

   

   

   

   

 9 

   

   

   

   

   

   

   

 9  D  D Q G  9  E

 3 U H G L F W L R Q  D F F X U D F \  I U R P  I Z 5 )   3 & & 

Fig 9. Comparison of prediction accuracy between the proposed method and the fwRF. Each of
the six axes displays a comparison between the prediction accuracy of the two models in specific
visual ROI (V3a and V3b are plotted in the same axe). In all six scatter plots, the ordinate and
abscissa represent the prediction accuracy values of the proposed method and the fwRF. The
blue dots indicate the voxels cannot be significantly encoded (under 0.27) by either of the two
models. The red dots indicate the voxels that can be better predicted by the proposed method
than the fwRF and vice versa for the cyan dots.

 2 X U V
 I Z 5 )
 & 6 6
 6 2 &

    

    
    
    
    
    

 2 X U V
 I Z 5 )
 & 6 6
 6 2 &

    

 $ Y H U D J H  & 2 '
   D Y H U D J H G  R Y H U  Y R [ H O V  L Q  5 2 , 

 $ Y H U D J H  0 6 ( 
   D Y H U D J H G  R Y H U  Y R [ H O V  L Q  5 2 , 

    

    

    

    

    

    
    

    
 9 

 9 

 9 

 9 

 5 2 ,

(a) MSE

 9  D

 9  E

 / 2

 / 2

 9 

 9 

 9 
 5 2 ,

 9 

 9  D

 9  E

(b) COD

Fig 10. Preformance within different ROIs in terms of MSE and COD. The proposed method
outperforms other baseline methods in most brain ROIs

November 27, 2019

15/22

bioRxiv preprint doi: https://doi.org/10.1101/861989; this version posted December 2, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available
under aCC-BY 4.0 International license.

Table 2. Performance of several neural encoding models for Subject 1 of the Vim-1 dataset. The
best performance (averaged over voxels) within each ROI was highlighted.
Evaluation

Algorithms

V1

V2

V3

V4

LO

V3a

V3b

MSE

CSS
SOC
fwRF
Ours

0.3183
0.3126
0.2666
0.2555

0.3026
0.3039
0.2717
0.2630

0.2577
0.2636
0.2730
0.2444

0.2565
0.2578
0.2562
0.2447

0.2630
0.2687
0.2747
0.2589

0.2445
0.2425
0.2548
0.2428

0.2507
0.2487
0.2625
0.2446

PCC

CSS
SOC
fwRF
Ours

0.0448
0.0911
0.2886
0.2943

0.0463
0.0611
0.2400
0.2468

0.0330
0.0483
0.1600
0.1730

0.0321
0.0496
0.1517
0.1565

0.0221
0.0535
0.1257
0.1453

0.0261
0.0171
0.1097
0.1077

0.0222
0.03041
0.1251
0.1363

COD

CSS
SOC
fwRF
Ours

0.0187
0.0088
0.2548
0.2619

0.0237
0.0076
0.2113
0.2281

0.0188
0.0067
0.1346
0.1408

0.0133
0.0078
0.1073
0.1141

0.0114
0.0071
0.0959
0.1028

0.0099
0.0055
0.0513
0.0561

0.0102
0.0062
0.0847
0.0776

Sensitivity analysis

399

The sensitivity analysis is crucial to DNN-based models. Since weak prior assumptions
(sparsity and smoothness regularizations) are adopted in our model, we also performed the
sensitivity analysis. It involves two hyper-parameters Œò = (Œªs , Œªl ), which need setting properly.
For the sake of studying the sensitivity of the model with respect to different values of these
parameters, we plot effective encoding results (average over voxels with the prediction accuracy
higher than 0.5) with different values of regularization parameters, and displayed the results in
terms of PCC and MSE in Fig 11. The figures for variation of MSE and PCC show the same
pattern, and model performs stably with the variation of different Œªs and Œªl . The results also
suggests that the best regularization parameters Œªl and Œªs can be chosen from [0.5, 1, 5] and
[1,5], where the proposed model achieves good results.

403
404
405
406
407
408
409

    
   
 

s

   

      
      

 

    
   
   

402

      

 

s

      

401

      

      
      

400

      

 

      

      

  

  

      

    

   

   

 

 

  

      

l
(a) MSE

    

   

   

 

 

  

l
(b) PCC

Fig 11. Encoding perfomance with the variation of Œªl and Œªs in terms of MSE and PCC. The
best regularization parameter Œªl and Œªs can be chosen from [0.5, 1, 5] and [1,5], where the
proposed model achieves good results.

Extension for multi-voxel joint encoding

410

In the visual cortex, the responses to stimulation in the classical receptive fields can be
modulated by stimulation in the extra-classical receptive field [7]. These modulations can be
excitatory or inhibitory and have been characterized in detail by electrophysiological and

November 27, 2019

16/22

411
412
413

bioRxiv preprint doi: https://doi.org/10.1101/861989; this version posted December 2, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available
under aCC-BY 4.0 International license.

psychophysical studies [38‚Äì40]. It inspired us to relate receptive fields of multiple voxels to
mimic this modulations and construct multi-voxel joint encoding models. Here, we use the
similarity loss to relate the adjecent voxels, as voxels in the same area tend to perform similar
computations. Taking two-voxel cooperative encoding as an example, for voxel j and voxel k, to
ensure their receptive fields as similar as possible, we use an L2 penalty on the difference
between receptive fields with the strength Œªm :
Lsimilarity = ||pj ‚àí pk ||F

Tjoint = Tj + Tk + Œªm Lsimilarity

   

 3 & &      
 P X O W L  Y R [ H O
 V L Q J O H  Y R [ H O

   

   
   

   

 & R X Q W

 & R X Q W

   

PCCm > 0.27 = 1286
PCCs > 0.27 = 1000

   

  

PCCm > 0.27 = 331
PCCs > 0.27 = 178

  

  
  
  
  

  
 

   

   

   

   

   

   

   

 3 U H G L F W L R Q  D F F X U D F \   3 & & 

(a) Early visual areas (V1, V2, V3)

   

   

 

   

   

   

   

   

   

   

   

 3 U H G L F W L R Q  D F F X U D F \   3 & & 

(b) Higher visual areas (V4, V3a, V3b, LO)

Fig 12. Comparisons between the single-voxel encoding and the multi-voxel joint encoding. (a)
Histogram of voxels within early visual areas (V1,V2,V3). There are 1286 significant voxels in
the multi-voxel joint encoding model, in contrast to 1000 significant voxels in the single-voxel
encoding model. (b) Histogram of voxels within higher visual areas (V4,V3a,V3b,LO). The
number of significant voxels in the multi-voxel joint encoding is 331 while that in the
single-voxel encoding model is 178. Results in both early visual areas and higher visual areas
demonstrate that the multi-voxel joint encoding is conducive to rescuing voxels with poor
signal-to-noise characteristics

November 27, 2019

416
417
418
419

420

(9)

Here, Tj and Tk are the objective function of voxel j and voxel k, respectively, and Lsimilarity
is the bond between two voxels. This bond can be extended to three or more voxels according to
appropriate definitions of voxel neighborhood. In the present study, we did a preliminary
verification of the multi-voxel joint encoding.
In consideration of computational similarities in the same visual area, we define a voxel
neighborhood that consists of three voxels according to their location index in the Vim-1 dataset.
Those voxels (up to three voxels) whose location index are adjacent are chosen to be jointly
optimized. The hyper-parameter Œªsimilarity is set to 1, while five-fold cross-validation is carried
out to choose the better regularization parameter from [0.1, 0.5, 1, 5]. We compare the
multi-voxel joint encoding with the single-voxel encoding, and the results are shown in Fig 12.
It can be found that, both in early visual areas (V1, V2, V3) and higher visual areas (V4, V3a,
V3b and LO), after the multi-voxel joint encoding, a slight shift of the voxels toward the right
indicates an advantage for the multi-voxel joint encoding. The results suggest that, owing to
voxel neighborhood information, the multi-voxel joint encoding is conducive to rescuing voxels
with poor signal-to-noise characteristics.

 3 & &      
 P X O W L  Y R [ H O
 V L Q J O H  Y R [ H O

415

(8)

The joint objective function of the multi-voxel joint encoding model is formulated as follow:

   

414

17/22

421
422
423
424
425
426
427
428
429
430
431
432
433
434
435

bioRxiv preprint doi: https://doi.org/10.1101/861989; this version posted December 2, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available
under aCC-BY 4.0 International license.

Discussion

436

The proposed model is a new approach to building voxel-wise neural encoding models.
Attributing to the nonlinear feature refinement and separability of ‚Äúwhat‚Äù and ‚Äúwhere‚Äù
parameters, the proposed model is endowed with explicit receptive fields and feature maps for
each voxel, which facilitates the interpretability of neural encoding models. Compared with
either traditional or deep learning methods, the proposed model achieved the superior
performance in visual brain areas.

Relationship to previous work

438
439
440
441
442

443

Preceding the proposed modeling approach, a number of important voxel-wise modeling
approaches have made significant progress in neural encoding for human visual cortex. One
important class of models are those designed specifically for retinotopic modeling. This class
involves the inverse retinotopy method [41] and the population receptive field methods [6‚Äì8].
These methods implement estimation and analysis of the locations and sizes of voxel-wise
receptive fields, with the dedicated retinotopic mapping experiments that utilize the artificially
designed stimuli. However, they do not provide explicit feature maps and turning functions
directly and require fussy in-silico experiments. Another effective class of models are those
designed based on DNNs [13]. These methods are special cases of the general linearized
regression approach, they construct a set of nonlinear features from deep neural networks and
present explicit feature map. Nevertheless, they still limit the receptive field with Gaussian
assumptions in advance. The strong prior assumptions about receptive field have the advantage
of reducing the number of parameters of the model, whereas they reduces their expressiveness as
well.
The proposed modeling approach with weak prior assumptions about receptive fields is a
more special case of the general linearized regression approach, and it also depends on the
construction of a set of nonlinear features that result from deep neural networks. This modeling
approach overcomes two limitations of previous general regression approaches. First, in general
regression approaches, the shape of receptive field is pre-defined, which makes the model prone
to erroneous eatimation of receptive field characteristics. Second, under the general approach
deriving an explicit receptive field and feature tuning function often requires grid searching from
plenty of candidate receptive fields. However, it is an kind of effective but not sensible way to
set search parameters (such as total number and the search interval of candidate receptive fields)
according to experience, as it demands too much manual effort.

Advantages of the proposed model

444
445
446
447
448
449
450
451
452
453
454
455
456
457
458
459
460
461
462
463
464
465
466
467

468

There are at least three possible reasons for why the proposed model outperforms the other
compared methods. One primary possible reason is the flexible receptive field of the proposed
modeling approach. Structure analysis of receptive fields and preformance comparisons with
fwRF suggest that the model takes advantage of this flexibility by the weak assumptions about
spatial characteristic of rereceptive fields. Without defining the shape of receptive field in
advance, the model can make full use of the distribution of training data and estimate the optimal
receptive field automatically, revealing the efficient location and extend of visual features.
Another possible reason is the usage of channel attention module. On account of it, our model is
able to select inter-layer feature attributes and reducing redundant information. Furthermore,
strengthening important ones and weakening unimportant ones are conducive to subsequent
cross-layer regression. The third possibility is the space-feature separability derived from the
proposed model. Explicit receptive fields in the nonlinear feature refinement module are able to
capture spatial information, whereas explicit weights of feature maps allow the proposed model
to select cross-layer combinations of visual features that closely resemble the feature selectivity

November 27, 2019

437

18/22

469
470
471
472
473
474
475
476
477
478
479
480
481
482

bioRxiv preprint doi: https://doi.org/10.1101/861989; this version posted December 2, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available
under aCC-BY 4.0 International license.

of each voxel. We infer that the increased expressiveness is made possible by this space-feature
separability.

Limitations of the proposed model

484

485

While the proposed modeling approach exhibits good encoding performance, there is still an
imperfection worthy of consideration: the measurement of receptive field size. The proposed
modeling approach yields an explicit receptive fields with weak prior assumptions, which
provides pros and cons. It facilitates the expressiveness of model, whereas brings the difficulties
to the measurement of receptive field size. The receptive field size is different from classical
population receptive field (pRF) [6]. The differences underline the fact that the eatimation of
receptive field in our method depends entirely upon the feature maps. As the proposed modeling
approach does not impose an strong assumptions (regular shapes) on receptive fileds, it limits
the measurement of receptive field size to a certain esxtent. However, for irregular receptive
field, we can supply an alterbnative measurement method, which is the blob detection. In light
of different pixels of the receptive field, blob detection can detect an approximate regular shape,
such as the Gaussian shape (areas circled by red line in Fig 7 (e)).

The receptive field structure

486
487
488
489
490
491
492
493
494
495
496
497

498

The receptive field presented here is a mask with sparsity and smoothness regularizations,
and the salient characteristic is that its shapes can be irregular. While keeping the shape regular
has the advantage of reducing model parameters of the model, it also reduces the expressiveness.
In particular, the other appropriate operators instead of Laplacian operator can be adopted in our
method, exploring more appropriate receptive field structures. For example, the Laplacian of
Gaussian operator may allow the model to explicitly capture receptive fields with a ‚ÄúMexican
hat‚Äù profile that enforce a suppressive band around an excitatory center. Furthermore, the
receptive field with any appropriate weak prior assumptions could be trained using the proposed
modeling approach presented here. There may be a more optimal assumption about receptive
field structure, which needs future studies to confirm.

Encoding in higher visual areas (V4 and LO)

499
500
501
502
503
504
505
506
507
508

509

It is worth noting that all the presented methods obtain good performance in early visual
areas (V1,V2 and V3) while little effect on higher visual areas (V4 and LO). Nevertheless, the
proposed modeling approach is still superior to other methods, which makes progress in higher
visual areas. However, it is meaningful to build an effective encoding model in higher visual
areas, as it may reveal the high-level visual information processing in the visual cortex. Deep
neural network has mapped the function of the human visual cortex [35] and revealed a gradient
in the complexity of neural representations across the ventral stream [21]. The convolutional
layer encoded location and orientation-selective features while the fully-connected layer
encoded semantic features. In order to make progress in higher visual areas, perhaps more
attention to the fully connected layer will make sense. The receptive fields easimated in our
model focus on the convolutional layer, whereas the fully-connected layer are neglected. It is
worth our while further improving the encoding performance of our proposed modeling
approach with appropriate operations on the fully-connected layer.

November 27, 2019

483

510
511
512
513
514
515
516
517
518
519
520
521
522

Supporting information

523

S1 Table. The details of the selected data from subject 2.

524

S2 Table. The quantitative results of encoding performance for Subject 2.

525

19/22

bioRxiv preprint doi: https://doi.org/10.1101/861989; this version posted December 2, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available
under aCC-BY 4.0 International license.

Acknowledgments

526

This work was supported in part by the National Key Research and Development Program of
China (2018YFC2001302), in part by National Natural Science Foundation of China 91520202,
61602449, in part by Chinese Academy of Sciences Scientific Equipment Development Project
(YJKYYQ20170050), in part by Beijing Municipal Science and Technology Commission
(Z181100008918010), in part by Youth Innovation Promotion Association of Chinese Academy
of Sciences, and in part by Strategic Priority Research Program of Chinese Academy of Sciences
(XDB32040200).

References
1. Engel SA, Rumelhart DE, Wandell BA, Lee AT, Glover GH, Chichilnisky EJ, et al. fMRI
of human visual cortex. Nature. 1994;.
2. Sereno MI, Dale A, Reppas J, Kwong K, Belliveau J, Brady T, et al. Borders of multiple
visual areas in humans revealed by functional magnetic resonance imaging. Science.
1995;268(5212):889‚Äì893.
3. Paninski L, Pillow J, Lewi J. Statistical models for neural encoding, decoding, and
optimal stimulus design. Progress in brain research. 2007;165:493‚Äì507.
4. Kay KN, Naselaris T, Prenger RJ, Gallant JL. Identifying natural images from human
brain activity. Nature. 2008;452(7185):352‚Äì355.
5. Kriegeskorte N. Deep neural networks: a new framework for modeling biological vision
and brain information processing. Annual review of vision science. 2015;1:417‚Äì446.
6. Dumoulin SO, Wandell BA. Population receptive field estimates in human visual cortex.
Neuroimage. 2008;39(2):647‚Äì660.
7. Zuiderbaan W, Harvey BM, Dumoulin SO. Modeling center‚Äìsurround configurations in
population receptive fields using fMRI. Journal of vision. 2012;12(3):10‚Äì10.
8. Lee S, Papanikolaou A, Logothetis NK, Smirnakis SM, Keliris GA. A new method for
estimating population receptive field topography in visual cortex. Neuroimage.
2013;81(11):144‚Äì157.
9. Kay KN, Winawer J, Mezer A, Wandell BA. Compressive spatial summation in human
visual cortex. Journal of neurophysiology. 2013;110(2):481‚Äì494.
10. Zeidman P, Silson EH, Schwarzkopf DS, Baker CI, Penny W. Bayesian population
receptive field modelling. NeuroImage. 2018;180:173‚Äì187.
11. Nishimoto S, Vu AT, Naselaris T, Benjamini Y, Yu B, Gallant JL. Reconstructing visual
experiences from brain activity evoked by natural movies. Current Biology.
2011;21(19):1641‚Äì1646.
12. Naselaris T, Stansbury DE, Gallant JL. Cortical representation of animate and inanimate
objects in complex natural scenes. Journal of Physiology - Paris. 2012;106(5-6):239‚Äì249.
13. St-Yves G, Naselaris T. The feature-weighted receptive field: an interpretable encoding
model for complex feature spaces. NeuroImage. 2018;180:188‚Äì202.
14. Wang C. Variational Bayesian approach to canonical correlation analysis. IEEE
Transactions on Neural Networks. 2007;18(3):905‚Äì910.

November 27, 2019

20/22

527
528
529
530
531
532
533

bioRxiv preprint doi: https://doi.org/10.1101/861989; this version posted December 2, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available
under aCC-BY 4.0 International license.

15. Kay KN, Winawer J, Rokem A, Mezer A, Wandell BA. A two-stage cascade model of
BOLD responses in human visual cortex. PLoS Comput Biol. 2013;9(5):e1003079.
16. Du C, Du C, Huang L, He H. Reconstructing Perceived Images from Human Brain
Activities with Bayesian Deep Multi-view Learning. IEEE Transactions on Neural
Networks and Learning Systems. 2018;.
17. LeCun Y, Boser BE, Denker JS, Henderson D, Howard RE, Hubbard WE, et al.
Handwritten digit recognition with a back-propagation network. In: Advances in neural
information processing systems; 1990. p. 396‚Äì404.
18. Krizhevsky A, Sutskever I, Hinton GE. Imagenet classification with deep convolutional
neural networks. In: Advances in neural information processing systems; 2012. p.
1097‚Äì1105.
19. He K, Zhang X, Ren S, Sun J. Deep residual learning for image recognition. In:
Proceedings of the IEEE conference on computer vision and pattern recognition; 2016. p.
770‚Äì778.
20. Yamins DL, Hong H, Cadieu CF, Solomon EA, Seibert D, DiCarlo JJ.
Performance-optimized hierarchical models predict neural responses in higher visual
cortex. Proceedings of the National Academy of Sciences. 2014;111(23):8619‚Äì8624.
21. GuÃàcÃßluÃà U, van Gerven MA. Deep neural networks reveal a gradient in the complexity of
neural representations across the ventral stream. Journal of Neuroscience.
2015;35(27):10005‚Äì10014.
22. Kietzmann TC, McClure P, Kriegeskorte N. Deep neural networks in computational
neuroscience. bioRxiv. 2018; p. 133504.
23. Huth AG, Nishimoto S, Vu AT, Gallant JL. A continuous semantic space describes the
representation of thousands of object and action categories across the human brain.
Neuron. 2012;76(6):1210‚Äì1224.
24. GuÃàcÃßluÃà U, van Gerven MA. Increasingly complex representations of natural movies across
the dorsal stream are shared between subjects. NeuroImage. 2017;145:329‚Äì336.
25. Wen H, Shi J, Chen W, Liu Z. Deep residual network predicts cortical representation and
organization of visual features for rapid categorization. Scientific reports.
2018;8(1):3752.
26. Eickenberg M, Gramfort A, Varoquaux G, Thirion B. Seeing it all: Convolutional
network layers map the function of the human visual system. NeuroImage.
2017;152:184‚Äì194.
27. Han K, Wen H, Shi J, Lu KH, Zhang Y, Fu D, et al. Variational autoencoder: An
unsupervised model for encoding and decoding fMRI activity in visual cortex.
NeuroImage. 2019;198:125‚Äì136.
28. Wen H, Shi J, Zhang Y, Lu KH, Cao J, Liu Z. Neural encoding and decoding with deep
learning for dynamic natural vision. Cerebral Cortex. 2017;28(12):4136‚Äì4160.
29. Itti L, Koch C, Niebur E. A model of saliency-based visual attention for rapid scene
analysis. IEEE Transactions on Pattern Analysis & Machine Intelligence.
1998;(11):1254‚Äì1259.
30. Rensink RA. The dynamic representation of scenes. Visual cognition.
2000;7(1-3):17‚Äì42.

November 27, 2019

21/22

bioRxiv preprint doi: https://doi.org/10.1101/861989; this version posted December 2, 2019. The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available
under aCC-BY 4.0 International license.

31. Corbetta M, Shulman GL. Control of goal-directed and stimulus-driven attention in the
brain. Nature reviews neuroscience. 2002;3(3):201.
32. Larochelle H, Hinton GE. Learning to combine foveal glimpses with a third-order
Boltzmann machine. In: Advances in neural information processing systems; 2010. p.
1243‚Äì1251.
33. Chen L, Zhang H, Xiao J, Nie L, Shao J, Liu W, et al. Sca-cnn: Spatial and channel-wise
attention in convolutional networks for image captioning. In: Proceedings of the IEEE
conference on computer vision and pattern recognition; 2017. p. 5659‚Äì5667.
34. Hu J, Shen L, Sun G. Squeeze-and-excitation networks. In: Proceedings of the IEEE
conference on computer vision and pattern recognition; 2018. p. 7132‚Äì7141.
35. Eickenberg M, Varoquaux G, Thirion B, Gramfort A. Convolutional Network Layers
Map the Function of the Human Visual Cortex. ERCIM NEWS. 2017;(108):12‚Äì13.
36. Kingma DP, Ba J. Adam: A method for stochastic optimization. arXiv preprint
arXiv:14126980. 2014;.
37. Gao JS, Huth AG, Lescroart MD, Gallant JL. Pycortex: an interactive surface visualizer
for fMRI. Frontiers in neuroinformatics. 2015;9:23.
38. Carandini M. Receptive fields and suppressive fields in the early visual system. The
cognitive neurosciences. 2004;3:313‚Äì326.
39. Cavanaugh JR, Bair W, Movshon JA. Nature and interaction of signals from the receptive
field center and surround in macaque V1 neurons. Journal of neurophysiology.
2002;88(5):2530‚Äì2546.
40. Fitzpatrick D. Seeing beyond the receptive field in primary visual cortex. Current
opinion in neurobiology. 2000;10(4):438‚Äì443.
41. Thirion B, Duchesnay E, Hubbard E, Dubois J, Poline JB, Lebihan D, et al. Inverse
retinotopy: inferring the visual content of images from brain activation patterns.
Neuroimage. 2006;33(4):1104‚Äì1116.

November 27, 2019

22/22

