The information-theoretic value of unlabeled data in
semi-supervised learning

arXiv:1901.05515v2 [cs.LG] 13 May 2019

Alexander Golovnevâˆ—

DaÌvid PaÌlâ€ 

BalaÌzs SzoÌˆreÌnyiâ€ 

May 15, 2019

Abstract
We quantify the separation between the numbers of labeled examples required to learn in
two settings: Settings with and without the knowledge of the distribution of the unlabeled
data. More specifically, we prove a separation by Î˜(log n) multiplicative factor for the class of
projections over the Boolean hypercube of dimension n. We prove that there is no separation
for the class of all functions on domain of any size.
Learning with the knowledge of the distribution (a.k.a. fixed-distribution learning) can be
viewed as an idealized scenario of semi-supervised learning where the number of unlabeled data
points is so great that the unlabeled distribution is known exactly. For this reason, we call the
separation the value of unlabeled data.

1

Introduction

Hanneke [2016] showed that for any class C of Vapnik-Chervonenkis dimension d there

 exists an
algorithm that -learns any target function from C under any distribution from O d+log(1/Î´)

labeled examples with probability at least 1 âˆ’ Î´. For this paper, it is important to stress that
Hannekeâ€™s algorithm does not receive the distribution of unlabeled data as input. On the other
hand, Benedek and Itai [1991] showed that for any class C and any
 distribution there exists an
log N/2 +log(1/Î´)
algorithm that -learns any target from C from O
labeled examples with prob

ability at least 1 âˆ’ Î´ where N/2 is the size of an 2 -cover of C with respect to the disagreement
metric d(f, g) = Pr[f (x) 6= g(x)]. Here, it is important to note that Benedek and Itai construct
for each distribution a separate algorithm. In other words, they construct a family of algorithms
indexed by the (uncountably many) distributions over the domain. Alternatively, we can think of
Benedek-Itaiâ€™s family of algorithms as a single algorithm that receives the distribution as an input.
It is known that N = O(1/)O(d) ; see Dudley [1978]. Thus, ignoring log(1/) factor, Benedek-Itai
bound is never worse than Hannekeâ€™s bound.
As we already mentioned, Benedek-Itaiâ€™s algorithm receives as input the distribution of unlabeled data. The algorithm uses it to construct an 2 -cover. Unsurprisingly, there exist distributions
which have a small 2 -cover and thus sample complexity of Benedek-Itaiâ€™s algorithm on such distributions is significantly lower then the Hannekeâ€™s bound. For instance, a distribution concentrated
on a single point has an 2 -cover of size 2 for any positive .
âˆ—
â€ 

Harvard University, Cambridge, MA, USA. Supported by a Rabin Postdoctoral Fellowship.
Yahoo Research, New York, NY, USA

1

However, an algorithm does not need to receive the unlabeled distribution in order to enjoy
low sample complexity. For example, empirical risk minimization (ERM) algorithm needs significantly less labeled examples to learn any target under some unlabeled distributions. For instance,
if the distribution is concentrated on a single point, ERM needs only one labeled example to learn
any target. One could be lead to believe that there exists an algorithm that does not receive
the unlabeled distribution as input and achieves Benedek-Itai bound (or a slightly worse bound)
for every distribution. In fact, one could think that ERM or Hannekeâ€™s algorithm could be such
algorithms. If ERM, Hannekeâ€™s algorithm, or some other distribution-independent algorithm had
sample complexity that matches (or nearly matches) the optimal distribution-specific sample complexity for every distribution, we could conclude that the knowledge of unlabeled data distribution
is completely useless.
As DarnstaÌˆdt et al. [2013] showed this is not the case. They showed that any algorithm for learning projections over {0, 1}n that does not receive the unlabeled distribution as input, requires, for
some data unlabeled distributions, more labeled examples than the Benedek-Itai bound. However,
they did not quantify this gap beside stating that it grows without bound as n goes to infinity.
In this paper, we quantify the gap by showing that any distribution-independent algorithm for
learning the class of projections over {0, 1}n requires, for some unlabeled distributions, â„¦(log n)
times as many labeled examples as Benedek-Itai bound. DarnstaÌˆdt et al. [2013] showed the gap for
any class with Vapnik-Chervonenkis dimension d is at most O(d). It is well known that VapnikChervonenkis dimensions of projections over {0, 1}n is Î˜(log n). Thus our lower bound matches
the upper bound O(d). To better understand the relationship of the upper and lower bounds, we
illustrate the situation for the class of projections over {0, 1}n in Figure 1.
In contrast, we show that for the class of all functions (on any domain) there is no gap between
the two settings. In other words, for learning a target from the class of all functions, unlabeled
data are in fact useless. This illustrates the point that the gap depends in a non-trivial way on
the combinatorial structure of the function class rather than just on the Vapnik-Chervonenkis
dimension.
The paper is organized as follows. In Section 2 we review prior work. Section 3 gives the
necessary definitions and basic probabilistic tools. In Section 4 we give the proof of the separation
result for projections. In Section 5 we prove that there is no gap for the class of all functions. For
O(d) on the size of the
completeness, in Appendix A we give a proof of a simple upper bound O(1/)


log N/2 +log(1/Î´)
minimum -cover, and in Appendix B we give a proof of Benedek-Itaiâ€™s O
sample

complexity upper bound.

2

Related work

The question of whether knowledge of unlabeled data distribution helps was proposed and initially
studied by Ben-David et al. [2008]; see also Lu [2009]. However, they considered only classes with
Vapnik-Chervonenkis dimension at most 1, or classes with Vapnik-Chervonenkis dimension d but
only distributions for which the size of the -cover is Î˜(1/)Î˜(d) , i.e. the -cover is as large as it
can be.1 In these settings, for constant  and Î´, the separation of labeled sample complexities is
at most a constant factor, which is exactly what Ben-David et al. [2008] proved. In these settings,
1
For any concept class with Vapnik-Chervonenkis dimension d and any distribution, the size of the smallest -cover
is at most O(1/)O(d) .

2

labeled sample complexity

distribution on a shattered set
Î˜ (log(n))

an algorithm that does not â€œknowâ€ the distribution
fixed-distribution learning
distributions over {0, 1}n

Figure 1: The graph shows sample complexity bounds of learning a class of projections over the domain
1
{0, 1}n under various unlabeled distributions. We assume that  and Î´ are constant, say,  = Î´ = 100
. The
graph shows three lines. The red horizontal line is Hannekeâ€™s bound for the class of projections, which is
Î˜(VC(Cn )) = Î˜(log n). The green line is the Benedek-Itai bound. The green line touches the red line for
certain distributions, but is lower for other distributions. In particular, for certain distributions the green
line is O(1). The dashed line corresponds to a particular distribution on a shattered set. This is where the
green line and red line touch. Furthermore, here the upper bound coincides with the lower bound for that
particular distribution. The black line is the sample complexity of an arbitrary distribution-independent
algorithm. For example, the reader can think of the ERM or Hannekeâ€™s algorithm. We prove that there exist
a distribution where the black line is â„¦(log n) times higher than the green line. This separation is indicated
by the double arrow.

unlabeled data are indeed useless. However, these results say nothing about distributions with
-cover of small size and it ignores the dependency on the Vapnik-Chervonenkis dimension.
The question was studied in earnest by DarnstaÌˆdt et al. [2013] who showed two major results.
First, they show that for any non-trivial concept class C and for every distribution, the ratio
of the labeled sample complexities between distribution-independent and distribution-dependent
algorithms is bounded by the Vapnik-Chervonenkis dimension. Second, they show that for the
class of projections over {0, 1}n , there are distributions where the ratio grows to infinity as a
function of n.
In learning theory, the disagreement metric and -cover were introduced by Benedek and Itai
[1991] but the ideas are much older; see e.g. Dudley [1978, 1984]. The O(1/)O(d) upper bound on
size of the smallest -cover is by Dudley [1978, Lemma 7.13]; see also Devroye and Lugosi [2000,
Chapter 4] and Haussler [1995]. We present a proof of O(1/)O(d) upper bound in Appendix A.
For any distribution-independent algorithm and any class C of Vapnik-Chervonenkis dimension
d â‰¥ 2 and any  âˆˆ (0, 1) and
 Î´ âˆˆ (0,1), there exists a distribution over the domain and a concept
which requires at least â„¦ d+log(1/Î´)
labeled examples to -learn with probability at least 1 âˆ’ Î´;

see Anthony and Bartlett [1999, Theorem 5.3] and Blumer et al. [1989], Ehrenfeucht et al. [1989].
The proof of the lower bound constructs a distribution that does not depend on the algorithm.
The distribution is a particular distribution
over a fixed set shattered by C. So even an algorithm

d+log(1/Î´)
that knows the distribution requires â„¦
labeled examples.


3

3

Preliminaries

Let X be a non-empty set. We denote by {0, 1}X the class of all functions from X to {0, 1}.
A concept class over a domain X is a subset C âŠ† {0, 1}X . A labeled example is a pair (x, y) âˆˆ
X Ã— {0, 1}.
S
m
X
A distribution-independent learning algorithm is a function A : âˆž
m=0 (X Ã— {0, 1}) â†’ {0, 1} .
In other words, the algorithm gets as input a sequence of labeled examples (x1 , y1 ), (x2 , y2 ), . . . , (xm , ym )
and outputs a function from X to {0, 1}. We allow the algorithm to output a function that does not
belong to C, i.e., the algorithm can be improper. A distribution-dependent algorithm is a function
that maps any probability distribution over X to a distribution-independent algorithm.
Let P be a probability distribution over a domain X . For any two functions f : X â†’ {0, 1},
g : X â†’ {0, 1} we define the disagreement pseudo-metric
dP (f, g) = Pr [f (X) 6= g(X)] .
Xâˆ¼P

Let C be a concept class over X , let c âˆˆ C, let , Î´ âˆˆ (0, 1). Let X1 , X2 , . . . , Xm be an i.i.d. sample
from P . We define the corresponding labeled sample T = ((X1 , c(X1 )), (X2 , c(X2 )), . . . , (Xm , c(Xm ))).
We say that an algorithm A, -learns target c from m samples with probability at least 1 âˆ’ Î´ if
Pr [dP (c, A(T )) â‰¤ ] â‰¥ 1 âˆ’ Î´ .
The smallest non-negative integer m such that for any target c âˆˆ C, the algorithm A, -learns the
target c from m samples with probability at least 1 âˆ’ Î´ is denoted by m(A, C, P, , Î´).
We recall the standard definitions from learning theory. For any concept c : X â†’ {0, 1} and
any S âŠ† X we define Ï€(c, S) = {x âˆˆ S : c(x) = 1}. In other words, Ï€(c, S) is the set of examples
in S which c labels 1. A set S âŠ† X is shattered by a concept class C if for any subset S 0 âŠ† S there
exists a classifier c âˆˆ C such that Ï€(c, S) = S 0 . Vapnik-Chervonenkis dimension of a concept class
C is the size of the largest set S âŠ† X shattered by C. A subset C 0 of a concept class C is an -cover
of C for a probability distribution P if for any c âˆˆ C there exists c0 âˆˆ C 0 such that dP (c, c0 ) â‰¤ .
To prove our lower bounds we need three general probabilistic results. The first one is the standard Hoeffding bound. The other two are simple and intuitive propositions. The first proposition
says that if average error dP (c, A(T )) is high, the algorithm fails to -learn with high probability. The second proposition says that the best algorithm for predicting a bit based on some side
information, is to compute conditional expectation of the bit and thresholds it at 1/2.
Theorem 1 (Hoeffding bound). Let XP
1 , X2 , . . . , Xn be i.i.d. random variables that lie in interval
1
[a, b] with probability one and let p = n ni=1 E[Xi ]. Then, for any t â‰¥ 0,
" n
#
1X
2
2
Pr
Xi â‰¥ p + t â‰¤ eâˆ’2nt /(aâˆ’b) ,
n
i=1
#
" n
1X
2
2
Xi â‰¤ p âˆ’ t â‰¤ eâˆ’2nt /(aâˆ’b) .
Pr
n
i=1

Proposition 2 (Error probability vs. Expected error). Let Z be a random variable such that Z â‰¤ 1
with probability one. Then,
Pr[Z > t] â‰¥

E[Z] âˆ’ t
1âˆ’t

for any t âˆˆ [0, 1).
4

Proof. We have
E[Z] â‰¤ t Â· Pr[Z â‰¤ t] + 1 Â· Pr[Z > t] = t Â· (1 âˆ’ Pr[Z > t]) + Pr[Z > t] .
Solving for Pr[Z > t] finishes the proof.
Proposition 3 (Predicting Single Bit). Let U be a finite non-empty set. Let U, V be random
variables (possibly correlated) such that U âˆˆ U and V âˆˆ {0, 1} with probability one. Let f : U â†’
{0, 1} be a predictor. Then,

X 1
1
Pr [f (U ) 6= V ] â‰¥
âˆ’ âˆ’ E [V | U = u] Â· Pr[U = u] .
2
2
uâˆˆU

Proof. We have
Pr [f (U ) 6= V ] =

X

Pr [f (U ) 6= V | U = u] Â· Pr[U = u] .

uâˆˆU

It remains to show that
Pr [f (U ) 6= V | U = u] â‰¥

1
1
âˆ’ âˆ’ E [V | U = u] .
2
2

Since if U = u, the value f (U ) = f (u) is fixed, and hence
Pr [f (U ) 6= V | U = u] â‰¥ min {Pr [V = 1 | U = u] , Pr [V = 0 | U = u]}
= min {E [V | U = u] , 1 âˆ’ E [V | U = u]}
=

1
1
âˆ’ âˆ’ E [V | U = u]
2
2

We used the fact that min{x, 1 âˆ’ x} = 12 âˆ’
considering two cases: x â‰¥ 21 and x < 12 .

4

1
2

âˆ’ x for all x âˆˆ R which can be easily verified by

Projections

In this section, we denote by Cn the class of projections over the domain X = {0, 1}n . The class
Cn consists of n functions c1 , c2 , . . . , cn from {0, 1}n to {0, 1}. For any i âˆˆ {1, 2, . . . , n}, for any
x âˆˆ {0, 1}n , the function ci is defined as ci ((x[1], x[2], . . . , x[n])) = x[i].
For any  âˆˆ (0, 12 ) and n â‰¥ 2, we consider a family Pn, consisting of n probability distributions
P1 , P2 , . . . , Pn over the Boolean hypercube {0, 1}n . In order to describe the distribution Pi , for
some i, consider a random vector X = (X[1],
QnX[2], . . . , X[n]) drawn from Pi . Then distribution Pi
is a product distribution, i.e., Pr[X = x] = j=1 Pr[X[j] = x[j]] for any x âˆˆ {0, 1} . The marginal
distributions of the coordinates are
(
1
if j = i,
Pr[X[j] = 1] = 2
for j = 1, 2, . . . , n.
 if j 6= i,
The reader should think of  as a constant that does not depend on n, say,  =
The following result is folklore. We include its proof for completeness.
5

1
100 .

Proposition 4. Vapnik-Chervonenkis dimension of Cn is blog2 nc.
Proof. Let us denote the Vapnik-Chervonenkis dimension by d. Recall that d is the size of the
largest shattered set. Let S be any shattered set of size d. Then, there must be at least 2d distinct
functions in Cn . Hence, d â‰¤ log2 |Cn | = log2 n. Since d is an integer, we conclude that d â‰¤ blog2 nc.
On the other hand, we construct a shattered set of size blog2 nc. The set will consists of points
x1 , x2 , . . . , xblog2 nc âˆˆ {0, 1}n . For any i âˆˆ {1, 2, . . . , blog2 nc} and any j âˆˆ {0, 1, 2, . . . , n âˆ’ 1}, we
define xi [j] to be the i-th bit in the binary representation of the number j. (The bit at position
i = 1 is the least significant bit.) It is not hard to see that for any v âˆˆ {0, 1}blog2 nc , there exists
c âˆˆ Cn such that v = (c(x1 ), c(x2 ), . . . , c(xblog2 nc )). Indeed, given v, let k âˆˆ {0, 1, . . . , 2blog2 nc âˆ’ 1}
be the number with binary representation v, then we can take c = ck+1 .
Lemma 5 (Small cover). Let n â‰¥ 2 and  âˆˆ (0, 12 ). Any distribution in Pn, has 2-cover of size 2.
Proof. Consider a distribution Pi âˆˆ Pn, for some i âˆˆ {1, 2, . . . , n}. Let j be an arbitrary index in
{1, 2, . . . , n} \ {i}. Consider the projections ci , cj âˆˆ Cn . We claim that C 0 = {ci , cj } is a 2-cover of
Cn .
To see that C 0 is a 2-cover of Cn , consider any ck âˆˆ Cn . We need to show that dPi (ci , ck ) â‰¤ 2
or dPi (cj , ck ) â‰¤ 2. If k = i or k = j, the condition is trivially satisfied. Consider k âˆˆ {1, 2, . . . , n} \
{i, j}. Let X âˆ¼ Pi . Then,
dPi (cj , ck ) = Pr[cj (X) 6= ck (X)]
= Pr[cj (X) = 1 âˆ§ ck (X) = 0] + Pr[cj (X) = 0 âˆ§ ck (X) = 1]
= Pr[X[j] = 1 âˆ§ X[k] = 0] + Pr[X[j] = 0 âˆ§ X[k] = 1]
= Pr[X[j] = 1] Pr[X[k] = 0] + Pr[X[j] = 0] Pr[X[k] = 1]
= 2 (1 âˆ’ )
< 2 .

Using Benedek-Itai bound (Theorem 12 in Appendix B) we obtain the corollary below. The
corollary states that the distribution-dependent sample complexity of learning target in Cn under
any distribution from Pn, does not depend on n.
Corollary 6 (Learning with knowledge of the distribution). Let n â‰¥ 2 and  âˆˆ (0, 12 ). There exists
a distribution-dependent algorithm such that for any distribution from Pn, , any Î´ âˆˆ (0, 1), any
target function c âˆˆ Cn , if the algorithm gets
mâ‰¥

12 ln(2/Î´)


labeled examples, with probability at least 1 âˆ’ Î´, it 4-learns the target.
The next theorem states that without knowing the distribution, learning a target under a
distribution from Pn, requires at least â„¦(log n) labeled examples. It is important to note that  in
this bound is the parameter of the distribution, and not the accuracy of the PAC learning model.

6

Theorem 7 (Learning without knowledge of the distribution). For any distribution-independent
algorithm, any  âˆˆ (0, 14 ) and any n â‰¥ 600/3 there exists a distribution P âˆˆ Pn, and a target
concept c âˆˆ Cn such that if the algorithm gets
ln n
mâ‰¤
3 ln(1/)
labeled examples, it fails to

1
16 -learn

the target concept with probability more than

1
16 .

The main idea of the proof is the following. Assume that the learner is restricted to output some
function that belongs to Cn (i.e., the learner is proper ). Then with high probability, the number
of coordinates that coincide with the target on a random sample is â„¦(n), and, thus, the number
of projections that output the same value on each of the m random samples is â„¦(m n). Therefore,
with high probability, at least one other projection produces the exact same output as the target.
In this case, the learner has to choose randomly, and the probability of choosing a wrong answer is
ln n
at least 1/2. This implies that the learner must see at least m â‰¥ â„¦( ln(1/)
) samples. In the proof
below we make this intuition formal, and generalize it to the case of improper learners, too.
Proof of Theorem 7. Let A be any learning algorithm. For ease of notation, we formalize it is a
function
âˆž
[

n
A:
{0, 1}mÃ—n Ã— {0, 1}m â†’ {0, 1}{0,1} .
m=0

The algorithm receives an m Ã— n matrix and a binary vector of length m. The rows of the matrix
corresponds to unlabeled examples and the vector encodes the labels. The output of A is any
function from {0, 1}n â†’ {0, 1}.
We demonstrate the existence of a pair (P, c) âˆˆ Pn, Ã— Cn which cannot be learned with m
samples by the probabilistic method. Let I be chosen uniformly at random from {1, 2, . . . , n}. We
consider the distribution PI âˆˆ Pn, and target cI âˆˆ Cn . Let X1 , X2 , . . . , Xm be an i.i.d. sample
from PI and let Y1 = cI (X1 ), Y2 = cI (X2 ), . . . , Ym = cI (Xm ) be the target labels. Let X be the
m Ã— n matrix with entries Xi [j] and let Y = (Y1 , Y2 , . . . , Ym ) be the vector of labels. The output
of the algorithm is A(X, Y ). We will show that
1
E [dPI (cI , A(X, Y ))] â‰¥ .
(1)
8
This means that there exists i âˆˆ {1, 2, . . . , n} such that
1
E [dPi (ci , A(X, Y )) | I = i] â‰¥ .
8
By Proposition 2,


1
âˆ’ 1
1
1
Pr dPi (ci , A(X, Y )) >
I = i â‰¥ 8 16
>
.
1
16
16
1 âˆ’ 16
It remains to prove (1). Let Z be a test sample drawn from PI . That is, conditioned on I, the
sequence X1 , X2 , . . . , Xm , Z is i.i.d. drawn from PI . Then, by Proposition 3,
E [dPI (cI , A(X, Y ))] = Pr [A(X, Y )(Z) 6= cI (Z)]


X
1
1
â‰¥
âˆ’ âˆ’ E [cI (Z) | X = x, Y = y, Z = z] Â· Pr [X = x, Y = y, Z = z] . (2)
2
2
mÃ—n
xâˆˆ{0,1}
yâˆˆ{0,1}m
zâˆˆ{0,1}n

7

We need to compute E [cI (Z) | X = x, Y = y, Z = z]. For that we need some additional notation.
For any matrix x âˆˆ {0, 1}mÃ—n , let x[1], x[2], . . . , x[n] be its columns. For any matrix x âˆˆ {0, 1}mÃ—n
and vector y âˆˆ {0, 1}m let
k(x, y) = {i âˆˆ {1, 2, . . . , n} : x[i] = y}
be the set of indices of columns of x equal to the vector y. Also, we define kÂ·k to be the sum of
absolute values of entries of a vector or a matrix. (Since we use kÂ·k only for binary matrices and
binary vectors, it will be just the number of ones.)
For any i âˆˆ {1, 2, . . . , n},
(

1 1 m kxkâˆ’kyk

(1 âˆ’ )mnâˆ’kxk+kyk if i âˆˆ k(x, y),
n
2
Pr [I = i, X = x, Y = y] =
0
if i 6âˆˆ k(x, y).
Therefore, for any i âˆˆ {1, 2, . . . , n},
Pr [I = i, X = x, Y = y]
Pr [X = x, Y = y]
Pr [I = i, X = x, Y = y]
=P
jâˆˆk(x,y) Pr [I = j, X = x, Y = y]
(
1
if i âˆˆ k(x, y),
= |k(x,y)|
0
if i 6âˆˆ k(x, y).

Pr [I = i | X = x, Y = y] =

Conditioned on I, the variables Z and (X, Y ) are independent. Thus, for any x âˆˆ {0, 1}n , and
i = 1, 2, . . . , n,
Pr [Z = z | I = i, X = x, Y = y] = Pr [Z = z | I = i]
(
1 kzkâˆ’1

(1 âˆ’ )nâˆ’kzk
= 12 kzk
(1 âˆ’ )nâˆ’1âˆ’kzk
2

if z[i] = 1,
if z[i] = 0.

This allows us to compute the conditional probability
Pr [I = i, Z = z | X = x, Y = y]
= Pr [Z = z | I = i, X = x, Y = y] Â· Pr [I = i | X = x, Y = y]
ï£±
1
kzkâˆ’1 (1 âˆ’ )nâˆ’kzk if i âˆˆ k(x, y) and z[i] = 1,
ï£´
ï£´
ï£² 2|k(x,y)| 
1
= 2|k(x,y)|
kzk (1 âˆ’ )nâˆ’1âˆ’kzk if i âˆˆ k(x, y) and z[i] = 0,
ï£´
ï£´
ï£³0
if i 6âˆˆ k(x, y).
For any z âˆˆ {0, 1}n , let
s(x, y, z) = {i âˆˆ k(x, y) : z[i] = 1} ,

8

and note that s(x, y, z) âŠ† k(x, y). Then,
Pr [Z = z | X = x, Y = y]
n
X
=
Pr [Z = z, I = i | X = x, Y = y]
i=1

X

=

Pr [Z = z, I = i | X = x, Y = y]

iâˆˆk(x,y)

X

=

X

Pr [Z = z, I = i | X = x, Y = y] +

Pr [Z = z, I = i | X = x, Y = y]

iâˆˆk(x,y)\s(x,y,z)

iâˆˆs(x,y,x)

1
1
=
Â· |s(x, y, z)| Â· kzkâˆ’1 (1 âˆ’ )nâˆ’kzk +
Â· (|k(x, y)| âˆ’ |s(x, y, z)|) Â· kzk (1 âˆ’ )nâˆ’1âˆ’kzk
2|k(x, y)|
2|k(x, y)|
=

kzkâˆ’1 (1 âˆ’ )nâˆ’1âˆ’kzk
Â· (|s(x, y, z)| Â· (1 âˆ’ 2) + |k(x, y)| Â· ) .
2|k(x, y)|

Hence,
E [cI (Z) | X = x, Y = y, Z = z]
= Pr [Z[I] = 1 | X = x, Y = y, Z = z]
Pr [Z[I] = 1, Z = z | X = x, Y = y]
=
Pr [Z = z | X = x, Y = y]
n
X
Pr [I = i, Z[i] = 1, Z = z | X = x, Y = y]
=

i=1

Pr [Z = z | X = x, Y = y]
|s(x, y, z)| kzkâˆ’1
Â·
(1 âˆ’ )nâˆ’kzk
2|k(x, y)|
= kzkâˆ’1

(1 âˆ’ )nâˆ’1âˆ’kzk
Â· (|s(x, y, z)| Â· (1 âˆ’ 2) + |k(x, y, z)| Â· )
2|k(x, y)|
|s(x, y, z)| Â· (1 âˆ’ )
=
|s(x, y, z)| Â· (1 âˆ’ 2) + |k(x, y)| Â· 
1âˆ’
=
|k(x, y)| Â· 
1 âˆ’ 2 +
|s(x, y, z)|

We now show that the last expression is close to 1/2. It is easy to check that




|k(x, y)| Â· 
5
1âˆ’
1 3
âˆˆ
,2
=â‡’
âˆˆ
,
.
|k(x, y)| Â· 
|s(x, y, z)|
6
4 4
1 âˆ’ 2 +
|s(x, y, z)|
Indeed, since  âˆˆ (0, 14 ),
1âˆ’
1âˆ’
1 âˆ’ 1/4
1
â‰¥
â‰¥
=
|k(x, y)| Â· 
1 âˆ’ 2 + 2
1+2
4
1 âˆ’ 2 +
|s(x, y, z)|
9

and
1âˆ’
1âˆ’
1
3
â‰¤
â‰¤
= .
|k(x, y)| Â· 
1 âˆ’ 2 + 5/6
1 âˆ’ 1/2 + 5/6
4
1 âˆ’ 2 +
|s(x, y, z)|
We now substitute this into the (2). We have


X
1
1
âˆ’ âˆ’ E [cI (Z) | X = x, Y = y, Z = z] Â· Pr [X = x, Y = y, Z = z]
2
2
mÃ—n
xâˆˆ{0,1}
yâˆˆ{0,1}m
zâˆˆ{0,1}n

ï£«

ï£¶

ï£¬1
ï£¬ âˆ’ 1âˆ’
ï£­2
2
mÃ—n

ï£·
1âˆ’
ï£· Â· Pr [X = x, Y = y, Z = z]
|k(x, y)| Â·  ï£¸
1 âˆ’ 2 +
|s(x, y, z)|

ï£«

ï£¶

X

=

xâˆˆ{0,1}
yâˆˆ{0,1}m
zâˆˆ{0,1}n

X

â‰¥

xâˆˆ{0,1}mÃ—n
yâˆˆ{0,1}m
zâˆˆ{0,1}n
|k(x,y,z)|
âˆˆ[ 56 ,2]
|s(x,y,z)|

X

â‰¥

xâˆˆ{0,1}mÃ—n
yâˆˆ{0,1}m
zâˆˆ{0,1}n
|k(x,y,z)|
âˆˆ[ 56 ,2]
|s(x,y,z)|

ï£¬1
ï£¬ âˆ’ 1âˆ’
ï£­2
2



1 1
âˆ’
2 4

ï£·
1âˆ’
ï£· Â· Pr [X = x, Y = y, Z = z]
|k(x, y)| Â·  ï£¸
1 âˆ’ 2 +
|s(x, y, z)|


Â· Pr [X = x, Y = y, Z = z]




|k(X, Y )| Â· 
5
1
âˆˆ
,2 .
= Pr
4
|s(X, Y, Z)|
6
|k(X,Y )|Â·
In order to prove (1), we need to show that |s(X,Y,Z)|
âˆˆ
end, we define two additional random variables

The condition

|k(X,Y )|Â·
|s(X,Y,Z)|

K = |k(X, Y )|
and


âˆˆ 65 , 2 is equivalent to

5



6, 2

with probability at least 1/2. To that

S = |s(X, Y, Z)| .

1
S
6
â‰¤
â‰¤ .
2
K
5

(3)

First, we lower bound K. For any y âˆˆ {0, 1}m and any i, j âˆˆ {1, 2, . . . , n},
(
1
if j = i,
Pr [j âˆˆ k(X, Y ) | Y = y, I = i] = kyk
mâˆ’kyk
 (1 âˆ’ )
if j 6= j.
Conditioned on Y = y and I = i, the random variable K âˆ’ 1 = |k(X, Y ) \ {I}| is a sum of n âˆ’ 1
Bernoulli variables with parameter kyk (1 âˆ’ )mâˆ’kyk , one for each column except for column i.
10

Hoeffding bound with t = m /2 and the loose lower bound kyk (1 âˆ’ )mâˆ’kyk â‰¥ m gives




K âˆ’1
K âˆ’1
m
Pr
Y = y, I = i = Pr
>
> m âˆ’ t Y = y, I = i
nâˆ’1
2
nâˆ’1


K âˆ’1
kyk
mâˆ’kyk
>  (1 âˆ’ )
âˆ’ t Y = y, I = i
â‰¥ Pr
nâˆ’1
2

â‰¥ 1 âˆ’ eâˆ’2(nâˆ’1)t .
Since m â‰¤

ln n
3 ln(1/) ,

we lower bound t =

m
2

as

1 ln n
1
.
t = m /2 >  3 ln(1/) = âˆš
2
23n
Since the lower bound is uniform for all choices of y and i, we can remove the conditioning and
conclude that




(n âˆ’ 1)
(n âˆ’ 1)
âˆš
Pr K > 1 +
â‰¥ 1 âˆ’ exp âˆ’
.
23n
2n2/3
For n â‰¥ 25, we can simplify it further to
"
#
n2/3
3
Pr K â‰¥
â‰¥ .
2
4
Second, conditioned on K = r, the random variable S is a sum of r âˆ’ 1 Bernoulli random
variables with parameter  and one Bernoulli random variable with parameter 1/2. Hoeffding
bound for any t â‰¥ 0 gives that


(K âˆ’ 1) + 1/2
S
2
âˆ’
< t K = r â‰¥ 1 âˆ’ 2eâˆ’2rt .
Pr
K
K
Thus,
"

#
S
(K âˆ’ 1) + 1/2
n2/3
Pr
âˆ’
< t and K â‰¥
K
K
2


n
X
(K âˆ’ 1) + 1/2
S
Pr
âˆ’
â‰¥
< t K = r Â· Pr[K = r]
K
K
2/3
â‰¥

r=dn /2e
n

X

1 âˆ’ 2eâˆ’2rt

2



Â· Pr[K = r]

"

#

r=dn2/3 /2e



âˆ’n2/3 t2 /2

â‰¥ 1 âˆ’ 2e



n2/3
Â· Pr K â‰¥
2
2/3 2

.

We choose t = /4. Since n â‰¥ 600/3 , we have eâˆ’n t /2 < 81 and thus
"
#
"
#


S
(K âˆ’ 1) + 1/2
n2/3
n2/3
âˆ’n2/3 t2 /2
Pr
âˆ’
< t and K â‰¥
â‰¥ 1 âˆ’ 2e
Â· Pr K â‰¥
K
K
2
2

3
2/3 2
â‰¥
1 âˆ’ 2eâˆ’n t /2
4

1
3
1
9
>
1âˆ’
=
> .
4
4
16
2
11

We claim that t = /4,
S
K

âˆ’

(Kâˆ’1)+1/2
K

S
K

âˆ’

(Kâˆ’1)+1/2
K

< t and K â‰¥

n2/3
2

imply (3). To see that, note that

< t is equivalent to
(K âˆ’ 1) + 1/2
S
(K âˆ’ 1) + 1/2
âˆ’t<
<
+t
K
K
K

which implies that




S
1
1
1
âˆ’t<
+
p 1âˆ’
< 1âˆ’
+t.
K
K
K
2K
Since K â‰¥

n2/3
2

and n â‰¥ 25 we have K > 4, which implies that
S
3
1
3
âˆ’t<
< +
+t.
4
K
4
2K

Since K â‰¥

n2/3
2

and n â‰¥

12
3/2

we have K >

5
2 ,

which implies that

3
S
3

âˆ’t<
< + +t.
4
K
4
5
Since t = /4, the condition (3) follows.

5

All functions

Let X be some finite domain. We say a sample T = ((x1 , y1 ), . . . , (xm , ym )) âˆˆ (X Ã— {0, 1})m
of size m is self-consistent if for any i, j âˆˆ {1, 2, . . . , m}, xi = xj implies that yi = yj . A
distribution independent algorithm A is said to be consistent if for any self-consistent sample
T = ((x1 , y1 ), . . . , (xm , ym )) âˆˆ (X Ã— {0, 1})m , A(T )(xi ) = yi holds for any i = 1, 2, . . . , m.
In this section we show that for Call = {0, 1}X , any consistent distribution independent learner
is almost as powerful as any distribution independent learner. Note that, in particular, the ERM
algorithm for Call is consistent. In other words, for the class Call unlabeled data do not have any
information theoretic value.
Theorem 8 (No Gap). Let X be some finite domain, Call = {0, 1}X and A be any consistent
learning algorithm. Then, for any distribution P over X , any (possibly distribution dependent)
learning algorithm B and any , Î´ âˆˆ (0, 1),
m(A, Call , P, 2, 2Î´) â‰¤ m(B, Call , P, , Î´) .
Proof. Fix any integer m â‰¥ 0 and any distribution P over X . Let X, X1 , X2 , . . . , Xm be an i.i.d.
sample from P . Define the random variable
Z = Pr[X 6âˆˆ {X1 , X2 , . . . , Xm } | X1 , X2 , . . . , Xm ] .
In other words, Z is the probability mass not covered by X1 , X2 , . . . , Xm . For any c âˆˆ Call , let
Tc = ((X1 , c(X1 )), (X2 , c(X2 )), . . . , (Xm , c(Xm ))) be the sample labeled according to c. Since A is
consistent, with probability one, for any c âˆˆ Call ,
dP (A(Tc ), c) â‰¤ Z .
12

(4)

Let e
c be chosen uniformly at random from Call , independently of X, X1 , X2 , . . . , Xm . Additionally,
define b
c âˆˆ Call as
(
e
c(x)
if x âˆˆ {X1 , X2 , . . . , Xm },
b
c(x) =
1âˆ’e
c(x) otherwise.
and note that b
c and e
c are distributed identically and Tec = Tbc , and thus
E [1 [dP (B (Tec ) , e
c) â‰¥ ] | Tec ] = E [1 [dP (B (Tbc ) , b
c) â‰¥ ] | Tec ]

(5)

We have
sup Pr[dP (B(Tc ), c) â‰¥ ] = sup E [1 [dP (B (Tc ) , c) â‰¥ ]]
câˆˆCall

câˆˆCall

â‰¥ E [1 [dP (B (Tec ) , e
c) â‰¥ ]]
= E [E [1 [dP (B (Tec ) , e
c) â‰¥ ] | Tec ]]
 
1
1
= E E 1 [dP (B (Tec ) , e
c) â‰¥ ] + 1 [dP (B (Tbc ) , b
c) â‰¥ ]
2
2
 

1
â‰¥ E E 1 [Z â‰¥ 2] Tec
2
1
= E [1 [Z â‰¥ 2]]
2
1
= Pr [Z â‰¥ 2]
2
1
= sup Pr [Z â‰¥ 2]
2 câˆˆC
1
â‰¥ sup Pr [dP (A(Tc ), c) â‰¥ 2] .
2 câˆˆC


Tec

(6)
(7)

(8)

Equation (6) follows from (5). To justify inequality (7), note that since the classifiers e
c and b
c
disagree on the missing mass, if Z â‰¥ 2 then dP (B(Tec ), e
c) â‰¥  or dP (B(Tbc ), b
c) â‰¥  or both. By
symmetry between e
c and b
c, if Z â‰¥ 2 then with probability at least 1/2, dP (B(Tec ), e
c) â‰¥ . Inequality
(8) follows from (4).
Since the inequality
sup Pr[dP (B(Tc ), c) â‰¥ ] â‰¥
câˆˆCall

1
sup Pr [dP (A(Tc ), c) â‰¥ 2]
2 câˆˆC

holds for arbitrary m, it implies m(A, Call , P, 2, 2Î´) â‰¤ m(B, Call , P, , Î´) for any , Î´ âˆˆ (0, 1).

6

Conclusion and open problems

DarnstaÌˆdt et al. [2013] showed that the gap between the number of samples needed to learn a class
of functions of Vapnik-Chervonenkis dimension d with and without knowledge of the distribution
is upper-bounded by O(d). We show that this bound is tight for the class of Boolean projections.
On the other hand, for the class of all functions, this gap is only constant. These observations lead
to the following research directions.
13

First, it will be interesting to understand the value of the gap for larger classes of functions.
For example, one might consider the classes of (monotone) disjunctions over {0, 1}n , (monotone)
conjuctions over {0, 1}n , parities over {0, 1}n , and halfspaces over Rn . The Vapnik-Chervonenkis
dimension of these classes is Î˜(n) thus the gap for these classes is at least â„¦(1) and at most O(n).
Other than these crude bounds, the question of what is the gap for these classes is wide open.
Second, as the example with class of all functions shows, the gap is not characterized by the
Vapnik-Chervonenkis dimension. It will be interesting to study other parameters which determine
this gap. In particular, it will be interesting to obtain upper bounds on the gap in terms of other
quantities.
Finally, we believe that studying this question in the agnostic extension of the PAC model
[Anthony and Bartlett, 1999, Chapter 2] will be of great interest, too.

Acknowledgements
We thank the anonymous reviewers for their valuable comments.

References
Martin Anthony and Peter L. Bartlett. Neural Network Learning: Theoretical Foundations. Cambridge University Press, 1999.
Shai Ben-David, Tyler Lu, and DaÌvid PaÌl. Does unlabeled data provably help? Worst-case analysis of the sample complexity of semi-supervised learning. In Proceedings of the 21st Annual
Conference on Learning Theory, Helsinki, Finland, 9â€“12, July 2008, pages 33â€“44. Omnipress,
2008.
Gyora M. Benedek and Alon Itai. Learnability with respect to fixed distributions. Theoretical
Computer Science, 86(2):377â€“389, 1991.
Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K. Warmuth. Learnability
and the Vapnik-Chervonenkis dimension. Journal of the ACM (JACM), 36(4):929â€“965, 1989.
Malte DarnstaÌˆdt, Hans Ulrich Simon, and BalaÌzs SzoÌˆreÌnyi. Unlabeled data does provably help.
In 30th International Symposium on Theoretical Aspects of Computer Science, STACS 2013,
February 27 - March 2, 2013, Kiel, Germany, pages 185â€“196, 2013.
Luc Devroye and GaÌbor Lugosi. Combinatorial Methods in Density Estimation. Springer, 2000.
Richard M. Dudley. Central limit theorems for empirical measures. The Annals of Probability,
pages 899â€“929, 1978.
Richard M. Dudley. A course on empirical processes, pages 1â€“142. Springer, 1984.
Andrzej Ehrenfeucht, David Haussler, Michael Kearns, and Leslie Valiant. A general lower bound
on the number of examples needed for learning. Information and Computation, 82(3):247â€“261,
1989.

14

Steve Hanneke. The optimal sample complexity of PAC learning. Journal of Machine Learning
Research, 17(38):1â€“15, 2016.
David Haussler. Sphere packing numbers for subsets of the Boolean n-cube with bounded VapnikChervonenkis dimension. Journal of Combinatorial Theory, Series A, 69(2):217â€“232, 1995.
Wassily Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the
American Statistical Association, 58(301):13â€“30, 1963.
Tyler Lu. Fundamental limitations of semi-supervised learning. Masterâ€™s thesis, David R. Cheriton School of Computer Science, University of Waterloo, Waterloo, Ontario, Canada N2L
3G1, 2009. Available at https://uwspace.uwaterloo.ca/bitstream/handle/10012/4387/
lumastersthesis_electronic.pdf.

A

Size of -cover

In this section, we present (4e/)d/(1âˆ’1/e) upper bound on the size of the -cover of any concept
class of Vapnik-Chervonenkis dimension d. To prove our result, we need Sauerâ€™s lemma. Its proof
can be found, for example, in Anthony and Bartlett [1999, Chapter 3].
Lemma 9 (Sauerâ€™s lemma). Let X be a non-empty domain and let C âŠ† {0, 1}X be a concept class
with Vapnik-Chervonenkis dimension d. Then, for any S âŠ† X ,
|{Ï€(c, S) : c âˆˆ C}| â‰¤


d 
X
|S|
i=0

i

.

We remark that if n â‰¥ d â‰¥ 1 then
d  
X
n
i=0

i

â‰¤

 ne d
d

(9)

where e = 2.71828 . . . is the base of the natural logarithm. This follows from the following calculation
 d X
d  
d    i
X
n
d
d
n
Â·
â‰¤
i
i
n
n
i=0
i=0




n
X
n
d i
â‰¤
i
n
i=0

n
d
= 1+
â‰¤ ed
n
where we used in the last step that 1 + x â‰¤ ex for any x âˆˆ R.

15

Theorem 10 (Size of -cover). Let X be a non-empty domain and let C âŠ† {0, 1}X be a concept
class with Vapnik-Chervonenkis dimension d. Let P be any distribution over X . For any  âˆˆ (0, 1],
there exists a set C 0 âŠ† C such that


0

|C | â‰¤

4e


d/(1âˆ’1/e)
(10)

and for any c âˆˆ C there exists c0 âˆˆ C 0 such that dP (c, c0 ) â‰¤ .
Proof. We say that a set B âŠ† C is an -packing if
âˆ€c, c0 âˆˆ B

c 6= c0

=â‡’

dP (c, c0 ) > 

We claim that there exists a maximal -packing. In order to show that a maximal set exists we
to appeal to Zornâ€™s lemma. Consider the collection of all -packings. We impose partial order on
them by set inclusion.
Notice that any totally ordered collection
has
S
S {Bi : i âˆˆ I} of -packings
0
0
an upper bound iâˆˆI Bi that is an -packing. Indeed, if c, c âˆˆ iâˆˆI Bi such that c 6= c then there
exists i âˆˆ I such that c, c0 âˆˆ BiSsince {Bi : i âˆˆ I} is totally ordered. Since Bi is an -packing,
dP (c, c0 ) > . We conclude that iâˆˆI Bi is an -packing. By Zornâ€™s lemma, there exists a maximal
-packing.
Let C 0 be a maximal -packing. We claim that C 0 is also an -cover of C. Indeed, for any c âˆˆ C
there exists c0 âˆˆ C 0 such that dP (c, c0 ) â‰¤  since otherwise C 0 âˆª {c} would be an -packing, which
would contradict maximality of C 0 .
It remains to upper bound |C 0 |. Consider any finite subset C 00 âŠ† C 0 . It suffices to show an
upper bound on |C 00 | and since C 00 is arbitrary, the same upper bound holds for |C 0 |. Let M = |C 00 |
and let c1 , c2 , . . . , cM be concepts in C 00 . For any i, j âˆˆ {1, 2, . . . , M }, i < j, let
Ai,j = {x âˆˆ X : ci (x) 6= cj (x)} .
Let X1 , X2 , . . . , XK be an i.i.d. sample from P . We will choose K later. Since dP (ci , cj ) > ,
Pr[Xk âˆˆ Ai,j ] > 
Since there are

M
2



for k = 1, 2, . . . , K.

subsets Ai,j , we have
Pr [âˆ€i, j, i < j, âˆƒk, Xk âˆˆ Ai,j ]
= 1 âˆ’ Pr [âˆƒi, j, i < j, âˆ€k, Xk 6âˆˆ Ai,j ]
X
â‰¥1âˆ’
Pr [âˆ€k, Xk 6âˆˆ Ai,j ]
1â‰¤i<jâ‰¤M

â‰¥1âˆ’

X

(1 âˆ’ )K

1â‰¤i<jâ‰¤M

 
M
=1âˆ’
(1 âˆ’ )K .
2

For K =

ln (M
2)



+ 1, the above probability is strictly positive. This means there exists a set

S = {x1 , x2 , . . . , xK } âŠ† X such that Ai,j âˆ© S is non-empty for every i < j. This means that for
16

every for every i 6= j, ci (S) 6= cj (S) and hence M = |C 00 | = |{Ï€(c, S) : c âˆˆ C}|. Thus by Sauerâ€™s
lemma
d  
X
K
Mâ‰¤
.
i
i=0

We now show that this inequality implies that M â‰¤ (4e/)d/(1âˆ’1/e) . We consider several cases.
Case 1: d = âˆ’âˆž. That is, no set is shattered, and C = âˆ…. Then, M = 0 and inequality trivially
follows.
Case 2: d = 0. Then, M â‰¤ 1 and the inequality trivially follows.
Case 3a: d â‰¥ 1 and M â‰¤ ed . Clearly, M â‰¤ ed â‰¤ (4e/)d/(1âˆ’1/e) .
Case 3b: d â‰¥ 1 and M > ed . Then, K â‰¥ ln M â‰¥ d and hence by (9),


d  
X
K
Ke d
Mâ‰¤
â‰¤
.
i
d
i=0

Thus,



Ke
ln M â‰¤ d ln
d
ï£¶
ï£«  M 
ln ( 2 )
+1 ï£·

ï£¬e
ï£·
â‰¤ d ln ï£¬
ï£­
ï£¸
d
ï£¶
ï£«  M
ln ( 2 )
+2 ï£·

ï£¬e
ï£·
ï£¬
â‰¤ d ln ï£­
ï£¸
d
ï£¶
ï£«  M
ln ( 2 )+2

ï£¬e
ï£·
ï£·
â‰¤ d ln ï£¬
ï£­
ï£¸
d

â‰¤ d ln
â‰¤ d ln

e

2 ln M +2


!

d
e

4 ln M


!

d
  


4e
ln M
= d ln
+ ln

d
 
1
4e
â‰¤ d ln
+ ln M .

e
where in the last step we used that ln x â‰¤ x/e for any x > 0. Hence,
 
4e
(1 âˆ’ 1/e) ln M â‰¤ d ln

which implies the lemma.
17

B

Fixed distribution learning

Theorem 11 (Chernoffâ€“Hoeffding bound, Hoeffding [1963]). Let X1 , X2 , . . . , Xn be i.i.d. Bernoulli
random variables with E[Xi ] = p. Then, for any  âˆˆ [0, min{p, 1 âˆ’ p}),
" n
#
1X
Pr
Xi â‰¥ p +  â‰¤ eâˆ’nD(p+kp) ,
n
i=1
" n
#
1X
Pr
Xi â‰¤ p âˆ’  â‰¤ eâˆ’nD(pâˆ’kp) .
n
i=1

where



 
1âˆ’x
x
+ (1 âˆ’ x) ln
D (xky) = x ln
y
1âˆ’y

is the Kullback-Leibler divergence between Bernoulli distributions with parameters x, y âˆˆ [0, 1].
We further use the following inequality
D (xky) â‰¥

(x âˆ’ y)2
2 max{x, y}

Theorem 12 (Benedek-Itai). Let C âŠ† {0, 1}X be a concept class over a non-empty domain X .
Let P be a distribution over X . Let  âˆˆ (0, 1] and assume that C has an 2 -cover of size at most N .
Then, there exists an algorithm, such that for any Î´ âˆˆ (0, 1), any target c âˆˆ C, if it gets


ln N + ln(1/Î´)
m â‰¥ 48

labeled samples then with probability at least 1 âˆ’ Î´, it -learns the target.
Proof. Given a labeled sample T = ((x1 , y1 ), . . . , (xm , ym )), for any c âˆˆ C, we define
m

errT (c) =

1 X
1 [c(xi ) 6= yi ] .
m
i=1

Let C 0 âŠ† C be an (/2)-cover of size at most N . Consider the algorithm A that given a labeled
sample T outputs
b
c = argmin errT (c0 )
c0 âˆˆC 0

breaking ties arbitrarily. We prove that A, with probability at least 1 âˆ’ Î´, -learns any target c âˆˆ C
under the distribution P .
Consider any target c âˆˆ C. Then there exists e
c âˆˆ C 0 such that dP (c, e
c) â‰¤ /2. Let C 00 =
{c0 : dP (c, c0 ) > }. We claim that with probability at least 1 âˆ’ Î´, for all c0 âˆˆ C 00 , errT (c0 ) > 32 
and errT (e
c) < 23  and hence A outputs b
c âˆˆ C 0 \ C 00 .

18

Consider any c0 âˆˆ C 00 and note that errT (c0 ) is an average of Bernoulli random variables with
mean dP (c, c0 ) > . Thus, by Chernoff bound,





2
2
0
0
Pr errT (c ) >  > 1 âˆ’ exp âˆ’mD
 dP (c, c )
3
3
!
( 32  âˆ’ dP (c, c0 ))2
> 1 âˆ’ exp âˆ’m
2dP (c, c0 )
> 1 âˆ’ exp (âˆ’m/18)
where the last inequality follows from the inequality


2
âˆ’x
3

2

1
â‰¥ x
9

valid for any x â‰¥  > 0. Similarly, errT (e
c) is an average of Bernoulli random variables with mean
dP (c, e
c) < /2. Thus, by Chernoff bound,





2
2
Pr errT (e
c) <  > 1 âˆ’ exp âˆ’mD
 dP (c, e
c)
3
3
!
( 32  âˆ’ dP (c, e
c))2
> 1 âˆ’ exp âˆ’m
4
3
> 1 âˆ’ exp (âˆ’m/48) .
Since |C 00 | â‰¤ N âˆ’ 1, by union bound, with probability at least 1 âˆ’ (N âˆ’ 1) exp(âˆ’m/48), for all
c0 âˆˆ C 00 , errT (c0 ) > 23 . Finally, with probability at least 1 âˆ’ N exp(âˆ’m/48) â‰¥ 1 âˆ’ Î´, errT (e
c) < 32 
2
0
00
0
and for all c âˆˆ C , errT (c ) > 3 .

19

