Revisiting the Asymptotic Optimality of RRT*

arXiv:1909.09688v2 [cs.RO] 22 Apr 2020

Kiril Solovey1 , Lucas Janson2 , Edward Schmerling3 , Emilio Frazzoli4 , and Marco Pavone1

Abstractâ€” RRTâˆ— is one of the most widely used samplingbased algorithms for asymptotically-optimal motion planning.
RRTâˆ— laid the foundations for optimality in motion planning
as a whole, and inspired the development of numerous new
algorithms in the field, many of which build upon RRTâˆ— itself.
In this paper, we first identify a logical gap in the optimality
proof of RRTâˆ— , which was developed by Karaman and Frazzoli
(2011). Then, we present an alternative and mathematicallyrigorous proof for asymptotic optimality. Our proof suggests
that the connection radius used by RRTâˆ— should be increased


n 1/d
n 1/(d+1)
from Î³ log
to Î³ 0 log
in order to account
n
n
for the additional dimension of time that dictates the samplesâ€™
ordering. Here Î³, Î³ 0 are constants, and n, d are the number
of samples and the dimension of the problem, respectively.

I. I NTRODUCTION
For many robot motion-planning applications, feasibility
is not enoughâ€”we further desire path plans that are of high
quality, reflecting a need for robots that can achieve their
goals with efficiency, alacrity, and economy of motion. To
this end we seek planning algorithms that can be trusted,
whatever obstacle environment a robot faces, to produce
optimal or near-optimal plans with minimal scenario-specific
tuning. The advent of the asymptotically-optimal rapidlyexploring random tree (RRTâˆ— ) algorithm [1] has ushered in a
decade of theoretical and practical successes in the development of optimal sampling-based motion-planning algorithms.
Although proposed in its initial form for the case of
minimum-length path planning for robots without dynamic
constraints, RRTâˆ— has been extended to handle kinodynamic
planning problems [2] including robotic systems governed
by non-holonomic constraints [3], more expressive costs
accounting for robot energy expenditure [4], [5], and even to
plan paths that minimize violation of safety rules [6] or that
otherwise balance performance considerations with safety
constraints [7]. Heuristic modifications to the core algorithm
have also been demonstrated that improve practical RRTâˆ—
implementations [8], [9].
Each of these extensions leverages the simple yet powerful
iterative local graph-rewiring technique introduced by RRTâˆ—
to enable convergence to the optimal solution (as computation budget increases), provided an appropriate choice for the
scaling of the rewiring radius as a function of sample count.
Moreover, each of these extensions draws upon the original
analysis presented in [1] for the fundamental asymptotic
1 Department of Aeronautics and Astronautics, Stanford University, CA,
USA.
2 Department of Statistics, Harvard University, MA, USA.
3 Waymo Research, CA, USA.
4 Institute for Dynamic Systems and Control, ETH Zurich, Switzerland.

scaling of this algorithm parameter; this analysis is therefore
core to each of their optimality guarantees.
Contribution. The primary contribution of this paper is an
in-depth study of the theoretical analysis underpinning the
asymptotic-optimality criterion for the RRTâˆ— algorithm. In
revisiting this analysis, we identify a logical gap in the original proof and provide an amended proof suggesting a larger
radius scaling exponent to ensure asymptotic optimality. The
impact of this paper is potentially far-reaching in the large
number of works that currently appeal to RRTâˆ— optimality
to make their theoretical guarantees.
The paper is organized as follows. Section II provides
preliminaries and a description of RRTâˆ— . In Section III we
review the original optimality proof of RRTâˆ— and identify a
logical gap within it. In Section IV we provide the main
contribution of this paper, which is an alternative proof
that circumvents this logical gap. We conclude the paper in
Section V.
II. P RELIMINARIES
We provide several basic definitions that will be used
throughout the paper. Given two points x, y âˆˆ Rd , denote
by kx âˆ’ yk the standard Euclidean distance. Denote by
Br (x) the d-dimensional ballS of radius r > 0 centered at
x âˆˆ Rd . Define Br (Î“) := xâˆˆÎ“ Br (x) for any Î“ âŠ† Rd .
Similarly,
given a curve Ïƒ : [0, 1] â†’ Rd , define Br (Ïƒ) =
S
d
B
Ï„ âˆˆ[0,1] r (Ïƒ(Ï„ )). For a subset D âŠ‚ R , |D| denotes its
Lebesgue measure. All logarithms used herein are to base e.
A. Motion planning
Denote by C the robotâ€™s configuration space, and by F âŠ† C
the free space, i.e., the set of all collision free configurations.
We assume that C is a subset of the Euclidean space. For
simplicity, let C = [0, 1]d âŠ‚ Rd for some fixed d â‰¥ 2. Given
start and target configurations s, t âˆˆ F, the motion-planning
problem consists of finding a continuous path (curve) Ïƒ :
[0, 1] â†’ F such that Ïƒ(0) = s and Ïƒ(1) = t. That is,
the robot starts its motion along Ïƒ at s, and ends at t, while
avoiding collisions. An instance of the problem is defined by
(F, s, t). We consider the standard path length as a measure
of quality:
Definition 1. Given a path Ïƒ, its length (cost), which
corresponds to its Hausdorff measure, is represented by
n
X
c(Ïƒ) =
sup
kÏƒ(Ï„i ) âˆ’ Ïƒ(Ï„iâˆ’1 )k.
nâˆˆN+ ,0=Ï„1 â‰¤...â‰¤Ï„n =1 i=2

We proceed to describe the notion of robustness, which is
essential when discussing theoretical properties of samplingbased planners. Given a subset Î“ âŠ‚ C and two configurations

x, y âˆˆ Î“, denote by Î£Î“x,y the set of all continuous paths,
whose image is in Î“, that start in x and end in y, i.e., if
Ïƒ âˆˆ Î£Î“x,y then Ïƒ : [0, 1] â†’ Î“ and Ïƒ(0) = x, Ïƒ(1) = y.
We mention that the following definition is slightly different
than the one used in [1], [10].
Definition 2. Let (F, s, t) be a motion-planning problem.
A path Ïƒ âˆˆ Î£F
s,t is robust if there exists Î´ > 0 such that
BÎ´ (Ïƒ) âŠ‚ F. We also say that (F, s, t) is robustly feasible if
there exists such a robust path.
Definition 3. The robust optimum is defined as

câˆ— = inf c(Ïƒ) Ïƒ âˆˆ Î£F
s,t is robust .
B. Algorithms
While our main focus in this paper is the RRTâˆ— algorithm,
we also rely on the properties of the RRT algorithm, which is
described first. The following description of the (geometric)
RRT algorithm is based on [11] and [1].
Algorithm 1 RRT(xinit := s, xgoal := t, n, Î·)
1: V = {xinit }
2: for j = 1 to n do
3:
xrand â† SAMPLE - FREE( )
4:
xnear â† NEAREST(xrand , V )
5:
xnew â† STEER(xnear , xrand , Î·)
6:
if COLLISION - FREE(xnear , xnew ) then
7:
V = V âˆª {xnew }; E = E âˆª {(xnear , xnew )}
8: return G = (V, E)

The input for RRT (Algorithm 1) is an initial and goal
configurations xinit , xgoal , number of iterations n, and a
steering parameter Î· > 0. RRT constructs a tree G = (V, E)
by performing n iterations. In each iteration, a new sample
xrand is returned from F uniformly at random by calling
SAMPLE - FREE. Then, the vertex xnear âˆˆ V that is nearest
(according to k Â· k) to xrand is found using NEAREST. A new
configuration xnew âˆˆ X is then returned by STEER, such that
xnew is on the line segment between xnear and xrand , and the
distance kxnear âˆ’ xnew k is at most Î·. Finally, COLLISION FREE(xnear , xnew ) checks whether the straight-line path from
xnear to xnew is collision free. If so, xnew is added as a vertex
to G and is connected by an edge from xnear .
We proceed to describe RRTâˆ— [1] in Algorithm 2. Every RRTâˆ— iteration begins with an RRT-style extension.
The difference lies in the subsequent lines. First, RRTâˆ—
attempts to connect the tree to xnew from all its neighbors in V within a min{r(|V |), Î·} vicinity (lines 7-15).
Notice that the expression r(|V |) determines the radius
based on the current number of vertices in V . (The operation NEAR(xnew , V, min{r(|V |), Î·}) returns the subset
V âˆ© Bmin{r(|V |),Î·} (xnew ), i.e., the vertices that are within
a distance of min{r(|V |), Î·} from xnew .) However, it only
adds a single edge to xnew from the neighbor xmin âˆˆ Xnear
such that COST(xnew ) is minimized (line 16). In the next
step, RRTâˆ— attempts to perform rewires (lines 17-21): with
the addition of xnew , it may be beneficial to reroute the
existing path of xnear to use xnew . RRTâˆ— checks whether

changing the parent of xnear to be xnew reduces COST(xnear ).
(PARENT(xnear ) returns the immediate predecessor of xnear in
G. COST(x) for x âˆˆ V returns the cost of the path leading
from xinit to x in G.)
Algorithm 2 RRTâˆ— (xinit := s, xgoal := t, n, r, Î·)
1: V = {xinit }
2: for j = 1 to n do
3:
xrand â† SAMPLE - FREE( )
4:
xnear â† NEAREST(xrand , V )
5:
xnew â† STEER(xnear , xrand , Î·)
6:
if COLLISION - FREE(xnear , xnew ) then
7:
Xnear = NEAR(xnew , V, min{r(|V |), Î·})
8:
V = V âˆª {xnew }
9:
xmin = xnear
10:
cmin = COST(xnear ) + kxnew âˆ’ xnear k
11:
for xnear âˆˆ Xnear do
12:
if COLLISION - FREE(xnear , xnew ) then
13:
if COST(xnear ) + kxnew âˆ’ xnear k < cmin then
14:
xmin = xnear
15:
cmin = COST(xnear ) + kxnew âˆ’ xnear k

E = E âˆª {(xmin , xnew )}
for xnear âˆˆ Xnear do
if COLLISION - FREE(xnew , xnear ) then
if COST(xnew ) + kxnear âˆ’ xnew k < COST(xnear ) then
xparent = PARENT(xnear )
E = E âˆª {(xnew , xnear )} \ {(xparent , xnear )}
22: return G = (V, E)
16:
17:
18:
19:
20:
21:

Remark 1. As mentioned above, RRTâˆ— performs extensions
of the tree in a manner similar to RRT. That is, STEER
generates xnew , which lies on the straight line connecting
xnear , xrand , such that kxnew âˆ’ xnear k â‰¤ Î·. Note that initially
xnew 6= xrand , but once
S the space is sufficiently covered by
G, i.e., when F âŠ‚ vâˆˆV BÎ· (v), then in all the following
iterations it will hold that xnew = xrand . This property will
be important in the analysis of RRTâˆ— , as it indicates that
xnew is uniformly sampled from F. This notion will be
formalized below. For now, it is useful to note that given
the same sequence of samples, RRT and RRTâˆ— will generate
two (possibly distinct) graphs that have a common vertex set.
III. O RIGINAL OPTIMALITY PROOF
In this section we review the original proof [1] for
asymptotic optimality of RRTâˆ— , and point out a logical gap.
Specifically, Theorem 38 in [1] states that if the connection
radius used by RRTâˆ— is of the form

1/d
log n
KF
KF
r (n) = Î³
,
(1)
n
where n âˆˆ N+ , and for some constant Î³ KF > 0, the cost
of the solution obtained by RRTâˆ— converges to the robust
optimum câˆ— as n â†’ âˆ, almost surely.
A. Review of previous proof
We provide a sketch of the original proof and identify a
logical gap. We mention that our definitions of robustness
(Definition 2) and robust optimum (Definition 3) are simplified versions of the ones used originally in [1], where the

G
Xj i

0
Xji+1

0
Xji+2

s

Ïƒn
ÏƒÎµ

t

Bn,i Bn,i+1 B
n,i+2
Fig. 1. Illustration of the components in the original proof [1]. (a) The robustly-optimal path ÏƒÎµ is drawn as a black curve. (b) Discs represent the balls
Bn,1 , . . . , Bn,Mn , whose centers are denoted as red bullets along ÏƒÎµ . The path Ïƒn connecting samples between adjacent balls in an increasing order is
illustrated as a blue curve. (c) A problematic scenario (Section III-B), where the RRTâˆ— tree G yields a suboptimal solution, is depicted in green.

latter are slightly less convenient to work with (especially in
correction of the proof which we give in Section IV). We
thus adapt the original proof details presented in this section
to our setting. We emphasize that the logical gap is unrelated
to those definitions, and our argument presented below can
be easily remapped to the original formulation.
Recall that the sample set of RRTâˆ— consists of n timelabeled configurations. Denote by {X1 , . . . , Xn } the sample
set, where indices denote the order in which the samples
are drawn. Fix Îµ > 0 and let ÏƒÎµ be a robust solution
path such that c(ÏƒÎµ ) â‰¤ (1 + Îµ)câˆ— . The proof constructs a
sequence of Mn â‰¤ n identical balls Bn,1 , . . . , Bn,Mn that
are centered on some equally-spaced points along ÏƒÎµ . The
size and spacing of balls
is set so that (a) ÏƒÎµ is completely
SM
n
covered by them, (b) i=1
Bn,i âŠ† F, and (c) for every
0
1 â‰¤ i â‰¤ Mn , x âˆˆ Bn,i , x âˆˆ Bn,i+1 it holds that kx âˆ’ x0 k â‰¤
rKF (n) â‰¤ rKF (|V |). Furthermore, it is shown in [1] that given
xi âˆˆ Bn,i for every 1 â‰¤ i â‰¤ Mn , the length of the path Ïƒ
connecting each xi to the point in the next ball with a straight
line converges (as n â†’ âˆ) to the length of ÏƒÎµ (see Figure 1).
The proof establishes that if for every 1 â‰¤ i < Mn there
exist Xji , Xji+1 such that (i) Xji âˆˆ Bn,i , Xji+1 âˆˆ Bn,i+1
and (ii) ji < ji+1 , then RRTâˆ— is asymptotically optimal (see
Section G.3 in [1]). The rationale behind these conditions
is as follows. Condition (i) makes sure that the optimal
path is approximated by samples drawn by RRTâˆ— , i.e., for
every point along ÏƒÎµ there is a sample point in its vicinity.
Condition (ii) ensures that RRTâˆ— will have the opportunity to
add a directed edge from Xji to Xji+1 : as Xji+1 is sampled
after Xji then RRTâˆ— would consider drawing a directed
edge from the latter to the former, considering the fact that
Xji âˆˆ NEAR(Xji+1 , V, min{r(n), Î·}) (this is formalized in
Claim 1 below). Observe that r(n) is used as a conservative
lower-bound for r(|V |) throughout [1], as we do too.
Consequently, the proof deduces that if these conditions
are met RRTâˆ— is guaranteed to find a solution with cost at
most câˆ— (1 + Îµ) with probability that converges to 1 as n â†’
âˆ. In particular, denote by Xj1 , . . . , XjMn the sequence of
samples satisfying the conditions above, and let Ïƒn be a path
that is induced by those Mn samples in the prescribed order.
Then the claim is that the solution returned by RRTâˆ— is of
length c(Ïƒn ), if not shorter.

B. A logical gap
We identify an issue with the proof technique described
above, and in particular with the conditions (i) and (ii).
We assert that the line of reasoning mentioned above
overlooks the fact that the existence of pairwise sequential
samples does not directly imply the existence of a whole
chain of samples with a proper ordering such that a path
in G traces through all the balls in sequence. That is, the
fact that for every 1 â‰¤ i < Mn (i) there exist Xji , Xji+1
such that Xji âˆˆ Bn,i , Xji+1 âˆˆ Bn,i+1 and (ii) ji < ji+1 ,
does not necessarily mean that (iii) there exists a sequence
j1 â‰¤ j2 â‰¤ . . . â‰¤ jMn such that Xji âˆˆ Bn,i for every
1 â‰¤ i < Mn ; (iii) is a sufficient (but not necessary) condition
for recovering a path that is at least as good as Ïƒn .
Consider for instance the case where Xji âˆˆ Bn,i , Xji+1 âˆˆ
0
0
âˆˆ Bn,i+2 and ji <
âˆˆ Bn,i+1 , Xji+2
Bn,i+1 , Xji+1
0
0
0
ji+1 , ji+1
< ji+2
, but ji+2
< ji , where there are two points
0
Xji+1 , Xji+1
that fall into the same ball Bn,i+1 (see Figure 1
Siâˆ’1
(c)). Define X1ji âˆ’1 = {X1 , . . . , Xji âˆ’1 }, B1iâˆ’1 = k=1 Bn,k ,
and let X B = X1ji âˆ’1 âˆ© B1iâˆ’1 âˆ© Br(ji )KF (Xji ). Namely, X B
contains all the sampled points that were drawn before Xji ,
which lie in previous balls along ÏƒÎµ , and whose distance
from Xji is at most r(ji )KF .
Now assume that X B = âˆ…. We can choose the current
0
structure of G and the locations of Xji , Xji+1
such that
the only directed edge that is added in iteration ji is
0
0
(Xji+1
, Xji ), i.e., from Xji+1
to Xji (rather than the other
way around). Note that in iteration ji+1 the addition of
sample Xji+1 would not resolve this problematic wiring
since the latter sample will be connected by a directed edge
0
either from Xji or Xji+1
. Moreover, we can repeat this
argument for preceding balls to yield a long chain of samples
that are connected in the opposite direction.
In this discussion it is important to keep in mind that
RRTâˆ— performs rewiring (i.e., changing the predecessor of
a given vertex) only locally (lines 17-21 of Algorithm 2).
That is, in order to force a rewiring of a given vertex Xj
RRTâˆ— must sample a vertex xnew in the vicinity of Xj , and
this rewiring would not cause a chain of rewires for Xj s
predecessors or successors in G. Consequently, in order to
0
reverse the direction of the aforementioned chain from Xji+1
,
RRTâˆ— would need to sample new vertices along the chain

in the correct order. For a more detailed example see the
appendix.
As we show in our proof in the next section, condition
(iii) is in fact sufficient to guarantee asymptotic optimality,
and we prove that it indeed holds with high probability when
we slightly increase the connection radius from Equation (1),
and modify the constant Î³ KF .
IV. A LTERNATIVE PROOF
In order to account for the additional dimension of time,
1
 d+1

,
we set the connection radius to be r(n) = Î³ logn n
where Î³ is a constant that will be determined below. We
state our main theorem and provide an overview of the proof.
The full proof is presented later on. Note that our result
suggests that the exponent should be decreased from 1/d
to 1/(d + 1), which yields a larger radius overall. Denote
by Ïƒn the path connecting s to t returned by RRTâˆ— after n
iterations. Recall that c(Ïƒn ) denotes its length (in case that
no solution is found, the length of Ïƒn is assumed to be âˆ).
Our main theorem, which appears below, states that if Î³ is
set correctly, then the cost of the solution returned by RRTâˆ—
is upper-bounded asymptotically by (1 + Îµ)câˆ— , where câˆ— is
the robust optimum, and Îµ is a tuning parameter. Additional
tuning parameters that appear in the theorem are as follows:
Î· is the steering size of RRTâˆ— (Algorithm 2, line 5), while
Âµ and Î¸ are constants whose purpose will become clear in
the proof of the theorem.
Theorem 1. Suppose that (F, s, t) is robustly feasible and
fix Î· > 0, Îµ âˆˆ (0, 1),1 Î¸ âˆˆ (0, 1/4), and Âµ âˆˆ (0, 1). Define
the radius of RRTâˆ— to be
 1

log n d+1
,
(2)
r(n) = Î³
n
such that

Î³ â‰¥ (2 + Î¸)

|F|
(1 + Îµ/4)câˆ—
Â·
(d + 1)Î¸(1 âˆ’ Âµ) Î¶d

1
 d+1

,

(3)

where Î¶d is the volume of a unit d-dimensional hypersphere.
Then
lim Pr[c(Ïƒn ) â‰¤ (1 + Îµ)câˆ— ] = 1.
nâ†’âˆ

Our proof of Theorem 1 proceeds similarly to the proof
of the asymptotic optimality of FMTâˆ— [10] (which is in turn
based on [1]), but with additional complications due to the
time dimension and the coupling with the RRT algorithm.
We proceed to describe the main ingredients of the proof.
Fix the parameters Îµ âˆˆ (0, 1), Î¸ âˆˆ (0, 1/4), Âµ âˆˆ (0, 1), Î· >
0. Due to the fact that (F, s, t) is robustly feasible, there
exists a robust path ÏƒÎµ âˆˆ Î£F
s,t and Î´ > 0 such that c(ÏƒÎµ ) â‰¤
(1 + Îµ/4)câˆ— and BÎ´ (ÏƒÎµ ) âŠ‚ F. We will show that the RRTâˆ—
graph G contains a path that is in the vicinity of ÏƒÎµ , which
implies that the solution returned by RRTâˆ— is of cost at most
(1 + Îµ)câˆ— (which is slightly larger than (1 + Îµ/4)câˆ— due to
the fact that this is still an approximation of the path ÏƒÎµ ).
1 For simplicity, we upper-bound Îµ with 1 although the proof can be
adapted to accommodate larger stretch factors.

The first part of the proof deals with the technicality
involved with the samples produced by the algorithm. Denote
by V = {X1 . . . , Xn } the sequence of vertices produced
by RRTâˆ— , where Xj is equal to xnew generated in iteration
j. Due to the fact that RRTâˆ— (and RRT) perform steering
(line 5), samples are not distributed in a uniform manner,
as xrand is not necessarily identical to xnew (see Remark 1).
However, we do show that most of the vertices in V that
are in the vicinity of ÏƒÎµ are distributed uniformly at random,
with probability approaching 1 (see Lemma 1). This event
is denoted by E1 (see Definition 4).
Next, we proceed in a manner similar to other proofs of
asymptotic optimality (see, [1], [10], [13]), by defining a
sequence of points x1 , . . . , xMn along the path ÏƒÎµ and specifying a sequence of balls Bn,1 , . . . , Bn,Mn that are centered
on those points respectively, and whose radius
is 
proportional
 m
l
âˆ’1

,
to r(n). More formally, define Mn = c(ÏƒÎµ ) Â· r(n)
2+Î¸
and let x1 , . . . , xMn be a sequence of points along ÏƒÎµ such
that kxi âˆ’ xiâˆ’1 k â‰¤ Î¸r(n)
2+Î¸ , x1 = s, xMn = t. For every
1 â‰¤ i â‰¤ Mn define Bn,i := B r(n) (xi ).
2+Î¸
As suggested in Section III, we need to reason both about
the existence of samples inside those balls, and the order
of those samples. We assign to every ball Bn,i a specific
time window Ti , corresponding to allowed timestamps of
samples, and partition the sample set V = {X1 , . . . , Xn }
into the subsets V0 , V1 , . . . , VMn , where Xj âˆˆ Vi if j âˆˆ Ti . In
particular, T0 consists of the first n0 indices, where n0 = Âµn,
and every Ti , where i > 1 consists of (n âˆ’ n0 )/Mn indices,
and Âµ âˆˆ (0, 1) is a constant:
n
j
k
j
ko
0
0
nâˆ’n0
Ti6=0 = n0 + (i âˆ’ 1) Â· nâˆ’n
+
1,
.
.
.
,
n
+
i
Â·
.
Mn
Mn
We show that the event E2 (Definition 5) indicating that
every Bn,i contains a vertex from Vi occurs with probability
approaching 1 as well (Lemma 2). The motivation for this
event is the following claim, which indicates that edges
between points in consecutive balls are added if deemed
beneficial.
Claim 1. There exists n âˆˆ N+ large enough such that
the following holds with respect to Gji+1 = (Vji+1 , Eji+1 ):
Suppose that there exist Xji âˆˆ Vi âˆ© Bn,i , Xji+1 âˆˆ Vi+1 âˆ©
Bn,i+1 and denote by Gji+1 the RRTâˆ— graph at the end of
iteration ji+1 . Then in Gji +1 it follows that COST(Xji+1 ) â‰¤
COST (Xji ) + kXji âˆ’ Xji+1 k.
Proof. Recall that Bn,i = B r(n) (xi ) and kxi âˆ’ xi+1 k â‰¤
2+Î¸

Î¸r(n)
2+Î¸ .

For any x âˆˆ Bn,i , x0 âˆˆ Bn,i+1 it follows that
kx âˆ’ x0 k â‰¤

r(n)
2+Î¸

+

Î¸r(n)
2+Î¸

+

r(n)
2+Î¸

= r(n).

This implies that Xji âˆˆ Xnear = NEAR(Xji+1 , Vji , r(n)),
which will cause the execution of the test
COLLISION - FREE (Xji , Xji+1 ) (line 12 of RRTâˆ— ). The
latter will be evaluated to be true since BÎ´ (ÏƒÎµ ) âŠ† F and
r(n)  Î´ (for n large enough). Thus, in line 13 the edge
(Xji , Xji+1 ) will be added to the graph, unless there is a
lower-cost alternative for connection.

Thus, E2 guarantees that the RRTâˆ— tree G contains a path
connecting s to t that follows ÏƒÎµ closely. In order to
ensure that c(Ïƒn0 ) â‰¤ (1 + Îµ)câˆ— we need one more step, since
Ïƒn0 could stay close to ÏƒÎµ but zig-zag around it, resulting in
a high-cost solution.
Define the constants Î± âˆˆ (0, Î¸Îµ/16), Î² âˆˆ (0, Î¸Îµ/16).
Î²
Additionally, define for every 1 â‰¤ i â‰¤ Mn the ball Bn,i
:=
3
B Î²r(n) (xi ). The event E (Definition 6) indicates that a

the latter is the vertex set of RRTâˆ— after n0 iterations, and
1
assume that E0 n holds . Fix an iteration n0 < j < n and
some 1 â‰¤ k â‰¤ `. Due to the fact that Î· > 0 is fixed, by the
proof of Lemma 1 in [14] it follows that if xjrand âˆˆ BÎº (zj )
then xjnear âˆˆ B5Îº (zj ), and consequently

Î²
fraction of at most Î± of the smaller balls Bn,i
does not
3
contain samples from Vi . We show that E occurs with
probability approaching 1 (Lemma 3). We then proceed
to show that if E2 , E3 occur simultaneously then RRTâˆ— is
guaranteed to return a solution with cost at most (1 + Îµ)câˆ—
(Lemma 4).

This implies that xjnew = xjrand . Additionally, observe that
due to the fact that the straight-line path from xjnear to xjrand
is contained in BÎº (zj ), where Îº < Î´/5, it is also collision
free. Thus, at the end of iteration j, xrand will be added to
the RRTâˆ— graph as a vertex.

Ïƒn0

kxjrand âˆ’ xjnear k = kxjrand âˆ’ zj + zj âˆ’ xjnear k
â‰¤ kxjrand âˆ’ zj k + kzj âˆ’ xjnear k â‰¤ Îº + 5Îº â‰¤ Î·.

2+Î¸

A. Proof of Theorem 1
We start with a formal definition of E1 :
xjrand , xjnew

Definition 4. For every 1 â‰¤ j â‰¤ n denote by
the
random and new samples of RRTâˆ— in iteration j (line 3 and
line 5 in Algorithm 2, respectively). Define n0 := Âµn and
E1n := {âˆ€1 â‰¤ i â‰¤ Mn , n0 â‰¤ j â‰¤ n :
if xjrand âˆˆ Bn,i then xjrand = xjnew }.

We will prove that the following event E2 holds with
probability approaching 1 by conditioning on E1 .
Definition 5. E2n represents the event that every Bn,i contains at least one vertex from Vi . That is,
E2n := {âˆ€1 â‰¤ i â‰¤ Mn , Vi âˆ© Bn,i 6= âˆ…}.
Lemma 2. limnâ†’âˆ Pr[E2n ] = 1.
Proof. Observe that

That is, E1n is the event that all xjrand âˆˆ Bn,i for j between
n0 and n satisfy xjrand = xjnew .

Pr[E2n ] = Pr[E2n |E1n ] Â· Pr[E1n ] + Pr[E2n |E1n ] Â· Pr[E1n ]

Remark 2. We wish to stress that the following lemma,
which lower bounds the probability of E1n , is a key ingredient
in our proof. As we shall see below, this would allow us
to treat some of the vertices added by RRTâˆ— as uniformly
sampled, which is not true for all samples, as some are
perturbed by the STEER operation. We mention that this
issue was not addressed in the original proof in [1], where
the RRTâˆ— nodes were assumed (incorrectly) to be uniformly
distributed. Furthermore, setting the steering step Î· = âˆ
does not resolve this issue.

We shall lower-bound the expression Pr[E2n |E1n ]. By definition of E1n , for every n0 < j â‰¤ n, and i such that j âˆˆ Ti , if
xjrand âˆˆ Bn,i , then xjnew = xjrand is a valid vertex of the RRTâˆ—
graph. Thus, by conditioning on E1n we can treat V \ V0 as
uniform random samples from F. This will come in handy
in bounding the probability of E2 :

Lemma 1. There exist two constants a, b > 0 such that
Pr[E1n ] â‰¥ 1 âˆ’ a Â· eâˆ’bn .
Proof. A similar proof appears in [13, Claim 6], albeit for a
different type of sampling scheme and in the context of an
RRG analysis. The main challenge here is to show that while
it is not true that all the new samples xnew are distributed
uniformly randomly (due to lines 4,5 in Algorithm 2), most
of them are. Define Îº := min{Î·, Î´}/10 and set z1 , . . . , z`
to be a sequence of points placed along ÏƒÎµ , such that ` =
c(ÏƒÎµ )/Îº, and kzk âˆ’ S
zk+1 k â‰¤ Îº. S
Observe that for n large
Mn
`
enough it holds that i=1
Bn,i âŠ‚ k=1 BÎº (zk ).
Denote by VnRRT
the vertex set of RRT after n0 iterations.
0
Theorem 1 in [14] states that there exist constants a, c > 0
such that the probability that for every 1 â‰¤ k â‰¤ ` it holds
0
that VnRRT
âˆ© BÎº (zk ) 6= âˆ… is at least a Â· eâˆ’cn = a Â· eâˆ’bn , where
0
b := cÂµ. Notice that this theorem requires Î· to be fixed (i.e.,
independent of n) and strictly positive.
1
Denote the latter event to be E0 n . Next, we show that
âˆ—
01
1
E n implies En . First, observe that VnRRT
= VnRRT
, where
0
0

â‰¥ Pr[E2n |E1n ] Â· Pr[E1n ].

Pr[E2n |E1n ] = Pr [âˆƒ1 â‰¤ i â‰¤ Mn , Vi âˆ© Bn,i = âˆ…]
|Ti |
Mn 
Mn
X
X
|Bn,i |
1âˆ’
Pr[Vi âˆ© Bn,i = âˆ…] =
â‰¤
|F|
i=1
i=1

d !(nâˆ’n0 )/Mn
â‰¤ Mn

1âˆ’

Î¶d

r(n)
2+Î¸

|F |



r(n)d
n âˆ’ n0 Î¶d
â‰¤ Mn exp âˆ’
Â·
Â·
Mn
|F| (2 + Î¸)d


r(n)d
nr(n)Î¸(1 âˆ’ Âµ) Î¶d
â‰¤ Mn exp âˆ’
Â·
Â·
c(ÏƒÎµ )(2 + Î¸) |F| (2 + Î¸)d


Î¸Î¶d (1 âˆ’ Âµ)
d+1
= Mn exp âˆ’
Â·
n
Â·
r(n)
c(ÏƒÎµ )(2 + Î¸)d+1 |F|


log n
=: Mn exp âˆ’Î¾ Â· n Â· Î³ d+1
n


âˆ’1 
n
o
= c(ÏƒÎµ ) Â· r(n)
exp âˆ’Î¾Î³ d+1 log n
2+Î¸



âˆ’1
n
o
< c(ÏƒÎµ ) Â· r(n)
+
1
exp âˆ’Î¾Î³ d+1 log n
2+Î¸
=

(4)

(5)

(6)

d+1
c(ÏƒÎµ )(2+Î¸)
(log n)âˆ’1/(d+1) n1/(d+1)âˆ’Î¾Î³
Î¸Î³

n
o
+ exp âˆ’Î¾Î³ d+1 log n ,

(7)

where (4) is due to the union bound and the fact that Vi
is uniformly sampled at random from F, (5) is due to the

inequality 1 âˆ’ x â‰¤ eâˆ’x for x âˆˆ (0, 1) which applies here
d (1âˆ’Âµ)
for n large enough, and (6) defines Î¾ := c(ÏƒÎµÎ¸Î¶
. If
)(2+Î¸)d+1 |F |
âˆ’1
d+1
(d + 1) âˆ’ Î¾Î³
â‰¤ 0 then the final expression tends to 0.
Indeed,
Î¸Î¶d (1 âˆ’ Âµ)
1
âˆ’
Â· Î³ d+1 â‰¤ 0 â‡â‡’
d+1
c(ÏƒÎµ )(2 + Î¸)d+1 |F|

 1
 âˆ—
 1
d+1
d+1
c(ÏƒÎµ )|F |
c (1+Îµ/4)|F |
(2 + Î¸) (d+1)Î¸Î¶
â‰¤
(2
+
Î¸)
â‰¤ Î³.
(1âˆ’Âµ)
(d+1)Î¸Î¶
(1âˆ’Âµ)
d
d

It remains to show that limnâ†’âˆ Pr[E2n |E1n ] Â· Pr[E1n ] = 1:
Pr[E2n |E1n ] Â· Pr[E1n ] = (1 âˆ’ Pr[E2n |E1n ])(1 âˆ’ Pr[E1n ])
= 1 + Pr[E2n |E1n ] Â· Pr[E1n ] âˆ’ Pr[E2n |E1n ] âˆ’ Pr[E1n ]
> 1 âˆ’ Pr[E2n |E1n ] âˆ’ Pr[E1n ],
where the final expression converges to 1, according to
Equation 7 and Lemma 1.
Next we consider the existence of samples in a collection
of smaller balls.
Î²
Definition 6. Let KnÎ² := |{i âˆˆ {1, . . . , Mn } : Bn,i
âˆ© Vi = âˆ…}|.
3
Î²
En := {Kn â‰¤ Î±Mn } is the event that at most Î±Mn of the
Î²
smaller balls Bn,i
do not contain any samples from Vi .

Lemma 3. limnâ†’âˆ Pr[E3n ] = 1.
Proof. Similarly to Lemma 2, it is sufficient to show that
limnâ†’âˆ Pr[E3n |E1n ] = 0. We shall upper bound the probability that KnÎ² > Î±Mn assuming that E1n holds. To this
end, we compute the expectation of KnÎ² and apply Markovâ€™s
inequality.
For every 1 â‰¤ i â‰¤ Mn , denote by Ii the indicatorP
variable
Mn
Î²
for the event that Bn,i
âˆ©Vi = âˆ…. Observe that KnÎ² = i=1
Ii .
For n large enough we have that

E[Ii ] = Pr[Ii = 1] =
â‰¤

1âˆ’

Î² d Î¶d



1âˆ’

r(n)
2+Î¸

Î²
|Bn,i |

|Ti |

|F |

Lemma 4. For n large enough, if the events E2n , E3n occur,
then c(Ïƒn ) â‰¤ (1 + Îµ)câˆ— .
Proof. As E2n âˆ§ E3n we may define the sequence of vertices
Xj1 , . . . , XjMn âˆˆ V , such that Xj1 = s, XjMn = t, and for
Î²
Î²
every 1 < i < Mn , Xji âˆˆ Vi âˆ© Bn,i
if Vi âˆ© Bn,i
6= âˆ…, and
Xji âˆˆ Vi âˆ© Bn,i otherwise.
Denote by Ïƒn0 the path induced by concatenating those
points, and notice that it is collision free by definition of
Bn,i and ÏƒÎµ . Next, we claim that the cost of the path Ïƒn
âˆ—
0
obtained by RRT
PMn is upper-bounded by the cost of Ïƒn , which
is equal to
i=2 kXji âˆ’ Xjiâˆ’1 k. Consider iteration ji of
i
= Xji ,
RRTâˆ— , for 1 < i â‰¤ Mn and observe that (i) xjnew
ji
(ii) Xjiâˆ’1 âˆˆ Xnear . By Claim 1, it follows that COST(Xji ) â‰¤
Pi
0
k=2 kXj(k) âˆ’ Xj(kâˆ’1) k, as desired. Thus, c(Ïƒn ) â‰¤ c(Ïƒn ).
0
We proceed to bound c(Ïƒn ). Observe that for any 1 < i â‰¤
Mn it holds that kXji âˆ’ Xjiâˆ’1 k is at most
ï£± Î¸r(n)
ï£´
ï£² 2+Î¸ +
Î¸r(n)
+
2+Î¸
ï£´
ï£³ Î¸r(n) +
2+Î¸

Î²r(n)
+ Î²r(n)
,
2+Î¸
2+Î¸
Î²r(n)
r(n)
+
,
2+Î¸
2+Î¸
r(n)
+ r(n)
,
2+Î¸
2+Î¸

Î²
Î²
AND Xji âˆˆ Bn,i
if Xjiâˆ’1 âˆˆ Bn,iâˆ’1
Î²
Î²
if Xjiâˆ’1 âˆˆ Bn,iâˆ’1 XOR Xji âˆˆ Bn,i .
otherwise

Thus,
c(Ïƒn0 ) â‰¤

Mn
X

kXji âˆ’ Xjiâˆ’1 k

i=2

â‰¤ (Mn âˆ’ 1) Î¸r(n)
+ d(1 âˆ’ Î±)(Mn âˆ’ 1)e 2Î²r(n)
2+Î¸
2+Î¸
Î¸ + 2Î² + 2Î±
2r(n)
+ bÎ±(Mn âˆ’ 1)c 2+Î¸ â‰¤ (Mn âˆ’ 1)r(n)
2+Î¸
 âˆ— Î¸ + 2Î² + 2Î±
c(ÏƒÎµ )(2 + Î¸)
Î¸ + 2Î² + 2Î±
Îµ
â‰¤
r(n)
â‰¤ 1+ 4 c Â·
Î¸r(n)
2+Î¸
Î¸
2Î¸Îµ
2Î¸Îµ


Î¸
+
+
2
16
16
= 1 + 4Îµ câˆ—
< 1 + 4Îµ câˆ—
Î¸


 âˆ—
2
Îµ
= 1 + 2Îµ + Îµ16 câˆ— < 1 + 2Îµ + 16
c < (1 + Îµ)câˆ— .

It remains to show that E2 âˆ§ E3 occurs with probability
approaching 1:
lim Pr[E2 âˆ§ E3 ] = 1 âˆ’ lim Pr[E2 âˆ¨ E3 ]
nâ†’âˆ


â‰¥ 1 âˆ’ lim Pr[E2 ] + Pr[E3 ] = 1.

d !n(1âˆ’Âµ)/Mn

nâ†’âˆ

|F |

nâ†’âˆ

n
o
d
d+1
Î¸Î¶d (1âˆ’Âµ)
â‰¤ exp âˆ’ c(ÏƒÎ² )(2+Î¸)
d+1 |F | Â· n Â· r(n)
Îµ
n
o
d
d+1
Î¸Î¶d (1âˆ’Âµ)
â‰¤ exp âˆ’ c(ÏƒÎ² )(2+Î¸)
Â· log n
d+1 |F | Â· Î³
Îµ
n
o
d
Î²d
= exp âˆ’ d+1 log n = nâˆ’Î² /(d+1) .

PMn
d
Thus, E[KnÎ² ] = i=1
E[Ii ] â‰¤ Mn nâˆ’Î² /(d+1) . By Markovâ€™s
inequality, it follows that
Pr[KnÎ² > Î±Mn ] â‰¤

Î²
E[Kn
]
Î±Mn

d

â‰¤

Mn nâˆ’Î² /(d+1)
Î±Mn

=

d
nâˆ’Î² /(d+1)
.
Î±

(8)

As Î± is fixed, the last expression tends to 0 as n tends to
âˆ. While the upper bound obtained in (8) is sufficient for
our purpose, we mention that a tighter bound can be derived
by using a slightly more complex Poissonization argument
similar to that used in [10].
Next, we show that if E2 , E3 occur simultaneously, then
the cost of c(Ïƒn ) is bounded by (1 + Îµ)câˆ— .

V. C ONCLUSION
In this paper we revisited the original asymptoticoptimality proof of RRTâˆ— in [1], and discussed an apparent
logical gap within it. We then introduced an alternative proof
that amends this logical gap. Our new proof suggests that
the connection radius of RRTâˆ— should be slightly larger than
the original bound on the radius that was developed in [1].
We leave the question of whether our bound is tight, i.e.,
whether the exponent of 1/(d + 1) in Equation (2) can be
lowered to 1/d, to future research. The practical successes
of the algorithm and its extensions, using the exponent 1/d,
provide some evidence that this might be the case.
ACKNOWLEDGMENTS
We thank Sertac Karaman for insightful discussions on his
work [1]. We also thank Michal Kleinbort for feedback on
the manuscript. This work was supported in part by NSF,
Award Number: 1931815.

A PPENDIX
We provide a detailed counter example (Figures 2-12)
illustrating our argument that the fact that for every 1 â‰¤ i <
Mn (i) there exist Xji , Xji+1 such that Xji âˆˆ Bn,i , Xji+1 âˆˆ
Bn,i+1 and (ii) ji < ji+1 , does not necessarily mean that
(iii) there exists a sequence j1 â‰¤ j2 â‰¤ . . . â‰¤ jMn such that
Xji âˆˆ Bn,i for every 1 â‰¤ i < Mn (see Section III-B).
R EFERENCES
[1] S. Karaman and E. Frazzoli, â€œSampling-based algorithms for optimal
motion planning,â€ International Journal of Robotics Research, vol. 30,
no. 7, pp. 846â€“894, 2011. 1, 2, 3, 4, 5, 6
[2] â€”â€”, â€œOptimal kinodynamic motion planning using incremental
sampling-based methods,â€ in IEEE Conference on Decision and Control, 2010, pp. 7681â€“7687. 1
[3] â€”â€”, â€œSampling-based optimal motion planning for non-holonomic
dynamical systems,â€ in IEEE International Conference on Robotics
and Automation, 2013, pp. 5041â€“5047. 1
[4] G. Goretkin, A. Perez, R. Platt, and G. Konidaris, â€œOptimal samplingbased planning for linear-quadratic kinodynamic systems,â€ in IEEE
International Conference on Robotics and Automation, 2013, pp.
2429â€“2436. 1
[5] D. J. Webb and J. van den Berg, â€œKinodynamic RRT*: Optimal
motion planning for systems with linear differential constraints,â€ in
IEEE International Conference on Robotics and Automation, 2013,
pp. 5054â€“5061. 1
[6] L. I. Reyes Castro, P. Chaudhari, J. Tumova, S. Karaman, E. Frazzoli,
and D. Rus, â€œIncremental sampling-based algorithm for minimumviolation motion planning,â€ in IEEE Conference on Decision and
Control, 2013, pp. 3217â€“3224. 1
[7] W. Liu and M. H. Ang, Jr., â€œIncremental sampling-based algorithm for
risk-aware planning under motion uncertainty,â€ in IEEE International
Conference on Robotics and Automation, 2014, pp. 2051â€“2058. 1
[8] B. Akgun and M. Stilman, â€œSampling heuristics for optimal motion
planning in high dimensions,â€ in IEEE/RSJ International Conference
on Intelligent Robots and Systems, 2011, pp. 2640â€“2645. 1
[9] J. D. Gammell, S. S. Srinivasa, and T. D. Barfoot, â€œInformed RRT*:
Optimal sampling-based path planning focused via direct sampling
of an admissible ellipsoidal heuristic,â€ in IEEE/RSJ International
Conference on Intelligent Robots and Systems, 2014, pp. 2997â€“3004.
1
[10] L. Janson, E. Schmerling, A. A. Clark, and M. Pavone, â€œFast marching
tree: A fast marching sampling-based method for optimal motion
planning in many dimensions,â€ International Journal of Robotics
Research, vol. 34, no. 7, pp. 883â€“921, 2015. 2, 4, 6
[11] J. J. Kuffner and S. M. LaValle, â€œRRT-Connect: An efficient approach
to single-query path planning,â€ in IEEE International Conference on
Robotics and Automation, 2000, pp. 995â€“1001. 2
[12] K. Solovey, L. Janson, E. Schmerling, E. Frazzoli, and M. Pavone,
â€œRevisiting the asymptotic optimality of RRT,â€ CoRR, vol.
abs/1909.09688, 2019.
[13] K. Solovey and M. Kleinbort, â€œThe critical radius in sampling-based
motion planning,â€ International Journal of Robotics Reseasrch, 2019.
4, 5
[14] M. Kleinbort, K. Solovey, Z. Littlefield, K. E. Bekris, and D. Halperin,
â€œProbabilistic completeness of RRT for geometric and kinodynamic
planning with forward propagation,â€ IEEE Robotics and Automation
Letters, 2019. 5

s

t

Fig. 2. The input scenario for the counter example. The goal is to find a path from configuration s on the left to t on the right, while avoiding the two
gray obstacles. The path ÏƒÎµ is drawn as a black curve.

X1

s

X2

t= X3

Fig. 3. The first three samples X1 , X2 , X3 are drawn by the algorithm, where X3 = t. The edge (s, X1 ) is added first through line 5 of Algorithm 2.
The edges (X1 , X2 ), (X2 , X3 ) are added in a similar fashion. We assume that no rewiring occurs in those steps due to the smaller magnitude of r1
in comparison to kX1 âˆ’ X3 k. We also mention that the length of the new path to t just discovered can be made arbitrarily long with respect to ÏƒÎµ by
moving X1 , X2 further away from s and t respectively, and setting the steering parameter Î· to be large enough to support such long connections.

X1

X2

b8

b1

b9

b10

b11

s

t= X3

b2

b12

b7

b3

b4

b5

b6

Fig. 4. We construct a sequence of Mn balls Bn,1 , . . . , Bn,Mn , which we denote for simplicity by b1 , . . . , bMn , and we fix n = 23. For simplicity,
we set Mn = 12 in the illustration, to avoid unnecessarily complicating the visualization. Below we also assume that the connection radius r23 used by
RRTâˆ— is equal to the diameter of any ball bi , although a similar out come will follow when r23 is much larger (as long as r23 < kX2 âˆ’ X3 k).

X1

X2

b8

b1

b9

b10

b11

s

t= X3

b2

b7

b12

X4

b3

b4

b5

b6

Fig. 5. Next, we generate the sample X4 âˆˆ b11 , which introduces the edge (X3 , X4 ) and does not result in rewiring. As we mentioned earlier, we
assume that rn < kX2 âˆ’ X3 k, which implies that the edge (X2 , X4 ) will not be considered.

X1

X2

b8

b1

b9

b10

b11

s

t= X3

b2

X4

b7

b12

X5

b3

b4

b5

b6

Fig. 6. Next, we generate the sample X5 âˆˆ b12 , which introduces the edge (X4 , X5 ), since the cost-to-come via X3 is smaller than through a
connection from X4 . Clearly, the edge (X5 , X4 ) cannot improve the cost-to-come of X4 , and it is therefore not added in the rewiring stage. Observe that
X4 âˆˆ b11 , X5 âˆˆ b12 , and X4 was sampled before X5 , which implies that conditions (i), (ii) are satisfied locally for b11 , b12 .

X1

X2

b8

b1

b9

b10

b11

s

t= X3

b2

b7

X6

X4

b12

X5

b3

b4
Fig. 7.

b5

b6

Similarly to X4 , the sample X6 is produced in b10 , which yields the edge (X4 , X6 ), and introduces no rewiring.

X1

X2

b8

b1

b9

b10

b11

s

t= X3

b2

b7

X6

b12

X4 X7 X
5

b3

b4
Fig. 8.

b5

b6

Similarly to X5 , the sample X7 is produced in b11 , and the edge (X4 , X6 ) is left intact.

X1

X2

b8

b1

b9

b10

s

b11

t= X3

b12

X9
b2

X8

b7

X6

X4 X7 X
5

b3

b4

b5

b6

Fig. 9. In a similar manner, we introduce incrementally the samples X8 âˆˆ b9 , X9 âˆˆ b10 . Notice that a path from t in the opposite direction of the balls
towards s (currently till X8 ) is beginning to form.

X1

X2

b8

b9

b10

b11

X11

s

b1

t= X3
X13

b2
X22

X10
X15 X8 b
7
X12

b3
X20

X18

X19
X21
X16
b4
b5

b12

X9
X6

X4 X7 X
5

X17

X14
b6

Fig. 10. This sample scheme can be repeated until the sample X22 âˆˆ b2 is produced, which is within r23 distance from s. This introduces the edge
(s, X22 ), which minimizes the cost-to-come to X22 .

X1

X2

b8

b9

b10

b11

X11

s

b1

t= X3
X13

b2
X22

X10
X15 X8 b
7
X12

b3
X20

X18

X19
X21
X16
b4
b5

b12

X9

X14

X6

X4 X7 X
5

X17

b6

Fig. 11. The introduction of X22 forces a rewire of X20 and X18 within the same iteration: the edges (X22 , X20 ), (X22 , X18 ) are added, whereas
(X18 , X20 ) and (X16 , X18 ) are removed. It is important to note that by definition of RRTâˆ— , this rewire does not promote further rewiring to the
predecessors of X18 , X20 in G.

X1

X2

b8

b9

b10

b11

X11

s

b1

t= X3
X13

b2
X22

X23

X10
X15 X8 b
7
X12

b3
X20

X18

X19
X21
X16
b4
b5

b12

X9

X14

X6

X4 X7 X
5

X17

b6

Fig. 12. Finally, the sample X23 âˆˆ b3 is drawn, and the edge (s, X23 ) is added. This will promote additional rewires to X16 , X19 , X18 , X21 ,in the
vicinity of X23 , although as earlier those rewires will not propagate to other vertices of G. It is clear that at this point for every 1 â‰¤ i < Mn it holds
that there exist (i) Xji âˆˆ bi , Xji+1 âˆˆ bi+1 and (ii) ji < ji+1 . Unfortunately, the graph G does not contain a path starting at s and going sequentially
through the balls b1 , . . . , b12 until t is reached. To conclude, even though conditions (i), (ii) hold, RRTâˆ— will return the path consisting of the vertices
s, X1 , X2 , t, which is substantially longer than ÏƒÎµ .

