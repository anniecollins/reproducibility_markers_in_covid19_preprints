2019-03-04

The StreetLearn Environment and Dataset
Piotr Mirowski*,1 , Andras Banki-Horvath*,1 , Keith Anderson*,1 , Denis Teplyashin*,1 ,
Karl Moritz Hermann2 , Mateusz Malinowski1 , Matthew Koichi Grimes1 , Karen Simonyan1 ,
Koray Kavukcuoglu1 , Andrew Zisserman1 and Raia Hadsell1

arXiv:1903.01292v1 [cs.AI] 4 Mar 2019

* Equal

contributions, 1 DeepMind, London, United Kingdom, 2 DeepMind, Berlin, Germany

Navigation is a rich and well-grounded problem domain that drives progress in many different areas of research: perception, planning, memory, exploration, and optimisation in particular. Historically these challenges have been separately considered and solutions built that rely on stationary
datasetsâ€”for example, recorded trajectories through an environment. These datasets cannot be used
for decision-making and reinforcement learning, however, and in general the perspective of navigation as an interactive learning task, where the actions and behaviours of a learning agent are learned
simultaneously with the perception and planning, is relatively unsupported. Thus, existing navigation benchmarks generally rely on static datasets (Geiger et al., 2013; Kendall et al., 2015) or simulators (Beattie et al., 2016; Shah et al., 2018). To support and validate research in end-to-end navigation,
we present StreetLearn: an interactive, first-person, partially-observed visual environment that uses
Google Street View for its photographic content and broad coverage, and give performance baselines
for a challenging goal-driven navigation task. The environment code, baseline agent code, and the
dataset are available at http://streetlearn.cc.

Keywords: navigation, environment, reinforcement learning, deep learning, end-to-end

1. Introduction

Figure 1 | Our environment is built of real-world
places from StreetView. The figure shows diverse
views and corresponding local maps in New York
City (Times Square, Central Park) and London
(St. Paulâ€™s Cathedral). The green cone represents
the agentâ€™s location and orientation.

The subject of navigation is attractive to various research disciplines and technology domains
alike, being at once a subject of inquiry from the
point of view of neuroscientists wishing to crack
the code of grid and place cells (Banino et al.,
Correspondence to piotrmirowski@google.com

2018; Cueva and Wei, 2018), as well as a fundamental aspect of robotics research wishing to
build mobile robots that can reach a given destination. The majority of navigation algorithms
involve building an explicit map during an exploration phase and then planning and acting via
that representation. More recently, researchers
have sought to directly learn a navigation policy through exploration and interaction with the
environment, for instance by using end-to-end
deep reinforcement learning (Lample and Chaplot, 2017; Mirowski et al., 2017; Wu et al., 2018;
Zhu et al., 2017). To support this research, we
have designed an interactive environment called
StreetLearn that uses the images and underlying connectivity information from Google Street
View (see Fig. 1) in two large areas comprising
Pittsburgh and New York City. The environment
features high-resolution photographic images displaying a diversity of urban settings, and spans
city-scale areas with real-world street connectivity graphs. Within this environment we have
developed several traversal tasks that requires
that the agent navigates from goal to goal over
long distances. One such task has a real-world

The StreetLearn Environment and Dataset

tasks in Section 2, explain the environment code
in Section 3, describe implemented approaches
and baseline methods in Section 4 with results in
Section 5, and detail related work in Section 6.

2. Environment
This section presents StreetLearn, an interactive
environment constructed using Google Street
View. Since Street View data has been collected
worldwide, and includes both high-resolution
imagery and graph connectivity, it is a valuable
resource for studying navigation (Fig.1).

Figure 2 | Maps with bounding boxes indicating
the dataset coverage in New York City (top) and
Pittsburgh (bottom).

analogy of a courier operating in a given city that
starts at an arbitrary location called â€œAâ€ and then
is directed to go to a specific location â€œBâ€ defined
using absolute coordinates, without having ever
been shown the map featuring these locations or
the path going from A to B, or been told its own
position. Another task consists in following stepby-step directions consisting of natural language
navigation instructions and image thumbnails,
mimicking Google Maps. Additional navigation
tasks can be developed in the StreetLearn environment.
We describe the dataset, environment, and

Street View provides a set of geolocated 360Â°
panoramic images which form the nodes of an
undirected graph (we use the term node and
panorama interchangeably). We selected regions
in New York City and Pittsburgh (see Fig.2).
The area of New York City which is available
for download is Manhattan south of 81st Street.
This comprises approximates 56K panoramic images within a lat/long bounding box defined by
p40.695, Â´74.028q and p40.788, Â´73.940q. Note
that Brooklyn, Queens, Roosevelt Island as well
as the bridges and tunnels out of Manhattan are
excluded, and we include only panoramas inside a polygon that follows the waterfront of
Manhattan and 79th / 81st Street, covering an
area of 31.6 km2 . The Pittsburgh dataset comprises 58K images and is defined by a lat/long
bounding box between p40.425, Â´80.035q and
p40.460, Â´79.930q, covering an area of 8.9 km
by 3.9km. Additionally, we identify three regions
in each city which can be used individually for
training or for transfer learning experiments. The
statistics of each region are given in Table 1.
The undirected graph edges define the proximity and accessibility of nodes to other nodes. We
do not reduce or simplify the underlying connectivity but rather use the full graph; thus there are
congested areas with many nodes, complex occluded intersections, tunnels and footpaths, and
other ephemera. The average node spacing is
10m, with higher densities at intersections. Although the graph is used to construct the environment, the agent never observes the underlying
graphâ€”only the RGB images are observed (overlay information, such as arrows, that are visible
2

The StreetLearn Environment and Dataset

list of directly connected neighbours.
2.1. Defining Areas Within the Dataset
The whole Manhattan and Pittsburgh environments in the StreetLearn dataset encompass
large urban areas that represent over 56k Street
View panoramas each, and traversing these areas
from one extremity to another could entail going through close to 1k nodes in the Street View
graph. To make learning tractable and also to
define distinct regions for training and transfer,
one can cut the environment into smaller areas.
For instance, Figure 3 illustrates a cut of Manhattan and Pittsburgh into 6 regions ("Wall Street",
"Union Square", "Hudson", "CMU", "Allegheny"
and "South Shore") that used in our experiments
in Section 5.

Figure 3 | Maps with polygons delimiting the
Wall Street (1), Union Square (2) and Hudson
(3) regions in New York City (top) and the CMU
(4), Allegheny (5) and South Shore (6) regions in
Pittsburgh (bottom).

in the public Street View product are also not
seen by the agent). Examples of the RGB images
and the graph are shown in Figure 1.
In our dataset, each panorama is stored as
a Protocol Buffer (Google, 2008) object, containing a string in high-quality compressed JPEG
format that encodes the equirectangular image,
and decorated with the following attributes: a
unique string identifier, the position (lat/long coordinates and altitude in meters) and orientation
(pitch, roll and yaw angles) of the panoramic
camera, date of acquisition of the image, and a

There are many possibilities to define areas inside a street graph: the most obvious is to cut the
graph using a latitude/longitude bounding box,
with the disadvantage of creating unconnected
components. The second is to cut the graph using a polygon, with the inconvenience of having
to specify all the vertices of that polygon, relying on convex hulls to select the nodes included
within the polygon. We chose a third approach
for defining our areas, by growing graph areas by Breadth-First Search (BFS)(Moore, 1959;
Zuse, 1972) from a given node, which requires
to choose only a central panorama and a graph
depth, and which ensure that the resulting graph
is connected. We list in Table 1 the size (in nodes,
edges and area coverage), the elevation changes
and a description of those areas, including the
central panorama ID and the BFS graph depth.
2.2. Agent Interface and the Courier Task
An RL environment needs to specify the observations and action space of the agent as well as
define the task. The StreetLearn environment
provides a visual observation at each timestep,
xt . The visual inputs are meant to simulate a firstperson, partially observed environment, thus xt
is a cropped, 60Â° square, RGB image that is scaled
to 84 Ë† 84 pixels (i.e. not the entire panorama).
The action space is composed of five discrete

3

The StreetLearn Environment and Dataset

actions: â€œslow" rotate left or right (Ë˜22.5Â°), â€œfast"
rotate left or right (Ë˜67.5Â°), or move forward
(this action becomes a noop if there is not an
edge in view from the current agent pose). If
there are multiple edges in the viewing cone of
the agent, then the most central one is chosen.
StreetLearn provides an additional observation, the goal descriptor gt , which communicates
the task objective to the agentâ€”where to go to
receive the next reward. There are many options
for how to specify the goal: e.g., images are a natural choice (as in (Zhu et al., 2017)) but quickly
become ambiguous at city scale; language-based
directions or street addresses could be used (as
in (Chen et al., 2018)) though this would place
the emphasis on language grounding rather than
navigation; and landmarks could be used to encode the target location in a scalable, coordinatefree way (Mirowski et al., 2018). For this courier
task we take the simplest route and define goal
locations straightforwardly as continuous-valued
coordinates pLatgt , Longtg q. Note that the goal
description is absolute; it is not relative to the
agentâ€™s position and only changes when a new
goal is drawn (either upon successful goal acquisition or at the beginning of an episode).
In the courier task, which can be summarised
as the problem of navigating to a series of random locations in a city, the agent starts each
episode from a randomly sampled position and
orientation within the StreetLearn graph. A goal
location is randomly sampled from the graph and
the goal descriptor g0 is computed and input to
the agent. If the agent reaches a node that is
near to the goal (100m, or approximately one
city block), the agent is rewarded and the next
goal is randomly chosen and input to the agent.
Each episode ends after 1000 agent steps. The
reward that the agent gets upon reaching a goal
is proportional to the shortest path between the
goal and the agentâ€™s position when the goal is
first assigned; much like a delivery service, the
agent receives a higher reward for longer journeys.
Intuitively, in order to solve the courier task,
the agent will need to learn to associate the goal
encoding with the images observed at the goal
location, as well as to associate the images ob-

served at the current location with the policy to
reach different goal locations.
2.3. Curriculum
Curriculum learning gradually increases the complexity of the learning task by choosing more
and more difficult examples to present to the
learning algorithm (Bengio et al., 2009; Graves
et al., 2017; Zaremba and Sutskever, 2014). We
have found that a curriculum may be important
for the courier task with more distant destinations. Similar to other RL problems such as Montezumaâ€™s Revenge, the courier task suffers from
very sparse rewards; unlike that game, we are
able to define a natural curriculum scheme. We
start by sampling new goals within 500m of the
agentâ€™s position (phase 1). In phase 2, we progressively grow the maximum range of allowed
goals to cover the full graph.
Note that while this paper focuses on the
courier task, but as described in the following Section 3, the environment has been enriched with
the possibility of specifying directions through
step-by-step pairs of (image, natural language
instruction) and goal image.

3. Code
3.1. Code Structure
We have made the environment and agent code
available at https://github.com/deepmind/streetlearn. The
code repository contains the following components:
â€¢ Our C++ StreetLearn engine for loading,
caching and serving Google Street View
panoramas as well as for handling navigation (moving from one panorama to another) depending on the city street graph
and the current position and orientation of
the agent. Each panorama is projected from
its equirectangular (Wikipedia, 2005) representations to a first-person view for which
one can specify the yaw, pitch and field of
view angles.
â€¢ The message protocol buffers (Google,
2008) used to store the panoramas and the
4

The StreetLearn Environment and Dataset

Region

#nodes

#edges

av. edge len.

elev. change

area

description
Southernmost area of Manhattan, skyscrapers,
narrow streets and highways with irregular intersections. Graph of depth 215, centered at pano
6rIMyvAZUW4sT3ffqYOg0w.

Wall Street

7224

7496

9.8m

31m

3.8km2

Union Square

15525

16094

9.8m

40m

9.7km2

Between Downtown and Midtown Manhattan;
skyscrapers, brownstones and townhouses; parks
and regular street grid. Graph of depth 200, centered at pano dFbip4uo7CNu86y52Axc5g.

Hudson River

18085

18676

9.9m

56m

11.7km2

Riverside along Hudson River and near Central
Park; skyscrapers, regular street grid and highways. Graph of depth 400, centered at pano
PreXwwylmG23hnheZ__zGw.

CMU

15947

16339

9.9m

146m

11.2km2

Suburban areas of Oakland near CMU, suburban, leafy streets with high altitude differentials. Graph of depth 400, centered at pano
r5DqC1wcUi2Lw6T4GvUxwQ.

Allegheny

14073

14567

9.8m

104m

7.2km2

Downtown Pittsburgh and historic district, large avenues, highways and bridges.
Graph of depth 320, centered at pano
ohwj1wXoJ3KOPwnSPaAMCw.

South Shore

14967

15370

9.9m

151m

9.4km2

Downtown Pittsburgh, South Shore, South Side
Flats and Duquesne Heights, highways, bridges
and long tunnels, funicular. Graph of depth 350,
centered at pano ljBFHUcoonDeE2omJ7PrOQ.

Table 1 | Relevant information for the three regions in New York (Wall Street, Union Square, and
Hudson River) and three regions in Pittsburgh (CMU, Allegheny, and South Shore).
street graph.
â€¢ A Python-based interface for calling the
StreetLearn environment with custom action
spaces.
â€¢ Within the Python StreetLearn interface, several games are defined in individual files
whose names end with game.py.
â€¢ A simple human agent, implemented in
Python using Pygame1 , that instantiates the
StreetLearn environment on the requested
map and enables a user to play the courier
or the instruction-following games.
â€¢ Oracle agents, similar to the human agent,
which automatically navigate towards a
specified goal and reports oracle performance on the courier or instructionfollowing games.
â€¢ TensorFlow implementation of agents.
1

https://www.pygame.org

3.2. Code Interface
Our Python StreetLearn environment follows the
specifications from OpenAI Gym2 (Brockman
et al., 2016).

After instantiating a specific game and the environment, the environment can be initialised
by calling function resetpq. Note that if the flag
auto_reset is set to True at construction, resetpq
will be called automatically every time that an
episode ends.
As illustrated in Listing 4, the agent plays
within the environment by iteratively producing
an action, sending it to (stepping through) the
environment, and processing the observations
and rewards returned by the environment. The
call to function step(action) returns:
â€¢ observation (tuple of observations arrays
and scalars that are requested at construction),
â€¢ reward (a floating-point scalar number with
the current reward of the agent),
â€¢ done (boolean indicating whether a game
episode has ended and been reset),
â€¢ and info (a dictionary of environment state
variables, which is useful for debugging the
2

https://gym.openai.com/

5

The StreetLearn Environment and Dataset

agent behaviour or for accessing privileged
environment information for visualisation
and analysis).
3.3. Actions and observations
We have made four actions available to the agent:
â€¢ Rotate left or right in the panorama, by
a specified angle (change the yaw of the
agent).
â€¢ Rotate up or down in the panorama, by
a specified angle (change the pitch of the
agent).
â€¢ Move from current panorama A forward to
another panorama B if the current bearing
of the agent from A to B is within a tolerance
angle of 30 degrees.
â€¢ Zoom in and out in the panorama.
As such, agent actions are sent to the environment via step(action) as tuples of 4 scalar
numbers. However, for training discrete policy
agents via reinforcement learning, action spaces
are discretised into integers. For instance, we
used 5 actions in (Mirowski et al., 2018): (move
forward, turn left by 22.5 deg, turn left by 67.5
deg, turn right by 22.5 deg, turn right by 67.5
deg).
The following observations can currently be
requested from the environment:
â€¢ view_image: RGB image for the first-person
view image returned from the environment
and seen by the agent,
â€¢ graph_image: RGB image for the top-down
street graph image, usually not seen by the
agent,
â€¢ pitch: Scalar value of the pitch angle of
the agent, in degrees (zero corresponds to
horizontal),
â€¢ yaw: Scalar value of the yaw angle of
the agent, in degrees (zero corresponds to
North),
â€¢ yaw_label: Integer discretized value of the
agent yaw using 16 bins,
â€¢ metadata: Message protocol buffer of type
Pano with the metadata of the current

panorama,
â€¢ target_metadata: Message protocol buffer
of type Pano with the metadata of the target/goal panorama,
â€¢ latlng: Tuple of lat/lng scalar values for the
current position of the agent,
â€¢ latlng: Integer discretized value of the current agent position using 1024 bins (32 bins
for latitude and 32 bins for longitude),
â€¢ target_latlng: Tuple of lat/lng scalar values
for the target/goal position,
â€¢ target_latlng: Integer discretized value of
the target position using 1024 bins (32 bins
for latitude and 32 bins for longitude),
â€¢ thumbnails: set of n ` 1 RGB images for
the first-person view image returned from
the environment, that should be seen by the
agent at specific waypoints and goal locations when playing the instruction-following
game with n instructions,
â€¢ instructions: set of n instructions for the
agent at specific waypoints and goal locations when playing the instruction-following
game with n instructions,
â€¢ neighbors: Vector of immediate neighbor
egocentric traversability grid around the
agent, with 16 bins for the directions around
the agent and bin 0 corresponding to the
traversability straight ahead of the agent.
â€¢ ground_truth_direction: Scalar value of
the relative ground truth direction to be
taken by the agent in order to follow a shortest path to the next goal or waypoint. This
observation should be requested only for
agents trained using imitation learning.
3.4. Games
The following games are available in the
StreetLearn environment:
3.4.1. coin_game
In the coin_game, the rewards consist in invisible
coins scattered throughout the map, yielding a
reward of 1 for each. Once picked up, these
rewards do not reappear until the end of the
episode.

6

The StreetLearn Environment and Dataset

3.4.2. courier_game
In the courier_game, the agent is given a goal
destination, specified as lat/long pairs. Once the
goal is reached (with 100m tolerance), a new
goal is sampled, until the end of the episode. Rewards at a goal are proportional to the number of
panoramas on the shortest path from the agentâ€™s
position when it gets the new goal assignment
to that goal position. Additional reward shaping
consists in early rewards when the agent gets
within a range of 200m of the goal. Additional
coins can also be scattered throughout the environment. The proportion of coins, the goal
radius and the early reward radius are parameterizable. The curriculum_courier_game is similar
to the courier_game, but with a curriculum on
the difficulty of the task (maximum straight-line
distance from the agentâ€™s position to the goal
when it is assigned).
3.4.3. Instruction games
The goal_instruction_game and its variations
incremental_instruction_game
and
step_by_step_instruction_game use navigation instructions to direct agents to a goal.
Agents are provided with a list of instructions
as well as thumbnails that guide the agent
from its starting position to the goal location.
In step_by_step, agents are provided one
instruction and two thumbnails at a time,
in the other game variants the whole list is
available throughout the whole game. Reward
is granted upon reaching the goal location (all
variants), as well as when hitting individual
waypoints (incremental and step_by_step only).
During training various curriculum strategies
are available to the agents, and reward shaping
can be employed to provide fractional rewards
when the agent gets within a range of 50m of a
waypoint or goal.

4. Methods
This section briefly describes the set of approaches which are evaluated on the courier task.

4.1. Goal-dependent Actor-Critic Reinforcement Learning
We formalise the learning problem as a Markov
Decision Process, with state space S, action space
A, environment E, and a set of possible goals G.
The reward function
on the current
Åšdepends
Åš
goal and state: R : S
G
A Ã‘ R. The usual
reinforcement learning objective is to find the policy that maximises the expected return defined
as the sum of discounted rewards starting from
state s0 with discount Î³. In this navigation task,
the expected return from a state st also depends
on the series of sampled goals tgk uk . The policy is
a distribution over actions given the current state
st and the goal gt : Ï€pa|s, gq â€œ P rpat â€œ a|st â€œ
s, gt â€œ gq. We define the value function to be the
expected return for the agent that is sampling
actions from policy Ï€ from
st with goal gt :
Å™ state
kr
V Ï€ ps, gq â€œ ErRt s â€œ Er 8
Î³
t`k |st â€œ s, gt â€œ
kâ€œ0
gs.
We hypothesise that an agent should benefit from two types of learning: first, learning
a general and location-agnostic representation
and exploration behaviour, and second, learning
locale-specific structure and features. A navigating agent not only needs an internal representation that is general, to support cognitive
processes such as scene understanding, but also
needs to organise and remember the features and
structures that are unique to a place. Therefore,
to support both types of learning, we focus on
neural architectures with multiple pathways.
We evaluate two agents on the six regions described in Table 1. We give here a summary of the
approach, as the full architectural details of these
agents have been previously described (Mirowski
et al., 2018). The policy and the value function are both parameterised by a neural network
which shares all layers except the final linear outputs. The agent operates on raw pixel images
xt , which are passed through a convolutional
network as in (Mnih et al., 2016). A Long ShortTerm Memory (LSTM) (Hochreiter and Schmidhuber, 1997) receives the output of the convolutional encoder as well as the past reward rtÂ´1
and previous action atÂ´1 . The two different architectures are described below.
The CityNav architecture (Fig. 5b) has a con7

The StreetLearn Environment and Dataset

# I n s t a n t i a t e a game ( e a c h game has i t s own c l a s s and c o n s t r u c t o r ) .
game = courier_game . CourierGame ( c o n f i g )
env = s t r e e t l e a r n . S t r e e t L e a r n ( FLAGS . d a t a s e t _ p a t h , config , game)
env . r e s e t ( )
a c t i o n = np . a r r a y ( [ 0 , 0 , 0 , 0 ] )
sum_rewards = 0
while True :
observation , reward , done , i n f o = env . s t e p ( a c t i o n )
# Plot the observations .
# [...]
# Keep t r a c k o f e p i s o d e e n d s and o f r e w a r d s .
sum_rewards += reward
i f done :
sum_rewards = 0
# Use i n f o f o r a n a l y s i n g t h e a g e n t p e r f o r m a n c e on t h e game .
# [...]
# Take an a c t i o n
a c t i o n = s o m e _ a g e n t _ f u n c t i o n ( observation )

Figure 4 | Main loop for interacting with the environment.

ð‘½

ð›‘
ð›‰

ð›‰i

ð›‰j

envi

ð›‰k

envj

envk

conv

gt

xt

ð‘½

ð›‘

conv

at-1,rt-1

CityNav agent

gt

xt

at-1,rt-1

MultiCityNav agent

Figure 5 | Comparison of architectures. Left: CityNav is a single-city navigation architecture with
a policy LSTM, a separate goal LSTM, and optional auxiliary heading (Î¸). Right: MultiCityNav
is a multi-city architecture with individual goal
LSTM pathways for each city.

volutional encoder and two LSTM layers, which
are designated as a policy LSTM and a goal LSTM.
The goal description gt is input to the goal LSTM
along with the previous action and reward, as
well as the visual features from the convolutional
encoder. The CityNav agent also adds an auxiliary heading (Î¸) prediction task on the outputs
of the goal LSTM.

The MultiCityNav architecture (Fig. 5c) extends the CityNav agent to learn in different
cities. The remit of the goal LSTM is to encode and encapsulate locale-specific features and
topology such that multiple pathways may be
added, one per city or region. Moreover, after
training on a number of cities, we demonstrate
that the convolutional encoder and the policy
LSTM become general enough that only a new
goal LSTM needs to be trained for new cities.
To train the agents, we use IMPALA (Espeholt
et al., 2018), an actor-critic implementation that
decouples acting and learning. In our experiments, IMPALA results in similar performance to
A3C (Mnih et al., 2016). We use 256 actors for
CityNav and 512 actors for MultiCityNav, with
batch sizes of 256 or 512 respectively, and sequences are unrolled to length 50.
We note that these computational resources
are not available for all, so we have verified that
comparable results are attained using only 16
actors and 1 learner, running on a single desktop
computer with a Graphics Processing Unit (GPU).
The desktop we used had large memory (192 GB)
for instantiating 16 StreetLearn environments
(each environment requiring a large cache memory for caching panoramas), but smaller memory
could be used as well with the trade-off of more

8

The StreetLearn Environment and Dataset

frequent disk accesses.
A TensorFlow implementation of the CityNav
and baseline architectures from (Mirowski et al.,
2018) is made available on the code repository at https://github.com/deepmind/streetlearn. The
trainer code is a directy modification of (Espeholt
et al., 2018) from https://github.com/deepmind/scalable_
agent and is made available separately.
4.2. Oracle
We also compute an upper bound for all the
tasks by computing the shortest path from all
panorama positions to the specified goal position
using breadth-first search (Moore, 1959; Zuse,
1972) on the panorama connectivity graph. This
enables us to calculate both which is the next
panorama that agent should go to and the direction that the agent should align with in order
to move forward to that panorama, repeating
this process until arriving at destination. This
ground_truth_position can be requested as an observation (for imitation learning agents) or be
taken from the info dictionary returned by the environment. Listing 6 shows how the oracle agent
can be implemented to provide with a valuable
measure to benchmark the tasks.

5. Results on the Courier Task
To evaluate the described approaches, we give
the individual performance in each region as
well as the result of training jointly over multiple regions. We also show the capability of the
approach to generalise by evaluating goals in
held-out areas, and by training only part of the
agent for an entirely new region.
Table 2 gives the average total reward per
1000-step episode achieved by different agents in
six different regions of New York City and Pittsburgh defined on Figure 3 and Table 1. Although
the agents were trained with reward shaping
(i.e., they receive partial rewards when they are
within a small radius of the goal), the per-episode
returns given here only include the full reward
which is given when the goal is reached. The
experiments are all replicated with 5 different
seeds.

In Table 2, Oracle results are the result of
breadth-first search directly on the graph; hence
they reflect perfect performance. Single results
show the performance of agents trained individually for each region using the CityNav architecture. The trained agents do well in New York City,
achieving 85% to 97% of oracle returns, and do
less well in Pittsburgh, particularly in the South
Shore region where the agent fails completely.
This is presumably due to the challenging elevation changes in the region which give rise to
convoluted routes even between nearby nodes,
and is an artifact of how we specify the curriculum task (based on the maximum Euclidean distance from the agent position to the goal, not
accounting for actual travel time). Specifically,
when the agent is at the top of Duquesne Hill in
South Shore, a goal location on the other side
of the river and that is 500m away by bird flight
might be kilometres away by road distance.
Joint results show the per-region performance
of a MultiCityNav agent that is trained jointly
across five regions (South Shore is excluded).
The resulting agent suffers only a small drop
in performance even though it is now trained
across a much broader area: two cities and five
regions. Finally, transfer gives the performance
of an agent that is trained on four regions (given
in italics) and then transferred to a fifth region
(Wall Street). In this transfer, only the goal LSTM
is modified; there are no gradient updates to the
other two components of the architecture (the
convolutional encoder or the policy LSTM).
City

Oracle

Single

Joint

Transfer

Wall Street
Union Square
Hudson River

809
750
721

782
721
615

745
681
621

541
667
601

CMU
Allegheny
South Shore

755
760
737

473
669
1

313
571
-

355
562
-

Table 2 | Per-city goal rewards for Oracle, singletrained CityNav as well as MultiCityNav agents
trained jointly on 5 cities (Wall Street, Union
Square and Hudson River in Manhattan, CMU
and Allegheny in Pittsburgh) or jointly on 4
cities (Union Square, Hudson River, CMU and
Allegheny) then transferred to Wall Street.

9

The StreetLearn Environment and Dataset

game = courier_game . CourierGame ( c o n f i g )
env = s t r e e t l e a r n . S t r e e t L e a r n ( FLAGS . d a t a s e t _ p a t h , config , game)
env . r e s e t ( )
a c t i o n = np . a r r a y ( [ 0 , 0 , 0 , 0 ] )
a c ti o n _s p e c = env . a c ti o n _s p e c ( )
while True :
observation , reward , done , i n f o = env . s t e p ( a c t i o n )
# Plot the observations .
# [...]
bearing = i n f o [ " bearing_to_next_pano " ]
i f bearing > FLAGS . h o r i z o n t a l _ r o t :
a c t i o n = FLAGS . h o r i z o n t a l _ r o t * a ct i o n _s p e c [ " h o r i z o n t a l _ r o t a t i o n " ]
e l i f bearing < Â´FLAGS . h o r i z o n t a l _ r o t :
a c t i o n = Â´FLAGS . h o r i z o n t a l _ r o t * a c ti o n _s p e c [ " h o r i z o n t a l _ r o t a t i o n " ]
else :
a c t i o n = a ct i o n _s p e c [ " move_forward " ]

Figure 6 | Oracle implementation using the ground truth direction/bearing to the next panorama.
Grid size

Area

Goal rewards

Fail

T 21

No grid

Train

719

0%

133

Held-out
Held-out

724
605

0%
2%

126
164

Medium grid
Coarse grid

Table 3 | CityNav agent generalisation performance (reward and fail metrics) on a set of heldout goal locations (medium and coarse grids).
We also compute the half-trip time (T 12 ), to reach
halfway to the goal.
To investigate the generalisation capability of a
trained agent, we mask 25% of the possible goals
and train on the remaining ones (see Figure 5
in (Mirowski et al., 2018) for an illustration).
At test time we evaluate the agent only on its
ability to reach goals in the held-out areas. Note
that the agent is still able to traverse through
these areas, it just never samples a goal there.
More precisely, the held-out areas are squares
sized 0.01Â° (coarse grid) or 0.005Â° (medium grid)
of latitude and longitude (respectively roughly
about 1km2 and 0.5km2 ).
In the experiments, we train the CityNav agent
for 1B steps, and next freeze the weights of the
agent and evaluate its performance on held-out
areas for 100M steps. Table 3 shows some decreasing performance of the agents as the heldout area size increases. To gain further understanding, in addition to Test Reward metric, we
also use missed goals (Fail) and half-trip time

(T 1 ) metrics. The missed goals metric measures
2
the percentage of times goals were not reached.
The half-trip time measures the number of agent
steps necessary to cover half the distance separating the agent from the goal.
We also compare, in Table 4, the performance
achieved when using (lat, long) goal descriptors
versus the previously proposed landmark descriptors (Mirowski et al., 2018). Although the landmark scheme has advantages, such as avoiding
fixed coordinate frames, the (lat, long) descriptor
is shown to out-perform landmarks on the Union
Square region in New York.
Target representation

Goal rewards

Oracle

750

(lat, long) scalars
Landmarks

721
700

Table 4 | CityNav agent performance on Union
Square with different types of target representations: (lat, long) scalars vs. landmarks.

6. Related Work
The StreetLearn environment is related to a number of other simulators and datasets that have
emerged in recent years in response to a greater
interest in reinforcement learning and, more
generally, learning navigation through interac10

The StreetLearn Environment and Dataset

tion. We focus on enumerating these related
datasets and environments, referring the reader
to Mirowski et al. (2018) for a more complete
discussion of related approaches.
Many RL-based approaches for navigation
rely on simulators which have the benefit of
features like procedurally generated variations
but tend to be visually simple and unrealistic, including synthetic 3D environments such
as VizDoom (Kempka et al., 2016), DeepMind
Lab (Beattie et al., 2016), HoME (Brodeur et al.,
2017), House 3D (Wu et al., 2018), Chalet (Yan
et al., 2018), or AI2-THOR (Kolve et al., 2017).
To bridge the gap between simulation and reality, researchers have developed more realistic,
higher-fidelity simulated environments (Dosovitskiy et al., 2017; Kolve et al., 2017; Shah et al.,
2018; Wu et al., 2018). However, in spite of their
increasing photo-realism, the inherent problems
of simulated environments lie in the limited diversity of the environments and the antiseptic
cleanliness of the observations. Our real-world
dataset is diverse and visually realistic, comprising scenes with pedestrians, cars, buses or trucks,
diverse weather conditions and vegetation and
covering large geographic areas. However, we
note that there are obvious limitations of our
environment: It does not contain dynamic elements, the action space is necessarily discrete
as it must jump between panoramas, and the
street topology cannot be arbitrarily altered or
regenerated.
More visually realistic environments such as
Matterport Room-to-Room (Chang et al., 2017),
AdobeIndoorNav (Mo et al., 2018), Stanford 2D3D-S (Armeni et al., 2016), ScanNet (Dai et al.,
2017), Gibson Env (Xia et al., 2018), and MINOS (Savva et al., 2017) have been recently introduced to represent indoor scenes, some augmented with navigational instructions.
Using New York imagery, de Vries et al. (2018)
use navigation instructions but rely on categorical annotation of nearby landmarks rather than
visual observations and use a dataset of 500
panoramas only (ours is two orders of magnitude larger). Very recently, Cirik et al. (2018)
and particularly Chen et al. (2018) have also

proposed larger datasets of driving instructions
grounded in Street View imagery.

7. Conclusion
Navigation is an important cognitive task that
enables humans and animals to traverse a complex world without maps. To help understand
this cognitive skill, its emergence and robustness, and its application to real-world settings,
we have made public a dataset and an interactive environment based on Google Street View.
Our carefully curated dataset has been constituted from photographic images that have been
manually reviewed and vetted for privacy - we
took these extra precautions to ensure that all
faces and license plates are blurred appropriately.
The dataset is made available at http://streetlearn.cc
and is distributed on request; in the case when
an individual requests a specific panorama to
be taken down or to be blurred on the Google
Street View website, we propagate their request
to the users of the StreetLearn dataset and provide users with an updated version that complies
with the takedown request.
Our environment enables the training of
agents to navigate to different goal locations
based purely on visual observations and absolute
target position representations. We have also
expanded that dataset with text instructions to
enable reward-based task focused on following
relative directions to reach a goal. We will rely
on this dataset and environment to address the
fundamental problem of grounded, long-range,
goal-driven navigation.

Acknowledgements
The authors wish to acknowledge Lasse Espeholt
and Hubert Soyer for technical help with the IMPALA algorithm, Razvan Pascanu, Ross Goroshin,
Phil Blunsom, and Nando de Freitas for their
feedback, Chloe Hillier, Razia Ahamed, Richard
Ives and Vishal Maini for help with the project,
and the Google Maps and Google Street View
teams for their support in accessing the data.

11

The StreetLearn Environment and Dataset

References
Iro Armeni, Ozan Sener, Amir R. Zamir, Helen
Jiang, Ioannis Brilakis, Martin Fischer, and Silvio Savarese. 3d semantic parsing of large-scale
indoor spaces. In Proceedings of the IEEE International Conference on Computer Vision and Pattern
Recognition (CVPR), 2016.
Andrea Banino, Caswell Barry, Benigno Uria,
Charles Blundell, Timothy Lillicrap, Piotr
Mirowski, Alexander Pritzel, Martin J Chadwick, Thomas Degris, Joseph Modayil, Greg
Wayne, Hubert Soyer, Fabio Viola, Brian Zhang,
Ross Goroshin, Neil Rabinowitz, Razvan Pascanu, Charlie Beattie, Stig Petersen, Amir Sadik,
Stephen Gaffney, Helen King, Koray Kavukcuoglu,
Demis Hassabis, Raia Hadsell, and Dharshan Kumaran. Vector-based navigation using grid-like
representations in artificial agents. Nature, 557
(7705):429, 2018.

ral language navigation and spatial reasoning
in visual street environments. arXiv preprint
arXiv:1811.12354, 2018.
Volkan Cirik, Yuan Zhang, and Jason Baldridge.
Following formulaic map instructions in a street
simulation environment. Visually Grounded Interaction and Language (ViGIL) Workshop, NeurIPS,
2018.
Christopher J Cueva and Xue-Xin Wei. Emergence of grid-like representations by training recurrent neural networks to perform spatial localization. International Conference on Learning
Representations, 2018.
Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas A Funkhouser, and Matthias
NieÃŸner. ScanNet: Richly-annotated 3D reconstructions of indoor scenes. In Proceedings of the
IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), volume 2, page 10, 2017.

Charles Beattie, Joel Z Leibo, Denis Teplyashin,
Tom Ward, Marcus Wainwright, Heinrich KÃ¼ttler,
Andrew Lefrancq, Simon Green, VÃ­ctor ValdÃ©s,
Amir Sadik, et al. Deepmind lab. arXiv preprint
arXiv:1612.03801, 2016.

Harm de Vries, Kurt Shuster, Dhruv Batra,
Devi Parikh, Jason Weston, and Douwe Kiela.
Talk the walk: Navigating New York City
through grounded dialogue. arXiv preprint
arXiv:1807.03367, 2018.

Yoshua Bengio, JÃ©rÃ´me Louradour, Ronan Collobert, and Jason Weston. Curriculum learning.
In Proceedings of the 26th annual international
conference on machine learning, pages 41â€“48.
ACM, 2009.

Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio LÃ³pez, and Vladlen Koltun. Carla:
An open urban driving simulator. arXiv preprint
arXiv:1711.03938, 2017.

Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang,
and Wojciech Zaremba. Openai gym. arXiv
preprint arXiv:1606.01540, 2016.
Simon Brodeur, Ethan Perez, Ankesh Anand, Florian Golemo, Luca Celotti, Florian Strub, Jean
Rouat, Hugo Larochelle, and Aaron Courville.
HoME: A household multimodal environment.
arXiv preprint arXiv:1711.11017, 2017.
Angel Chang, Angela Dai, Thomas Funkhouser,
Maciej Halber, Matthias NieÃŸner, Manolis Savva,
Shuran Song, Andy Zeng, and Yinda Zhang. Matterport3d: Learning from RGB-d data in indoor
environments. International Conference on 3D
Vision (3DV), 2017.
Howard Chen, Alane Shur, Dipendra Misra, Noah
Snavely, and Yoav Artzi. Touchdown: Natu-

Lasse Espeholt, Hubert Soyer, Remi Munos,
Karen Simonyan, Volodymir Mnih, Tom Ward,
Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, and Koray Kavukcuoglu. Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures. In
International Conference on Machine Learning
(ICML), 2018.
Andreas Geiger, Philip Lenz, Christoph Stiller,
and Raquel Urtasun. Vision meets robotics: The
kitti dataset. The International Journal of Robotics
Research, 32(11):1231â€“1237, 2013.
Google.

Protocol Buffers, 2008.

//developers.google.com/protocol-buffers/.

URL https:
(accessed 1

March 2019).
Alex Graves, Marc G Bellemare, Jacob Menick,
Remi Munos, and Koray Kavukcuoglu. Automated curriculum learning for neural networks.
12

The StreetLearn Environment and Dataset

In International Conference on Machine Learning
(ICML), 2017.
Sepp Hochreiter and JÃ¼rgen Schmidhuber. Long
short-term memory. Neural computation, 9(8):
1735â€“1780, 1997.
MichaÅ‚ Kempka, Marek Wydmuch, Grzegorz
Runc, Jakub Toczek, and Wojciech JasÌkowski.
Vizdoom: A doom-based ai research platform
for visual reinforcement learning. In Computational Intelligence and Games (CIG), 2016 IEEE
Conference on, pages 1â€“8. IEEE, 2016.

wards deep reinforcement learning based realworld indoor robot visual navigation. arXiv
preprint arXiv:1802.08824, 2018.
Edward F Moore. The shortest path through a
maze. In Proc. Int. Symp. Switching Theory, 1959,
pages 285â€“292, 1959.
Manolis Savva, Angel X Chang, Alexey Dosovitskiy, Thomas Funkhouser, and Vladlen Koltun.
Minos: Multimodal indoor simulator for navigation in complex environments. arXiv preprint
arXiv:1712.03931, 2017.

Alex Kendall, Matthew Grimes, and Roberto
Cipolla. Posenet: A convolutional network for
real-time 6-dof camera relocalization. In Computer Vision (ICCV), 2015 IEEE International Conference on, pages 2938â€“2946. IEEE, 2015.

Shital Shah, Debadeepta Dey, Chris Lovett, and
Ashish Kapoor. Airsim: High-fidelity visual and
physical simulation for autonomous vehicles.
In Field and Service Robotics, pages 621â€“635.
Springer, 2018.

Eric Kolve, Roozbeh Mottaghi, Daniel Gordon,
Yuke Zhu, Abhinav Gupta, and Ali Farhadi. Ai2thor: An interactive 3d environment for visual ai.
arXiv preprint arXiv:1712.05474, 2017.

Wikipedia. Equirectangular projection, 2005. URL
https://en.wikipedia.org/wiki/Equirectangular_projection. (accessed 1 March 2019).

Guillaume Lample and Devendra Singh Chaplot. Playing FPS games with deep reinforcement
learning. In Proceedings of the Thirty-First AAAI
Conference on Artificial Intelligence, 2017.
Piotr Mirowski, Razvan Pascanu, Fabio Viola, Hubert Soyer, Andrew J Ballard, Andrea Banino,
Misha Denil, Ross Goroshin, Laurent Sifre, Koray
Kavukcuoglu, et al. Learning to navigate in complex environments. In International Conference
on Learning Representations (ICLR), 2017.
Piotr Mirowski, Matthew Koichi Grimes, Mateusz
Malinowski, Karl Moritz Hermann, Keith Anderson, Denis Teplyashin, Karen Simonyan, Koray
Kavukcuoglu, Andrew Zisserman, and Raia Hadsell. Learning to navigate in cities without a
map. Advances in Neural Information Processing
Systems (NeurIPS), 2018.
Volodymyr Mnih, Adria Puigdomenech Badia,
Mehdi Mirza, Alex Graves, Timothy Lillicrap,
Tim Harley, David Silver, and Koray Kavukcuoglu.
Asynchronous methods for deep reinforcement
learning. In International Conference on Machine
Learning, pages 1928â€“1937, 2016.
Kaichun Mo, Haoxiang Li, Zhe Lin, and JoonYoung Lee. The AdobeIndoorNav dataset: To-

Yi Wu, Yuxin Wu, Georgia Gkioxari, and Yuandong Tian. Building generalizable agents with a
realistic and rich 3d environment. In European
Conference on Computer Vision (ECCV), 2018.
Fei Xia, Amir R Zamir, Zhiyang He, Alexander
Sax, Jitendra Malik, and Silvio Savarese. Gibson
Env: Real-world perception for embodied agents.
In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 9068â€“9079,
2018.
Claudia Yan, Dipendra Misra, Andrew Bennnett,
Aaron Walsman, Yonatan Bisk, and Yoav Artzi.
CHALET: Cornell house agent learning environment. arXiv preprint arXiv:1801.07357, 2018.
Wojciech Zaremba and Ilya Sutskever. Learning to execute. arXiv preprint arXiv:1410.4615,
2014.
Yuke Zhu, Roozbeh Mottaghi, Eric Kolve,
Joseph J. Lim, Abhinav Gupta, Li Fei-Fei, and
Ali Farhadi. Target-driven visual navigation in
indoor scenes using deep reinforcement learning.
In 2017 IEEE International Conference on Robotics
and Automation, ICRA, pages 3357â€“3364, 2017.
Konrad Zuse. Der PlankalkÃ¼l. Number 63.
Gesellschaft fÃ¼r Mathematik und Datenverarbeitung, 1972.

13

