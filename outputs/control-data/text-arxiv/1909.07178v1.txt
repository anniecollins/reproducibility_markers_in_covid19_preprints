arXiv:1909.07178v1 [math.ST] 16 Sep 2019

Estimating change points in nonparametric
time series regression models
Maria Mohr and Leonie Selkâˆ—
Department of Mathematics, University of Hamburg
September 17, 2019

Abstract
In this paper we consider a regression model that allows for time series covariates
as well as heteroscedasticity with a regression function that is modelled nonparametrically. We assume that the regression function changes at some unknown time
bns0 c, s0 âˆˆ (0, 1), and our aim is to estimate the (rescaled) change point s0 . The
considered estimator is based on a Kolmogorov-Smirnov functional of the marked
empirical process of residuals. We show consistency of the estimator and prove a
rate of convergence of OP (nâˆ’1 ) which in this case is clearly optimal as there are
only n points in the sequence. Additionally we investigate the case of lagged dependent covariates, that is, autoregression models with a change in the nonparametric
(auto-) regression function and give a consistency result. The method of proof also
allows for different kinds of functionals such that CrameÌr-von Mises type estimators
can be considered similarly. The approach extends existing literature by allowing
nonparametric models, time series data as well as heteroscedasticity. Finite sample
simulations indicate the good performance of our estimator in regression as well as
autoregression models and a real data example shows its applicability in practise.

Key words: change point estimation, time series, nonparametric regression, autoregression, conditional heteroscedasticity, consistency, rates of convergence
AMS 2010 Classification: Primary 62G05, Secondary 62G08, 62G20, 62M10

1

Introduction

Change point analysis has gained attention for decades in mathematical statistics. There
is a vast literature on testing for structural breaks when the possible timing of such a
âˆ—

Financial support by the DFG (Research Unit FOR 1735 Structural Inference in Statistics: Adaptation and Efficiency) is gratefully acknowledged.

1

break, the change point, is unknown, see for instance Kirch and Kamgaing (2012) and
reference mentioned therein. This paper, however, is concerned with the estimation of
the change point when assuming its existence.
The most simple set of models can be described as follows
Yt = Âµ1 I{t â‰¤ bns0 c} + Âµ2 I{t > bns0 c} + Îµt , t = 1, . . . , n,
where s0 âˆˆ (0, 1) is the (rescaled) change point, Âµ1 and Âµ2 the signal before and after the
break, respectively, and (Îµt )t being stationary and centred errors. These models are often
referred to as AMOC-models (at most one change). The problem naturally moved from
the standard case with independent errors (see Ferger and Stute (1992) among others)
to the time series context. Both Bai (1994) and Antoch et al. (1997) allow for linear
processes and HusÌŒkovaÌ and Kirch (2008) more generally for dependent errors.
Additional information on the form of the signal can be expressed through a process
of covariates (Xt )t resulting in linear regression models with a change in the regression
parameter, such as
Yt = Î²1 Xt I{t â‰¤ bns0 c} + Î²2 Xt I{t > bns0 c} + Îµt , t = 1, . . . , n,
where Î²1 and Î²2 are the regression coefficients before and after the break, respectively.
Bai (1997), HorvaÌth et al. (1997), Aue et al. (2012) among others consider the estimation
of a change point in (multiple) linear regression models making use of least squares estimation. Considering Xt = Ytâˆ’1 in the linear regression model from above, one obtains
autoregressive models with one change in the autoregressive parameter. The estimation
of the parameters and the unknown change point in AR(1) models was for instance considered by Chong (2001), Pang et al. (2014) and Pang and Zhang (2015).
Our aim is to propose an estimator for the change point s0 in a nonparametric version
of the regression model from above, namely
Yt = m(1) (Xt )I{t â‰¤ bns0 c} + m(2) (Xt )I{t > bns0 c} + Îµt , t = 1, . . . , n,
for some nonparametric regression functions m(1) , m(2) (before and after the break) and
in addition also investigate the autoregressive case where Xt = Ytâˆ’1 . While the investigation of points of discontinuity in (nonparametric) regression functions has been studied
to some extend (see for instance DoÌˆring and Jensen (2015) for an overview), not that
much research has been devoted to change point analysis in nonparametric models as
the one above, where the change occurs in time. Delgado and Hidalgo (2000) propose
estimators for the location and size of structural breaks in a nonparametric regression
model imposing scalar breaks in time or values taken by some regressors, as in threshold
models. Their rates of convergence and limiting distribution depends on a bandwidth,
chosen for the kernel estimation. Chen et al. (2005) estimate the time of a scalar change
in the conditional variance function in nonparametric heteroscedastic regression models
using a hybrid procedure that combines the least squares and nonparametric methods.
2

The paper at hand extends existing literature, on the one hand by allowing for nonparametric heteroscedastic regression models with a general change in the unknown regression
function where both errors and covariates are allowed to be time series, and on the other
hand by investigating the autoregressive case. The achieved rate of convergence for the
proposed estimator of OP (nâˆ’1 ) is optimal as described in Hariz et al. (2007).
The remainder of the paper is organized as follows. The model and the considered
estimator are introduced in section 2. Section 3 contains the regularity assumptions as
well as the asymptotic results for the proposed estimator. Section 4 is concerned with the
special case of lagged dependent covariates, that is the autoregressive case. In section 5 we
describe a simulation study and discuss a real data example, whereas section 6 concludes
the paper. Proofs of the main results as well as auxiliary lemmata can be found in the
appendix.

2

The model and estimator

Let {(Yt , Xt ) : t âˆˆ N} be a weakly dependent stochastic process in R Ã— Rd following the
regression model
Yt = mt (Xt ) + Ut , t âˆˆ N.
(2.1)
The unobservable innovations are assumed to fulfill E[Ut |F t ] = 0 almost surely for
the sigma-field F t = Ïƒ(Ujâˆ’1 , Xj : j â‰¤ t). We assume there exists a change point in the
regression function such that
(
m(1) (Â·),
t = 1, . . . , bns0 c
mn,t (Â·) = mt (Â·) =
,
m(1) 6â‰¡ m(2)
(2.2)
m(2) (Â·),
t = bns0 c + 1, . . . , n
where bns0 c with s0 âˆˆ (0, 1) is the unknown time the change occurs. Note that we
keep above notations for simplicity reasons, however, the considered process is in fact a
triangular array process {(Yn,t , Xn,t ) : 1 â‰¤ t â‰¤ n, n âˆˆ N} and will be treated appropriately.
Assuming (Y1 , X1 ), . . . , (Yn , Xn ) have been observed, the aim is to estimate s0 . The
idea is to base the estimator on the sequential marked empirical process of residuals,
namely
bnsc
1X
TÌ‚n (s, z) :=
(Yi âˆ’ mÌ‚n (Xi ))Ï‰n (Xi )I{Xi â‰¤ z},
n i=1
for s âˆˆ [0, 1] and z âˆˆ Rd , where x â‰¤ y is short for xj â‰¤ yj for all j = 1, . . . , d,
Ï‰n (Â·) = I{Â· âˆˆ Jn } being from assumption (J) below and mÌ‚n being the Nadaraya-Watson
estimator, that is


Pn
xâˆ’Xj
Yj
j=1 K
hn

 ,
mÌ‚n (x) = P
xâˆ’Xj
n
j=1 K
hn

3

with kernel function K and bandwidth hn as considered in the assumptions below. Then
we want to estimate s0 by
)
(
sÌ‚n := min s : sup |TÌ‚n (s, z)| = sup sup |TÌ‚n (s, z)| .

(2.3)

sâˆˆ[0,1] zâˆˆRd

zâˆˆRd

Note that sÌ‚n = bnsÌ‚n c /n.
Remark. The advantage of using marked residuals in comparison to using the classical
CUSUM TÌ‚n (s, âˆ) to estimate the change point is that in the first case the estimator is
consistent for all changes of the form (2.2) whereas there are several examples in which
the use of TÌ‚n (s, âˆ) leads to a non-consistent estimator. To this end see the remark below
the proof of Theorem 3.1 and compare to Mohr and Neumeyer (2019).
Remark. Mohr and Neumeyer (2019) constructed procedures based on functionals of
TÌ‚n , e.g. a Kolmogorov-Smirnov test statistic supsâˆˆ[0,1] supzâˆˆRd |TÌ‚n (s, z)|, to test the null
hypothesis of no changes in the unknown regression function against change point alternatives as in (2.2). Given that such a test has rejected the null, the use of an M-estimator
as in (2.3) seems natural. Furthermore, CrameÌr-von Mises type test statistics of the form
R
supsâˆˆ[0,1] Rd |TÌ‚n (s, z)|2 Î½(z)dz for some integrable Î½ : Rd â†’ R were also considered by
Mohr and Neumeyer (2019). Assuming strict stationarity of the covariates and the existence of a density f such that Xt âˆ¼ f for all t, as in (X1) below, the CrameÌr-von Mises
approach from above with Î½ â‰¡ f leads to an alternative estimator for s0 , namely
( Z

 )
Z
1/2

|TÌ‚n (s, z)|2 f (z)dz

sÌƒn := min s :

1/2

|TÌ‚n (s, z)|2 f (z)dz

= sup

Rd

sâˆˆ[0,1]

.

Rd

R
However, to obtain a feasible estimator one needs to replace the integral Rd |TÌ‚n (s, z)|2 f (z)dz
P
by its empirical counterpart n1 nk=1 |TÌ‚n (s, Xk )|2 in practise as f is not known.

3

Asymptotic results

In this section we will derive asymptotic properties for sÌ‚n . To this end we introduce the
following assumptions.
(U) For all t âˆˆ Z let E[Ut |F t ] = 0 a.s. for F t = Ïƒ(Ujâˆ’1 , Xj : j â‰¤ t) and E[|Ut |q ] â‰¤ CU
for some CU < âˆ and q > 2.
(M) For all t âˆˆ Z let E[|m(1) (Xt ) âˆ’ m(2) (Xt )|r ] â‰¤ Cm for some Cm < âˆ and r > 2.
(P) Let {(Yt , Xt ) : 1 â‰¤ t â‰¤ n, n âˆˆ N} be strongly mixing with mixing coefficient Î±(Â·).
For q, r from assumptions (U) and (M) and b := min(q, r) let Î±(t) = O(tâˆ’Î±Ì„ ) with
some Î±Ì„ > (1 + (b âˆ’ 1)(1 + d))/(b âˆ’ 2).

4

(N) For b from assumption (P) let E[|Yt |b ] < âˆ and let Xt be absolutely continuous
with density function ft : Rd â†’ R that satisfies supxâˆˆRd E[|Yt |b |Xt = x]ft (x) < âˆ
and supxâˆˆRd ft (x) < âˆ for all t âˆˆ {1, . . . , n} and n âˆˆ N. Let there exist some
N â‰¥ 0 such that sup|iâˆ’j|â‰¥N supxi ,xj E[|Yi Yj ||Xi = xi , Xj = xj ]fij (xi , xj ) < âˆ for
all n âˆˆ N, where fij is the density function of (Xi , Xj ).
(J) Let (cn )nâˆˆN be a positive sequence of real valued numbers satisfying cn â†’ âˆ and
cn = O((log n)1/d ) and let Jn = [âˆ’cn , cn ]d .
(F) For some C < âˆ and cn from assumption (J) let In = [âˆ’cn âˆ’ Chn , cn + Chn ]d and
let Î´nâˆ’1 = inf xâˆˆJn inf 1â‰¤tâ‰¤n ft (x) > 0 for all n âˆˆ N. Further, let for all n âˆˆ N
pn = max sup sup |Dk ft (x)| < âˆ
|k|=1 xâˆˆIn 1â‰¤tâ‰¤n

0 < qn = max sup max |Dk m(j) (x)| < âˆ,
0â‰¤|k|â‰¤1 xâˆˆIn j=1,2

where |i| =

Pd

j=1 ij

and Di =

âˆ‚ |i|
i
i
âˆ‚x11 ...âˆ‚xdd

for i = (i1 , . . . , id ) âˆˆ Nd0 .

R
(K) Let K : Rd â†’ R be symmetric in each component with Rd K(z)dz = 1 and compact
support [âˆ’C, C]d . Additionally let |K(u)| < âˆ for all u âˆˆ Rd and |K(u)âˆ’K(u0 )| â‰¤
Î›ku âˆ’ u0 k for some Î› < âˆ and for all u, u0 âˆˆ Rd , where kxk = maxi=1,...,d |xi |.
(B) With b and Î±Ì„ from assumption (P) let
Î±Ì„ âˆ’ 1 âˆ’ d âˆ’
log (n)
=
o(1)
for
Î¸
=
nÎ¸ hdn
Î±Ì„ + 3 âˆ’ d âˆ’

1+Î±Ì„
bâˆ’1
1+Î±Ì„
bâˆ’1

.

For Î´n , pn , qn from assumption (F) let
s
!
log(n)
+ hn pn pn qn Î´n = o(nâˆ’Î¶ )
nhdn
for some Î¶ > 0.
(X1) For all 1 â‰¤ t â‰¤ n, n âˆˆ N let ft (Â·) = f (Â·), for some density f .
(X2) For all 1 â‰¤ t â‰¤ n, n âˆˆ N let ft (Â·) = f(1) (Â·) for all t = 1, . . . , bns0 c and ft (Â·) = f(2) (Â·)
for all t = bns0 c + 1, . . . , n, for some densities f(1) , f(2) .
Remark. The assumptions on the error terms and the mixing assumptions particularly
allow for conditional heteroscedasticity. Assumptions (U), (M) and (P) are a trade off
between the existence of moments and the rate of decay of the mixing coefficient. Assumptions (P), (N), (K) and the first part of (B) are reproduced from Kristensen (2012).
Together with (J) and (F), they are used to obtain uniform rates of convergence for mÌ‚n
stated in Lemma A.1 in the appendix. In (X1), we assume stationarity of the covariates
for the whole observation period, while in the case of (X2) we assume stationarity before
5

and right after the change occurs. Nevertheless both assumptions rule out general autoregressive effects such as Xt = (Ytâˆ’1 , . . . , Ytâˆ’d ). We will address this issue separately in
section 4.
Theorem 3.1. Assume (U), (M), (P), (N), (J), (F), (K) and (B). Furthermore let either
(X1) or (X2) hold. Then the change point estimator sÌ‚n is consistent, i. e.
|sÌ‚n âˆ’ s0 | = oP (1).
Theorem 3.2. Under the assumptions of Theorem 3.1 for the change point estimator sÌ‚n
it holds that
|sÌ‚n âˆ’ s0 | = OP (rnâˆ’1 ),
where rn = n.
The proofs of the theorems can be found in appendix A.2. We state both theorems
seperately since we need Theorem 3.1 to prove Theorem 3.2.
Remark. To obtain the rates of convergence we make use of the fact that sÌ‚n can be
expressed using the sup norm on lâˆ (Rd ), i.e.
N : lâˆ (Rd ) â†’ R, g 7â†’ N (g) := sup |g(z)|,
zâˆˆRd

where lâˆ (Rd ) is the space of all uniformly bounded real valued functions on Rd . Note that
similarly sÌƒn can be expressed using the L2 (P ) norm, when (Xt )t is strictly stationary with
marginal distribution P , namely
1/2
Z
2
âˆ
d
|g(z)| f (z)dz
.
NÌƒ : l (R ) â†’ R, g 7â†’ NÌƒ (g) :=
Rd

Using NÌƒ (g) â‰¤ N (g) for all g âˆˆ lâˆ (Rd ), corresponding results for sÌƒn as in Theorem 3.1
and Theorem 3.2 can be proven in a similar matter.

4

The autoregressive case

In this section we will consider the case where the exogenous variables include finitely
many lagged values of the endogenous variable, we will refer to this model as the autoregressive case. We will focus on one dimensional covariates, however, the results do
not depend on the dimension and can also be formulated for higher order autoregression
models. Consider the nonparametric autoregression
Yt = mt (Ytâˆ’1 ) + Ut , t = 1, . . . , n,

(4.1)

with unobservable innovations Ut and one change in the regression function occurring at
some unknown time bns0 c as in (2.2).
Furthermore assume the following.
6

(X3) For all 1 â‰¤ t â‰¤ n, n âˆˆ N let Xt := Ytâˆ’1 be absolutely continuous with density ft .
Let there exist densities f(1) and f(2) such that ft (Â·) = f(1) (Â·) for all t = 1, . . . , bns0 c
P
0c
and Rn (x) := n1 nj=bns0 c+1 fj (x) âˆ’ nâˆ’bns
f(2) (x) â†’ 0 for all x âˆˆ R and n â†’ âˆ.
n
Remark. Note that (X3) requires on the one hand strict stationarity up to the time of
change bns0 c. On the other hand the time series needs to reach its (new) stationary
distribution fast enough after the change. This is a generalization of (X2) where we
assumed stationarity both before and right after the change point, which can not be fulfilled
in the model (4.1). A necessary condition then is that there exists a stationary solution
of equation (4.1) under both m(1) (Â·) and m(2) (Â·) as regression functions.
Example. Consider the AR(1)-model
Yt = at Â· Ytâˆ’1 + Îµt
with standard normally distributed innovations (Îµt )t and at = a âˆˆ (âˆ’1, 1) for t â‰¤ bns0 c,
at = b âˆˆ (âˆ’1, 1) for t > bns0 c, a 6= b. Then assumption (X3) is fulfilled. Note to this
end that Xt := Ytâˆ’1 âˆ¼ N (0, 1/(1 âˆ’ a2 )) for t â‰¤ bns0 c. The distribution after the change

P
point is given by Xbns0 c+1+k âˆ¼ N 0, b2(k+1) /(1 âˆ’ a2 ) + ki=0 b2i for all k > 0. Thus with
Pjâˆ’bns0 câˆ’1 2i
b by the mean value theorem it holds for some Î¾j
Ïƒj2 := b2(jâˆ’bns0 c) /(1 âˆ’ a2 ) + i=0
2
2 âˆ’1
between Ïƒj and (1 âˆ’ b ) that
ï£«
ï£¶




n
2
2
X
1
x
x
1
ï£­q 1
ï£¸
Rn (x) =
exp âˆ’ 2 âˆ’ p
exp âˆ’
2 )âˆ’1
2
âˆ’1
n
2Ïƒ
2(1
âˆ’
b
2
2Ï€(1
âˆ’
b
)
j
2Ï€Ïƒj
j=bns0 c+1
!




n
2
2
1
1
1 X
1
x
1
x
âˆ’ Â·
=
Ïƒj2 âˆ’
exp âˆ’
+
1 Â·
2
n
1 âˆ’ b2
2Î¾j
2 (2Ï€Î¾j ) 32
(2Ï€Î¾j ) 2 2Î¾j
â‰¤ C

j=bns0 c+1
n
X

1
n

j=bns0 c+1

Ïƒj2 âˆ’

1
1 âˆ’ b2

for some constant C < âˆ for all x âˆˆ R. Further we can conclude
1
n

n
X

Ïƒj2

j=bns0 c+1

1
1
âˆ’
=
2
1âˆ’b
n

n
X
j=bns0 c+1

b2(jâˆ’bns0 c) 1 âˆ’ b2(jâˆ’bns0 c)
1
+
âˆ’
2
2
1âˆ’a
1âˆ’b
1 âˆ’ b2

1
1
1
=
âˆ’
2
2
1âˆ’a
1âˆ’b n

n
X

b2(jâˆ’bns0 c)

j=bns0 c+1

and thus Rn (x) âˆ’âˆ’âˆ’â†’ 0 for all x âˆˆ R.
nâ†’âˆ

In general verifying assumption (X3) for model (4.1) means to compare the distribution
of a stochastic process that is not yet in balance with its stationary distribution. A well
known technique to deal with this task is coupling, see e. g. Franke et al. (2002).
Under (X3) we get the following consistency result for our change point estimator in
the autoregressive case.
7

Theorem 4.1. Assume model (4.1) under (U), (M), (P), (N), (J), (F), (K), (B) and
(X3). Then the change point estimator sÌ‚n is consistent, i. e.
|sÌ‚n âˆ’ s0 | = oP (1).
The proof can be found in appendix A.2.
Remark. Another possibility to handle the autoregressive case would be to model the
change in a different way, namely
( (1)
(1) 
(1)
Yt = m(1) Ytâˆ’1 + Ut ,
t = 1, . . . , bns0 c
Yt =
,
m(1) 6â‰¡ m(2) ,
(2)
(2) 
(2)
Yt = m(2) Ytâˆ’1 + Ut ,
t = bns0 c + 1, . . . , n
(1) 
(2) 
for two stationary processes Yt t , Yt t , see e. g. Kirch et al. (2015). In this case
assumption (X2) is fulfilled and thus Theorem 3.1 and Theorem 3.2 apply.

5
5.1

Finite sample properties
Simulations

To investigate the finite sample performance of our estimator, we generate data from two
different basic models, namely
(IID) Yt = mt (Xt ) + Ïƒ(Xt )Îµt , where the observations (Xt )t are i.i.d., univariate and standard normally distributed, just as the errors (Îµt )t .
(TS) Yt = mt (Xt ) + Ïƒ(Xt )Îµt , where (Îµt )t i.i.d. âˆ¼ N (0, 1) and the univariate observations
(Xt )t stem from a time series Xt = 0.4Xtâˆ’1 + Î·t with standard normal innovations
(Î·t )t .
For both models we generate data both for the homoscedastic case Ïƒ â‰¡ 1 as well as for
âˆš
the heteroscedastic case Ïƒ(x) = 1 + 0.5x2 . The results for both are very similar in all
situations, thus we only present the results for the heteroscedastic case. To model the
change in the regression function we use three different scenarios
(
âˆ’0.5x,
t = 1, . . . , bns0 c
(C1) mt =
0.5x
t = bns0 c + 1, . . . , n,
(C2) mt =

(C3) mt =

(
0.1x,
0.9x

t = 1, . . . , bns0 c
t = bns0 c + 1, . . . , n,

(
0.5x,
(0.5 + 3 exp(âˆ’0.8x2 ))x

t = 1, . . . , bns0 c
t = bns0 c + 1, . . . , n,

8

where we let s0 range from 0.1 to 0.9. In Figure 1 the results for 1000 replications and
sample sizes n = 100, 500, 1000 are shown, where we plot s0 against the estimated mean
squared error of our estimator sÌ‚n . The kernel for mÌ‚n is chosen as the Epanechnikov
kernel of order four and the bandwidth is determined by a cross-validation method. It
can be seen that our estimator performs quite well even for the smallest sample size
n = 100 when s0 is 0.5 or close to it whereas for a change point that lies closer to the
boundaries of the observation interval a larger sample size is needed to get satisfying
results. This is due to the fact that if s0 = 0.1 or s0 = 0.9 there are only 10 observations
before and after the change point respectively for n = 100 and thus the estimation of
m(1) and m(2) respectively are poor. Moreover an asymmetry in the results is striking.
This stems from the CUSUM type statistic that our estimator is based on. For s0 = 0.1
e. g. the sum consists of only 0.1n summands and thus the estimation of e. g. E[Ut ] is
worse than if s0 = 0.9 and the estimation is based on 0.9n summands. The effect of a
decreasing performance of the estimators the closer s0 gets to the boundaries is typical
for change point estimators based on CUSUM statistics and can be antagonized by the
use of appropriate weights, see e. g. Ferger (2005).
To stress our estimator a little further we simulate the scenario that there is also a
change in the variance function Ïƒ at a different time point than the change in the regression
function m. In this situation the estimator should still be able to detect s0 , the change
point in the regression function. The results are shown in Figure 2 for model (IID) and
âˆš
model (TS) with change point scenario (C1) where Ïƒt (x) = 1 + 0.1x2 for t â‰¤ 0.4n and
âˆš
Ïƒt (x) = 1 + 0.8x2 for t > 0.4n. They confirm the good performance of our estimator
even in this more difficult situation.
As discussed in section 4 our estimator can also be applied to the autoregressive case.
To investigate the finite sample performance in this situation we generate data according
to the model
(AR) Yt = mt (Ytâˆ’1 ) + Ïƒ(Ytâˆ’1 )Îµt , where (Îµt )t i.i.d. âˆ¼ N (0, 1).
For Ïƒ â‰¡ 1 and change point scenario (C1) as well as (C2) assumption (X3) is fulfilled,
see the example in section 4. Simulation results for these cases are shown in Figure 3
where the setting is the same as described above. They look very similar to the results
of model (IID) and (TS) and thus confirm the theoretical result of Theorem 4.1. Even
for examples where assumption (X3) can not be verified easily the performance of our
estimator is satisfying, see Figure 4 for model (AR) with Ïƒ â‰¡ 1 and change point scenario
âˆš
(C3) as well as the heteroscedastic model (AR) with Ïƒ = 1 + 0.5x2 and change point
scenario (C1).
As stated in the remark in section 2 it is also possible to base the estimator on a
CrameÌr-von Mises type functional of the marked empirical process of residuals. The
simulation results for this type of estimator are very similar to those presented here for
the Kolmogorov-Smirnov type estimator sÌ‚n and are omitted for the sake of brevity.

9

0.10

MSE

n=100
n=500
n=1000

0.00

0.10

n=100
n=500
n=1000

0.00

MSE

0.20

(TS)

0.20

(IID)

0.2

0.4

0.6

0.8

0.2

0.4

0.8

0.20
0.10

MSE

n=100
n=500
n=1000

0.00

0.10
0.00

MSE

n=100
n=500
n=1000

0.2

0.4

0.6

0.8

0.2

0.4

0.6

0.8

0.20

s0

MSE

n=100
n=500
n=1000

0.00

0.00

0.10

n=100
n=500
n=1000

0.10

0.20

s0

MSE

0.6
s0

0.20

s0

0.2

0.4

0.6

0.8

0.2

s0

0.4

0.6

0.8

s0

Figure 1: Simulation results for model (IID) (left), model (TS) (right) and change point
scenario (C1) (top), change point scenario (C2) (middle), change point scenario (C3)
(bottom)

5.2

Data example

Finally, we will consider a real data example. The data at hand contains 36 measurements
of the annual flow volume of the small Czech river, RaÌztoka, recorded between 1954 and
1989 as well as the annual rainfall during that time. It was considered by HusÌŒkovaÌ and
Antoch (2003) to investigate the effect of controlled deforestation on the capability for
water retention of the soil. To this end it is of interest if and when the relationship
between rainfall and flow volume changes. We set Xt as the annual rainfall and Yt as
the annual flow volume. Mohr and Neumeyer (2019) applied their Kolmogorov-Smirnov
test to this data set, which clearly rejects the null of no change in the conditional mean
function, indicating the existence of a change in the relationship between rainfall and flow
volume. Using sÌ‚n to estimate the unknown time of change suggests a change in 1979.
Note that this is consistent with the literature. As was pointed out by HusÌŒkovaÌ and
Antoch (2003) large scale deforestation had started around that time. Figure 5 shows on
the left-hand side the scatterplot Xt against Yt using dots for the observations after the
10

0.10

MSE

n=100
n=500
n=1000

0.00

0.10

n=100
n=500
n=1000

0.00

MSE

0.20

(TS)

0.20

(IID)

0.2

0.4

0.6

0.8

0.2

0.4

s0

0.6

0.8

s0

0.20
0.10

MSE

n=100
n=500
n=1000

0.00

0.10

n=100
n=500
n=1000

0.00

MSE

0.20

Figure 2: Simulation results for model (IID) (left) and model (TS) (right) with change
point scenario (C1) and an additional change in the variance function

0.2

0.4

0.6

0.8

0.2

s0

0.4

0.6

0.8

s0

Figure 3: Simulation results for model (AR) and change point scenario (C1) (left), change
point scenario (C2) (right)
estimated change and crosses for the observations before the estimated change. On the
right-hand side the figure shows the cumulative sum, n1/2 supzâˆˆR |TÌ‚n (Â·, z)|, as well as the
critical value of the test used in Mohr and Neumeyer (2019) (red horizontal line) and the
estimated change (green vertical line). Note that sÌƒn leads to the same result.

6

Concluding remarks

In this paper we consider nonparametric regression models with a change in the unknown
regression function that allows for time series data as well as conditional heteroscedasticity.
We propose an estimator for the rescaled change point that is based on the sequential
marked empirical process of residuals and show consistency as well as a rate of convergence
of OP (nâˆ’1 ). In an autoregressive setting we additionally give a consistency result for the
proposed estimator.
If more than one change occurs, the proposed estimator is not consistent for one of
the changes in some situations. For detecting multiple changes we refer the reader to
alternative procedures such as the MOSUM procedure proposed by Kirch and Eichinger
(2018) or the wild binary segmentation procedure by Fryzlewicz (2014) (see also Fryzlewicz

11

0.20
0.00

MSE

n=100
n=500
n=1000

0.10

0.20
0.10
0.00

MSE

n=100
n=500
n=1000

0.2

0.4

0.6

0.8

0.2

0.4

0.6

s0

0.8

s0

2.0
1.5
0.5

1.0

Cumulative sum

600
500
400
300

0.0

100

200

Annual flow volume

700

Figure 4: Simulation results for homoscedastic model (AR) with change point scenario
(C3) (left) and heteroscedastic model (AR) with change point scenario (C1) (right)

400

500

600

700

800

900

1000

1955

Annual rainfall

1965

1975

1985

Year

Figure 5: RaÌztoka data: scatterplot (left) and CUSUM (right)
(2019)).
Investigating the asymptotic distribution of the proposed estimator is a subsequent
issue. Certainly, it is of great interest as it can be used to obtain confidence intervals.
However, this subject goes beyond the scope of the paper at hand and is postponed to
future research.

A
A.1

Proofs
Auxiliary results

Lemma A.1. Under the assumptions (P), (N), (J), (F), (K) and (B), it holds that
s
!
!
log(n)
sup |mÌ‚n (x) âˆ’ mÌ„n (x)| = OP
+ hn pn Î´n pn qn ,
nhdn
xâˆˆJn
where

Pn
f (x)mi (x)
Pn i
mÌ„n (x) = i=1
.
i=1 fi (x)
12

The proof is similar to the proof of Lemma 2.2 in Mohr (2018). The key tool is an
application of Theorem 1 in Kristensen (2009). Details are omitted for the sake of brevity.
Remark. Under (X1) we have
mÌ„n (x) =

bns0 c
n âˆ’ bns0 c
m(1) (x) +
m(2) (x),
n
n

under (X2) and (X3) we have
bns0 c
f(1) (x)
n

mÌ„n (x) =

fÂ¯n (x)

(m(1) (x) âˆ’ m(2) (x)) + m(2) (x),

where
n

1X
fÂ¯n (x) :=
fi (x) =
n i=1

(

bns0 c
f(1) (x)
n
bns0 c
f(1) (x)
n

+
+

nâˆ’bns0 c
f(2) (x),
n
nâˆ’bns0 c
f(2) (x) +
n

for (X2)
Rn (x), for (X3)

with Rn (Â·) from assumption (X3).
The proofs of the following lemmata can be found in appendix A.3.
Lemma A.2. Under the assumptions of Theorem 3.1 as well as under those of Theorem
4.1 there exists a constant CÌ„ = CÌ„(C) < âˆ such that
ï£«
ï£¶
L+bÎºn sc
1
X
âˆ’1
P ï£­ sup sup
Ui Ï‰n (Xi )I{Xi â‰¤ z} > CÎºn ï£¸ â‰¤ CÌ„Îºnq
sâˆˆ[0,1] zâˆˆRd

i=L+1

for all L = 0, 1, . . . , n âˆ’ Îºn , 1 â‰¤ Îºn â‰¤ n, n âˆˆ N and all C > 0 with q from assumption
(U).
Lemma A.3. Under the assumptions of Theorem 3.1 as well as under those of Theorem
4.1 there exists a constant CÌ„ = CÌ„(C) < âˆ such that
P

(L+bÎºn sc)âˆ§bns0 c 

X

sup sup
sâˆˆ[0,1] zâˆˆRd

(m(1) (Xi ) âˆ’ mÌ„n (Xi ))Ï‰n (Xi )I{Xi â‰¤ z}

i=L+1



âˆ’E (m(1) (Xi ) âˆ’ mÌ„n (Xi ))Ï‰n (Xi )I{Xi â‰¤ z}



!
> CÎºn

1

â‰¤ CÌ„Îºnr

âˆ’1

and
L+bÎºn sc

P

sup sup
sâˆˆ[0,1] zâˆˆRd



X

(m(2) (Xi ) âˆ’ mÌ„n (Xi ))Ï‰n (Xi )I{Xi â‰¤ z}

i=Lâˆ¨bns0 c+1



âˆ’E (m(2) (Xi ) âˆ’ mÌ„n (Xi ))Ï‰n (Xi )I{Xi â‰¤ z}



!
> CÎºn

1

â‰¤ CÌ„Îºnr

for all L = 0, 1, . . . , n âˆ’ Îºn , 1 â‰¤ Îºn â‰¤ n, n âˆˆ N and all C > 0 with r from assumption
(M).
13

âˆ’1

Lemma A.4. Under the assumptions of Theorem 3.1 as well as under those of Theorem
4.1 it holds
ï£«
ï£¶
L+bÎºn sc
X
P ï£­ sup sup
(mÌ„n (Xi ) âˆ’ mÌ‚n (Xi ))Ï‰n (Xi )I{Xi â‰¤ z} > CÎºn ï£¸ â‰¤ C âˆ’1 Îºâˆ’Î¶
n
sâˆˆ[0,1] zâˆˆRd

i=L+1

for all L = 0, 1, . . . , n âˆ’ Îºn , 1 â‰¤ Îºn â‰¤ n, n âˆˆ N and all C > 0 with Î¶ > 0 from assumption
(B).

A.2

Proof of main results

We will proof Theorem 3.1 under the assumption (X1) and simply make a note on the
parts that change under (X2).
Proof of Theorem 3.1. First note that for all s âˆˆ [0, 1] and z âˆˆ Rd
TÌ‚n (s, z) = An (s, z) + âˆ†n,1 (s)âˆ†n,2 (z),

(A.1)

where An (s, z) = An,1 (s, z) + An,2 (s, z) + An,3 (s, z) + An,4 (s, z) with
bnsc

1X
An,1 (s, z) :=
Ui Ï‰n (Xi )I{Xi â‰¤ z}
n i=1
1
An,2 (s, z) :=
n

bn(sâˆ§s0 )c 

X

(A.2)

(m(1) (Xi ) âˆ’ mÌ„n (Xi ))Ï‰n (Xi )I{Xi â‰¤ z}

i=1



âˆ’ E (m(1) (Xi ) âˆ’ mÌ„n (Xi ))Ï‰n (Xi )I{Xi â‰¤ z}
1
An,3 (s, z) := I{s > s0 }
n

bnsc
X





(A.3)

(m(2) (Xi ) âˆ’ mÌ„n (Xi ))Ï‰n (Xi )I{Xi â‰¤ z}

i=bns0 c+1



âˆ’ E (m(2) (Xi ) âˆ’ mÌ„n (Xi ))Ï‰n (Xi )I{Xi â‰¤ z}

(A.4)

bnsc

1X
An,4 (s, z) :=
(mÌ„n (Xi ) âˆ’ mÌ‚n (Xi ))Ï‰n (Xi )I{Xi â‰¤ z}
n i=1
and
n âˆ’ bns0 c bnsc
n âˆ’ bnsc bns0 c
âˆ†n,1 (s) := I{s â‰¤ s0 }
+ I{s > s0 }
n
n
n
n
Z
âˆ†n,2 (z) :=
(m(1) (x) âˆ’ m(2) (x))f (x)Ï‰n (x)dx,
(âˆ’âˆ,z]

since by inserting the definition of mÌ„n we obtain for s â‰¤ s0
bnsc


1X 
E (m(1) (Xi ) âˆ’ mÌ„n (Xi ))Ï‰n (Xi )I{Xi â‰¤ z}
n i=1
14

(A.5)

bnsc


n âˆ’ bns0 c 1 X 
=
E (m(1) (Xi ) âˆ’ m(2) (Xi ))Ï‰n (Xi )I{Xi â‰¤ z}
n
n i=1
=

n âˆ’ bns0 c bnsc
âˆ†n,2 (z)
n
n

and for s > s0
bns0 c

1 X 
E (m(1) (Xi ) âˆ’ mÌ„n (Xi ))Ï‰n (Xi )I{Xi â‰¤ z}
n i=1

1
+
n

bnsc
X



E (m(2) (Xi ) âˆ’ mÌ„n (Xi ))Ï‰n (Xi )I{Xi â‰¤ z}

i=bns0 c+1

bns0 c

n âˆ’ bns0 c 1 X 
E (m(1) (Xi ) âˆ’ m(2) (Xi ))Ï‰n (Xi )I{Xi â‰¤ z}
=
n
n i=1

bns0 c 1
âˆ’
n n

bnsc
X



E (m(1) (Xi ) âˆ’ m(2) (Xi ))Ï‰n (Xi )I{Xi â‰¤ z}

i=bns0 c+1

n âˆ’ bnsc bns0 c
âˆ†n,2 (z).
n
n
R
R zd
R z1
Note that we use the notation (âˆ,z] g(x)dx = âˆ’âˆ
Â· Â· Â· âˆ’âˆ
g(x1 , . . . , xd )dx1 . . . dxd here.
Due to the dominated convergence theorem and assumption (M), it holds that
=

âˆ†n,1 (s)âˆ†n,2 (z) = âˆ†1 (s)âˆ†2 (z) + o(1),
uniformly in s âˆˆ [0, 1] and z âˆˆ Rd , where
âˆ†1 (s) := I{s â‰¤ s0 }(1 âˆ’ s0 )s + I{s > s0 }(1 âˆ’ s)s0 ,
Z
âˆ†2 (z) :=
(m(1) (x) âˆ’ m(2) (x))f (x)dx.
(âˆ’âˆ,z]

Note that under (X2) the same assertion holds with
Z
f(1) (x)f(2) (x)
âˆ†n,2 (z) :=
(m(1) (x) âˆ’ m(2) (x)) bns0 c
Ï‰n (x)dx
nâˆ’bns0 c
f
(x)
+
f
(x)
(âˆ’âˆ,z]
(1)
(2)
n
n
and
Z
(m(1) (x) âˆ’ m(2) (x))

âˆ†2 (z) :=
(âˆ’âˆ,z]

f(1) (x)f(2) (x)
dx.
s0 f(1) (x) + (1 âˆ’ s0 )f(2) (x)

By Lemma A.2, Lemma A.3 and Lemma A.4 with Îºn = n, it holds that An (s, z) = oP (1)
uniformly in s âˆˆ [0, 1] and z âˆˆ Rd . Hence, we have shown that
sup |TÌ‚n (s, z)| = âˆ†1 (s) sup |âˆ†2 (z)| + oP (1)
zâˆˆRd

zâˆˆRd

uniformly in s âˆˆ [0, 1] under both cases (X1) and (X2). The assertion then follows by
Theorem 2.12 in Kosorok (2008) as s0 is well-separated maximum of [0, 1] â†’ R, s 7â†’
âˆ†1 (s).
15

Remark. Note that there are examples of m(1) , m(2) and f resp. f(1) , f(2) that lead to
âˆ†2 (âˆ) = 0. In those cases a change point estimator based on the classical CUSUM
TÌ‚n (s, âˆ) is not consistent.
Proof of Theorem 3.2. First note that s0 =
consider

bnsÌ‚n c
n

âˆ’

bns0 c
n

bns0 c
n

+ O(nâˆ’1 ) and sÌ‚n =

bnsÌ‚n c
.
n

Thus we can

instead of |sÌ‚n âˆ’ s0 |. The proof follows mainly along the same lines

as the proof of Theorem 1 in Hariz et al. (2007). Consider the norm N : lâˆ (Rd ) â†’ R, g 7â†’
supzâˆˆRd |g(z)| and let M > 0. We will show below that for all Î· > 0 and b, c > 0 it holds




bnsÌ‚n c bns0 c
bnsÌ‚n c bns0 c
âˆ’1 M
M
= P rn 2 <
P rn
âˆ’
>2
âˆ’
â‰¤Î·
n
n
n
n


bnsÌ‚n c bns0 c
+P
âˆ’
>Î·
n
n
â‰¤ En,1 + En,2 + En,3 + En,4 ,

(A.6)

where

En,1 := P

rnâˆ’1 2M

bnsÌ‚n c bns0 c
bnsÌ‚n c bns0 c
âˆ’
â‰¤ Î·, N (An (sÌ‚n , Â·) âˆ’ An (s0 , Â·)) â‰¥ C
âˆ’
<
n
n
n
n



En,2 := P (N (An (s0 , Â·)) > c)
En,3 := P (âˆ†n,1 (s0 )N (âˆ†n,2 (Â·)) â‰¤ b)
En,4 := P (|sÌ‚n âˆ’ s0 | > Î·) ,
with C := b âˆ’ 2c. Now it holds that En,4 â†’ 0 for all Î· > 0, due to Theorem 3.1.
Further, En,2 â†’ 0 for all c > 0 as An (s0 , z) = oP (1) holds uniformly in z âˆˆ Rd . Finally
choose b > 0 and n0 = n0 (b) âˆˆ N such that En,3 = 0 for all n â‰¥ n0 , which exists as
âˆ†1 (s0 )N (âˆ†2 (Â·)) > 0 and âˆ†n,1 (s0 )N (âˆ†n,2 (Â·)) = âˆ†1 (s0 )N (âˆ†2 (Â·)) + o(1). We then choose
c > 0 such that b âˆ’ 2c > 0. To see the validity of (A.6) first note that for all s âˆˆ [0, 1]
TÌ‚n (s, Â·) = An (s, Â·) + âˆ†n,1 (s)âˆ†n,2 (Â·)


âˆ†n,1 (s)
âˆ†n,1 (s)
= An (s, Â·) âˆ’ An (s0 , Â·) + An (s0 , Â·) 1 âˆ’
+
TÌ‚n (s0 , Â·).
âˆ†n,1 (s0 )
âˆ†n,1 (s0 )
Applying the norm and triangular inequality we obtain for all s âˆˆ [0, 1]




âˆ†n,1 (s)
âˆ†n,1 (s)
N (TÌ‚n (s, Â·)) â‰¤ N (An (s, Â·) âˆ’ An (s0 , Â·)) + 1 âˆ’
N (An (s0 , Â·)) +
N (TÌ‚n (s0 , Â·))
âˆ†n,1 (s0 )
âˆ†n,1 (s0 )
which is equivalent to

N (TÌ‚n (s, Â·)) âˆ’ N (TÌ‚n (s0 , Â·)) â‰¤ N (An (s, Â·) âˆ’ An (s0 , Â·)) +



âˆ†n,1 (s)
âˆ’ 1 N (TÌ‚n (s0 , Â·)) âˆ’ N (An (s0 , Â·)) .
âˆ†n,1 (s0 )

Due to the definition of sÌ‚n it holds that N (TÌ‚n (sÌ‚n , Â·)) âˆ’ N (TÌ‚n (s0 , Â·)) â‰¥ 0. Additionally
using the specific definition of âˆ†n,1 we obtain



âˆ†n,1 (sÌ‚n ) 
N (An (sÌ‚n , Â·) âˆ’ An (s0 , Â·)) â‰¥ 1 âˆ’
N (TÌ‚n (s0 , Â·)) âˆ’ N (An (s0 , Â·))
âˆ†n,1 (s0 )
16


â‰¥ min
|
â‰¥



bnsÌ‚n c bns0 c 
n
n
,
âˆ’
N (TÌ‚n (s0 , Â·)) âˆ’ N (An (s0 , Â·))
bns0 c n âˆ’ bns0 c
n
n
{z
}
>1

bnsÌ‚n c bns0 c
âˆ’
(âˆ†n,1 (s0 )N (âˆ†n,2 (Â·)) âˆ’ 2N (An (s0 , Â·))) ,
n
n

where we again make use of the triangular inequality in the last step. Putting the results
together we obtain


bnsÌ‚n c bns0 c
âˆ’1 M
P rn 2 <
âˆ’
â‰¤Î·
n
n


bnsÌ‚n c bns0 c
âˆ’1 M
âˆ’
â‰¤ Î·, âˆ†n,1 (s0 )N (âˆ†n,2 (Â·)) > b, N (An (s0 , Â·)) â‰¤ c
â‰¤ P rn 2 <
n
n
+ P (âˆ†n,1 (s0 )N (âˆ†n,2 (Â·)) â‰¤ b) + P (N (An (s0 , Â·)) > c)


bnsÌ‚n c bns0 c
bnsÌ‚n c bns0 c
âˆ’1 M
âˆ’
â‰¤ Î·, N (An (sÌ‚n , Â·) âˆ’ An (s0 , Â·)) â‰¥ C
âˆ’
â‰¤ P rn 2 <
n
n
n
n
+ P (âˆ†n,1 (s0 )N (âˆ†n,2 (Â·)) â‰¤ b) + P (N (An (s0 , Â·)) > c).
Finally we will investigate En,1 . To do this we define shells


bns0 c
l
l+1
Sn,l = t âˆˆ [0, 1] : 2 < rn t âˆ’
â‰¤2
n
and choose Ln = Ln (Î·) such that 2Ln < rn Î· â‰¤ 2Ln +1 for some Î· â‰¤ 21 . Then
En,1

Ln
X



bnsÌ‚n c
bnsÌ‚n c bns0 c
P
âˆˆ Sn,l , N (An (sÌ‚n , Â·) âˆ’ An (s0 , Â·)) â‰¥ C
âˆ’
â‰¤
n
n
n
l=M
ï£«
ï£¶
Ln
X
Pï£­
sup
â‰¤
N (An (s, Â·) âˆ’ An (s0 , Â·)) â‰¥ C2l rnâˆ’1 ï£¸
s:

l=M

â‰¤

Ln X
4
X

â‰¤ CÌƒ

âˆ’1
â‰¤2l+1 rn

ï£«

ï£¶

Pï£­

n
rn

N (An,i (s, Â·) âˆ’ An,i (s0 , Â·)) â‰¥

sup
s:

l=M i=1



bns c
bnsc
âˆ’ n0
n



bns c
bnsc
âˆ’ n0
n

 1q âˆ’1 X
Ln
l=M

(2

1
âˆ’1
q

âˆ’1
â‰¤2l+1 rn

l

) +



n
rn

 r1 âˆ’1 X
Ln
l=M

(2

1
âˆ’1
r

l

) +



n
rn

C l âˆ’1 ï£¸
2r
4 n

âˆ’Î¶ X
Ln

!
âˆ’Î¶ l

(2 )

l=M

j
k
for some constant CÌƒ < âˆ by Lemmata A.2, A.3 and A.4 with Îºn = 2l+1 rnn with q from
assumption (U), r from assumption (M) and Î¶ > 0 from assumption (B). Now choosing
rn = n and letting n and thus Ln tend to infinity and then M to infinity, the assertion of
Theorem 3.2 follows.
Proof of Theorem 4.1. Under (X3) we have for all s âˆˆ [0, 1] and z âˆˆ R
Ëœ n (s, z),
TÌ‚n (s, z) = An (s, z) + âˆ†n,1 (s)âˆ†n,2 (z) + âˆ†
17

with An (s, z) and âˆ†n,1 (s) from the proof of Theorem 3.1, and with
Z
f(1) (x)f(2) (x)
(m(1) (x) âˆ’ m(2) (x)) bns0 c
Ï‰n (x)dx
âˆ†n,2 (z) :=
nâˆ’bns0 c
f
(x)
+
f
(x)
+
R
(x)
(âˆ’âˆ,z]
n
(1)
(2)
n
n
and
Ëœ n (s, z) :=
âˆ†

Z
(m(1) (x)âˆ’m(2) (x))I{s â‰¤ s0 }
(âˆ’âˆ,z]

f(1) (x)Rn (x)
bnsc
Ï‰n (x)dx.
bns
c
0
n
f(1) (x) + nâˆ’bns0 c f(2) (x) + Rn (x)
n

n

Now it holds that
Z
âˆ†n,2 (z) â†’

(m(1) (x) âˆ’ m(2) (x))
(âˆ’âˆ,z]

f(1) (x)f(2) (x)
dx =: âˆ†2 (z)
s0 f(1) (x) + (1 âˆ’ s0 )f(2) (x)

Ëœ n (s, z) â†’ 0 uniformly in s âˆˆ [0, 1] and z âˆˆ R, due to dominated convergence and
and âˆ†
assumption (M). Hence we have uniformly in s and z
TÌ‚n (s, z) = An (s, z) + âˆ†1 (s)âˆ†2 (z) + o(1),
with âˆ†1 (s) as in the proof of Theorem 3.1. The rest goes analogously to the proof of
Theorem 3.1.
Remark. Note that for finite n âˆˆ N we do not get the decomposition of TÌ‚n as in (A.1) in
the proof of Theorem 3.1. We only obtain this kind of decomposition when letting n tend
to infinity. The decomposition for finite n, however, is essential for the proof of the rates
of convergence in Theorem 3.2.

A.3

Proofs of lemmata

Proof of Lemma A.2. The proof follows along similar lines as the proof of Lemma A.3 in
Mohr (2018). Throughout the proof the values of C and CÌ„ may vary from line to line
but they are always positive, finite and independent of n. Further note that deterministic
terms that are of order O(Îºn ) can be omitted as we can choose constants appropriately.
It holds that
L+bÎºn sc

sup sup
sâˆˆ[0,1] zâˆˆRd

X

Ui Ï‰n (Xi )I{Xi â‰¤ z}

i=L+1

ï£®

L+bÎºn sc

= sup sup
sâˆˆ[0,1] zâˆˆRd

X

Ui Ï‰n (Xi )I{Xi â‰¤ z} âˆ’ E ï£°

i=L+1

sâˆˆ[0,1] zâˆˆRd

X

X

Ui Ï‰n (Xi )I{Xi â‰¤ z}ï£»

i=L+1

L+bÎºn sc

â‰¤ sup sup

ï£¹

L+bÎºn sc

1

Ui I{|Ui | > Îºnq }Ï‰n (Xi )I{Xi â‰¤ z}

i=L+1

ï£®

ï£¹

L+bÎºn sc

âˆ’E ï£°

X

1
q

Ui I{|Ui | > Îºn }Ï‰n (Xi )I{Xi â‰¤ z}ï£»

i=L+1

18

(A.7)

L+bÎºn sc

+ sup sup
sâˆˆ[0,1] zâˆˆRd

1

X

Ui I{|Ui | â‰¤ Îºnq }Ï‰n (Xi )I{Xi â‰¤ z}

i=L+1

ï£¹

ï£®

L+bÎºn sc

1
q

X

âˆ’E ï£°

Ui I{|Ui | â‰¤ Îºn }Ï‰n (Xi )I{Xi â‰¤ z}ï£»

(A.8)

i=L+1

where (A.7) is of the desired rate in probability since
!
L+Îº
1
1
Xn
âˆ’1
q
P
|Ui |I{|Ui | > Îºn } > CÎºn â‰¤ C âˆ’1 CU Îºnq
i=L+1

by Markovâ€™s inequality with




1
1
q
q
q
âˆ’(qâˆ’1)
E |Ui |I{|Ui | > Îºn } = E |Ui | |Ui |
I{|Ui | > Îºn }
âˆ’ qâˆ’1
q

â‰¤ Îºn

E[|Ui |q ]

1
âˆ’1
q

â‰¤ CU Îºn

for all i and for CU < âˆ from assumption (U).

Considering the term (A.8) we define the function class


1
q
d
Fn := (u, x) 7â†’ uI{|u| â‰¤ Îºn }Ï‰n (x)I{x â‰¤ z} : z âˆˆ R
to rewrite the assertion as
ï£«
P ï£­ sup sup

sâˆˆ[0,1] Ï•âˆˆFn

L+bÎºn sc 

X



Z

Ï•(Ui , Xi ) âˆ’

Ï•dP

ï£¶
1

> CÎºn ï£¸ â‰¤ CÌ„Îºnq

âˆ’1

.

i=L+1

Now we will cover [0, 1] by finitely many intervals and Fn by finitely many brackets to
replace the supremum by a maximum. Let therefore
0 = s1 < . . . < sKn = 1
âˆ’1

part the interval [0, 1] in Kn subintervals of length Â¯n with Â¯n = Îºn q . Then
L+bÎºn sc 

sup sup
sâˆˆ[0,1] Ï•âˆˆFn

X



Z
Ï•(Ui , Xi ) âˆ’

Ï•dP

i=L+1
L+bÎºn sc 

= max
k

sup

sup

sâˆˆ[0,1] Ï•âˆˆFn
|sâˆ’sk |â‰¤Â¯
n

X

Ï•(Ui , Xi ) âˆ’

â‰¤ max sup
k

Ï•âˆˆFn

X

Ï•dP

i=L+1

L+bÎºn sk c 

Ï•(Ui , Xi ) âˆ’



Z



Z
Ï•dP

i=L+1

19

+ max
k

sup

L+Îº
Xn

sup



Z
Ï•(Ui , Xi ) âˆ’
{z

sâˆˆ[0,1] Ï•âˆˆFn i=L+1
|
|sâˆ’sk |â‰¤Â¯
n

Ï•dP I
}

1




iâˆ’L
iâˆ’L
â‰¤s âˆ’I
â‰¤ sk
Îºn
Îºn

â‰¤2Îºnq
L+bÎºn sk c 

X

â‰¤ max sup
k

Ï•âˆˆFn



Z

Ï•(Ui , Xi ) âˆ’

1

+ 2Îºnq (Îºn Â¯n + 1)

Ï•dP

i=L+1

1

1

and 2Îºnq (Îºn Â¯n + 1) = 2(Îºn + Îºnq ) = O(Îºn ). Further let
1

1

Ï•uj (u, x) := uI{|u| â‰¤ Îºnq }I{u â‰¥ 0}Ï‰n (x)I{x â‰¤ zj }+uI{|u| â‰¤ Îºnq }I{u < 0}Ï‰n (x)I{x â‰¤ zjâˆ’1 }
and
1

1

Ï•lj (u, x) := uI{|u| â‰¤ Îºnq }I{u â‰¥ 0}Ï‰n (x)I{x â‰¤ zjâˆ’1 }+uI{|u| â‰¤ Îºnq }I{u < 0}Ï‰n (x)I{x â‰¤ zj }
form the brackets [Ï•lj , Ï•uj ]jâˆˆÃ—di=1 {1,...,Ni } of Fn , where zj = (zj1 ,1 , . . . , zjd ,d ) and
âˆ’âˆ = z0,i < . . . < zNi ,i = âˆ
gives a partition of R for all i = 1, . . . , d. The total number of brackets Jn := N[ ] (n , Fn , kÂ·
kL1 (P ) ) needed to cover Fn is of order Jn = O(âˆ’d
n ), which follows analogously to but easier
than the proof of Lemma A.7 in Mohr (2018).
For all Ï• âˆˆ Fn there exists a j with Ï•lj â‰¤ Ï• â‰¤ Ï•uj and thus
Z
Z
Z
u
u
Ï• âˆ’ Ï•dP â‰¤ Ï•j âˆ’ Ï•j dP + (Ï•uj âˆ’ Ï•lj )dP
and

Z
Ï•âˆ’

Ï•dP â‰¥

Ï•lj

Z
âˆ’

Ï•lj dP

Z
âˆ’

(Ï•uj âˆ’ Ï•lj )dP.

Therefore for all s âˆˆ [0, 1]
L+bÎºn sc 

X

sup
Ï•âˆˆFn



Z
Ï•(Ui , Xi ) âˆ’

Ï•dP

i=L+1
L+bÎºn sc 

= max
j

sup
Ï•âˆˆ[Ï•lj ,Ï•u
j]

â‰¤ max max
j

ï£±
ï£²
ï£³
Z

+Îºn max
j

X



Z
Ï•(Ui , Xi ) âˆ’

Ï•dP

i=L+1
L+bÎºn sc 

X

Ï•uj (Ui , Xi ) âˆ’

Z

Ï•uj dP

i=L+1

L+bÎºn sc 


,

X

i=L+1

(Ï•uj âˆ’ Ï•lj )dP
{z
}
|
â‰¤n

20

Ï•lj (Ui , Xi ) âˆ’

Z

Ï•lj dP

ï£¼
ï£½
ï£¾

and Îºn n = O(Îºn ) if we choose n constant. Thus it remains to show that
ï£«
ï£¶

Z
L+bÎºn sk c 
1
X
âˆ’1
Ï•uj (Ui , Xi ) âˆ’ Ï•uj dP > CÎºn ï£¸ â‰¤ CÌ„Îºnq
P ï£­max
j,k

i=L+1

and the same with Ï•uj replaced by Ï•lj . Recall that
L+bÎºn sk c 

X

max
j,k

â‰¤ max

Ï•uj (Ui , Xi )

Z

Ï•uj dP

âˆ’



i=L+1
L+bÎºn sk c 

X

j,k

1

Ui I{|Ui | â‰¤ Îºnq }I{Ui â‰¥ 0}Ï‰n (Xi )I{Xi â‰¤ zj }

i=L+1



1
q
âˆ’E Ui I{|Ui | â‰¤ Îºn }I{Ui â‰¥ 0}Ï‰n (Xi )I{Xi â‰¤ zj }
+ max
j,k

L+bÎºn sk c 

X

(A.9)

1

Ui I{|Ui | â‰¤ Îºnq }I{Ui < 0}Ï‰n (Xi )I{Xi â‰¤ zjâˆ’1 }

i=L+1



1
q
.
âˆ’E Ui I{|Ui | â‰¤ Îºn }I{Ui < 0}Ï‰n (Xi )I{Xi â‰¤ zjâˆ’1 }
We will only consider the first summand in more detail since the rest works analogously.
To prove that (A.9) is stochastically of the desired rate we apply a Bernstein type inequality for Î±-mixing processes, see Liebscher (1996) Therorem 2.1. Following his notation we
define

1
Zi := Ui+L I{|Ui+L | â‰¤ Îºnq }I{Ui+L â‰¥ 0}Ï‰n (Xi+L )I{Xi+L â‰¤ z}


 
1
i
q
â‰¤ sk
âˆ’E Ui+L I{|Ui+L | â‰¤ Îºn }I{Ui+L â‰¥ 0}Ï‰n (Xi+L )I{Xi+L â‰¤ z} I
Îºn
1

for fixed z âˆˆ Rd and s âˆˆ [0, 1]. Note that S(Îºn ) := |Zi | â‰¤ 2Îºnq , Zi is centered and
ï£®ï£«
D(Îºn , N ) :=

sup
0â‰¤T â‰¤Îºn âˆ’1

(T +N )âˆ§Îºn

X

E ï£°ï£­

ï£¶2 ï£¹
Zj ï£¸ ï£» â‰¤ N 2 E[Zi2 ] â‰¤ CU N 2

j=T +1
1âˆ’ 2q

by assumption (U). Thus Liebscherâ€™s Theorem can be applied with N = bÎºn
means that
L+bÎºn sk c


P

max
j,k

X

1

Ui I{|Ui | â‰¤ Îºnq }I{Ui â‰¥ 0}Ï‰n (Xi )I{Xi â‰¤ zj }

i=L+1





1
q

âˆ’E Ui I{|Ui | â‰¤ Îºn }I{Ui â‰¥ 0}Ï‰n (Xi )I{Xi â‰¤ zj }

21


> CÎºn

c. This

X 
â‰¤
P

L+bÎºn sk c 

X

1

Ui I{|Ui | â‰¤ Îºnq }I{Ui â‰¥ 0}Ï‰n (Xi )I{Xi â‰¤ zj }

i=L+1

j,k



â‰¤
â‰¤
â‰¤
â‰¤



1
q



âˆ’E Ui I{|Ui | â‰¤ Îºn }I{Ui â‰¥ 0}Ï‰n (Xi )I{Xi â‰¤ zj }
> CÎºn




Îºn
C 2 Îº2n
+ 4 Î±(N )
Jn Kn 4 exp âˆ’ Îºn
N
64 N D(Îºn , N ) + 83 CÎºn N S(Îºn )
ï£¶
ï£«
ï£«
ï£¶
2 2
2
2
C Îºn
q ï£¸
ï£¸ + 4Îºnq Î±(Îº1âˆ’
Jn Kn ï£­4 exp ï£­âˆ’
)
n
2
1
2âˆ’ q
2âˆ’
q
16
64CU Îºn + 3 CÎºn




1
2
1âˆ’ 2q
q
q
Jn Kn 4 exp âˆ’C1 Îºn + 4Îºn Î±(Îºn )


1
1
2
âˆ’Î±Ì„+ 2qÎ±Ì„
q
q âˆ’q
q
C2 Îºn (C1 Îºn ) + Îºn
1

â‰¤ CÌ„Îºnq

âˆ’1

for some constants C1 , C2 , CÌ„ where the second to last inequality follows from the fact that
exp(âˆ’x) < xâˆ’k k! for all k âˆˆ N and x âˆˆ R>0 and the last inequality is true by assumption
(P) which implies Î±Ì„ > (q + 2)/(q âˆ’ 2). This completes the proof.
Proof of Lemma A.3. First we will distinguish between the cases L + bÎºn sc â‰¤ bns0 c and
L + bÎºn sc > bns0 c. In the first case we can write
L+bÎºn sc

X

(m(1) (Xi ) âˆ’ mÌ„n (Xi ))Ï‰n (Xi )I{Xi â‰¤ z}

i=L+1

=

m(1) (Xi )
Pn
m(1) (Xi ) âˆ’

X

Pbns0 c

Pn

fj (Xi ) m(2) (Xi ) j=bns0 c+1 fj (Xi )
Pn
âˆ’
j=1 fj (Xi )
j=1 fj (Xi )

L+bÎºn sc 
i=L+1

j=1

Â·Ï‰n (Xi )I{Xi â‰¤ z}
Pn

L+bÎºn sc

=

X

(m(1) (Xi ) âˆ’ m(2) (Xi ))

i=L+1

j=bns0 c+1 fj (Xi )
Pn
Ï‰n (Xi )I{Xi
j=1 fj (Xi )

â‰¤ z}

and analogously for the second case
L+bÎºn sc

X

(m(2) (Xi ) âˆ’ mÌ„n (Xi ))Ï‰n (Xi )I{Xi â‰¤ z}

i=Lâˆ¨bns0 c+1
L+bÎºn sc

=

X

Pbns0 c
j=1 fj (Xi )
(m(2) (Xi ) âˆ’ m(1) (Xi )) Pn
Ï‰n (Xi )I{Xi â‰¤ z}.
j=1 fj (Xi )

i=Lâˆ¨bns0 c+1

We will only examine the case L + bÎºn sc â‰¤ bns0 c in detail since the other case works
analogously.
22

The remainder of the proof is similar Pto the proof of Lemma A.2. With g(Xi ) :=
n
fj (Xi )
(s )
Pn 0 c+1
(m(1) (Xi ) âˆ’ m(2) (Xi )) and fÂ¯n 0 (Xi ) = j=bns
it holds
fj (Xi )
j=1

L+bÎºn sc

sup sup
sâˆˆ[0,1] zâˆˆRd

â‰¤

L+Îº
Xn

X

1

g(Xi )I{|g(Xi )| > Îºnr }fÂ¯n(s0 ) (Xi )Ï‰n (Xi )I{Xi â‰¤ z}

i=L+1
1

|g(Xi )|I{|g(Xi )| > Îºnr }

i=L+1

and further
P

L+Îº
Xn

!

1
r

|g(Xi )|I{|g(Xi )| > Îºn } > CÎºn

1

r
â‰¤ C âˆ’1 Îºâˆ’1
n Cm Îºn Îºn

âˆ’1

i=L+1

by the Markov inequality with
h
h
1 i
1 i
r
âˆ’(râˆ’1)
r
r
E |g(Xi )|I{|g(Xi )| > Îºn } = E |g(Xi )| |g(Xi )|
I{|g(Xi )| > Îºn }
âˆ’ râˆ’1
r

â‰¤ Îºn

1

â‰¤ Cm Îºnr

E[|g(Xi )|r ]
âˆ’1

for all i and for some Cm < âˆ by assumption (M). Thus we can rewrite our assertion as
ï£«
ï£¶
ï£«
ï£¶
Z
L+bÎºn sc
X
1
âˆ’1
P ï£­ sup sup ï£­
Ï•(Xi ) âˆ’ Ï•dP ï£¸ > CÎºn ï£¸ â‰¤ CÌ„Îºnr ,
sâˆˆ[0,1] Ï•âˆˆFn

i=L+1

with the function class
o
n
1
Fn := x 7â†’ g(x)I{|g(x)| â‰¤ Îºnr }fÂ¯n(s0 ) (x)Ï‰n (x)I{x â‰¤ z} : z âˆˆ Rd .
To replace the supremum over Ï• by a maximum we cover Fn by finitely many brackets
[Ï•lj , Ï•uj ]jâˆˆÃ—di=1 {1,...,Ni } where
1

Ï•uj (x) := g(x)I{|g(x)| â‰¤ Îºnr }I{g(x) â‰¥ 0}fÂ¯n(s0 ) (x)Ï‰n (x)I{x â‰¤ zj }
1

+g(x)I{|g(x)| â‰¤ Îºnr }I{g(x) < 0}fÂ¯n(s0 ) (x)Ï‰n (x)I{x â‰¤ zjâˆ’1 }
and
1

Ï•uj (x) := g(x)I{|g(x)| â‰¤ Îºnr }I{g(x) â‰¥ 0}fÂ¯n(s0 ) (x)Ï‰n (x)I{x â‰¤ zjâˆ’1 }
1

+g(x)I{|g(x)| â‰¤ Îºnr }I{g(x) < 0}fÂ¯n(s0 ) (x)Ï‰n (x)I{x â‰¤ zj }
and j, zj are defined as in the proof of Lemma A.2. The total number of brackets
Jn := N[ ] (n , Fn , k Â· kL1 (P ) ) needed to cover Fn is again of order Jn = O(âˆ’d
n ), which
follows analogously to but easier than the proof of Lemma A.7 in Mohr (2018). Now we
proceed completely analogously to the proof of Lemma A.2 by replacing the supremum
over s by a maximum as well and applying Liebscherâ€™s Theorem. Since the arguments are
the same as in the aforementioned proof we omit this part for the sake of brevity.
23

Proof of Lemma A.4. It holds
ï£«
ï£¶
L+bÎºn sc
X
P ï£­ sup sup
(mÌ„n (Xi ) âˆ’ mÌ‚n (Xi ))Ï‰n (Xi )I{Xi â‰¤ z} > CÎºn ï£¸
sâˆˆ[0,1] zâˆˆRd
L+Îº
Xn

â‰¤ P

i=L+1

!
|mÌ„n (Xi ) âˆ’ mÌ‚n (Xi )|Ï‰n (Xi ) > CÎºn

i=L+1




sup |mÌ„n (x) âˆ’ mÌ‚n (x)| > C

â‰¤ P
â‰¤ C

xâˆˆJn
âˆ’1

E[ sup |mÌ„n (x) âˆ’ mÌ‚n (x)|]
xâˆˆJn

by the Markov inequality. Further by Lemma A.1 with assumption (B) it holds that
supxâˆˆJn |mÌ„n (x) âˆ’ mÌ‚n (x)| P
âˆ’âˆ’âˆ’â†’ 0
nâ†’âˆ
nâˆ’Î¶
which implies
E[supxâˆˆJn |mÌ„n (x) âˆ’ mÌ‚n (x)|]
âˆ’âˆ’âˆ’â†’ 0
nâ†’âˆ
nâˆ’Î¶
and thus for sufficiently large n
E[ sup |mÌ„n (x) âˆ’ mÌ‚n (x)|] â‰¤ nâˆ’Î¶
xâˆˆJn

â‰¤ Îºâˆ’Î¶
n
for Îºn â‰¤ n. This completes the proof.

References
Antoch, J., HusÌŒkovaÌ, M., and PraÌsÌŒkovaÌ, Z. (1997). Effect of dependence on statistics for
determination of change. J. Stat. Plan. Inference, 60:291â€“310.
Aue, A., HorvaÌth, L., and HusÌŒkovaÌ, M. (2012). Segmenting mean-nonstationary time
series via trending regressions. J. Econom., 168:367â€“381.
Bai, J. (1994). Least squares estimation of a shift in linear processes. J. Time Ser. Anal.,
15:435â€“472.
Bai, J. (1997). Estimation of a Change Point in Multiple Regression Models. Rev. Econ.
Stat., 79:551â€“563.
Chen, G., Choi, Y. K., and Zhou, Y. (2005). Nonparametric estimation of structural
change points in volatility models for time series. J. Econom., 126:79â€“114.
Chong, T. T.-L. (2001). Structural Change in AR(1) Models. Econom. Theory, 17:87â€“155.
24

Delgado, M. A. and Hidalgo, J. (2000). Nonparametric inference on structural breaks. J.
Econom., 96:113â€“144.
DoÌˆring, M. and Jensen, U. (2015). Smooth change point estimation in regression models
with random design. Ann. Inst. Stat. Math., 67:595â€“619.
Ferger, D. (2005). Weighted Least Squares Estimators for a Change-Point. Econ. Qual.
Contr., 20:255â€“270.
Ferger, D. and Stute, W. (1992). Convergence of changepoint estimators. Stochastic
Process. Appl., 42:345â€“351.
Franke, J., Kreiss, J.-P., Mammen, E., and Neumann, M. (2002). Properties of the
nonparametric autoregressive bootstrap. J. Time Ser. Anal., 23:555â€“585.
Fryzlewicz, P. (2014). Wild binary segmentation for multiple change-point detection.
Ann. Statist., 42:2243â€“2281.
Fryzlewicz, P. (2019). Detecting possibly frequent change-points: Wild Binary Segmentation 2 and steepest-drop model selection. preprint on arXiv. https://arxiv.org/
abs/1812.06880.
Hariz, S. B., Wylie, J. J., and Zhang, Q. (2007). Optimal rate of convergence for nonparametric change-point estimators for nonstationary sequences. Ann. Statist., 35:1802â€“
1826.
HorvaÌth, L., HusÌŒkovaÌ, M., and Serbinowska, M. (1997). Estimators for the Time of
Change in Linear Models. Statistics, 29:109â€“130.
HusÌŒkovaÌ, M. and Antoch, J. (2003). Detection of structural changes in regression. Tatra
Mt. Math. Publ., 26:201â€“215.
HusÌŒkovaÌ, M. and Kirch, C. (2008).
Bootstrapping confidence intervals for the
change?point of time series. J. Time Ser. Anal., 29:947â€“972.
Kirch, C. and Eichinger, B. (2018). A MOSUM procedure for the estimation of multiple
random change points. Bernoulli, 24:526â€“564.
Kirch, C. and Kamgaing, J. T. (2012). Testing for parameter stability in nonlinear autoregressive models. J. Time Ser. Anal., 33:365â€“385.
Kirch, C., Muhsal, B., and Ombao, H. (2015). Detection of Changes in Multivariate Time
Series With Application to EEG Data. J. Amer. Statist. Assoc., 110:1197â€“1216.
Kosorok, M. R. (2008). Introduction to empirical processes and semiparametric inference.
Springer, New York.

25

Kristensen, D. (2009). Uniform convergence rates of kernel estimators with heterogeneous
dependent data. Econom. Theory, 25:1433â€“1445.
Kristensen, D. (2012). Non-parametric detection and estimation of structural change.
Econom. J., 15:420â€“461.
Liebscher, E. (1996). Strong convergence of sums of Î±-mixing random variables with
applications to density estimation. Stochastic Process. Appl., 65:69â€“80.
Mohr, M. (2018). Changepoint detection in a nonparametric time series regression model.
PhD thesis, University of Hamburg. http://ediss.sub.uni-hamburg.de/volltexte/
2018/9416/.
Mohr, M. and Neumeyer, N. (2019). Consistent nonparametric change point detection combining CUSUM and marked empirical processes. preprint on arXiv. https:
//arxiv.org/abs/1901.08491.
Pang, T. and Zhang, D. (2015). Asymptotic Inferences for an AR(1) Model with a Change
Point and Possibly Infinite Variance. Commun. Stat. - Theory Methods, 44:4848â€“4865.
Pang, T., Zhang, D., and Chong, T. T.-L. (2014). Asymptotic inferences for an AR(1)
model with a change point: stationary and nearly non-stationary cases. J. Time Ser.
Anal., 35:133â€“150.

26

