MEAN FIELD INTERACTION ON RANDOM GRAPHS WITH
DYNAMICALLY CHANGING MULTI-COLOR EDGES

arXiv:1912.01785v3 [math.PR] 20 Sep 2020

ERHAN BAYRAKTAR AND RUOYU WU
Abstract. We consider weakly interacting jump processes on time-varying random graphs
with dynamically changing multi-color edges. The system consists of a large number of nodes
in which the node dynamics depends on the joint empirical distribution of all the other nodes
and the edges connected to it, while the edge dynamics depends only on the corresponding
nodes it connects. Asymptotic results, including law of large numbers, propagation of chaos,
and central limit theorems, are established. In contrast to the classic McKean-Vlasov limit,
the limiting system exhibits a path-dependent feature in that the evolution of a given particle
depends on its own conditional distribution given its past trajectory. We also analyze the
asymptotic behavior of the system when the edge dynamics is accelerated. A law of large
number and a propagation of chaos result is established, and the limiting system is given as
independent McKean-Vlasov processes. Error between the two limiting systems, with and
without acceleration in edge dynamics, is also analyzed.

Contents
1. Introduction
1.1. Organization
1.2. Notation
2. Systems with node-dependent edge dynamics
2.1. Systems with independent edge dynamics
3. Systems with accelerated edge dynamics
3.1. Approximation error
3.2. Riccati equation for limiting marginal probabilities
4. Fluctuations and central limit theorems
4.1. Canonical processes
4.2. Some integral operators
4.3. Central limit theorems
4.4. An example with explicit variance
5. Proofs of laws of large numbers and propagation of chaos results
5.1. Proofs of Theorems 2.1 and 2.2
5.2. Proofs of Theorems 3.1 and 3.2
6. Proofs of central limit theorems
6.1. Asymptotics of symmetric statistics
6.2. Girsanov change of measure
6.3. Asymptotics of J n
6.4. Completing the proof of Theorem 4.1

2
4
4
5
7
8
9
10
11
11
11
12
13
14
15
21
29
29
30
31
40

Date: September 22, 2020.
2010 Mathematics Subject Classification. 60F05 60K35 60J75 05C80 60G09 60J27 60K37.
Key words and phrases. Dynamical random graphs; Mean field interaction; Propagation of chaos; Central
limit theorems; Endogenous common noise; Exchangeability; Interacting particle systems.
1

2

BAYRAKTAR AND WU

6.5. Completing the proof of Theorem 4.2
Appendix A. Proof of Theorem 2.3
References

41
41
42

1. Introduction
In this work we study some asymptotic results for large particle systems with mean field
interactions on time varying random graphs. The model is described in terms of two collections
of countable-state pure jump processes, one that gives the evolution of (the states of) nodes in
the system, and the other that drives the evolution of (the colors of) edges which govern the
interaction between nodes in the system. We consider mean field interaction between nodes,
in that the node dynamics depends on the joint empirical distribution of all the other nodes
and the edges connected to it. The edge dynamics on the other hand depends only on the
nodes it connects. More precisely,
Z
y1[0,Î“(y,Xin (sâˆ’),Î½in (sâˆ’))] (z) Ni (ds dy dz),
Xin (t) = Xi (0) +
[0,t]Ã—ZÃ—R+
Z
n
y1[0,Î²(n)Î“(y,Î¾
Î¾ij (t) = Î¾ij (0) +
(1.1)
n (sâˆ’),X n (sâˆ’),X n (sâˆ’))] (z) Nij (ds dy dz),
e
[0,t]Ã—ZÃ—R+

Î½in (t) =

1
n

n
X

Î´(Xjn (t),Î¾ijn (t)) ,

ij

i

j

i, j = 1, . . . , n,

j=1

where {Xi (0)} are independent and identically distributed (i.i.d.) Z-valued random variables
with some probability distribution Âµ(0), {Î¾ij (0)} are i.i.d. Z-valued random variables with
some probability distribution Î¸(0), {Ni } and {Nij } are i.i.d. Poisson random measures (whose
precise definition will be introduced in Section 1.2) with intensity ds Ã— Ï(dy) Ã— dz for some
e are functions governing the jump rates with Î“(y, x, Î½) =
finite measure Ï on Z, Î“ and Î“
R
e Î½(de
e for some measurable function Î³. Here X n denotes the state of node
e, Î¾)
x dÎ¾)
i
Z2 Î³(y, x, x
n
i, Î¾ij describes the color of the edge between nodes i and j, and Î²(n) â‰¥ 0 is a sequence of real
numbers representing the scale of jump rates of edges.
A typical example where such a system arises is in the study of gossip algorithms [26],
n denotes whether there is an edge between nodes i, j in a graph with n nodes, and
where Î¾ij
n
Xi denotes whether certain information has spread to node i. In neuroscience, the system
in (1.1) may be used to describe a collection of interacting neurons, where Xin is the state
n
of each neuron, and connections between neurons are denoted by dynamically changing Î¾ij
(see e.g. [3, 30] for a diffusion setup with static graphs). In simpler terms, one may also view
the system as n children playing at M places with K types friendship between each pair of
children, where M and K could be infinity. The node Xin (t) âˆˆ {1, . . . , M } denotes the place
n (t) âˆˆ {1, . . . , K} denotes the type of friendship
at which the i-th child is, while the edge Î¾ij
in which the i-th child views the j-th child at time t. The jump rate of Xin depends on the
empirical distribution of all childrenâ€™s positions and their friendship from the viewpoint of the
i-th child, that is, Î½in .
n â‰¡ 1,
When there is only one possible color, i.e. the graph is simply a complete graph with Î¾ij
the model reduces to the classic mean-field system, the study of which dates back to works
of Boltzmann, Vlasov, McKean and others (see [17, 29] and references therein). The original
motivation for the study of mean-field systems came from statistical physics but in recent

MEAN FIELD INTERACTION ON DYNAMIC RANDOM GRAPHS

3

years similar models have arisen in many different application areas, ranging from economics
and chemical and biological systems to communication networks and social sciences (see e.g.
[6] for an extensive list of references). The asymptotic picture is well resolved and many
different results have been established, including laws of large numbers (LLN), propagation
of chaos (POC) properties, and central limit theorems (CLT), see e.g. [4] and the references
therein.
When there are two possible colors (denoted by 0 and 1 for example) and edges are independently drawn and fixed at time 0, i.e. the edges could be present or absent and form an
ErdoÌ‹s-ReÌnyi random graph, the model has recently drawn much attention. It has been shown
that the same LLN, CLT, and large deviation principles (LDP), as in the mean-field case,
hold under certain conditions. In particular for interacting diffusions, quenched and annealed
LLN are studied in [13], CLT is established in [4], and LDP is obtained in [11, 24]. For certain
pure jump processes arising from the study of large-scale queuing networks, LLN is studied
in [8]. Moreover, mean field games on ErdoÌ‹s-ReÌnyi random graphs are analyzed in [12], and
graphon mean field games on static graphs with possibly uncountable players have recently
been studied (see e.g. [9, 10, 25]).
The goal of the current work is to study asymptotic behaviors of the system (1.1) as n â†’ âˆ
and Î²(n) â†’ Î² âˆˆ [0, âˆ]. Our first main result is LLN, POC, and CLT (Theorems 2.2, 4.1
and 4.2) for node states and their empirical measures when Î² < âˆ. The proof of LLN
and POC relies on certain coupling arguments, exchangeability properties of the nodes and
edges, and a key conditional independence structure for the limiting system (Theorem 2.1).
Intuitively speaking, due to the state-dependent evolution of edges, one would not expect in
the limiting system that edges are independent, although nodes are i.i.d. Indeed, Theorem
2.1 states that conditioning on the path of a node, edges connected with this node are i.i.d.
The proof of such a statement follows from a careful time-discretization argument along with
applying de Finettiâ€™s theorem. Moreover, we generalize the special case Î²(n) â†’ 0, in which
n
the limit represents a random but a static graph, to a case in which the edge processes Î¾ij
are i.i.d. adapted and could be non-Markovian. LLN and POC for such systems are obtained
in Theorem 2.3. The CLT result characterizes the fluctuation of functionals of the empirical
measures of nodes and edges. The proof of CLT relies on a change of measure technique
using Girsanovâ€™s theorem, and this approach goes back to [27, 28]. This technique reduces the
problem to working with the limiting system where nodes are i.i.d. and edges are conditionally
independent, while the price to pay is that one must carefully analyze the asymptotic behavior
of the Radonâ€“Nikodym derivative. The presence of conditionally independent edges requires
more challenging work than what has been done in the single-color case (e.g. in [27, 28]) and
the two-color case (in [4]). In particular, the node plays the role of common noises in the
analysis (see Lemma 6.6) and as a result the CLT limit is not a Gaussian random variable
but rather a Gaussian mixture.
Our second main result is the study of the averaging principle of the system (1.1) when
Î²(n) â†’ âˆ. Systems of stochastic processes with fast components or regime-switching features
have a long history of applications and the averaging principle has been well studied, when
there is one fast component (see e.g. [7, 23, 32]). However, a collection of fast state-dependent
switching edges are present in the system considered here, and more careful analysis is needed.
In particular in the limiting system, the jump rate of the slow component Xi corresponding
to node i depends on its own probability distribution and the conditional invariant measure
of the fast component given slow components (see (3.1) for the precise form). LLN and
POC for (1.1) when Î²(n) â†’ âˆ are obtained in Theorem 3.1. Compared to the limiting
system in the regime Î²(n) â†’ Î² < âˆ, this one does not suffer from the path-dependent

4

BAYRAKTAR AND WU

conditional independence subtlety and serves as a nice approximation of the former, with the
approximation error analyzed in Theorem 3.2.
1.1. Organization. The paper is organized as follows. In Section 2 we analyze the system
(1.1) when Î²(n) â†’ Î² âˆˆ [0, âˆ). A basic condition (Condition 2.1) is stated, under which the
limiting system (2.1) has a unique solution and a certain (conditional) independence property
(Theorem 2.1). A law of large numbers and a propagation of chaos property are obtained in
Theorem 2.2. In Section 2.1, we also present a LLN and POC (Theorem 2.3) for a system
n . In Section 3, the system (1.1) with
with i.i.d. and possibly non-Markovian edge processes Î¾ij
accelerated edge dynamics, namely when Î²(n) â†’ âˆ, is studied. LLN and POC are obtained
in Theorem 3.1, and the approximation error, as Î² â†’ âˆ, between the corresponding limiting
system and (2.1) are characterized in Theorem 3.2. The convenience of this limiting system is
illustrated in Section 3.2, by characterizing the evolution of marginal distributions as Riccati
equations. In Section 4 we present a CLT (Theorem 4.1) for the fluctuation of functionals of
the empirical measures of nodes and edges connecting to a given node. As noted above, the
limit is not a Gaussian random variable but rather a Gaussian mixture. We also provide a
CLT (Theorem 4.2) for the fluctuation of functionals of the empirical measures of nodes. The
limit is given by a simpler form and this point is illustrated through an example in Section
4.4 where the variance of the limit Gaussian random variable has an explicit form. Proofs of
all LLN and POC are given in Section 5. Finally Section 6 contains proofs of Theorems 4.1
and 4.2.
1.2. Notation. Given a Polish space S, denote by B(S) the Borel Ïƒ-field. Let P(S) be
the space of probability measures on S endowed with the topology of weak convergence. A
convenient metric for this topology is the bounded-Lipschitz metric dBL , defined by
dBL (Î½1 , Î½2 ) = sup |hf, Î½1 âˆ’ Î½2 i| ,
f âˆˆB1

Î½1 , Î½2 âˆˆ P(S),

where B1 is the collection of all Lipschitz functions f that are bounded by 1 and
R such that
the corresponding Lipschitz constant is bounded by 1 as well; and hf, Î½i := f dÎ½ for a
signed measure Î½ on S and Î½-integrable f : S â†’ R. Given a collection of random probability
measures Î½ n , Î½ on S, we write Î½ n â†’ Î½ in P(S) in probability, if d(Î½ n , Î½) â†’ 0 in probability
as n â†’ âˆ, for any metric d on P(S) that metrizes the weak convergence topology. We say
a collection {Xn } of S-valued random variables is tight if the distributions of Xn are tight
d

in P(S). We use the symbol â€˜â‡’â€™ to denote convergence in distribution and â€˜=â€™ to denote
the equality in distribution. The probability law of a random variable X will be denoted by
L(X). For a measurable function f : S â†’ R, let kf kâˆ := supxâˆˆS |f (x)|. Fix T âˆˆ (0, âˆ).
Denote by C([0, T ] : S) (resp. D([0, T ] : S)) the space of continuous functions (resp. right
continuous functions with left limits) from [0, T ] to S, endowed with the uniform topology
(resp. Skorokhod topology). We will use the notations X(t) and Xt interchangeably for
stochastic processes. For x âˆˆ D([0, T ] : S), let kxkâˆ—,t := sup0â‰¤sâ‰¤t kx(s)k, x[t] := (x(s) : 0 â‰¤
s â‰¤ t), and x[tâˆ’] := (x(s) : 0 â‰¤ s < t). For a Hilbert space H, denote the norm and inner
product in H by k Â· kH and hÂ·, Â·iH , respectively. For a Ïƒ-finite measure Î½ on a Polish space S,
denote by L2 (S, Î½) the Hilbert space of Î½-square integrable functions from S to R. We denote
by Sâˆ the countable product space of copies of S, equipped with the usual product topology.
Let [k] := {1, . . . , k} for each k âˆˆ N. We will use Îº, Îº0 , Îº1 , . . . for constants in the proofs,
whose value may change over lines. Let Xt = [0, t]Ã—ZÃ—R+ for each t âˆˆ [0, T ], and let Mt be the
space of Ïƒ-finite measures on (Xt , B(Xt )) with the topology of vague convergence. A Poisson
random measure (PRM) N on XT with intensity measure Î½ âˆˆ MT is an MT -valued random

MEAN FIELD INTERACTION ON DYNAMIC RANDOM GRAPHS

5

variable such that for each A âˆˆ B(XT ) with Î½(A) < âˆ, N (A) is Poisson distributed with
mean Î½(A) and for disjoint A1 , . . . , Ak âˆˆ B(XT ), N (A1 ), . . . , N (Ak ) are mutually independent
random variables (cf. [16]).
Let (â„¦, F, P, {Ft }) be a filtered probability space on which we are given i.i.d. PRM {Ni , Nij :
i, j âˆˆ N} on XT with intensity measure ds Ã— Ï(dy) Ã— dz for some finite measure Ï on Z.
Expectations under P (resp. some other probability measure Q) will be denoted by E (resp.
EQ ).

2. Systems with node-dependent edge dynamics
In this section we study the system (1.1) when Î²(n) â†’ Î² âˆˆ [0, âˆ). Recall that Î“(y, x, Î½) =
e Î½(de
e for y, x âˆˆ Z and Î½ âˆˆ P(Z2 ), where Î³ is some measurable function
e, Î¾)
x dÎ¾)
Z2 Î³(y, x, x
e
from Z4 to R. We make the following assumptions on Î³ and Î“.
R

e â‰¤ Î³y and
e, Î¾)
Condition 2.1. (i) There exists {Î³y âˆˆ [0, âˆ) : y âˆˆ Z} such that 0 â‰¤ Î³(y, x, x
R
e
e
e Î¾, x, x
0 â‰¤ Î“(y,
e) â‰¤ Î³y for all y, x, x
e, Î¾ âˆˆ Z and CÎ³ := Z |y|Î³y Ï(dy) < âˆ.
(ii) {Xi (0)} are i.i.d. with some common probability distribution Âµ(0) âˆˆ P(Z) and E|Xi (0)| <
âˆ. {Î¾ij (0)} are i.i.d. with some common probability distribution Î¸(0) âˆˆ P(Z) and
E|Î¾ij (0)| < âˆ.
Remark 2.1. (a) Condition 2.1(i) holds clearly if the system is finite state, such as
e = 0 whenever |y + x| > r, for some r âˆˆ N.
Î³(y, x, x
e, Î¾)
(b) Noting that every bounded function on Zd is automatically Lipschitz, we see that
e is Î³y -Lipschitz (with respect to all variables) by Condition 2.1(i).
Î³(y, x, x
e, Î¾)

The next two theorems show that, under Condition 2.1, the limiting system is given by the
pathwise unique solution to the following equations:
Xi (t) = Xi (0) +
Î¾ij (t) = Î¾ij (0) +

Z

Z Xt
Xt

y1[0,Î“(y,Xi (sâˆ’),Î½i (sâˆ’))] (z) Ni (ds dy dz),
y1[0,Î² Î“(y,Î¾
(z) Nij (ds dy dz),
e
ij (sâˆ’),Xi (sâˆ’),Xj (sâˆ’))]

(2.1)

n

1X
Î´(Xj (t),Î¾ij (t)) ,
nâ†’âˆ n

Î½i (t) = lim

j=1

where the limit in Î½i (t) is understood as almost surely in P(Z2 ) for each i âˆˆ N and t âˆˆ [0, T ].
Note that the PRMs are the same as those in (1.1), for ease of deriving Theorem 2.2 below.
The proofs of Theorems 2.1 and 2.2 are given in Section 5.1.
Theorem 2.1. Suppose Condition 2.1 holds. Then
(a) The system (2.1) has a unique pathwise solution.
(b) Xi are i.i.d., {(Xj [t], Î¾ij [t]) : j âˆˆ N, j 6= i} are i.i.d. conditioning on Xi [t], and Î½i (t) =
L((Xj (t), Î¾ij (t)) | Xi [t]) = Î¦t (Xi [t]) for each j 6= i, where Î¦t : D([0, t] : Z) â†’ P(Z2 ) is
some measurable map independent of the choice of i.

6

BAYRAKTAR AND WU

Remark 2.2. Using Theorem 2.1, the system (2.1) could be rewritten in the following equivalent and perhaps more familiar form:
Z
y1[0,Î“(y,Xi (sâˆ’),Î½i (sâˆ’))] (z) Ni (ds dy dz),
Xi (t) = Xi (0) +
Xt
Z
y1[0,Î² Î“(y,Î¾
(z) Nij (ds dy dz),
Î¾ij (t) = Î¾ij (0) +
e
ij (sâˆ’),Xi (sâˆ’),Xj (sâˆ’))]
Xt

Î½i (t) = L((Xk (t), Î¾ik (t)) | Xi [t]) = Î¦t (Xi [t]),

For i âˆˆ [n] and t âˆˆ [0, T ], let
n

Î½in :=

1X
Î´(Xjn (Â·),Î¾ijn (Â·)) ,
n
j=1

âˆ€k 6= i.

n

Âµn :=

1X
Î´Xin (Â·) ,
n

n

Âµn (t) :=

i=1

1X
Î´Xin (t) .
n

(2.2)

i=1

Theorem 2.2. Suppose Condition 2.1 holds. Then
(a) There exists some Îº = Îº(T, Î²) < âˆ such that

Îº
n
max EkXin âˆ’ Xi kâˆ—,T + max EkÎ¾ij
âˆ’ Î¾ij kâˆ—,T â‰¤ âˆš + Îº|Î²(n) âˆ’ Î²|.
n
iâˆˆ[n]
i,jâˆˆ[n]

(2.3)

(b) (POC) For any k âˆˆ N, as n â†’ âˆ,
L(X1n , . . . , Xkn ) â†’ ÂµâŠ—k ,

L(X1n (t), . . . , Xkn (t)) â†’ [Âµ(t)]âŠ—k for each t âˆˆ [0, T ],

(2.4)

where Âµ := L(Xi ) âˆˆ P(D([0, T ] : Z)) and Âµ(t) := L(Xi (t)) âˆˆ P(Z) for i âˆˆ N.
(c) (LLN) As n â†’ âˆ,
n

1X
Î´(Xj (Â·),Î¾ij (Â·)) in P(D([0, T ] : Z2 )) in probability,
nâ†’âˆ n

Î½in â†’ Î½i := lim
Î½in (t)

(2.5)

j=1

2

â†’ Î½i (t) in P(Z ) in probability, for each t âˆˆ [0, T ],

(2.6)

Âµn â†’ Âµ in P(D([0, T ] : Z)) in probability,

(2.7)

for each i âˆˆ [n], and
n

Âµ (t) â†’ Âµ(t) in P(Z) in probability, for each t âˆˆ [0, T ].

(2.8)

Remark 2.3. (a) We abuse the notation to use Î½in , Î½i , Âµn , Âµ to denote the empirical measures
on the path space, and use Î½in (t), Î½i (t), Âµn (t), Âµ(t) to denote the processes of the marginal
empirical measures. We always precisely state the space to avoid the ambiguity in statements
such as (2.5) and (2.7).
(b) Although (2.7) only states the convergence on the path space, by applying standard
arguments (cf. [22, Theorem 4.7 and Lemma 4.8]), one can make use of the fact that Âµ
is deterministic and obtain suitable controls of jump sizes of Xi , to argue that the process
{Âµn (t) : t âˆˆ [0, T ]} converges in probability to {Âµ(t) : t âˆˆ [0, T ]} in the space D([0, T ] : P(Z))
endowed with the uniform topology.
n ) in (1.1) (resp. L(Î¾ ) 6= L(Î¾ ) in (2.1))
Remark 2.4. (a) We note that L(Î¾iin ) 6= L(Î¾ij
ii
ij
n
for j 6= i. However, the contribution of Î¾ii to Î½in (resp. Î¾ii to Î½i ) is negligible. Therefore
one does not
have to worry about the special evolution of Î¾iin . One may also simply define
1 Pn
n
Î½i (t) = n j6=i Î´(Xjn (t),Î¾ijn (t)) and the LLN, POC, and CLT results in this work will still be
valid.
(b) Although in this paper we consider the case of directed graphs, that is, we do not assume
n = Î¾ n for j 6= i, we note that the results naturally extend to the undirected graph scenario,
Î¾ij
ji

MEAN FIELD INTERACTION ON DYNAMIC RANDOM GRAPHS

7

e x, x
ex
e Î¾,
e Î¾,
with an additional symmetry assumption Î“(y,
e) = Î“(y,
e, x) and minor changes to the
proofs.

2.1. Systems with independent edge dynamics. In this section we consider a system that
generalizes the Î²(n) â†’ 0 limit of (1.1), in that we allow for non-Markovian edge processes
(such as processes with delays and renewal processes). Recall the node process Xin and the
local empirical measure process Î½in
Z
n
y1[0,Î“(y,Xin (sâˆ’),Î½in (sâˆ’))] (z) Ni (ds dy dz),
Xi (t) = Xi (0) +
Xt

Î½in (t) =

1
n

n
X

Î´(Xjn (t),Î¾ijn (t)) ,

(2.9)

i = 1, . . . , n.

j=1

n (t) = Î¾ (t) are adapted, i.i.d. with L(Î¾ ) = Î¸ âˆˆ P(D([0, T ] : Z)), and independent
Suppose Î¾ij
ij
ij
of {Xi (0), Ni }.
We make the following assumption on Î³.

e â‰¤ Î³y for all
Condition 2.2. There exists {Î³y âˆˆ [0, âˆ) : y âˆˆ Z} such that 0 â‰¤ Î³(y, x, x
e, Î¾)
R
y, x, x
e, Î¾e âˆˆ Z and CÎ³ := Z |y|Î³y Ï(dy) < âˆ.

The next theorem shows that, under Condition 2.2, the limiting system is given by the
unique solution to the following equations:
Z
y1[0,Î“(y,Xi (sâˆ’),Î½(sâˆ’))] (z) Ni (ds dy dz),
Xi (t) = Xi (0) +
(2.10)
Xt
Î½(t) := Âµ(t) âŠ— Î¸(t) := L(Xi (t)) âŠ— L(Î¾ij (t)), i, j âˆˆ N.
The proof of Theorem 2.3 is a standard application of a coupling argument. For completeness
it is given in Appendix A.

Theorem 2.3. Suppose Condition 2.2 hold. Then
(a) The system (2.10) has a unique pathwise solution.
(b) There exists some Îº = Îº(T ) < âˆ such that

Îº
max EkXin âˆ’ Xi kâˆ—,T â‰¤ âˆš .
n
iâˆˆ[n]

(c) (POC) For any k âˆˆ N, as n â†’ âˆ,
L(X1n , . . . , Xkn ) â†’ ÂµâŠ—k ,

L(X1n (t), . . . , Xkn (t)) â†’ [Âµ(t)]âŠ—k for each t âˆˆ [0, T ],

where Âµ := L(Xi ) âˆˆ P(D([0, T ] : Z)) for i âˆˆ N.
(d) (LLN) As n â†’ âˆ, for each i âˆˆ [n],
n
1X
Î½in :=
Î´(Xjn (Â·),Î¾ijn (Â·)) â†’ Î½ := Âµ âŠ— Î¸ in P(D([0, T ] : Z2 )) in probability,
n

(2.11)

(2.12)

(2.13)

j=1

Î½in (t) â†’ Î½(t) in P(Z2 ) in probability, for each t âˆˆ [0, T ].

Moreover, as n â†’ âˆ,
n
1X
n
Î´Xin (Â·) â†’ Âµ in P(D([0, T ] : Z)) in probability,
Âµ :=
n
Âµn (t) :=

1
n

i=1
n
X
i=1

Î´Xin (t) â†’ Âµ(t) in P(Z) in probability, for each t âˆˆ [0, T ].

(2.14)

(2.15)
(2.16)

8

BAYRAKTAR AND WU

Remark 2.5. Theorem 2.3 is not a simple consequence of Theorems 2.1 and 2.2 for the case
n
Î²(n) â†’ 0, although it seems to be. In particular, note that the system (2.9) only assumes Î¾ij
to be adapted, which may be a non-Markovian process.
3. Systems with accelerated edge dynamics
In this section we study the system (1.1) with accelerated edge dynamics compared to the
node dynamics, that is when Î²(n) â†’ âˆ. In addition to Condition 2.1, we make the following
e
assumption on Î“ and Î“.
(i) For Î¾e0 , x, x
e âˆˆ Z, denote by Ye(x,ex,Î¾e0 ) the continuous-time Markov chain
e x, x
e Î¾,
with transition rate matrix Î“(y,
e) (representing the rate of jumping from Î¾e to Î¾e+ y)
e of
starting at Î¾e0 . Suppose that there exists a unique invariant distribution Q(x, x
e, dÎ¾)
e
Y(x,ex,Î¾e0 ) and
Z
e
e â‰¤Îº
e
e, Î¾)Q(x,
x
e, dÎ¾)
EÎ³(y, x, x
e, Y(x,ex,Î¾e0 ) (t)) âˆ’ Î³(y, x, x
e(t)Î³y (1 + |x| + |e
x|)

Condition 3.1.

Z

Râˆ
for some Îº
e(t) such that 0 Îº
eR(t) dt < âˆ.
2
(ii) E[|Xi (0)| ] < âˆ and CÎ³,2 := Z |y|2 Î³y Ï(dy) < âˆ.

Remark 3.1. (a) Condition 3.1(i) holds if Ye(x,ex,Î¾e0 ) is uniformly exponentially ergodic in
(x, x
e, Î¾e0 ) in the following sense: There exists Î± > 0 and C > 0 such that
X
e âˆ’ Q(x, x
e â‰¤ Ceâˆ’Î±t (1 + |x| + |e
|P(Ye(x,ex,Î¾e0 ) (t) = Î¾)
e, {Î¾})|
x|),
e
Î¾âˆˆZ

e is given, one may refer to sufficient
for all Î¾e0 , x, x
e âˆˆ Z. Once the transition rate matrix Î“
criteria (see e.g. [31]) that guarantees such an assumption. In particular, it is satisfied if
the system is finite
R âˆstate (cf. [31] and [2, Theorem 6.5]).
(b) The assumption 0 Îº
e(t) dt < âˆ in Condition 3.1(i) is needed to obtain the rate of convergence in Theorem 3.1. If one is only interested in the convergence of Xin , Âµn , Î½in , then
Rt
Îº(s) ds = 0.
it would be sufficient (see Remark 5.1) to assume that limtâ†’âˆ 1t 0 e
The next theorem shows that, under Conditions 2.1 and 3.1, the limiting system is given
by the unique solution to the following equations:
Z
y1[0,Î“(y,Xi (sâˆ’),Î½i (sâˆ’))] (z) Ni (ds dy dz),
Xi (t) = Xi (0) +
Xt

e = Âµt (de
e
Î½i (t)(de
x dÎ¾)
x) Q(Xi (t), x
e, dÎ¾),
Âµ = L(Xi ),

Âµt = L(Xi (t)),

(3.1)
i âˆˆ N.

We note that here the system does not suffer from the path-dependent subtlety in (2.1), and
hence serves as a simpler approximation of (1.1) than (2.1), when Î²(n) is large.
The proof of Theorem 3.1 is given in Section 5.2.
Theorem 3.1. Suppose Conditions 2.1 and 3.1 hold. Then
(a) There is a unique pathwise solution {Xi } to the system (3.1).
(b) There exists some Îº = Îº(T ) < âˆ such that
Îº
Îº
EkXin âˆ’ Xi kâˆ—,T â‰¤ âˆš + p
,
n
Î²(n)

(3.2)

MEAN FIELD INTERACTION ON DYNAMIC RANDOM GRAPHS

9

(c) (POC) For any k âˆˆ N, as n â†’ âˆ,
L(X1n , . . . , Xkn ) â†’ ÂµâŠ—k ,

(d) (LLN) As n â†’ âˆ,

L(X1n (t), . . . , Xkn (t)) â†’ [Âµ(t)]âŠ—k for each t âˆˆ [0, T ].

Âµn â†’ Âµ in P(D[0, T ] : Z) and Âµn (t) â†’ Âµ(t) in P(Z), in probability,

for each t âˆˆ [0, T ], where Âµn and Âµn (t) are introduced in (2.2).
(e) Suppose in addition that
X
e âˆ’ Q(x, x
e â‰¤ C(t)(1 + |x| + |e
|P(Ye(x,ex,Î¾e0 ) (t) = Î¾)
e, {Î¾})|
x|)

(3.3)
(3.4)

(3.5)

e
Î¾âˆˆZ

e are as in
for some positive C(t) such that limtâ†’âˆ C(t) = 0, and Ye(x,ex,Î¾e0 ) and Q(x, x
e, {Î¾})
Condition 3.1. Then
Î½in (t) â†’ Î½i (t) in P(Z2 ) in probability, for each i âˆˆ [n] and t âˆˆ (0, T ]

(3.6)

e dt â†’ Î·i (dt de
e := Î½i (t)(de
e dt
e := Î½ n (t)(de
x dÎ¾)
x dÎ¾)
x dÎ¾)
x dÎ¾)
Î·in (dt de
i

(3.7)

and hence

in P([0, T ] Ã— Z2 ), in probability, for each i âˆˆ [n].

3.1. Approximation error. In this section we study the approximation error in terms of
Î² â†’ âˆ between two limiting systems (2.1) and (3.1) obtained as Î²(n) â†’ Î² âˆˆ [0, âˆ) and
Î²(n) â†’ âˆ. To distinguish the two systems, we rewrite (2.1) as
Z
Î²
y1[0,Î“(y,X Î² (sâˆ’),Î½ Î² (sâˆ’))] (z) Ni (ds dy dz),
Xi (t) = Xi (0) +
i
i
Xt
Z
Î²
y1[0,Î² Î“(y,Î¾
(z) Nij (ds dy dz),
Î¾ij
(t) = Î¾ij (0) +
Î²
e
(sâˆ’),X Î² (sâˆ’),X Î² (sâˆ’))]
Î½iÎ² (t) = lim

nâ†’âˆ

1
n

Xt
n
X

ij

i

j

Î´(X Î² (t),Î¾ Î² (t)) ,

j=1

j

ij

and recall (3.1). The following theorem characterizes the approximation error of these two
limiting systems as Î² â†’ âˆ. The proof is given in Section 5.2.
Theorem 3.2. Suppose Conditions 2.1 and 3.1 hold. Then
(a) There exists some Îº = Îº(T ) < âˆ such that
Îº
EkXiÎ² âˆ’ Xi kâˆ—,T â‰¤ âˆš
Î²
and
Îº
dBL (ÂµÎ² , Âµ) â‰¤ âˆš ,
Î²

(3.8)
(3.9)

where ÂµÎ² := L(XiÎ² ).
(b) Suppose in addition that (3.5) holds, then
Î½iÎ² (t) â†’ Î½i (t) in P(Z2 ), in probability, for each i âˆˆ N and t âˆˆ (0, T ]

(3.10)

e := Î½ Î² (t)(de
e dt â†’ Î·i (dt de
e := Î½i (t)(de
e dt
Î·iÎ² (dt de
x dÎ¾)
x dÎ¾)
x dÎ¾)
x dÎ¾)
i

(3.11)

and hence

in P([0, T ] Ã—

Z2 ),

in probability, for each i = 1, . . . , n.

10

BAYRAKTAR AND WU

Râˆ
Remark 3.2. Similar to Remark 3.1(b), the assumption 0 Îº
e(t) dt < âˆ in Condition 3.1(i)
is needed to obtain the rate of convergence in Theorem 3.2. If one is only interested in
the convergence of XiÎ² , ÂµÎ² , Î½iÎ² , then it would be sufficient (see Remark 5.2) to assume that
Rt
e(s) ds = 0.
limtâ†’âˆ 1t 0 Îº

3.2. Riccati equation for limiting marginal probabilities. In this section we will get a
Riccati equation for the evolution of Âµ = L(Xi ).
To simplify the notation for the evolution of Âµ, we will rewrite the limiting system as
follows:
Z
(y âˆ’ Xi (sâˆ’))1[0,Î“Ì„(y,Xi (sâˆ’),Î½i (sâˆ’))] (z) Ni (ds dy dz),
Xi (t) = Xi (0) +
Xt

where

e = Âµt (de
e
Î½i (t)(de
x dÎ¾)
x) Q(Xi (t), x
e, dÎ¾),

Î“Ì„(y, x, Î½) =

Z

Z2

e Î½(de
e :=
Î³Ì„(y, x, x
e, Î¾)
x dÎ¾)

Z

Z2

i âˆˆ N,

e Î½(de
e = Î“(y âˆ’ x, x, Î½)
Î³(y âˆ’ x, x, x
e, Î¾)
x dÎ¾)

e is still the unique stationis the rate of particles jumping from x to y, and Q(x, x
e, dÎ¾)
e x, x
e Î¾,
ary/invariant distribution of Markov chain with transition rate matrix Î“(y,
e) (repree
e
senting the rate of jumping from Î¾ to Î¾ + y). Let
pt (k) = Âµt ({k}) = P(Xi (t) = k).

Then
X
d
pt (k) = âˆ’pt (k)
dt
y6=k

+

X

pt (y)

y6=k

= âˆ’pt (k)
+

X
y6=k

Z

Z2

Z

Z2

e Âµt (de
e
Ï(y âˆ’ k)Î³Ì„(y, k, x
e, Î¾)
x) Q(k, x
e, dÎ¾)

e Âµt (de
e
Ï(k âˆ’ y)Î³Ì„(k, y, x
e, Î¾)
x) Q(y, x
e, dÎ¾)

XXZ

y6=k x
eâˆˆZ Z

pt (y)

XZ

x
eâˆˆZ Z

e t (e
e
Ï(y âˆ’ k)Î³Ì„(y, k, x
e, Î¾)p
x) Q(k, x
e, dÎ¾)

e t (e
e
Ï(k âˆ’ y)Î³Ì„(k, y, x
e, Î¾)p
x) Q(y, x
e, dÎ¾).

This is an infinite dimensional Riccati equation.

e =
Remark 3.3. If, for simplicity, the system is finite dimensional with Ï(y) = 1, Î³Ì„(y, x, x
e, Î¾)
e does not depend on x
e = Q0 (dÎ¾)
e does not depend on x or x
Î³Ì„0 (y, x, Î¾)
e, and Q(x, x
e, dÎ¾)
e, then the
above Riccati equation reduces to the following finite dimensional linear ordinary differential
equations:
Z
XZ
X
d
e Q0 (dÎ¾)
e +
e Q0 (dÎ¾).
e
pt (k) = âˆ’pt (k)
Î³Ì„0 (y, k, Î¾)
pt (y) Î³Ì„0 (k, y, Î¾)
dt
Z
Z
y6=k

y6=k

Note that the special choice of Î³Ì„ and Q does not mean that the particle system is trivial.
e still allows interaction as there is the dependence on relationship Î¾.
e Also
Indeed, Î³Ì„0 (y, x, Î¾)
e does not depend on x or x
e x, x
e Î¾,
note that although Q0 (dÎ¾)
e, one can still have general Î“(y,
e)
n
e
associated to Î¾ij such that the stationary distribution is given by Q0 (dÎ¾).

MEAN FIELD INTERACTION ON DYNAMIC RANDOM GRAPHS

11

4. Fluctuations and central limit theorems
Finally we will study the fluctuations of empirical measures about the law of large numbers
limit when Î²(n) â†’ Î² âˆˆ [0, âˆ). For simplicity, we assume Î²(n) = Î² âˆˆ [0, âˆ). In addition to
Condition 2.1, we make the following assumption on Î“.
e â‰¤
Condition 4.1. There exists Îµ âˆˆ (0, 1] such that Îµ â‰¤ Î³(y, x, x
e, Î¾)

1
Îµ

for all y, x, x
e, Î¾e âˆˆ Z.

The fluctuations will be characterized by CLT in Theorems 4.1 and 4.2 in Section 4.3.
Variances in these CLT will be expressed in terms of norms of certain integral operators
defined in Section 4.2 using canonical spaces and processes introduced in Section 4.1.
4.1. Canonical processes. We first introduce the following canonical spaces and stochastic
processes. Let â„¦v = MT Ã— D([0, T ] : Z)2 , â„¦e = D([0, T ] : Z), and â„¦0 = D([0, T ] : Z). Define
n(nâˆ’1)
for n âˆˆ N the probability measure P n on â„¦n := â„¦0 Ã— â„¦nv Ã— â„¦e
by
P n := L (X1 , (N1 , X1 , Î¾11 ), (N2 , X2 , Î¾12 ), . . . , (Nn , Xn , Î¾1n ), {Î¾ij : i = 2, . . . , n, j âˆˆ [n]}) .

For Ï‰ = (Ï‰0 , Ï‰1 , Ï‰2 , . . . , Ï‰n , Ï‰Ì„) âˆˆ â„¦n with Ï‰i = (Ï‰ik : k = 1, 2, 3) for i = 1, . . . , n and
Ï‰Ì„ = (Ï‰Ì„ij : i = 2, . . . , n, j âˆˆ [n]), let
V0 (Ï‰) := Ï‰0 ,

and, abusing notation, write
V0 := X1 ,

Vi (Ï‰) := Ï‰i = (Ï‰i1 , Ï‰i2 , Ï‰i3 ), i âˆˆ [n],

Vi := (Ni , Xi , Î¾1i ), i âˆˆ [n],

Î¾ij (Ï‰) := Ï‰Ì„ij , i = 2, . . . , n, j âˆˆ [n].

Note that such an abuse of notation just says that the distribution of the canonical processes
(Ni , Xi , Î¾ij : i, j = 1, . . . , n) under P n is the same as that of processes (Ni , Xi , Î¾ij : i, j =
1, . . . , n) in (2.1). Also define the canonical processes Vâˆ— := (Nâˆ— , Xâˆ— , Î¾âˆ— ) on â„¦v by
Vâˆ— (Ï‰) := (Nâˆ— (Ï‰), Xâˆ— (Ï‰), Î¾âˆ— (Ï‰)) := (Ï‰1 , Ï‰2 , Ï‰3 ),

Ï‰ = (Ï‰1 , Ï‰2 , Ï‰3 ) âˆˆ â„¦v .

eâˆ— on â„¦v by
Define the compensated PRM N
eâˆ— (Ï‰)(ds dy dz) := Nâˆ— (Ï‰)(ds dy dz) âˆ’ ds Ã— Ï(dy) Ã— dz.
N

4.2. Some integral operators. We will need some functions for stating our central limit
theorem. Let
Î±(x, Â·) := L((N2 , X2 , Î¾12 ) âˆˆ Â·|V0 = x)
and
Î¸(t, x[t], x
e[t]) := L(Î¾12 (t) | X1 [t] = x[t], X2 [t] = x
e[t])
be the corresponding regular conditional probabilities, for x, x
e âˆˆ D([0, T ] : Z). Let
P0 := L(X1 ),

Î := L(N2 , X2 , Î¾12 ).

Note that P0 is just Âµ but we write it in this way to emphasize its role as a common factor. Recall Î½i (t) = L((Xj (t), Î¾ij (t)) | Xi [t]) and we rewrite it as Î½(t, Xi [t]) to emphasize its
dependence on Xi [t]. Define the function h : â„¦v Ã— â„¦v â†’ R (Î Ã— Î a.s.) by
Z
1[0,Î“(y,Xâˆ— (Ï‰1 )(sâˆ’),Î½(sâˆ’,Xâˆ— (Ï‰1 )[sâˆ’]))] (z)
h(Ï‰1 , Ï‰2 ) :=
XT

where

hÎ³Ì„sâˆ’,y (Xâˆ— (Ï‰1 )[sâˆ’], Xâˆ— (Ï‰2 )(sâˆ’), Â·), Î¸(sâˆ’, Xâˆ— (Ï‰1 )[sâˆ’], Xâˆ— (Ï‰2 )[sâˆ’])i e
Â·
Nâˆ— (Ï‰1 )(ds dy dz),
Î“(y, Xâˆ— (Ï‰1 )(sâˆ’), Î½(sâˆ’, Xâˆ— (Ï‰1 )[sâˆ’]))
Î³Ì„t,y (x1 , x2 , Î¾) := Î³(y, x1 (t), x2 , Î¾) âˆ’ hÎ³(y, x1 (t), Â·, Â·), Î½(t, x1 [t])i,

12

BAYRAKTAR AND WU

for t âˆˆ [0, T ], x1 âˆˆ D([0, t] : Z) and y, x2 , Î¾ âˆˆ Z. Fix Ï‰0 âˆˆ â„¦0 and consider the Hilbert space
HÏ‰0 := L2 (â„¦v , Î±(Ï‰0 , Â·)). Define the integral operator AÏ‰0 on HÏ‰0 by
Z
g(Ï‰1 )h(Ï‰1 , Ï‰2 ) Î±(Ï‰0 , dÏ‰1 ), g âˆˆ HÏ‰0 , Ï‰2 âˆˆ â„¦v .
AÏ‰0 g(Ï‰2 ) =
â„¦v

Denote by I the identity operator.

4.3. Central limit theorems. We denote by A the collection of all measurable maps
Ï• : D([0, T ] : Z)2 â†’ R such that Ï•(Xâˆ— , Î¾âˆ— ) âˆˆ L2 (â„¦v , Î±(Ï‰0 , Â·)) for P0 a.e. Ï‰0 âˆˆ â„¦0 . For
Ï• âˆˆ A and Ï‰0 âˆˆ â„¦0 , let
Z
Ï•(Xâˆ— (Ï‰1 ), Î¾âˆ— (Ï‰1 )) Î±(Ï‰0 , dÏ‰1 ), Î¦Ï‰0 (Ï‰) := Ï•(Xâˆ— (Ï‰), Î¾âˆ— (Ï‰))âˆ’mÏ• (Ï‰0 ), Ï‰ âˆˆ â„¦v .
mÏ• (Ï‰0 ) :=
â„¦v

For Ï‰0 âˆˆ â„¦0 and Ï• âˆˆ A, define

ÏƒÏ‰Ï•0 := k(I âˆ’ AÏ‰0 )âˆ’1 Î¦Ï‰0 kHÏ‰0 ,

and denote by Ï€Ï‰Ï•0 the normal distribution with mean 0 and standard deviation ÏƒÏ‰Ï•0 . Here the
operator I âˆ’ AÏ‰0 is invertible by Lemma 6.7. Define Ï€ Ï• âˆˆ P(R) by
Z
Ï•
Ï€Ï‰Ï•0 P0 (dÏ‰0 ).
Ï€ :=
â„¦0

Finally let
Î· n (Ï•) :=

âˆš

âˆš

nhÏ•, Î½1n âˆ’ Î½1 i =

ï£¶
n
X
1
n
Ï•(Xjn , Î¾ij
) âˆ’ mÏ• (X1n )ï£¸ .
nï£­
n
ï£«

j=1

The following is the CLT for the empirical measure Î½1n of neighboring nodes and edges of
node 1. The proof is given in Section 6.
Theorem 4.1. Suppose that Conditions 2.1 and 4.1 hold. Then, for all Ï• âˆˆ A, L(Î· n (Ï•)) â†’
Ï€ Ï• weakly as n â†’ âˆ.

We note that the limit of the fluctuation of Î½1n is a Gaussian mixture. This is due to the
conditional independence of {Î¾ij : j 6= i} given Xi , so that Xi serves as a common noise which
would not be averaged out. However, the limit could be written as a single Gaussian random
variable if one is interested in the fluctuation of Âµn . To be precise, let Ax := L2 (D([0, T ] : Z), Âµ)
and H = L2 (â„¦v , Î). Define the integral operator A on H by
Z
g(Ï‰1 )h(Ï‰1 , Ï‰2 ) Î(dÏ‰1 ), g âˆˆ H, Ï‰2 âˆˆ â„¦v .
Ag(Ï‰2 ) =
â„¦v

For Ï• âˆˆ Ax , let

Î¦(Ï‰) := Ï•(Xâˆ— (Ï‰)) âˆ’
and

Z

â„¦v

Ï•(Xâˆ— (Ï‰1 )) Î(dÏ‰1 ),
ï£«

Ï‰ âˆˆ â„¦v ,

ï£¶
n
X
âˆš
âˆš
1
Î·xn (Ï•) := nhÏ•, Âµn âˆ’ Âµi = n ï£­
Ï•(Xjn ) âˆ’ EÏ•(X1 )ï£¸ .
n

The following is the CLT for empirical measure
given in Section 6.

j=1

Âµn

of all nodes in the system. The proof is

MEAN FIELD INTERACTION ON DYNAMIC RANDOM GRAPHS

13

Theorem 4.2. Suppose that Conditions 2.1 and 4.1 hold. Then {Î·xn (Ï•) : Ï• âˆˆ Ax } converges
as n â†’ âˆ to a mean 0 Gaussian field {Î·x (Ï•) : Ï• âˆˆ Ax } in the sense of convergence of finite
dimensional distributions, where for Ï•, Ïˆ âˆˆ Ax ,
E[Î·x (Ï•)Î·x (Ïˆ)] = h(I âˆ’ A)âˆ’1 Î¦, (I âˆ’ A)âˆ’1 Î¨iH ,

where Î¨ := Ïˆ(Xâˆ— ) âˆ’ EÏˆ(X1 ) âˆˆ H.

4.4. An example with explicit variance. In this section we provide an example where
one can get explicit variance from Theorem 4.2 for some functionals Ï• âˆˆ Ax .
Suppose Conditions 2.1 and 4.1 hold, and
e = c0 (y)b0 (x) + c1 (y)b1 (e
e + c3 (y),
Î³(y, x, x
e, Î¾)
x) + c2 (y)b2 (Î¾)

e x, x
e Î¾,
for some bounded functions b0 , b1 , b2 , c0 , c1 , c2 , c3 : Z â†’ R. Assume that Ï({Â·}), Î“(Â·,
e) are
even functions from Z to R+ , b1 , b2 are odd functions, and c0 , b0 , c3 are even functions. Also
suppose Î² = 1, Xi (0) = 0 and Î¾ij (0) = 0.
Consider the system
Z
y1[0,c0(y)b0 (Xi (sâˆ’))+c3 (y)] (z) Ni (ds dy dz),
Xi (t) =
Xt
Z
y1[0,Î² Î“(y,Î¾
(z) Nij (ds dy dz),
(4.1)
Î¾ij (t) =
e
ij (sâˆ’),Xi (sâˆ’),Xj (sâˆ’))]
Xt

n

1X
Î´(Xj (t),Î¾ij (t)) = L((Xk (t), Î¾ik (t)) | Xi [t]),
Î½i (t) = lim
nâ†’âˆ n
j=1

k 6= i.

e x, x
e Î¾,
From the even function property of Ï({Â·}), c0 , b0 , c3 and Î“(Â·,
e) we see that L(Xi (t)) =
L(âˆ’Xi (t)) and L(Î¾ij (t)|Xi [t], Xj [t]) = L(âˆ’Î¾ij (t)|Xi [t], Xj [t]). It then follows from the odd
function property of b1 , b2 that
Î“(y, Xi (s), Î½i (s)) = c0 (y)b0 (Xi (s)) + c3 (y)

(4.2)

and
hÎ³Ì„s,y (Xâˆ— (Ï‰1 )[s], Xâˆ— (Ï‰2 )(s), Â·), Î¸(s, Xâˆ— (Ï‰1 )[s], Xâˆ— (Ï‰2 )[s])i = c1 (y)b1 (Xâˆ— (Ï‰2 )(s))

(4.3)

for Ï‰1 , Ï‰2 âˆˆ â„¦v . The equality (4.2) and Theorem 2.1(a) imply that (4.1) is indeed the limiting
system (2.1).
Now consider Ï• âˆˆ Ax defined by
Z T
Z
Ï•(x) = xT âˆ’
b1 (xs ) ds yc1 (y) Ï(dy), x âˆˆ D([0, T ] : Z).
0

Z

For this example we can explicitly describe the asymptotic distribution of

Z
Z T
n 
âˆš
1 X
n
n
n
n
b1 (Xj (s)) ds yc1 (y) Ï(dy) .
Xj (T ) âˆ’
Î·x (Ï•) = nhÏ•, Âµ âˆ’ Âµi = âˆš
n
Z
0
j=1

Following the notation above Theorem 4.2, we have
Z
Z T
b1 (Xâˆ—,s (Ï‰)) ds yc1 (y) Ï(dy),
Î¦(Ï‰) = Xâˆ—,T (Ï‰) âˆ’
0

and from (4.3) we have
Z
1[0,c0 (y)b0 (Xâˆ—,s (Ï‰1 ))+c3 (y)] (z)
h(Ï‰1 , Ï‰2 ) :=
XT

Z

Ï‰ âˆˆ â„¦v ,

c1 (y)b1 (Xâˆ—,s (Ï‰2 ))
eâˆ— (Ï‰1 )(ds dy dz)
N
c0 (y)b0 (Xâˆ—,s (Ï‰1 )) + c3 (y)

14

BAYRAKTAR AND WU

for Ï‰1 , Ï‰2 âˆˆ â„¦v . The special form of Ï• allows us to determine (I âˆ’ A)âˆ’1 Î¦. Indeed, let
Z
eâˆ— (Ï‰)(ds dy dz)
y1[0,c0(y)b0 (Xâˆ—,s (Ï‰))+c3 (y)] (z) N
Î¨(Ï‰) =
XT
Z
y1[0,c0(y)b0 (Xâˆ—,s (Ï‰))+c3 (y)] (z) Nâˆ— (Ï‰)(ds dy dz) = Xâˆ—,T (Ï‰), Î-a.s. Ï‰ âˆˆ â„¦v ,
=
XT

where the second line uses the even function property of c0 , c3 and Ï({Â·}). Note that
Z
yc1 (y)b1 (Xâˆ—,s (Ï‰)) ds Ï(dy),
AÎ¨(Ï‰) =
[0,T ]Ã—Z

(I âˆ’ A)Î¨(Ï‰) = Xâˆ—,T (Ï‰) âˆ’

Z

T

b1 (Xâˆ—,s (Ï‰)) ds

0

Z

yc1 (y) Ï(dy) = Î¦(Ï‰),
Z

Î-a.s. Ï‰ âˆˆ â„¦v .

Therefore (I âˆ’A)âˆ’1 Î¦ = Î¨ and from Theorem 4.2 we have that Î·xn (Ï•) converges in distribution
to a mean zero Gaussian random variable with variance
i
h
ÏƒÏ•2 = kÎ¨k2H = E (Xi (T ))2 .

Remark 4.1. (a) We note that the variance is not simply the variance of Xi (T ) âˆ’
RT
R
b
(X
(s))
ds
1
i
0
Z yc1 (y) Ï(dy), since the propagation of chaos property in Theorem 2.2(b)
only guarantees asymptotic independence for finite collection of Xin .
(b) It is straightforward to extend above calculation to functionals that depend on states at
finitely many time instants. Indeed, taking


Z tk
Z
m
X
ak xtk âˆ’
b1 (xs ) ds yc1 (y) Ï(dy) , x âˆˆ D([0, T ] : Z),
Ï•(x) =
0

k=1

Z

for some 0 â‰¤ t1 < Â· Â· Â· < tm â‰¤ T , a1 , . . . , am âˆˆ R, m âˆˆ N, one has


Z tk
Z
m
X
ak Xâˆ—,tk (Ï‰) âˆ’
b1 (Xâˆ—,s (Ï‰)) ds yc1 (y) Ï(dy)
Î¦(Ï‰) =
k=1

0

Z

Pm

and (I âˆ’ A)âˆ’1 Î¦ = Î¨, where Î¨(Ï‰)âˆš= k=1 ak Xâˆ—,tk (Ï‰) for Îâˆ’a.s. Ï‰ âˆˆ â„¦v . It then follows
from Theorem 4.2 that Î·xn (Ï•) = nhÏ•, Âµn âˆ’ Âµi converges in distribution to a mean zero
Gaussian random variable with variance
ï£®
!2 ï£¹
m
X
ak Xi (tk ) ï£» .
ÏƒÏ•2 = kÎ¨k2H = E ï£°
k=1

5. Proofs of laws of large numbers and propagation of chaos results
We first state an elementary result on (conditionally) i.i.d. random variables. The proof is
omitted.
Lemma 5.1. Let {Yi : i = 1, . . . , n} be a collection of S-valued random variables defined on
some probability space (â„¦, F, P), where S is some Polish space. Suppose {Yi : i = 1, . . . , n} are
conditionally i.i.d. given some Ïƒ-field G âŠ‚ F. Then for each k âˆˆ N, there exists ak âˆˆ (0, âˆ)
such that
k
n
ak
1X
sup E
(f (Yi ) âˆ’ E[f (Yi )|G]) â‰¤ k/2 .
n
n
kf kâˆ â‰¤1
i=1

MEAN FIELD INTERACTION ON DYNAMIC RANDOM GRAPHS

15

5.1. Proofs of Theorems 2.1 and 2.2.
Proof of Theorem 2.1. (a) We first prove pathwise uniqueness. Suppose {(Xi , Î¾ij , Î½i ) : i, j âˆˆ
ei , Î¾eij , Î½ei ) : i, j âˆˆ N} are two solutions of (2.1) with Xi (0) = X(0)
e
N} and {(X
and Î¾ij (0) = Î¾eij (0)
for i, j âˆˆ N. By adding and subtracting terms, for t âˆˆ [0, T ],
Z
e
|y| 1[0,Î“(y,Xi (sâˆ’),Î½i (sâˆ’))] (z) âˆ’ 1[0,Î“(y,Xei (sâˆ’),eÎ½i (sâˆ’))] (z) Ni (ds dy dz)
EkXi âˆ’ Xi kâˆ—,t â‰¤ E
ZXt
ei (s), Î½ei (s)) ds Ï(dy)
|y| Î“(y, Xi (s), Î½i (s)) âˆ’ Î“(y, X
â‰¤E
[0,t]Ã—Z
Z
|y| |hÎ³(y, Xi (s), Â·), Î½i (s) âˆ’ Î½ei (s)i| ds Ï(dy)
â‰¤E
[0,t]Ã—Z
Z
ei (s), Â·), Î½ei (s)i ds Ï(dy)
|y| hÎ³(y, Xi (s), Â·) âˆ’ Î³(y, X
+E
[0,t]Ã—Z
Z


ei (s) ds Ï(dy)
|y|Î³y EdBL (Î½i (s), Î½ei (s)) + E Xi (s) âˆ’ X
â‰¤
[0,t]Ã—Z

â‰¤ CÎ³

Z

0

t

E

"

#

ei kâˆ—,s
sup dBL (Î½i (u), Î½ei (u)) + EkXi âˆ’ X

uâˆˆ[0,s]

!

ds,

where the fifth line uses the Lipschitz property of Î³ guaranteed by Condition 2.1(i) and
Remark 2.1(b). Similarly,
EkÎ¾ij âˆ’ Î¾eij kâˆ—,t
Z
|y| 1[0,Î² Î“(y,Î¾
(z) âˆ’ 1[0,Î² Î“(y,
â‰¤E
e
e Î¾eij (sâˆ’),X
ei (sâˆ’),X
ej (sâˆ’))] (z) Nij (ds dy dz)
ij (sâˆ’),Xi (sâˆ’),Xj (sâˆ’))]
Xt
Z
e Î¾ij (s), Xi (s), Xj (s)) âˆ’ Î“(y,
e Î¾eij (s), X
ei (s), X
ej (s)) ds Ï(dy)
|y|Î² Î“(y,
â‰¤E
[0,t]Ã—Z
Z


ei (s) + E Xj (s) âˆ’ X
ej (s) ds Ï(dy)
|y|Î²Î³y E Î¾ij (s) âˆ’ Î¾eij (s) + E Xi (s) âˆ’ X
â‰¤
[0,t]Ã—Z

Z t

ei kâˆ—,s + EkXj âˆ’ X
ej kâˆ—,s ds,
EkÎ¾ij âˆ’ Î¾eij kâˆ—,s + EkXi âˆ’ X
â‰¤ Î²CÎ³
0

e guaranteed by Condition 2.1(i) and
where the fourth line uses the Lipschitz property of Î“
Remark 2.1(b). Also by Fatouâ€™s lemma,
#
"
#
"
E

sup dBL (Î½i (s), Î½ei (s)) = E

sâˆˆ[0,t]

sup sup |hf, Î½i (s) âˆ’ Î½ei (s)i|

sâˆˆ[0,t] f âˆˆB1


1 X
ej kâˆ—,t + EkÎ¾ij âˆ’ Î¾eij kâˆ—,t
EkXj âˆ’ X
nâ†’âˆ n
j=1


ej kâˆ—,t + EkÎ¾ij âˆ’ Î¾eij kâˆ—,t .
â‰¤ sup EkXj âˆ’ X
n

â‰¤ lim inf
i,jâˆˆN

Combining these three estimates gives

ej kâˆ—,t + EkÎ¾ij âˆ’ Î¾eij kâˆ—,t â‰¤ 2(1 + Î²)CÎ³
EkXj âˆ’ X

Z

t



ej kâˆ—,s + EkÎ¾ij âˆ’ Î¾eij kâˆ—,s ds.
sup EkXj âˆ’ X

0 i,jâˆˆN

16

BAYRAKTAR AND WU

The pathwise uniqueness then follows from Gronwallâ€™s lemma.
Next we will adapt the argument in [20] to prove existence of the system (2.1). For m âˆˆ N,
let
Z
m
e
1[0,Î³y ] (z)Ni (ds dy dz),
(5.1)
Ni ([0, t] Ã— A) :=
]Ã—A
[0, âŒŠmtâŒ‹
m

e m ([0, t] Ã— A) :=
N
ij

Z

[0,

âŒŠmtâŒ‹
]Ã—A
m

1[0,Î²Î³y ] (z)Nij (ds dy dz),

(5.2)

for t âˆˆ [0, T ], A âŠ‚ B(Z Ã— R+ ) and i, j âˆˆ N. Consider an approximation system that is driven
e m, N
e m : i, j âˆˆ N} as follows:
by {N
i
ij
Z
m
e m (ds dy dz),
e
y1[0,Î“(y,Xe m (sâˆ’),eÎ½ m (sâˆ’))] (z) N
(5.3)
Xi (t) = Xi (0) +
i
i
i
Xt
Z
m
em
y1[0,Î² Î“(y,
(5.4)
Î¾eij
(t) = Î¾ij (0) +
e Î¾em (sâˆ’),X
e m (sâˆ’),X
e m (sâˆ’))] (z) Nij (ds dy dz),
Î½eim (t) = lim

nâ†’âˆ

1
n

1
nâ†’âˆ n

= lim

Xt
n
X
j=1
n
X
j=1

ij

i

Î´(Xe m (t),Î¾em (t)) = lim
j

nâ†’âˆ

ij

Î´(Xe m ( k ),Î¾em ( k ))
j

m

ij

m

for

j

1
n

n
X
j=1

Î´(Xe m ( âŒŠmtâŒ‹ ),Î¾em ( âŒŠmtâŒ‹ ))
j

m

k
k+1
â‰¤t<
,
m
m

ij

m

k âˆˆ N0 .

Note that the system is piece-wise constant and determined recursively over intervals of length
1
m . We claim that the following holds for each k âˆˆ N0 : There exists a unique solution
e m [ k ], Î¾em [ k ], Î½em [ k ]) : i, j âˆˆ N}, which is exchangeable, namely for each K âˆˆ N and
{(X
ij m
i m
i m
permutation Ï€ on {1, . . . , K},
eim [
{(X

k em k
k
k
k
d
m
e m [ k ], Î¾em
], Î¾ij [ ], Î½eim [ ]) : i, j âˆˆ N} = {(X
], Î½eÏ€(i)
[ ]) : i, j âˆˆ N}.
Ï€(i)
Ï€(i)Ï€(j) [
m
m
m
m
m
m

m : j âˆˆ N) for each fixed i, as the
Note that we are not claiming the exchangeability of (Î¾eij
m . Now we prove this claim by induction.
evolution of Î¾eiim is different from that of other Î¾eij
Since (Xi (0)) and (Î¾ij (0)) are all i.i.d., the claim holds for k = 0. Now assume the claim
k k+1
holds for some k âˆˆ N0 . Since the solution is linear on [ m
, m ), we have the existence and
k+1 em k+1
m
e
uniqueness of {(Xi [ m ], Î¾ij [ m ]) : i, j âˆˆ N} by Condition 2.1, and the exchangeability of
e m [ k+1 ], Î¾em [ k+1 ], Î½em [ k ]) : i, j âˆˆ N}. This further implies the existence and uniqueness of
{(X
i
ij m
i m
m
)
by
de
Finettiâ€™s
theorem (cf. [18, Theorem 4.1], see also [1, Theorem 3.1]), and the
Î½eim ( k+1
m
m
em k+1 em [ k+1 ]) : i, j âˆˆ N}. Therefore the claim holds for
e
exchangeability of {(Xi [ k+1
i
m ], Î¾ij [ m ], Î½
m
k + 1 and hence holds for each k âˆˆ N0 by induction.
Using Condition 2.1 and the evolution in (5.3) and (5.4), we have

eim kâˆ—,T â‰¤ E|Xi (0)| + T CÎ³ < âˆ,
EkX

m
EkÎ¾eij
kâˆ—,T â‰¤ E|Î¾ij (0)| + T Î²CÎ³ < âˆ,

e m (t), Î¾em (t)) : i, j âˆˆ N} in Zâˆ for each t âˆˆ [0, T ]. Moreover,
which implies the tightness of {(X
ij
i
for the fluctuations, one can easily verify that




1
1
m
m
m
m
e
e
e
e
E|Xi (Ï„ + Î´) âˆ’ Xi (Ï„ )| â‰¤ CÎ³ Î´ +
, E|Î¾ij (Ï„ + Î´) âˆ’ Î¾ij (Ï„ )| â‰¤ Î²CÎ³ Î´ +
, (5.5)
m
m

MEAN FIELD INTERACTION ON DYNAMIC RANDOM GRAPHS

17

for each Î´ âˆˆ (0, 1) and Ft -stopping times Ï„ with Ï„ âˆˆ [0, T âˆ’ Î´] a.s. Therefore the sequence
e m , Î¾em ) : i, j âˆˆ N} is tight in D([0, T ] : Zâˆ ) by applying Aldousâ€™ tightness criterion
of {(X
i
ij
[19, Theorem 2.7] with m â†’ âˆ and then Î´ â†’ 0.
e m , Î¾em ) : i, j âˆˆ N} â‡’ {(X
ei , Î¾eij ) : i, j âˆˆ
Taking a subsequence if necessary, we assume that {(X
i
ij
e Fe, P),
e as m â†’ âˆ. Since {(X
e m , Î¾em ) : j âˆˆ N, j 6= i}
N}, defined on some probability space (â„¦,
j
ij
ej , Î¾eij ) : j âˆˆ N, j 6= i}, and we can define
is exchangeable, so is {(X
n

1X
Î´(Xej (t),Î¾eij (t)) .
Î½ei (t) := lim
nâ†’âˆ n

(5.6)

j=1

It then follows from [20, Lemma 2.1] (see also [18, Lemma 4.2]), and appealing to the Skorokhod representation theorem, that
e m (tk ), Î¾em (tk ), Î½em (tk )) : i, j âˆˆ N, k âˆˆ [K]}
{(X
i
ij
i

ei (tk ), Î¾eij (tk ), Î½ei (tk )) : i, j âˆˆ N, k âˆˆ [K]}
â‡’ {(X

(5.7)

in Zâˆ Ã— P(Z4 )âˆ as m â†’ âˆ, for each K âˆˆ N and t1 , . . . , tK âˆˆ TC âŠ‚ [0, T ], where [0, T ] \ TC
is at most countable. Let
n
X
e m := lim 1
Î´(Xe m (Â·),Î¾em (Â·)) .
Î
i
nâ†’âˆ n
j
ij
j=1

Denoting by ÏÌ„ the Prohorov metric on P(Z2 ), we have
h
i
e m {x âˆˆ D([0, T ] : Z2 ) : |x(Ï„ + Î´) âˆ’ x(Ï„ )| â‰¥ Îµ} + Îµ
EÏÌ„(e
Î½im (Ï„ + Î´), Î½eim (Ï„ )) â‰¤ E Î
i

e m (Ï„ + Î´), Î¾em (Ï„ + Î´)) âˆ’ (X
e m (Ï„ ), Î¾em (Ï„ ))| â‰¥ Îµ) + Îµ
= P(|(X
j
ij
j
ij
â‰¤

(1 + Î²)CÎ³ (Î´ +
Îµ

1
m)

+Îµ

(5.8)

for each Îµ, Î´ âˆˆ (0, 1) and Ft -stopping times Ï„ with Ï„ âˆˆ [0, T âˆ’ Î´] a.s., where the last line uses
e m , Î¾em , Î½em ) : i, j âˆˆ N} is tight in D([0, T ] : Zâˆ Ã— (P(Z2 ))âˆ ) by applying
(5.5). Therefore {(X
i
ij
i
Aldousâ€™ tightness criterion [19, Theorem 2.7] with m â†’ âˆ, Î´ â†’ 0 and then Îµ â†’ 0. By (5.7),
the finite dimensional distributions converge for time instants in a dense subset set of [0, T ].
So
m m
eim , Î¾eij
ei , Î¾eij , Î½ei ) : i, j âˆˆ N}
{(X
, Î½ei ) : i, j âˆˆ N} â‡’ {(X
(5.9)

in D([0, T ] : Zâˆ Ã— (P(Z2 ))âˆ ).
ei , Î¾eij , Î½ei ) : i, j âˆˆ N} satisfies (2.1). We will need the weak conNext, we will verify that {(X
vergence of stochastic integrals with respect to Poisson random measures later, and the following notations from [21]. Let H := L2 (Z Ã— R+ , Ï(dy) dz). Given a polish space S, a collection
of S-valued stochastic processes S m , S, and Poisson random measures {Yim (ds dy dz) : i âˆˆ N}
and {Yi (ds dy dz) : i âˆˆ N} viewed as H# -semimartingales (see [21, Section 3.3] for the precise
definition), we say that (S m , {Yim : i âˆˆ N}) â‡’ (S, {Yi : i âˆˆ N}) in D([0, T ] : S) âŠ— H# if


Z
m
m
Ï•k (y, z) Yi (ds dy dz) : i âˆˆ N, k âˆˆ [K]
S ,
XÂ·

 Z
Ï•k (y, z) Yi (ds dy dz) : i âˆˆ N, k âˆˆ [K]
â‡’ S,
XÂ·

18

BAYRAKTAR AND WU

in D([0, T ] : S Ã— Râˆ ) for each K âˆˆ N and Ï•1 , . . . , Ï•K âˆˆ H. Now take a dense sequence
e m and N
e m defined in (5.1) and (5.2). Clearly
{Ï•k } âŠ‚ H and recall N
i
ij



Z
Z
m
m
m
m
m
e
e
e
e
Ï•k (y, z)Nij (ds dy dz) : i, j âˆˆ N
Ï•k (y, z)Ni (ds dy dz),
Xi (t), Î¾ij (t), Î½ei (t),
Xt

Xt

is tight in Zâˆ Ã— (P(Z2 ))âˆ Ã— Râˆ , for each t âˆˆ [0, T ]. Using Condition 2.1 and the Cauchyâ€“
Schwarz inequality we have
Z
Z
m
e m (ds dy dz)
e
E
Ï•k (y, z) N
Ï•k (y, z) Ni (ds dy dz) âˆ’
i
[0,Ï„ +Î´]Ã—ZÃ—R+

â‰¤

â‰¤
â‰¤
E

Z







Î´+

1
m

1
Î´+
m
Î´+

1
m





E
E

â‰¤

Î´+

1
m



[0,Ï„ ]Ã—ZÃ—R+

ZÃ—R+

"Z

kÏ•k kH

[0,Ï„ +Î´]Ã—ZÃ—R+



Z

|Ï•k (y, z)|1[0,Î³y ] (z) Ï(dy) dz

ZÃ—R+

p

|Ï•k (y, z)| Ï(dy) dz

kÏ•k kH

1#
2

ZÃ—R+

1[0,Î³y ] (z) Ï(dy) dz

CÎ³ ,

eijm (ds dy dz) âˆ’
Ï•k (y, z) N
p

 1 Z
2

2

Z

[0,Ï„ ]Ã—ZÃ—R+

Î²CÎ³ ,

eijm (ds dy dz)
Ï•k (y, z) N

for each Î´ âˆˆ (0, 1) and Ft -stopping times Ï„ with Ï„ âˆˆ [0, T âˆ’ Î´] a.s. Combining this with (5.5)
and (5.8) implies that



Z
Z
m
m
m em m
e
e
e
Ï•k (y, z)Nij (ds dy dz) : i, j âˆˆ N
Ï•k (y, z)Ni (ds dy dz),
Xi , Î¾ij , Î½ei ,
XÂ·

XÂ·

is tight in D([0, T ] : Zâˆ Ã— (P(Z2 ))âˆ Ã— Râˆ ), once again by Aldousâ€™ tightness criterion [19,
e m , Î¾em , Î½em , N
e m, N
e m) :
Theorem 2.7] (by taking m â†’ âˆ, Î´ â†’ 0 and then Îµ â†’ 0). Therefore {(X
i
ij
i
i
ij
i, j âˆˆ N} is tight in D([0, T ] : Zâˆ Ã— (P(Z2 ))âˆ ) âŠ— H# . From this, (5.9), (5.1) and (5.2) we have
that, taking a subsequence if necessary,
m m em em
eim , Î¾eij
ei , Î¾eij , Î½ei , N
ei , N
eij ) : i, j âˆˆ N}
{(X
, Î½ei , Ni , Nij ) : i, j âˆˆ N} â‡’ {(X

(5.10)

e Fe, P),
e in D([0, T ] : Zâˆ Ã— (P(Z2 ))âˆ ) âŠ— H# , where
defined again on the probability space (â„¦,
Z
Z
d
d
eij ([0, t] Ã— Â·) =
ei ([0, t] Ã— Â·) =
1[0,Î²Î³y ] (z)Nij (ds dy dz)
1[0,Î³y ] (z)Ni (ds dy dz), N
N
[0,t]Ã—Â·

[0,t]Ã—Â·

are mutually independent.
Noting that the maps

Z Ã— P(Z2 ) âˆ‹ (x, Î½) 7â†’ y1[0,Î“(y,x,Î½)] (z) âˆˆ H,

Z3 âˆ‹ (Î¾, x, xâ€² ) 7â†’ y1[0,Î² Î“(y,Î¾,x,x
â€² )] (z) âˆˆ H
e

are continuous, from (5.10) and the continuous mapping theorem we have
y1[0,Î“(y,Xe m (Â·),eÎ½ m (Â·))] (z) â‡’ y1[0,Î“(y,Xei (Â·),eÎ½i (Â·))] (z) in D([0, T ] : H),
i

i

y1[0,Î² Î“(y,
e Î¾em (Â·),X
e m (Â·),X
e m (Â·))] (z) â‡’ y1[0,Î² Î“(y,
e Î¾eij (Â·),X
ei (Â·),X
ej (Â·))] (z) in D([0, T ] : H),
ij

i

j

MEAN FIELD INTERACTION ON DYNAMIC RANDOM GRAPHS

19

jointly with the convergence in (5.10). Also note that for any stochastic process Z âˆˆ D([0, T ] :
H) with sup0â‰¤tâ‰¤T kZ(t)kH â‰¤ 1, we have
E

Z

XÂ·

â‰¤ 2E
â‰¤ 2E

ei (ds dy dz)
Z(s, y, z) N

Z

XT

+E
âˆ—,T

Z

XÂ·

eij (ds dy dz)
Z(s, y, z) N

âˆ—,T

|Z(s, y, z)|1[0,(Î²+1)Î³y ] (z) ds Ï(dy) dz

Z

2

XT

|Z(s, y, z)| ds Ï(dy) dz

âˆš q
â‰¤ 2 T T (Î² + 1)CÎ³

 1 Z
2

1

2

XT

1[0,(Î²+1)Î³y ] (z) ds Ï(dy) dz

by the Cauchyâ€“Schwarz inequality and Condition 2.1. It then follows from the convergence of
ei , Î¾eij , Î½ei ) : i, j âˆˆ N}
stochastic integrals (cf. [21, Theorem 4.2]), (5.3), (5.4), and (5.6) that {(X
is a solution to the limiting system (2.1). From the pathwise uniqueness established earlier it
now follows that there exists a pathwise solution of (2.1).
e m , Î¾em , Î½em ) given in (5.3) and (5.4). We claim that the
(b) Recall the discretized system (X
i
ij
i
following holds for each k âˆˆ N0 :
e m [ k ] : i âˆˆ N} are i.i.d.;
(i) {X
i m
e m ) : i, j âˆˆ N};
e m [ k ] : i âˆˆ N} are independent of {(Î¾ij (0), N
(ii) {X
ij
i m
e m ) : j âˆˆ N, j 6= i} are i.i.d. conditioning on X
e m [ k ], and the condie m [ k ], Î¾em [ k ], N
(iii) {(X
ij m
ij
i m
j m
e m [ k ], Î¾em [ k ], N
e m ) : j âˆˆ N, j 6= i} | X
e m [ k ]) = Î¦k,m(X
e m [ k ]) for some
tional law L({(X
j m
ij m
ij
i m
i m
k
k
k
] : Z) â†’ P((D([0, m
] : Z) Ã— D([0, m
] : Z) Ã— MT )âˆ )
measurable map Î¦k,m : D([0, m
independent of the choice of i.

Again, we will prove this by induction. Since (Xi (0)) and (Î¾ij (0)) are all i.i.d., the claim holds
for k = 0. Now assume the claim holds for some k âˆˆ N0 . From (iii) we have
Î½eim (

k
e m ( k ), Î¾em ( k )) | X
e m [ k ]) = Î¦
e k,m(X
e m [ k ])
) = L((X
j
ij
i
i
m
m
m
m
m

e k,m : D([0, k ] : Z) â†’ P(Z2 ) independent of the choice of i.
for some measurable map Î¦
m
e m we see that {X
e m [ k+1 ] : i âˆˆ N} are i.i.d., and
From (i), (ii) and the evolution of X
i
i
m
m
e ) : i, j âˆˆ N}. It then follows from the evolution of Î¾em that
independent of {(Î¾ij (0), N
ij
ij
k+1 em k+1 e m
k+1
m
m
e
e
{(Xj [ m ], Î¾ij [ m ], Nij ) : j âˆˆ N, j 6= i} are i.i.d. conditioning on Xi [ m ], and the cone m [ k+1 ], Î¾em [ k+1 ], N
e m ) : j âˆˆ N, j 6= i} | X
e m [ k+1 ]) = Î¦k+1,m (X
e m [ k+1 ]) for
ditional law L({(X
j
ij m
ij
i
i
m
m
m
k+1
k+1
k+1
some measurable map Î¦k+1,m : D([0, m ] : Z) â†’ P((D([0, m ] : Z) Ã— D([0, m ] : Z) Ã— MT )âˆ )
independent of the choice of i. Therefore the claim holds for k + 1 and hence holds for each
k âˆˆ N0 by induction.
Now from the convergence (5.10) we have that {Xi : i âˆˆ N} are i.i.d., and independent of
{(Î¾ij (0), Nij ) : i, j âˆˆ N}. From the evolution of Î¾ij we see that Î¾ij = Î¨(Î¾ij (0), Xi , Xj , Nij ) for
some measurable map Î¨. Therefore {(Xj [t], Î¾ij [t]) : j âˆˆ N, j 6= i} are i.i.d. conditioning on
Xi [t], and Î½i (t) = L((Î¾ij (t), Xj (t)) | Xi [t]) = Î¦t (Xi [t]) for some measurable map Î¦t : D([0, t] :
Z) â†’ P(Z2 ) independent of the choice of i. This gives (b) and completes the proof.


20

BAYRAKTAR AND WU

Proof of Theorem 2.2. (a) For each fixed i âˆˆ [n] and t âˆˆ [0, T ], we have

EkXin âˆ’ Xi kâˆ—,t
Z
|y| 1[0,Î“(y,Xin (sâˆ’),Î½in (sâˆ’))] (z) âˆ’ 1[0,Î“(y,Xi (sâˆ’),Î½i (sâˆ’))] (z) Ni (ds dy dz)
â‰¤E
Xt
Z
|y| |Î“(y, Xin (s), Î½in (s)) âˆ’ Î“(y, Xi (s), Î½i (s))| ds Ï(dy).
=E
[0,t]Ã—Z

Since Î³ is Lipschitz by Condition 2.1(i), and {(Xj (s), Î¾ij (s)) : j âˆˆ N, j 6= i} are conditionally
independent given Xi [s] with the conditional law L((Xj (s), Î¾ij (s))|Xi [s]) = Î½i (s), for j 6= i,
by Theorem 2.1(b), we have
E|Î“(y, Xin (s), Î½in (s)) âˆ’ Î“(y, Xi (s), Î½i (s))|
â‰¤E

n

n

j=1

j=1

1X
1X
n
Î³(y, Xin (s), Xjn (s), Î¾ij
(s)) âˆ’
Î³(y, Xi (s), Xj (s), Î¾ij (s))
n
n

Z
n
1X
e Î½i (s)(de
e
+E
Î³(y, Xi (s), x
e, Î¾)
x dÎ¾)
Î³(y, Xi (s), Xj (s), Î¾ij (s)) âˆ’
n
2
Z
j=1
ï£¶
ï£«
n
n
X
X
1
Îº1
1
n
E|Xjn (s) âˆ’ Xj (s)| +
E|Î¾ij
(s) âˆ’ Î¾ij (s)| + âˆš ï£¸
â‰¤ Î³y ï£­E|Xin (s) âˆ’ Xi (s)| +
n
n
n
j=1
j=1


Îº1
n
âˆ’ Î¾ij kâˆ—,s + âˆš
â‰¤ Î³y 2 max EkXin âˆ’ Xi kâˆ—,s + max EkÎ¾ij
,
n
iâˆˆ[n]
i,jâˆˆ[n]
Îº1
e is Lipschitz
in the second inequality follows from Lemma 5.1. Also since Î“
where the term âˆš
n
by Condition 2.1(i), we have
n
EkÎ¾ij
âˆ’ Î¾ij kâˆ—,t
Z
|y| 1[0,Î²(n)Î“(y,Î¾
(z) âˆ’ 1[0,Î² Î“(y,Î¾
(z) Nij (ds dy dz)
â‰¤E
n
n
n
e
e
ij (sâˆ’),Xi (sâˆ’),Xj (sâˆ’))]
ij (sâˆ’),Xi (sâˆ’),Xj (sâˆ’))]
ZXt
e Î¾ n (s), X n (s), X n (s)) âˆ’ Î² Î“(y,
e Î¾ij (s), Xi (s), Xj (s)) ds Ï(dy)
|y| Î²(n)Î“(y,
=E
ij
i
j
[0,t]Ã—Z
Z
|y| [Î³y |Î²(n) âˆ’ Î²|
â‰¤E
[0,t]Ã—Z


n
(s) âˆ’ Î¾ij (s)| + |Xin (s) âˆ’ Xi (s)| + |Xjn (s) âˆ’ Xj (s)| ds Ï(dy)
+Î³y Î² |Î¾ij
Z t
Z t
n
â‰¤ CÎ³ |Î²(n) âˆ’ Î²|t + CÎ³ Î²
âˆ’ Î¾ij kâˆ—,s ds + 2CÎ³ Î²
max EkÎ¾ij
max EkXin âˆ’ Xi kâˆ—,s ds.
0 i,jâˆˆ[n]

0 iâˆˆ[n]

Combining above three displays gives

n
max EkXin âˆ’ Xi kâˆ—,t + max EkÎ¾ij
âˆ’ Î¾ij kâˆ—,t
iâˆˆ[n]
i,jâˆˆ[n]

Z t
Îº1 CÎ³ t
n
n
â‰¤ 2CÎ³ (Î² + 1)
max EkXi âˆ’ Xi kâˆ—,s + max EkÎ¾ij âˆ’ Î¾ij kâˆ—,s ds + âˆš + CÎ³ |Î²(n) âˆ’ Î²|t.
n
iâˆˆ[n]
i,jâˆˆ[n]
0

From Gronwallâ€™s inequality we have (2.3).
(b) Using (a), Theorem 2.1(b) and a standard argument (see [29, Chapter 1]) one has (2.4).

MEAN FIELD INTERACTION ON DYNAMIC RANDOM GRAPHS

21

(c) Fix t âˆˆ [0, T ] and i âˆˆ N. First note that Î½i is well defined, thanks to the conditional
i.i.d. property in Theorem 2.1(b) of {(Xj , Î¾ij ) : j âˆˆ N, j 6= i} given Xi . Let
n

Î½Ì„in (t)

1X
Î´(Xi (t),Î¾ij (t)) ,
:=
n

n

1X
:=
Î´(Xi (Â·),Î¾ij (Â·)) .
n

Î½Ì„in

j=1

j=1

From (2.3) we have
EdBL (Î½in (t), Î½Ì„in (t)) â‰¤
EdBL (Î½in , Î½Ì„in ) â‰¤

n

1X
n
E |Xjn (t) âˆ’ Xj (t)| + |Î¾ij
(t) âˆ’ Î¾ij (t)| â†’ 0,
n

1
n

j=1
n
X
j=1


n
E kXjn âˆ’ Xj kâˆ—,T + kÎ¾ij
âˆ’ Î¾ij kâˆ—,T â†’ 0.

Using the definition of Î½i (t) in (2.1) and Î½i in (2.5), we have
Î½Ì„in (t) â†’ Î½i (t),

Î½Ì„in â†’ Î½i a.s.

Combining these gives (2.5) and (2.6). The last two statements (2.7) and (2.8) follow immediately from (2.5) and (2.6), respectively.

5.2. Proofs of Theorems 3.1 and 3.2.
Proof of Theorem 3.1. (a) Since the limiting system is McKeanâ€“Vlasov, the proof of existence
and uniqueness is standard (cf. [29, Chapter 1], see also [15, Theorem 2.1]) and hence omitted.
(b) It would be helpful to freeze the slow components X and analyze the averaging effect
of the fast component Î¾ first. Consider the following auxiliary process with âˆ† = âˆ†(n) â†’ 0
(whose precise value will be stated later):


t
âˆ†
Xi (t) = Xi âŒŠ âŒ‹âˆ† ,
âˆ†

 Z
t
n,âˆ†
n
Î¾ij (t) = Î¾ij âŒŠ âŒ‹âˆ† +
y1[0,Î²(n)Î“(y,Î¾
(z) Nij (ds dy dz).
n,âˆ†
âˆ†
âˆ†
e
ij (sâˆ’),Xi (sâˆ’),Xj (sâˆ’))]
âˆ†
[âŒŠ t âŒ‹âˆ†,t]Ã—ZÃ—R+
âˆ†

Note that for each t âˆˆ [0, T ],
E

sup
t
âŒ‹âˆ†,t]
sâˆˆ[âŒŠ âˆ†

|Xiâˆ† (s) âˆ’

Xi (s)| â‰¤ E
â‰¤âˆ†

Z

t
[âŒŠ âˆ†
âŒ‹âˆ†,t]Ã—ZÃ—R+

Z

R+

|y|1[0,Î“(y,Xi (sâˆ’),Î½i (sâˆ’))] (z) Ni (ds dy dz)

|y|Î³y Ï(dy) = CÎ³ âˆ†

(5.11)

by Condition 2.1(i). From this we have
t
âŒ‹âˆ†, t])
âˆ†
|Xin (s) âˆ’ Xi (s)|

n,âˆ†
n
(t) 6= Î¾ij
(t)) â‰¤ P((Xiâˆ† (s), Xjâˆ† (s)) 6= (Xin (s), Xjn (s)) for some s âˆˆ [âŒŠ
P(Î¾ij

â‰¤ 2E

sup
t
sâˆˆ[âŒŠ âˆ†
âŒ‹âˆ†,t]

|Xiâˆ† (s) âˆ’ Xi (s)| + 2E

â‰¤ 2CÎ³ âˆ† + 2EkXin âˆ’ Xi kâˆ—,t .

sup
t
sâˆˆ[âŒŠ âˆ†
âŒ‹âˆ†,t]

(5.12)

Also note that from Condition 2.1(i) and Condition 3.1(ii) we have
EkXiâˆ† k2âˆ—,T â‰¤ EkXi k2âˆ—,T â‰¤ Îº1 (E|Xi (0)|2 + CÎ³2 + CÎ³,2 ) < âˆ.

(5.13)

22

BAYRAKTAR AND WU

Let
n,âˆ†
(s)) âˆ’
(s) := Î³(y, Xiâˆ† (s), Xjâˆ† (s), Î¾ij
Qn,âˆ†,y
i,j

Z

Z

e Q(X âˆ† (s), X âˆ† (s), dÎ¾).
e
Î³(y, Xiâˆ† (s), Xjâˆ† (s), Î¾)
i
j



 n s
 n

s
âŒŠ âˆ† âŒ‹âˆ† , Î¾ik
âŒ‹âˆ† , Î¾ij
(s) and
âŒŠ âˆ†s âŒ‹âˆ† , clearly Qn,âˆ†,y
Given Xi âŒŠ âˆ†s âŒ‹âˆ† , Xj âŒŠ âˆ†s âŒ‹âˆ† , Xk âŒŠ âˆ†
i,j
Qn,âˆ†,y
(s) are conditionally independent. Using this, (5.13), Condition 3.1(i), and an api,k
plication of time change s 7â†’ s (so that the evolution of Î¾ n,âˆ† matches that of Ye introduced
ij

Î²(n)

in Condition 3.1) we have

h
i
n,âˆ†,y
E Qn,âˆ†,y
(s)Q
(s)
i,j
i,k
 s
 s
 s


 s


i
 s
n h
n
n
âŒŠ
âŒŠ
âŒŠ
âŒ‹âˆ†
,
X
âŒ‹âˆ†
,
X
âŒŠ
âŒ‹âˆ†
,
Î¾
âŒ‹âˆ†
,
Î¾
âŒ‹âˆ†
âŒŠ
= E E Qn,âˆ†,y
(s)
X
j
i
k
ij
ik
i,j
 sâˆ† 
h
 sâˆ† 
 sâˆ† io
 sâˆ† 
 sâˆ† 
n,âˆ†,y
n
n
Â·E Qi,k (s) Xi âŒŠ âŒ‹âˆ† , Xj âŒŠ âŒ‹âˆ† , Xk âŒŠ âŒ‹âˆ† , Î¾ij âŒŠ âŒ‹âˆ† , Î¾ik âŒŠ âŒ‹âˆ†
âˆ†
âˆ†
âˆ†
âˆ†
âˆ†





s
âˆ†
âˆ†
2
âˆ†
âˆ†
2
â‰¤Îº
e Î²(n) s âˆ’ âŒŠ âŒ‹âˆ† Î³y E 1 + kXi kâˆ—,T + kXj kâˆ—,T 1 + kXi kâˆ—,T + kXk kâˆ—,T

âˆ† s

2 2
â‰¤ Îº2 Î³y Îº
e Î²(n) s âˆ’ âŒŠ âŒ‹âˆ†
âˆ†

for each i, j, k âˆˆ [n] with j 6= k. Therefore

ï£¶2 ï£¹

n
n
n
n
2 
i
X
1 X
1 X X h n,âˆ†,y
1
n,âˆ†,y
n,âˆ†,y
n,âˆ†,y
ï£¸
ï£»
ï£°
ï£­
E Qi,j (s)
Qi,j (s)
E Qi,j (s)Qi,k (s) + 2
= 2
E
n
n
n
ï£®ï£«

j=1 k6=j

j=1



â‰¤ Îº2 Î³y2 Îº
e2 Î²(n) s âˆ’ âŒŠ

and hence
Z

n

kâˆ†

E
(kâˆ’1)âˆ†

j=1



1 X n,âˆ†,y
Qi,j (s) ds â‰¤
n
j=1

Z

kâˆ†
(kâˆ’1)âˆ†

âˆš



s
âŒ‹âˆ†
âˆ†



+

4Î³y2
n

,



 2Î³
âˆš
s
y
Îº2 Î³y Îº
e Î²(n) s âˆ’ âŒŠ âŒ‹âˆ† + âˆš
âˆ†
n

Z
Îº2 Î³y Î²(n)âˆ†
2âˆ†Î³y
Îº
e(s) ds + âˆš
=
Î²(n) 0
n
2âˆ†Î³y
Îº3 Î³y
+ âˆš
â‰¤
Î²(n)
n



for each k âˆˆ N.
Now we show (3.2). Note that
EkXin âˆ’ Xi kâˆ—,t
Z
|y| 1[0,Î“(y,Xin (sâˆ’),Î½in (sâˆ’))] (z) âˆ’ 1[0,Î“(y,Xi (sâˆ’),Î½i (sâˆ’))] (z) Ni (ds dy dz)
â‰¤E
Xt
Z
|y| |Î“(y, Xin (s), Î½in (s)) âˆ’ Î“(y, Xi (s), Î½i (s))| ds Ï(dy).
=E
[0,t]Ã—Z

ds

(5.14)

MEAN FIELD INTERACTION ON DYNAMIC RANDOM GRAPHS

23

Fixing y âˆˆ N and s âˆˆ [0, T ], we have

E|Î“(y, Xin (s), Î½in (s)) âˆ’ Î“(y, Xi (s), Î½i (s))|
Z
n
1X
n
n
n
e Âµs (de
e
Î³(y, Xi (s), x
e, Î¾)
x) Q(Xi (s), x
e, dÎ¾)
Î³(y, Xi (s), Xj (s), Î¾ij (s)) âˆ’
=E
n
Z2
j=1

â‰¤

1
n

n
X
j=1

n,âˆ†
n
E Î³(y, Xin (s), Xjn (s), Î¾ij
(s)) âˆ’ Î³(y, Xiâˆ† (s), Xjâˆ† (s), Î¾ij
(s))


Z
n 
1X
n,âˆ†
âˆ†
âˆ†
âˆ†
âˆ†
âˆ†
âˆ†
e
e
Î³(y, Xi (s), Xj (s), Î¾ij (s)) âˆ’ Î³(y, Xi (s), Xj (s), Î¾) Q(Xi (s), Xj (s), dÎ¾)
+E
n
Z
j=1

Z
n
1X
e Q(X âˆ† (s), X âˆ† (s), dÎ¾)
e
+
E
Î³(y, Xiâˆ† (s), Xjâˆ† (s), Î¾)
i
j
n
Z
j=1
Z
e Q(Xi (s), Xj (s), dÎ¾)
e
âˆ’ Î³(y, Xi (s), Xj (s), Î¾)
Z

n Z
1X
e Q(Xi (s), Xj (s), dÎ¾)
e
Î³(y, Xi (s), Xj (s), Î¾)
+E
n
j=1 Z
Z
e Âµs (de
e .
Î³(y, Xi (s), x
e, Î¾)
x) Q(Xi (s), x
e, dÎ¾)
âˆ’

(5.15)

Z2

Next we analyze each of the four terms on the RHS of (5.15). For the first term, it follows
from Condition 2.1(i), the exchangeability of {(Xjn , Xjâˆ† , Xj ) : j âˆˆ N}, (5.11) and (5.12) that
n,âˆ†
n
E Î³(y, Xin (s), Xjn (s), Î¾ij
(s)) âˆ’ Î³(y, Xiâˆ† (s), Xjâˆ† (s), Î¾ij
(s))

â‰¤ Î³y E|Xin (s) âˆ’ Xi (s)| + E|Xiâˆ† (s) âˆ’ Xi (s)| + E|Xjn (s) âˆ’ Xj (s)| + E|Xjâˆ† (s) âˆ’ Xj (s)|

n,âˆ†
n
+P(Î¾ij
(s) 6= Î¾ij
(s))
â‰¤ 4Î³y EkXin âˆ’ Xi kâˆ—,s + 4Î³y CÎ³ âˆ†.

For the second term, from (5.14) we have
Z

t

E

0

â‰¤


Z
n 
1X
n,âˆ†
e Q(X âˆ† (s), X âˆ† (s), dÎ¾)
e ds
(s)) âˆ’ Î³(y, Xiâˆ† (s), Xjâˆ† (s), Î¾)
Î³(y, Xiâˆ† (s), Xjâˆ† (s), Î¾ij
i
j
n
Z
j=1

t
âŒŠâˆ†
âŒ‹

XZ
k=1

â‰¤ Îº3

n

1 X n,âˆ†,y
Qi,j (s) ds + âˆ†Î³y
E
n
(kâˆ’1)âˆ†
kâˆ†

j=1

Î³y t
2Î³y t
+ âˆš + âˆ†Î³y .
Î²(n)âˆ†
n

For the third term, we have
Z
Z
âˆ†
âˆ†
âˆ†
âˆ†
e Q(Xi (s), Xj (s), dÎ¾)
e
e
e
E
Î³(y, Xi (s), Xj (s), Î¾) Q(Xi (s), Xj (s), dÎ¾) âˆ’ Î³(y, Xi (s), Xj (s), Î¾)
Z
Z

â‰¤ Î³y P(Xiâˆ† (s) 6= Xi (s)) + P(Xjâˆ† (s) 6= Xj (s)) â‰¤ 2Î³y CÎ³ âˆ†,

24

BAYRAKTAR AND WU

where the last inequality follows from (5.11). For the fourth term, since Xj are i.i.d. with law
Âµ, from Lemma 5.1 we have
n

E

1X
n
j=1

Z

Îº4 Î³y
â‰¤ âˆš .
n

Z

e Q(Xi (s), Xj (s), dÎ¾)
e âˆ’
Î³(y, Xi (s), Xj (s), Î¾)

Z

Z2

e Âµs (de
e
Î³(y, Xi (s), x
e, Î¾)
x) Q(Xi (s), x
e, dÎ¾)

Combining all of the above estimates and using Condition 2.1(i) gives
Z t
(Îº4 + 2)CÎ³ t
CÎ³ t
âˆš
+ CÎ³ âˆ† + 6CÎ³2 tâˆ† +
.
EkXin âˆ’ Xi kâˆ—,s ds + Îº3
EkXin âˆ’ Xi kâˆ—,t â‰¤ 4CÎ³
Î²(n)âˆ†
n
0
Using Gronwallâ€™s inequality and taking âˆ† = âˆ†(n) = âˆš 1

Î²(n)

, we have (3.2).

(c) Using (b), the independence of {Xi } and a standard argument (see [29, Chapter 1]) one
has (3.3).
(d) Fix t âˆˆ [0, T ]. Let
n

n

1X
Î´Xi (Â·) ,
ÂµÌ„ :=
n

1X
ÂµÌ„ (t) :=
Î´Xi (t) .
n
n

n

i=1

i=1

From (3.2) we have
n

EdBL (Âµn (t), ÂµÌ„n (t)) â‰¤
EdBL (Âµn , ÂµÌ„n ) â‰¤

1X
E|Xin (t) âˆ’ Xi (t)| â†’ 0,
n
1
n

i=1
n
X
i=1

EkXin âˆ’ Xi kâˆ—,T â†’ 0.

Also note that
ÂµÌ„n (t) â†’ Âµ(t),

ÂµÌ„n â†’ Âµ,

in probability by independence of Xi . Combining these gives (3.4).
(e) Fix t âˆˆ (0, T ]. Take âˆ† = âˆ†(n) = O( âˆš 1 ) such that (k + 21 )âˆ† â‰¤ t < (k + 1)âˆ† for some
Î²(n)

k = k(n) âˆˆ N. Define

n

Î½in,âˆ†,1 (t)

1X
Î´(X âˆ† (t),Î¾ n,âˆ† (t)) ,
=
j
ij
n

Î½in,âˆ†,2 (t) =
Î½in,âˆ†,3 (t) =
Î½in,âˆ†,4 (t) =

1
n
1
n
1
n

j=1
n
X
j=1
n
X
j=1
n
X
j=1

n,âˆ†
n
Î´X âˆ† (t) âŠ— L(Î¾ij
(t) | Î¾ij
(kâˆ†), Xiâˆ† (t), Xjâˆ† (t)),
j

Î´X âˆ† (t) âŠ— Q(Xiâˆ† (t), Xjâˆ† (t), Â·),
j

Î´Xj (t) âŠ— Q(Xi (t), Xj (t), Â·).

MEAN FIELD INTERACTION ON DYNAMIC RANDOM GRAPHS

25

From (3.2), (5.11) and (5.12) we have
n

EdBL (Î½in (t), Î½in,âˆ†,1 (t))

1X
n,âˆ†
n
(t)) âˆ’ f (Xjâˆ† (t), Î¾ij
(t))
sup f (Xjn (t), Î¾ij
â‰¤
n
f âˆˆB1
â‰¤

1
n

j=1
n 
X
j=1


n,âˆ†
n
(t)) â†’ 0,
E|Xjn (t) âˆ’ Xjâˆ† (t)| + 2P(Î¾ij
(t) 6= Î¾ij

n
1X
n,âˆ†,3
n,âˆ†,4
EdBL (Î½i
(t), Î½i
(t)) â‰¤
P(Xjâˆ† (t) 6= Xj (t)) â†’ 0.
n
j=1

n,âˆ†
Using the conditional independence of {Î¾ij
(t) : j âˆˆ [n]} given {Xjâˆ† (t) : j âˆˆ [n]}, we have


2 
n,âˆ†,1
n,âˆ†,2
(t)i âˆ’ hf, Î½i
(t)i
E hf, Î½i


n
2 
1 X
n,âˆ†
n,âˆ†
âˆ†
n
âˆ†
âˆ†
= 2
E f (Xj (t), Î¾ij (t)) âˆ’ hf, Î´X âˆ† (t) âŠ— L(Î¾ij (t) | Î¾ij (kâˆ†), Xi (t), Xj (t))i
j
n
j=1

kf k2âˆ
â‰¤
â†’0
n

for each bounded and continuous function f . Similarly, using the independence of {Xi : i âˆˆ
[n]} and the weak law of large numbers we have
Î½in,âˆ†,4 (t) â†’ Î½i (t)
n,âˆ†
in probability. Lastly, from (3.5) and the definition of Î¾ij
we have

E hf, Î½in,âˆ†,2 (t)i âˆ’ hf, Î½in,âˆ†,3 (t)i
ï£®
ï£¹
n
2kf kâˆ X ï£°X
n,âˆ†
n
e ï£»
E
P(Î¾ij
(t) = Î¾e| Î¾ij
(kâˆ†), Xiâˆ† (t), Xjâˆ† (t)) âˆ’ Q(Xiâˆ† (t), Xjâˆ† (t), {Î¾})
â‰¤
n
j=1

â‰¤

e
Î¾âˆˆZ

n

2kf kâˆ X 
E C(Î²(n)(t âˆ’ kâˆ†))(1 + |Xiâˆ† (t)| + |Xjâˆ† (t)|) â†’ 0
n
j=1

â‰¤ Îº5 C(Î²(n)(t âˆ’ kâˆ†)) â†’ 0

for each bounded and continuous function f . Combining above estimates gives (3.6). Finally,
(3.7) is a direct consequence of (3.6).

Remark 5.1. If we are only interested in the convergence of Xin , Âµn , Î½in , then it would be
Rt
Râˆ
e(s) ds = 0, instead of 0 Îº
e(t) dt < âˆ. We only have
sufficient to assume that limtâ†’âˆ 1t 0 Îº
R Î²(n)âˆ†
to replace Îº3 by Îº3 0
Îº
e(s) ds in (5.14) and in what follows.

Proof of Theorem 3.2. The proof is quite similar to that of Theorem 3.1, except that Î½in there
is replaced by Î½iÎ² here. So we will omit certain common arguments.

26

BAYRAKTAR AND WU

(a) First consider the following auxiliary process with âˆ† = âˆ†(Î²) â†’ 0 (whose precise value
will be stated later):

t
= Xi âŒŠ âŒ‹âˆ† ,
âˆ†
 Z

t
Î²,âˆ†
Î²
y1[0,Î² Î“(y,Î¾
(z) Nij (ds dy dz).
Î¾ij (t) = Î¾ij âŒŠ âŒ‹âˆ† +
Î²,âˆ†
âˆ†
âˆ†
e
ij (sâˆ’),Xi (sâˆ’),Xj (sâˆ’))]
âˆ†
[âŒŠ t âŒ‹âˆ†,t]Ã—ZÃ—R+


Xiâˆ† (t)

âˆ†

Using Condition 2.1(i), one can check that
|Xiâˆ† (s) âˆ’ Xi (s)| â‰¤ CÎ³ âˆ†,

(5.16)

Î²,âˆ†
Î²
P(Î¾ij
(t) 6= Î¾ij
(t)) â‰¤ 2CÎ³ âˆ† + 2EkXiÎ² âˆ’ Xi kâˆ—,t .

(5.17)

sup

E

t
sâˆˆ[âŒŠ âˆ†
âŒ‹âˆ†,t]

s
Î²

(so that the

2âˆ†Î³y
Îº1 Î³y
+ âˆš
Î²
n

(5.18)

From this, (5.13), Condition 3.1(i) and an application of time change s 7â†’
Î²,âˆ†
evolution of Î¾ij
matches that of Ye ) we have
Z

E

(kâˆ’1)âˆ†

âˆ’

1 X
Î²,âˆ†
(s))
Î³(y, Xiâˆ† (s), Xjâˆ† (s), Î¾ij
n
n

kâˆ†

Z

Z

j=1



e Q(X âˆ† (s), X âˆ† (s), dÎ¾)
e
Î³(y, Xiâˆ† (s), Xjâˆ† (s), Î¾)
i
j

ds â‰¤

for each k âˆˆ N.
Now we show (3.8). Since Xi are i.i.d. with law Âµ, we can write
n

1X
e
Î´Xj (t) (de
x) âŠ— Q(Xi (t), Xj (t), dÎ¾).
nâ†’âˆ n

e = lim
Î½i (t)(de
x dÎ¾)

j=1

Hence
EkXiÎ²

âˆ’ Xi kâˆ—,t â‰¤ E
â‰¤

Z

Z

[0,t]Ã—Z

|y| Î“(y, XiÎ² (s), Î½iÎ² (s)) âˆ’ Î“(y, Xi (s), Î½i (s)) ds Ï(dy)

1 X
Î²
Î³(y, XiÎ² (s), XjÎ² (s), Î¾ij
(s))
nâ†’âˆ
n
[0,t]Ã—Z
j=1

Z
e
e
âˆ’ Î³(y, Xi (s), Xj (s), Î¾) Q(Xi (s), Xj (s), dÎ¾) ds Ï(dy),
n

|y| lim E

Z

MEAN FIELD INTERACTION ON DYNAMIC RANDOM GRAPHS

27

where the last line uses the dominated convergence theorem. For each y âˆˆ Z, j âˆˆ N and
s âˆˆ [0, T ],

Z
n 
1X
Î²
Î²
Î²
e
e
Î³(y, Xi (s), Xj (s), Î¾ij (s)) âˆ’ Î³(y, Xi (s), Xj (s), Î¾) Q(Xi (s), Xj (s), dÎ¾)
E
n
Z
j=1
n

1X
Î²
Î²,âˆ†
â‰¤
E Î³(y, XiÎ² (s), XjÎ² (s), Î¾ij
(s)) âˆ’ Î³(y, Xiâˆ† (s), Xjâˆ† (s), Î¾ij
(s))
n
j=1


Z
n 
1X
Î²,âˆ†
âˆ†
âˆ†
âˆ†
âˆ†
âˆ†
âˆ†
e
e
Î³(y, Xi (s), Xj (s), Î¾ij (s)) âˆ’ Î³(y, Xi (s), Xj (s), Î¾) Q(Xi (s), Xj (s), dÎ¾)
+E
n
Z
j=1

Z
n
1X
e Q(X âˆ† (s), X âˆ† (s), dÎ¾)
e
+
E
Î³(y, Xiâˆ† (s), Xjâˆ† (s), Î¾)
i
j
n
Z
j=1
Z
e Q(Xi (s), Xj (s), dÎ¾)
e .
âˆ’ Î³(y, Xi (s), Xj (s), Î¾)
Z

Similar to the analysis of first three terms on the RHS of (5.15), it follows from Condition
2.1(i), (5.16), (5.17) and (5.18) that
Î²
Î²,âˆ†
E Î³(y, XiÎ² (s), XjÎ² (s), Î¾ij
(s)) âˆ’ Î³(y, Xiâˆ† (s), Xjâˆ† (s), Î¾ij
(s))

â‰¤ 4Î³y EkXiÎ² âˆ’ Xi kâˆ—,s + 4Î³y CÎ³ âˆ†,

Z t
Z
n 
1X
Î²,âˆ†
âˆ†
âˆ†
âˆ†
âˆ†
âˆ†
âˆ†
e
e
E
Î³(y, Xi (s), Xj (s), Î¾ij (s)) âˆ’ Î³(y, Xi (s), Xj (s), Î¾) Q(Xi (s), Xj (s), dÎ¾) ds
n
0
Z
j=1

E

Z

Z

â‰¤ Îº1

Î³y t 2Î³y t
+ âˆš + âˆ†Î³y ,
Î²âˆ†
n

e Q(X âˆ† (s), X âˆ† (s), dÎ¾)
e âˆ’
Î³(y, Xiâˆ† (s), Xjâˆ† (s), Î¾)
i
j

â‰¤ 2Î³y CÎ³ âˆ†.

Z

Z

e Q(Xi (s), Xj (s), dÎ¾)
e
Î³(y, Xi (s), Xj (s), Î¾)

Combining all of the above estimates and using Condition 2.1(i) gives
EkXiÎ²

âˆ’ Xi kâˆ—,t â‰¤ 4CÎ³

Z

t
0

EkXiÎ² âˆ’ Xi kâˆ—,s ds + Îº1

Using Gronwallâ€™s inequality and taking âˆ† = âˆ†(Î²) =
Using the definition of dBL and (3.8), we have

âˆš1
Î²

CÎ³ t
+ CÎ³ âˆ† + 6CÎ³2 tâˆ†.
Î²âˆ†

we have (3.8).

Îº
dBL (ÂµÎ² , Âµ) = sup Ef (XiÎ² ) âˆ’ Ef (Xi ) â‰¤ EkXiÎ² âˆ’ Xi kâˆ—,T â‰¤ âˆš .
Î²
f âˆˆB1
This gives (3.9).

28

BAYRAKTAR AND WU

(b) Fix t âˆˆ (0, T ]. Take âˆ† = âˆ†(Î²) = O( âˆš1Î² ) such that (k + 21 )âˆ† â‰¤ t < (k + 1)âˆ† for some
k = k(Î²) âˆˆ N. Define
n

1X
Î´(X âˆ† (t),Î¾ Î²,âˆ† (t)) ,
nâ†’âˆ n
j
ij

Î½iÎ²,âˆ†,1 (t) = lim

Î½iÎ²,âˆ†,2 (t) = lim

nâ†’âˆ

1
n

1
nâ†’âˆ n

Î½iÎ²,âˆ†,3 (t) = lim

Î½iÎ²,âˆ†,4 (t) = lim

nâ†’âˆ

1
n

j=1
n
X
j=1
n
X
j=1
n
X
j=1

Î²,âˆ†
Î²
(t) | Î¾ij
(kâˆ†), Xiâˆ† (t), Xjâˆ† (t)),
Î´X âˆ† (t) âŠ— L(Î¾ij
j

Î´X âˆ† (t) âŠ— Q(Xiâˆ† (t), Xjâˆ† (t), Â·),
j

Î´Xj (t) âŠ— Q(Xi (t), Xj (t), Â·).

From (3.8), (5.16) and (5.17) we have
EdBL (Î½iÎ² (t), Î½iÎ²,âˆ†,1 (t)) â‰¤ lim inf
nâ†’âˆ

n

1 X
Î²
Î²,âˆ†
E|XjÎ² (t) âˆ’ Xjâˆ† (t)| + 2P(Î¾ij
(t) 6= Î¾ij
(t)) â†’ 0,
n
j=1

n
1X
EdBL (Î½iÎ²,âˆ†,3 (t), Î½iÎ²,âˆ†,4 (t)) â‰¤ lim inf
P(Xjâˆ† (t) 6= Xj (t)) â†’ 0,
nâ†’âˆ n
j=1

Î²,âˆ†
(t) : j âˆˆ N} given {Xjâˆ† (t) : j âˆˆ N},
as Î² â†’ âˆ. Using the conditional independence of {Î¾ij
we have

2 
E hf, Î½iÎ²,âˆ†,1 (t)i âˆ’ hf, Î½iÎ²,âˆ†,2 (t)i


n
2 
1 X
Î²
Î²,âˆ†
Î²,âˆ†
âˆ†
âˆ†
âˆ†
â‰¤ lim inf 2
E f (Xj (t), Î¾ij (t)) âˆ’ hf, Î´X âˆ† (t) âŠ— L(Î¾ij (t) | Î¾ij (kâˆ†), Xi (t), Xj (t))i
j
nâ†’âˆ n
j=1

=0

for each bounded and continuous function f . This means
Î½iÎ²,âˆ†,1 (t) = Î½iÎ²,âˆ†,2 (t) a.s.
Similarly, using the independence of {Xi : i âˆˆ N} and the weak law of large numbers we have
Î½iÎ²,âˆ†,4 (t) = Î½i (t) a.s.

Î²,âˆ†
Moreover, from (3.5) and the definition of Î¾ij
we have

E hf, Î½iÎ²,âˆ†,2 (t)i âˆ’ hf, Î½iÎ²,âˆ†,3 (t)i
ï£¹
ï£®
n
X
X
2kf kâˆ
Î²,âˆ†
Î²
e ï£»
â‰¤ lim inf
Eï£°
P(Î¾ij
(t) = Î¾e| Î¾ij
(kâˆ†), Xiâˆ† (t), Xjâˆ† (t)) âˆ’ Q(Xiâˆ† (t), Xjâˆ† (t), {Î¾})
nâ†’âˆ
n
j=1

â‰¤ lim inf
nâ†’âˆ

e
Î¾âˆˆZ

n

2kf kâˆ X 
E C((t âˆ’ kâˆ†)Î²)(1 + |Xiâˆ† (t)| + |Xjâˆ† (t)|) â†’ 0
n
j=1

â‰¤ Îº2 C((t âˆ’ kâˆ†)Î²) â†’ 0

MEAN FIELD INTERACTION ON DYNAMIC RANDOM GRAPHS

29

as Î² â†’ âˆ, for each bounded and continuous function f . Combining above estimates gives
(3.10). Finally, (3.11) is a direct consequence of (3.10).

Remark 5.2. If we are only interested in the convergence of XiÎ² , ÂµÎ² , Î½iÎ² , then it would be
Rt
Râˆ
e(s) ds = 0, instead of 0 Îº
e(t) dt < âˆ. We only have
sufficient to assume that limtâ†’âˆ 1t 0 Îº
R Î²âˆ†
to replace Îº1 by Îº1 0 e
Îº(s) ds in (5.18) and in what follows.
6. Proofs of central limit theorems

In this section we prove Theorems 4.1 and 4.2.
6.1. Asymptotics of symmetric statistics. The proof of CLT crucially relies on certain
classical results from [14] on limit laws of degenerate symmetric statistics. In this section we
briefly review these results.
Let S be a Polish space and let {Yn }âˆ
n=1 be a sequence of i.i.d. S-valued random variables
having common probability law Î¸Ì„. For k âˆˆ N, let L2 (Î¸Ì„ âŠ—k ) be the space of all real-valued
square integrable functions on (Sk , B(S)âŠ—k , Î¸Ì„ âŠ—k ). Denote by L2c (Î¸Ì„ âŠ—k ) the subspace of centered
functions, namely Ï† âˆˆ L2 (Î¸Ì„ âŠ—k ) such that for all 1 â‰¤ j â‰¤ k,
Z
Ï†(x1 , . . . , xjâˆ’1 , x, xj+1 , . . . , xk ) Î¸Ì„(dx) = 0, Î¸Ì„ âŠ—kâˆ’1 a.e. (x1 , . . . , xjâˆ’1 , xj+1 , . . . , xk ).
S

Denote by L2sym (Î¸Ì„ âŠ—k ) the subspace of symmetric functions, namely Ï† âˆˆ L2 (Î¸Ì„ âŠ—k ) such that for
every permutation Ï€ on {1, . . . , k},
Ï†(x1 , . . . , xk ) = Ï†(xÏ€(1) , . . . , xÏ€(k) ),

Î¸Ì„ âŠ—k a.e. (x1 , . . . , xk ).

Also, denote by L2c,sym (Î¸Ì„ âŠ—k ) the subspace of centered symmetric functions in L2 (Î¸Ì„ âŠ—k ), namely
T
L2c,sym(Î¸Ì„ âŠ—k ) := L2c (Î¸Ì„ âŠ—k ) L2sym (Î¸Ì„ âŠ—k ). Given Ï†k âˆˆ L2sym (Î¸Ì„ âŠ—k ) define the symmetric statistic
Ukn (Ï†k ) as
X
ï£±
Ï†k (Yi1 , . . . , Yik ) for n â‰¥ k
ï£²
Ukn (Ï†k ) := 1â‰¤i1 <i2 <Â·Â·Â·<ik â‰¤n
ï£³
0
for n < k.
In order to describe the asymptotic distributions of such statistics consider a Gaussian field
{I1 (h) : h âˆˆ L2 (Î¸Ì„)} such that
h, g âˆˆ L2 (Î¸Ì„).

E (I1 (h)) = 0, E (I1 (h)I1 (g)) = hh, giL2 (Î¸Ì„) ,

For h âˆˆ L2 (Î¸Ì„), define Ï†hk âˆˆ L2sym (Î¸Ì„ âŠ—k ) as

Ï†hk (x1 , . . . , xk ) := h(x1 ) . . . h(xk )

and set Ï†h0 := 1.
The multiple Wiener integral (MWI) of Ï†hk , denoted as Ik (Ï†hk ), is defined through the
following formula. For k â‰¥ 1,
Ik (Ï†hk )

:=

âŒŠk/2âŒ‹

X
j=0

(âˆ’1)j Ck,j ||h||2j
(I (h))kâˆ’2j , where Ck,j :=
L2 (Î¸Ì„) 1

k!
, j = 0, . . . , âŒŠk/2âŒ‹.
(k âˆ’ 2j)!2j j!

The following representation gives an equivalent way to characterize the MWI of Ï†hk :


âˆ k
X
t2
t
h
2
Ik (Ï†k ) = exp tI1 (h) âˆ’ ||h||L2 (Î¸Ì„) , t âˆˆ R,
k!
2
k=0

30

BAYRAKTAR AND WU

where we set I0 (Ï†h0 ) := 1. We extend the definition of Ik to the linear span of {Ï†hk , h âˆˆ L2 (Î¸Ì„)}
by linearity. It can be checked that for all f in this linear span,
E(Ik (f ))2 = k! ||f ||2L2 (Î¸Ì„âŠ—k ) .

(6.1)

Using this identity and standard denseness arguments, the definition of Ik (f ) can be extended
to all f âˆˆ L2sym (Î¸Ì„ âŠ—k ) and the identity (6.1) holds for all f âˆˆ L2sym (Î¸Ì„ âŠ—k ). The following result
is taken from [14].
âŠ—k ) for each
2
Lemma 6.1 (Dynkin-Mandelbaum [14]). Let {Ï†k }âˆ
k=1 be such that Ï†k âˆˆ Lc,sym (Î¸Ì„
k â‰¥ 1. Then the following convergence holds as n â†’ âˆ:


 k

1
âˆ’2 n
n Uk (Ï†k )
Ik (Ï†k )
â‡’
k!
kâ‰¥1
kâ‰¥1

as a sequence of Râˆ -valued random variables.

6.2. Girsanov change of measure. Recall the probability space (â„¦, F, P) under which
systems (1.1) and (2.1) are defined. Recall the canonical spaces and processes introduced in
Section 4.1. Let
n
1X
Î´(Xj (t),Î¾ij (t)) .
Î½Ì„in (t) =
n
j=1

Define

n

J (t) :=

Z
n
X
i=1

where

Xt

n,i
(y, z)Ni (ds dy dz)
rsâˆ’

rsn,i (y, z) := 1[0,Î“(y,Xi (s),Î½i (s))] (z) log

âˆ’

Z

[0,t]Ã—Z

!

en,i
s (y) ds Ï(dy)

,

Î“(y, Xi (s), Î½Ì„in (s))
,
Î“(y, Xi (s), Î½i (s))

n
n
en,i
s (y) := Î“(y, Xi (s), Î½Ì„i (s)) âˆ’ Î“(y, Xi (s), Î½i (s)) = Î“(y, Xi (s), Î½Ì„i (s) âˆ’ Î½i (s)).

Let FÌ„tn := Ïƒ{Ni (A), Xi [t], Î¾ij [t] : i, j âˆˆ [n], A âˆˆ B(Xt )}. Note that {exp(J n (t))} is an FÌ„tn martingale under P n . Define a new probability measure Qn on â„¦n by
dQn
:= exp (J n (T )) .
dP n
By Girsanovâ€™s Theorem, {Xi , Î¾ij : i, j âˆˆ [n]} has the same probability distribution under
n : i, j âˆˆ [n]} under P. Let
Qn as {Xin , Î¾ij
ï£¶
ï£«
n
X
âˆš
1
Ï•(Xj , Î¾1j ) âˆ’ mÏ• (X1 )ï£¸ , Ï• âˆˆ A,
Î·Ì„ n (Ï•) := n ï£­
n
j=1
ï£«
ï£¶
n
X
âˆš
1
Î·Ì„xn (Ï•) := n ï£­
Ï•(Xj ) âˆ’ EÏ•(X1 )ï£¸ , Ï• âˆˆ Ax .
n
j=1

Thus in order to prove Theorems 4.1 and 4.2 it suffices to show that


Z
âˆš

1 Ï• 2
n
exp âˆ’ (ÏƒÏ‰0 ) P0 (dÏ‰0 ), Ï• âˆˆ A,
lim EQn exp âˆ’1Î·Ì„ (Ï•)) =
nâ†’âˆ
2
â„¦0


âˆš

1
âˆ’1
2
n
lim EQn exp âˆ’1Î·Ì„x (Ï•)) = exp âˆ’ k(I âˆ’ A) Î¦kH , Ï• âˆˆ Ax ,
nâ†’âˆ
2

MEAN FIELD INTERACTION ON DYNAMIC RANDOM GRAPHS

which is equivalent to showing
Z
âˆš

n
n
lim EP n exp âˆ’1Î·Ì„ (Ï•) + J (T ) =


1 Ï• 2
exp âˆ’ (ÏƒÏ‰0 ) P0 (dÏ‰0 ), Ï• âˆˆ A,
nâ†’âˆ
2
â„¦0


âˆš

1
âˆ’1
2
n
n
lim EP n exp âˆ’1Î·Ì„x (Ï•) + J (T ) = exp âˆ’ k(I âˆ’ A) Î¦kH , Ï• âˆˆ Ax .
nâ†’âˆ
2
n
For this we will need to study the asymptotics of J as n â†’ âˆ.


31

(6.2)
(6.3)

6.3. Asymptotics of J n . Now we analyze the asymptotics of J n . Recall the constant Îµ
from Condition 4.1. From Taylorâ€™s expansion, there exists a Îº0 âˆˆ (0, âˆ) such that for all
Î±, Î² âˆˆ [Îµ, 1/Îµ],
Î±
Î±
1 Î±
Î±
log = ( âˆ’ 1) âˆ’ ( âˆ’ 1)2 + Ï‘(Î±, Î²)( âˆ’ 1)3 ,
Î²
Î²
2 Î²
Î²
n,i
n
where |Ï‘(Î±, Î²)| â‰¤ Îº0 . Letting Ï‘s (y) := Ï‘(Î“(y, Xi (s), Î½Ì„i (s)), Î“(y, Xi (s), Î½i (s))), we have


2

Î“(y, Xi (s), Î½Ì„in (s))
Î“(y, Xi (s), Î½Ì„in (s))
1 Î“(y, Xi (s), Î½Ì„in (s))
log
=
âˆ’1 âˆ’
âˆ’1
Î“(y, Xi (s), Î½i (s))
Î“(y, Xi (s), Î½i (s))
2 Î“(y, Xi (s), Î½i (s))

3
Î“(y, Xi (s), Î½Ì„in (s))
n,i
+ Ï‘s (y)
âˆ’1 .
Î“(y, Xi (s), Î½i (s))
ei be the compensated PRM of Ni , we have
Letting N
n Z
n Z
X
X
n,i
en,i
(y, z)Ni (ds dy dz) âˆ’
rsâˆ’
J n (T ) =
s (y) ds Ï(dy)
=

i=1 XT
n Z
X

i=1

[0,T ]Ã—Z




Î“(y, Xi (sâˆ’), Î½Ì„in (sâˆ’))
ei (ds dy dz)
âˆ’1 N
Î“(y, Xi (sâˆ’), Î½i (sâˆ’))
X
T
i=1
2

n Z
Î“(y, Xi (sâˆ’), Î½Ì„in (sâˆ’))
1X
âˆ’ 1 Ni (ds dy dz)
âˆ’
1[0,Î“(y,Xi (sâˆ’),Î½i (sâˆ’))] (z)
2
Î“(y, Xi (sâˆ’), Î½i (sâˆ’))
i=1 XT

3
n Z
X
Î“(y, Xi (sâˆ’), Î½Ì„in (sâˆ’))
n,i
1[0,Î“(y,Xi (sâˆ’),Î½i (sâˆ’))] (z)Ï‘sâˆ’ (y)
+
âˆ’ 1 Ni (ds dy dz)
Î“(y, Xi (sâˆ’), Î½i (sâˆ’))
XT
1[0,Î“(y,Xi (sâˆ’),Î½i (sâˆ’))] (z)

i=1

1
=: J n,1 âˆ’ J n,2 + J n,3 .
2
First we analyze J n,3 .

(6.4)

Lemma 6.2. EP n |J n,3 | â†’ 0 as n â†’ âˆ.

Proof. Since |Ï‘n,i
s (y)| â‰¤ Îº0 and Î“ is bounded from above and away from 0 by Condition 4.1,
we have
n Z
X
Îº0
n,3
|Î“(y, Xi (s), Î½Ì„in (s) âˆ’ Î½i (s))|3 ds Ï(dy).
EP n |J | â‰¤ 2 EP n
Îµ
[0,T ]Ã—Z
i=1

Note that

max sup sup EP n |Î“(y, Xi (s), Î½Ì„in (s) âˆ’ Î½i (s))|3 â‰¤
iâˆˆ[n] yâˆˆZ sâˆˆ[0,T ]

Îº1
n3/2

by conditional i.i.d. property of (Xj , Î¾ij ) given Xi and Lemma 5.1. Therefore
Îº2
EP n |J n,3 | â‰¤ 1/2 â†’ 0
n

32

BAYRAKTAR AND WU

as n â†’ âˆ.



Before analyzing J n,2 , let
Î³sij,y := Î³(y, Xi (s), Xj (s), Î¾ij (s)) âˆ’ hÎ³(y, Xi (s), Â·, Â·), Î½i (s)i.

(6.5)

h
i
EP n Î³sij,y Xi = 0, for j 6= i.

(6.6)

Note that

We first show the following estimate.

Lemma 6.3. For each s âˆˆ [0, T ] and bounded measurable function f ,
EP n

1
n2
â‰¤

X

1â‰¤i<j<kâ‰¤n

i

h
Î³sij,y Î³sik,y f (Xi (s), Î½i (s)) âˆ’ EP n Î³sij,y Î³sik,y f (Xi (s), Î½i (s)) Xj , Xk

2

4kf k2âˆ
.
Îµ4 n

Proof. We claim that for i < j < k and ei < e
j<e
k,
h
i
h
EP n Î³sij,y Î³sik,y f (Xi (s), Î½i (s)) âˆ’ EP n Î³sij,y Î³sik,y f (Xi (s), Î½i (s)) Xj , Xk
ii
h

ee
ee
ee
ee
=0
Â· Î³sij,y Î³sik,y f (Xei (s), Î½ei (s)) âˆ’ EP n Î³sij,y Î³sik,y f (Xei (s), Î½ei (s)) Xej , Xek

(6.7)

except when i = ei, j = e
j and k = e
k.
e
To see this, first consider k < k. Using the independence of the collection {Xi , Î¾ij (0), Nij :
i, j âˆˆ N} and (6.6), we have
i
i
h
h
ee
ee
EP n EP n Î³sij,y Î³sik,y f (Xei (s), Î½ei (s)) Xej , Xek Xi , Xj , Xk , Î¾ij (0), Nij , Î¾ik (0), Nik , Xei , Xej
i
h
ee
ee
= EP n Î³sij,y Î³sik,y f (Xei (s), Î½ei (s)) Xej
i
i
h
h
ee
ee
= EP n EP n Î³sij,y Î³sik,y f (Xei (s), Î½ei (s)) Xei , Xej , Î¾eiej Xej = 0

and

h
i
ee
ee
EP n Î³sij,y Î³sik,y f (Xei (s), Î½ei (s)) Xi , Xj , Xk , Î¾ij (0), Nij , Î¾ik (0), Nik , Xei , Xej = 0.

Combining these two and conditioning on Xi , Xj , Xk , Î¾ij (0), Nij , Î¾ik (0), Nik , Xei , Xej in the LHS
of (6.7), we have verified (6.7) for k < e
k. Therefore (6.7) holds whenever k 6= e
k.
e but j < e
Next consider k = k
j. Note that we still have
h
i
i
h
ee
ee
EP n EP n Î³sij,y Î³sik,y f (Xei (s), Î½ei (s)) Xej , Xek Xi , Xj , Î¾ij (0), Nij , Î¾iek (0), Niek , Xei , Xek
i
h
ee
ee
= EP n Î³sij,y Î³sik,y f (Xei (s), Î½ei (s)) Xek
i
i
h
h
ee
ee
= EP n EP n Î³sij,y Î³sik,y f (Xei (s), Î½ei (s)) Xei , Xek , Î¾eiek Xek = 0

and

i
h
ee
ee
EP n Î³sij,y Î³sik,y f (Xei (s), Î½ei (s)) Xi , Xj , Î¾ij (0), Nij , Î¾iek (0), Niek , Xei , Xek = 0.

So again we have (6.7) for k = e
k and j < e
j. Therefore (6.7) holds whenever k 6= e
k or j 6= e
j.

MEAN FIELD INTERACTION ON DYNAMIC RANDOM GRAPHS

33

Consider k = e
k, j = e
j but i < ei. Note that
i
i
h
h
ee
ee
EP n EP n Î³sij,y Î³sik,y f (Xei (s), Î½ei (s)) Xej , Xek Xi , Î¾iej , Î¾iek , Xej , Xek
i
h
ee
ee
= EP n Î³sij,y Î³sik,y f (Xei (s), Î½ei (s)) Xej , Xek
i
h
ee
ee
= EP n Î³sij,y Î³sik,y f (Xei (s), Î½ei (s)) Xi , Î¾iej , Î¾iek , Xej , Xek .

k, j = e
j
Conditioning on Xi , Î¾iej , Î¾iek , Xej , Xek in the LHS of (6.7), we have verified (6.7) for k = e
e
e
e
e
but i < i. Therefore (6.7) holds whenever k 6= k, j 6= j, or i 6= i. So we have verified the
claim.
Using the claim and Condition 4.1 we have
1
n2

EP n
=

1
n4

X

1â‰¤i<j<kâ‰¤n

X

1â‰¤i<j<kâ‰¤n



i
h
Î³sij,y Î³sik,y f (Xi (s), Î½i (s)) âˆ’ EP n Î³sij,y Î³sik,y f (Xi (s), Î½i (s)) Xj , Xk

2


h
i2
EP n Î³sij,y Î³sik,y f (Xi (s), Î½i (s)) âˆ’ EP n Î³sij,y Î³sik,y f (Xi (s), Î½i (s)) Xj , Xk

4kf k2âˆ
.
Îµ4 n
This completes the proof.
â‰¤



Now we analyze J n,2 . Let
SÌ‚ n,2 := {(j, k) âˆˆ [n]2 : j 6= 1, k 6= 1, j 6= k}.

Recall Î³sij,y introduced in (6.5). Note that
Z 
2
ij,y 2
e âˆ’ hÎ³(y, Xi (s), Â·, Â·), Î½i (s)i Î½i (s)(de
e
Î³(y, Xi (s), x
e, Î¾)
x dÎ¾)
h(Î³s ) , Î½i (s)i =
2
Z
i
h
= EP n (Î³sij,y )2 Xi , j 6= i.

Lemma 6.4.
J

n,2

#
h(Î³s12,y )2 , Î½1 (s)i
ds Ï(dy)
EP n
=
Î“(y, X1 (s), Î½1 (s))
[0,T ]Ã—Z
"
#
Z
Î³s1j,y Î³s1k,y
1 X
EP n
Xj , Xk ds Ï(dy) + Rn,2 ,
+
n
Î“(y, X1 (s), Î½1 (s))
[0,T ]Ã—Z
"

Z

(j,k)âˆˆSÌ‚ n,2

where Rn,2 â†’ 0 in probability as n â†’ âˆ.
Proof. We can write
n Z
X
Î“2 (y, Xi (sâˆ’), Î½Ì„in (sâˆ’) âˆ’ Î½i (sâˆ’))
n,2
1[0,Î“(y,Xi (sâˆ’),Î½i (sâˆ’))] (z)
J =
Ni (ds dy dz)
Î“2 (y, Xi (sâˆ’), Î½i (sâˆ’))
i=1 XT
Z
n
ij,y ik,y
Î³sâˆ’
Î³sâˆ’
1 X
1[0,Î“(y,Xi (sâˆ’),Î½i (sâˆ’))] (z) 2
Ni (ds dy dz).
= 2
n
Î“ (y, Xi (sâˆ’), Î½i (sâˆ’))
XT
i,j,k=1

(6.8)

34

BAYRAKTAR AND WU

Let
Z
n
ij,y ik,y
Î³sâˆ’
Î³sâˆ’
1 X
n,2
e
ds Ï(dy).
J
:= 2
n
[0,T ]Ã—Z Î“(y, Xi (sâˆ’), Î½i (sâˆ’))
i,j,k=1

Note that
max
iâˆˆ[n]

Z

XT

ï£«

EP n ï£­

n
X

j,k=1

ï£¶2
Î³sij,y Î³sik,y
ï£¸ ds Ï(dy) dz â‰¤ Îº1 n2
1[0,Î“(y,Xi (s),Î½i (s))] (z) 2
Î“ (y, Xi (s), Î½i (s))

by conditional i.i.d. property of (Xj , Î¾ij ) given Xi and Lemma 5.1. Therefore
EP n |J n,2 âˆ’ Jen,2 |2

Z
n
ij,y ik,y
Î³sâˆ’
Î³sâˆ’
1 X
ei (ds dy dz)
= EP n 2
1[0,Î“(y,Xi (sâˆ’),Î½i (sâˆ’))] (z) 2
N
n
Î“ (y, Xi (sâˆ’), Î½i (sâˆ’))
X
T
i,j,k=1
ï£«
ï£¶2
n
n Z
ij,y ik,y
X
X
Î³s Î³s
1
ï£¸ ds Ï(dy) dz
EP n ï£­
1[0,Î“(y,Xi (s),Î½i (s))] (z) 2
= 4
n
Î“
(y,
Xi (s), Î½i (s))
XT
i=1

2

j,k=1

Îº1
â†’0
â‰¤
n
as n â†’ âˆ.
We rewrite Jen,2 as

5
1 X
Jen,2 = 2
n m=1

X

(i,j,k)âˆˆS n,m

Z

[0,T ]Ã—Z

5
X
Î³sij,y Î³sik,y
T n,m ,
ds Ï(dy) =:
Î“(y, Xi (s), Î½i (s))
m=1

where S n,1 , S n,2 , S n,3 , S n,4 , S n,5 are collections of (i, j, k) âˆˆ [n]3 which equal {i = j = k},
{i = j 6= k}, {i = k 6= j}, {i 6= j = k}, {i, j, k distinct}, respectively. Using Conditions 2.1
and 4.1, and conditional i.i.d. property of (Xj , Î¾ij ) given Xi , we can show
Îº2
Îº2
â†’ 0, EP n |T n,2 + T n,3 |2 â‰¤
â†’0
EP n |T n,1 | â‰¤
n
n
as n â†’ âˆ.
For the fourth term T n,4 , we have
Z
1 X
(Î³sij,y )2
n,4
T
= 2
ds Ï(dy)
n
Î“(y, Xi (s), Î½i (s))
1â‰¤i6=jâ‰¤n [0,T ]Ã—Z
Z
(Î³sij,y )2 âˆ’ h(Î³sij,y )2 , Î½i (s)i
1 X
ds Ï(dy)
= 2
n
Î“(y, Xi (s), Î½i (s))
[0,T
]Ã—Z
1â‰¤i6=jâ‰¤n
Z
h(Î³sij,y )2 , Î½i (s)i
1 X
ds Ï(dy).
+ 2
n
[0,T ]Ã—Z Î“(y, Xi (s), Î½i (s))
1â‰¤i6=jâ‰¤n

Here the first term
1
n2

X

Z

1â‰¤i6=jâ‰¤n [0,T ]Ã—Z

(Î³sij,y )2 âˆ’ h(Î³sij,y )2 , Î½i (s)i
ds Ï(dy) â†’ 0
Î“(y, Xi (s), Î½i (s))

MEAN FIELD INTERACTION ON DYNAMIC RANDOM GRAPHS

35

in probability, by conditional i.i.d. property of (Xj , Î¾ij ) given Xi , and the second term
#
"
Z
Z
h(Î³sij,y )2 , Î½i (s)i
h(Î³s12,y )2 , Î½1 (s)i
1 X
ds Ï(dy)
EP n
ds Ï(dy) â†’
n2
Î“(y, X1 (s), Î½1 (s))
[0,T ]Ã—Z
[0,T ]Ã—Z Î“(y, Xi (s), Î½i (s))
1â‰¤i6=jâ‰¤n

in probability, by i.i.d. property of (Xi , Î½i ), as n â†’ âˆ.
Finally for the last term T n,5 , we have
X Z
1
Î³sij,y Î³sik,y
T n,5 = 2
ds Ï(dy)
n
[0,T ]Ã—Z Î“(y, Xi (s), Î½i (s))
n,5
(i,j,k)âˆˆS
"
#!
X Z
Î³sij,y Î³sik,y
Î³sij,y Î³sik,y
1
âˆ’ EP n
Xj , Xk
ds Ï(dy)
= 2
n
Î“(y, Xi (s), Î½i (s))
Î“(y, Xi (s), Î½i (s))
[0,T
]Ã—Z
n,5
(i,j,k)âˆˆS
"
#
X Z
1
Î³sij,y Î³sik,y
+ 2
EP n
Xj , Xk ds Ï(dy).
(6.9)
n
Î“(y, Xi (s), Î½i (s))
n,5 [0,T ]Ã—Z
(i,j,k)âˆˆS

From Lemma 6.3 we have
ï£«
X Z
1
ï£­
EP n
n2
n,5 [0,T ]Ã—Z
(i,j,k)âˆˆS

ï£¶2
"
#!
Î³sij,y Î³sik,y
Î³sij,y Î³sik,y
âˆ’ EP n
Xj , Xk
ds Ï(dy)ï£¸
Î“(y, Xi (s), Î½i (s))
Î“(y, Xi (s), Î½i (s))

Îº3
â‰¤
.
n
So the first term in (6.9) goes to 0 in probability. The second term in (6.9) could be written
as
"
#
Z
X
Î³s1j,y Î³s1k,y
nâˆ’2
EP n
Xj , Xk ds Ï(dy)
n2
Î“(y, X1 (s), Î½1 (s))
[0,T ]Ã—Z
(j,k)âˆˆ[n]2 :j6=k
"
#
Z
1 X
Î³s1j,y Î³s1k,y
EP n
=
Xj , Xk ds Ï(dy) + Rn,5 ,
n
Î“(y, X1 (s), Î½1 (s))
[0,T ]Ã—Z
(j,k)âˆˆSÌ‚ n,2

Îº4
.
and one can easily check that EP n |Rn,5 | â‰¤ âˆš
n
Combining above estimates completes the proof.



Next we analyze J n,1 . Let Î¸ij (s) := L(Î¾ij (s) | Xi [s], Xj [s]) and note that

hÎ³sij,y , Î¸ij (s)i = hÎ³(y, Xi (s), Xj (s), Â·), Î¸ij (s)i âˆ’ hÎ³(y, Xi (s), Â·, Â·), Î½i (s)i
h
i
= EP n Î³sij,y Xi (s), Xj (s) .

Lemma 6.5.
J n,1 =

1
n

X

(i,j)âˆˆSÌ‚ n,2

1
+
n

Z

XT

X

(i,j)âˆˆSÌ‚ n,2

1[0,Î“(y,Xi (sâˆ’),Î½i (sâˆ’))] (z)
Z

XT

ij,y
ij,y
, Î¸ij i e
âˆ’ hÎ³sâˆ’
Î³sâˆ’
Ni (ds dy dz)
Î“(y, Xi (sâˆ’), Î½i (sâˆ’))

(6.10)

ij,y
, Î¸ij i
hÎ³sâˆ’
ei (ds dy dz) + Rn,1 ,
1[0,Î“(y,Xi (sâˆ’),Î½i (sâˆ’))] (z)
N
Î“(y, Xi (sâˆ’), Î½i (sâˆ’))

where EP n |Rn,1 | â†’ 0 as n â†’ âˆ.

36

BAYRAKTAR AND WU

Proof. We can write
n Z
X
Î“(y, Xi (sâˆ’), Î½Ì„in (sâˆ’) âˆ’ Î½i (sâˆ’)) e
Ni (ds dy dz)
1[0,Î“(y,Xi (sâˆ’),Î½i (sâˆ’))] (z)
J n,1 =
Î“(y, Xi (sâˆ’), Î½i (sâˆ’))
X
T
i=1
n Z
ij,y
Î³sâˆ’
1 X
ei (ds dy dz).
1[0,Î“(y,Xi (sâˆ’),Î½i (sâˆ’))] (z)
=
N
n
Î“(y, Xi (sâˆ’), Î½i (sâˆ’))
XT
i,j=1

Therefore (6.10) holds with
n Z
1j,y
Î³sâˆ’
1X
n,1
e1 (ds dy dz)
1[0,Î“(y,X1 (sâˆ’),Î½1 (sâˆ’))] (z)
N
R =
n
Î“(y,
X
(sâˆ’),
Î½
(sâˆ’))
1
1
X
T
j=1
Z
n
i1,y
i2,y
Î³sâˆ’
+ Î³sâˆ’
1X
ei (ds dy dz).
+
1[0,Î“(y,Xi (sâˆ’),Î½i (sâˆ’))] (z)
N
n
Î“(y, Xi (sâˆ’), Î½i (sâˆ’))
XT
i=2

Here for the first term,
ï£®ï£«
ï£¶2 ï£¹
n Z
1j,y
X
Î³sâˆ’
1
e1 (ds dy dz)ï£¸ ï£»
1[0,Î“(y,X1 (sâˆ’),Î½1 (sâˆ’))] (z)
N
EP n ï£°ï£­
n
Î“(y,
X
(sâˆ’),
Î½1 (sâˆ’))
1
X
T
j=1
2
 P
1j,y
n
1
Z
Î³
j=1 s
n
Îº1
ds Ï(dy) â‰¤
,
= EP n
n
[0,T ]Ã—Z Î“(y, X1 (s), Î½1 (s))

since
EP n

"

#
Î³s1j,y Î³s1k,y
= 0,
Î“(y, X1 (s), Î½1 (s))

whenever j 6= k.

ei clearly we have
For the second term, using the independence of N
n Z
i1,y
i2,y
Î³sâˆ’
+ Î³sâˆ’
1X
Îº2
ei (ds dy dz) â‰¤ âˆš
EP n
.
1[0,Î“(y,Xi (sâˆ’),Î½i (sâˆ’))] (z)
N
n
Î“(y,
X
(sâˆ’),
Î½
(sâˆ’))
n
i
i
XT
i=2

Therefore EP n |Rn,1 | â‰¤

Îº3
âˆš
n

â†’ 0 as n â†’ âˆ, and this completes the proof.

The most difficult part is to analyze the following term in J n,1 :
1 X
U n :=
un (i, j),
n



(6.11)

2â‰¤i<jâ‰¤n

where
un (i, j) :=

Z

XT

+

1[0,Î“(y,Xi (sâˆ’),Î½i (sâˆ’))] (z)
Z

XT

ij,y
ij,y
, Î¸ij i e
âˆ’ hÎ³sâˆ’
Î³sâˆ’
Ni (ds dy dz)
Î“(y, Xi (sâˆ’), Î½i (sâˆ’))

ji,y
ji,y
, Î¸ji i e
âˆ’ hÎ³sâˆ’
Î³sâˆ’
Nj (ds dy dz).
1[0,Î“(y,Xj (sâˆ’),Î½j (sâˆ’))] (z)
Î“(y, Xj (sâˆ’), Î½j (sâˆ’))

Recall â„¦v , â„¦0 , Î±(Ï‰0 , Â·), P0 introduced in Sections 4.1 and 4.2. We now use the results
from Section 6.1 with S = â„¦v and Î¸Ì„ = Î±(Ï‰0 , Â·), Ï‰0 âˆˆ â„¦0 to get the asymptotics of
(U n , J n,1 , J n,2 , J n,3 ). For each Ï‰0 âˆˆ â„¦0 , k â‰¥ 1 and g âˆˆ L2sym (Î±(Ï‰0 , Â·)âŠ—k ), the MWI IkÏ‰0 (g)

MEAN FIELD INTERACTION ON DYNAMIC RANDOM GRAPHS

37

is defined as in Section 6.1. More precisely, let Ak be the collection of all measurable
g : : â„¦0 Ã— â„¦kv â†’ R such that
Z
|g(Ï‰0 , Ï‰1 , . . . , Ï‰k )|2 Î±(Ï‰0 , dÏ‰1 ) Â· Â· Â· Î±(Ï‰0 , dÏ‰k ) < âˆ, P0 a.e. Ï‰0
â„¦kv

and for every permutation Ï€ on [k], g(Ï‰0 , Ï‰1 , . . . , Ï‰k ) = g(Ï‰0 , Ï‰Ï€(1) , . . . , Ï‰Ï€(k) ), P0 âŠ— Î±âŠ—k a.s.
Q
where P0 âŠ— Î±âŠ—k (dÏ‰0 , dÏ‰1 , . . . , dÏ‰k ) := P0 (dÏ‰0 ) ki=1 Î±(Ï‰0 , dÏ‰i ). Then there is a measurable
space (â„¦âˆ— , F âˆ— ) and a regular conditional probability distribution Î»âˆ— : â„¦0 Ã— F âˆ— â†’ [0, 1] such
that on the probability space (â„¦0 Ã— â„¦âˆ— , B(â„¦0 ) âŠ— F âˆ— , P0 âŠ— Î»âˆ— ), where
Z
âˆ—
Î»âˆ— (Ï‰0 , B) P0 (dÏ‰0 ), A Ã— B âˆˆ B(â„¦0 ) âŠ— F âˆ— ,
P0 âŠ— Î» (A Ã— B) :=
A

there is a collection of real-valued random variables {Ik (g) : g âˆˆ Ak , k â‰¥ 1} satisfying
(a) For all g âˆˆ A1 the conditional
of I1 (g) given G âˆ— := B(â„¦0 ) âŠ— {âˆ…, â„¦âˆ— } is Normal
R distribution
2
with mean 0 and variance â„¦v g (Ï‰0 , Ï‰1 ) Î±(Ï‰0 , dÏ‰1 );
(b) Ik is (a.s.) linear map on Ak ;
(c) For g âˆˆ Ak of the form
Z
k
Y
ge2 (Ï‰0 , Ï‰1 ) Î±(Ï‰0 , dÏ‰1 ) < âˆ, P0 a.e. Ï‰0 ,
ge(Ï‰0 , Ï‰i ), s.t.
g(Ï‰0 , Ï‰1 , . . . , Ï‰k ) =
â„¦v

i=1

we have

âˆ—

Ik (g)(Ï‰0 , Ï‰ ) =

âŒŠk/2âŒ‹

X

j

(âˆ’1) Ck,j

j=1

and

Z

P0 âŠ— Î»âˆ— a.e. (Ï‰0 , Ï‰ âˆ— )
Z

â„¦âˆ—

âˆ—

2

âˆ—

âˆ—

j
g )(Ï‰0 , Ï‰ âˆ— ))kâˆ’2j ,
ge (Ï‰0 , Ï‰1 ) Î±(Ï‰0 , dÏ‰1 ) (I1 (e
2

â„¦v

(Ik (g)(Ï‰0 , Ï‰ )) Î» (Ï‰0 , dÏ‰ ) = k!

Z

â„¦v

k
ge (Ï‰0 , Ï‰1 ) Î±(Ï‰0 , dÏ‰1 ) , P0 a.e. Ï‰0 ,
2

where Ck,j are as in (6.1).
We write Ik (g)(Ï‰0 , Â·) as IkÏ‰0 (g).
Recall U n from (6.11) and the canonical processes Vi from Section 4.1. The following lemma
is a key ingredient to get the asymptotics of (U n , J n,1 , J n,2 , J n,3 ).
k
Lemma 6.6. Let {Ï†k }âˆ
k=1 be such that Ï†k âˆˆ A for each k â‰¥ 1. Let
X
ï£±
ï£²
Ï†k (Vi1 , . . . , Vik ) for n â‰¥ k
UÌ‚kn (Ï†k ) := 2â‰¤i1 <i2 <Â·Â·Â·<ik â‰¤n
ï£³
0
for n < k.

Then the following convergence holds as n â†’ âˆ:
 !


 
 k
1
X
I 1 (Ï†k )
U n , nâˆ’ 2 UÌ‚kn (Ï†k )
â‡’ Z,
k! k
kâ‰¥1
kâ‰¥1

as a sequence of Râˆ -valued random variables, where Z is Gaussian with mean 0 variance
Z
ij,y
ij,y
, Î¸ij i)2
âˆ’ hÎ³sâˆ’
(Î³sâˆ’
Ïƒ 2 := EP n
ds Ï(dy)
(6.12)
[0,T ]Ã—Z Î“(y, Xi (sâˆ’), Î½i (sâˆ’))

38

BAYRAKTAR AND WU

and is independent of {IkX1 (Â·)}kâ‰¥1 .
Proof. Fix m âˆˆ N, Ï†k âˆˆ Ak and t, sk âˆˆ R for k = 1, . . . , m. Denote by EP n ,V the conditional expectation under P n given (Xi , Ni )ni=1 . Since Î¾ij is conditionally independent given
(Xi , Ni )ni=1 , we have
1 X
EP n,V [un (i, j)]2 .
Ïƒn2 := EP n ,V [U n ]2 = 2
n
2â‰¤i<jâ‰¤n

Note that
EP n [Ïƒn2 ]

1
= 2
n

X

n

2â‰¤i<jâ‰¤n

2

EP n [u (i, j)] â†’ EP n

Z

[0,T ]Ã—Z

ij,y
ij,y
(Î³sâˆ’
âˆ’ hÎ³sâˆ’
, Î¸ij i)2
ds Ï(dy) = Ïƒ 2
Î“(y, Xi (sâˆ’), Î½i (sâˆ’))

and (since the cross product term below is zero when (i, j, ei, e
j) are distinct)
ï£¹2
ï£®
X

1
Îº1
EP n [Ïƒn2 âˆ’ EP n [Ïƒn2 ]]2 = 4 EP n ï£°
EP n ,V [un (i, j)]2 âˆ’ EP n [un (i, j)]2 ï£» â‰¤
â†’ 0.
n
n
2â‰¤i<jâ‰¤n

So Ïƒn2 â†’ Ïƒ 2 in probability as n â†’ âˆ. Suppose without loss of generality that Ïƒ 2 > 0, since
otherwise we have that Z = 0, U n â†’ 0 in probability as n â†’ âˆ and the desired convergence
holds trivially by Lemma 6.1. Also note that
E

X

Pn

E

P n ,V

2â‰¤i6=jâ‰¤n

un (i, j)
n

4

â‰¤

Îº2
â†’0
n2

as n â†’ âˆ. Hence the Lyapunovâ€™s condition for CLT (see [5, Theorem 27.3]) holds with Î´ = 2:
lim

nâ†’âˆ

X

1
Ïƒn2+Î´

EP n ,V

2â‰¤i6=jâ‰¤n

un (i, j)
n

2+Î´

= 0,

where the convergence is in probability. It then follows from standard proofs of CLT and a
subsequence argument that for each t âˆˆ R,
i
h âˆš
2 2
n
EP n ,V e âˆ’1tU âˆ’ eâˆ’t Ïƒn /2 â†’ 0

in probability as n â†’ âˆ. This together with the convergence of Ïƒn2 â†’ Ïƒ 2 implies that
h âˆš
i
2 2
n
EP n ,V e âˆ’1tU â†’ eâˆ’t Ïƒ /2
(6.13)

in probability as n â†’ âˆ. Now let (t, s1 , . . . sm ) 7â†’ Ï•n (t, s1 , . . . , sm ) be the characteristic
function of
m
1
n
(Ï†m )),
(U n , nâˆ’ 2 UÌ‚1n (Ï†1 ), . . . , nâˆ’ 2 UÌ‚m
and

1 2 2
Ïƒ

Ï•(t, s1 , . . . , sm ) := eâˆ’ 2 t

Ïˆ(s1 , . . . , sm ), (t, s1 , . . . , sm ) âˆˆ Rm+1

1 X1
be that of (Z, I1X1 (Ï†1 ), . . . , m!
Im (Ï†m )). From Lemma 6.1 it follows that for all (s1 , . . . , sm ) âˆˆ
m
R
âˆš

EP n [e

âˆ’1

Pm

k=1 sk n

âˆ’k
2

UÌ‚kn (Ï†k )

] â†’ Ïˆ(s1 , . . . , sm ) as n â†’ âˆ.

(6.14)

MEAN FIELD INTERACTION ON DYNAMIC RANDOM GRAPHS

39

Thus as n â†’ âˆ,

Ï•n (t, s1 , . . . , sm ) âˆ’ Ï•(t, s1 , . . . , sm )

 âˆš
âˆš
Pm
âˆ’k
1 2 2
n + âˆ’1
2 UÌ‚ n (Ï†k )
âˆ’
âˆ’1tU
s
n
t
Ïƒ
k
k=1
k
âˆ’e 2
Ïˆ(s1 , . . . , sm )
= EP n e


 âˆš Pm
h âˆš
i
k
âˆ’1tU n
âˆ’1 k=1 sk nâˆ’ 2 UÌ‚kn (Ï†k )
âˆ’ 21 t2 Ïƒ2
n
n
e
EP ,V e
= EP
âˆ’e


âˆš
Pm
âˆ’k
1 2 2
2 UÌ‚ n (Ï†k )
âˆ’1
s
n
k
k=1
k
] âˆ’ Ïˆ(s1 , . . . , sm ) eâˆ’ 2 t Ïƒ
+ EP n [e
â†’ 0,

where the convergence follows from (6.13) and (6.14). This completes the proof.



Now we analyze the asymptotics of (U n , J n,1 , J n,2 , J n,3 ). Recall AÏ‰0 , h, Î introduced in
Section 4.2. For Ï‰0 âˆˆ â„¦0 , denote by Aâˆ—Ï‰0 the adjoint operator of AÏ‰0 , that is
Z
âˆ—
g(Ï‰2 )h(Ï‰1 , Ï‰2 ) Î±(Ï‰0 , dÏ‰2 ), g âˆˆ HÏ‰0 , Ï‰1 âˆˆ â„¦v .
AÏ‰0 g(Ï‰1 ) =
â„¦v

R
Lemma 6.7. For P0 a.e. Ï‰0 , (a) Trace(AÏ‰0 Aâˆ—Ï‰0 ) = â„¦2 h2 (Ï‰1 , Ï‰2 ) Î±(Ï‰0 , dÏ‰1 ) Î±(Ï‰0 , dÏ‰2 ) =
v
R
R
2 (Ï‰ , Ï‰ ) Î(dÏ‰ ) Î(dÏ‰ ) =
h
Î»(t,
y)
dt
Ï(dy),
where
2
1
2
1
2
â„¦v
[0,T ]Ã—Z


hÎ³Ì„t,y (X1 [t], X2 (t), Â·), Î¸(t, X1 [t], X2 [t])i2
, (t, y) âˆˆ [0, T ] Ã— Z.
Î»(t, y) := EP n
Î“(y, X1 (t), Î½1 (t))

(b) Trace(AnÏ‰0 ) = 0 for all n â‰¥ 2. (c) I âˆ’ AÏ‰0 is invertible.

Proof. The first equality in part (a) follows from the definition of AÏ‰0 , the second uses the
observation that h(Ï‰1 , Ï‰2 ) does not depend on Î¾âˆ— (Ï‰1 ) or Î¾âˆ— (Ï‰2 ), and the third follows from
the definition of Î» and h. Part (b) follows on noting that
Z
n
h(Ï‰1 , Ï‰2 )h(Ï‰2 , Ï‰3 ) Â· Â· Â· h(Ï‰n , Ï‰1 ) Î±(Ï‰0 , dÏ‰1 ) Î±(Ï‰0 , dÏ‰2 ) Â· Â· Â· Î±(Ï‰0 , dÏ‰n ) = 0;
Trace(AÏ‰0 ) =
â„¦n
v

see also [27, Lemma 2.7]. Part (c) is immediate from [27, Lemma 1.3].
Let
m(y, x[s], x
e[s]) := EP n

"



#
Î³s12,y Î³s13,y
X2 [s] = x[s], X3 [s] = x
e[s]
Î“(y, X1 (s), Î½1 (s))

and define functions â„“, F : â„¦v Ã— â„¦v â†’ R (Î Ã— Î a.s.) is by
Z
m(y, Xâˆ— (Ï‰1 )[s], Xâˆ— (Ï‰2 )[s]) ds Ï(dy),
â„“(Ï‰1 , Ï‰2 ) :=

(6.15)

[0,T ]Ã—Z

Note that

F (Ï‰1 , Ï‰2 ) := h(Ï‰1 , Ï‰2 ) + h(Ï‰2 , Ï‰1 ) âˆ’ â„“(Ï‰1 , Ï‰2 ).
â„“(Ï‰1 , Ï‰2 ) =

Z

h(Ï‰3 , Ï‰1 )h(Ï‰3 , Ï‰2 ) Î(dÏ‰3 ),

P0 a.s. Ï‰0 .

(6.16)
(6.17)

â„¦v

Recall the independent normal random variable Z from Lemma 6.6. Let
1
1
1
J := I2X1 (F ) âˆ’ Trace(AX1 Aâˆ—X1 ) + Z âˆ’ Ïƒ 2 .
2
2
2
The following lemma is the key step.

(6.18)

40

BAYRAKTAR AND WU

Lemma 6.8. As n â†’ âˆ,

âˆš

âˆ’1Î·Ì„ n (Ï•) + J n (T ) â‡’

âˆš
âˆ’1I1X1 (Ï•) + J.

Proof. Recall UÌ‚kn in Lemma 6.6, SÌ‚ n,2 in (6.8), U n in (6.11), and â„“ in (6.15). From Lemmas
6.4 and 6.5 we have
J n,1 = U n + UÌ‚2n (hsym ) + Rn,1 ,
"
#
Z
12,y 2
h(Î³
)
,
Î½
(s)i
s
1
EP n
J n,2 =
ds Ï(dy) + UÌ‚2n (â„“) + Rn,2 ,
Î“(y, X1 (s), Î½1 (s))
[0,T ]Ã—Z
where hsym (Ï‰1 , Ï‰2 ) := 12 [h(Ï‰1 , Ï‰2 ) + h(Ï‰2 , Ï‰1 )] for Ï‰1 , Ï‰2 âˆˆ â„¦v . It then follows from Lemmas
6.2, 6.4, 6.5 and 6.6 that

Î·Ì„ n (Ï•), J n,1 , J n,2 , J n,3
#
!
"
Z
h(Î³s12,y )2 , Î½1 (s)i
X1
X1
X1 sym
ds Ï(dy) + I2 (â„“), 0 .
EP n
â‡’ I1 (Ï•), Z + I2 (h
),
Î“(y, X1 (s), Î½1 (s))
[0,T ]Ã—Z
Also note that
#
"
h(Î³s12,y )2 , Î½1 (s)i
EP n
Î“(y, X1 (s), Î½1 (s))
#
"
EP n [h(Î³s12,y )2 , Î½1 (s)i | X1 ]
= EP n
Î“(y, X1 (s), Î½1 (s))
"
#
EP n [(Î³sij,y âˆ’ hÎ³sij,y , Î¸ij i)2 | X1 ] + EP n [hÎ³Ì„t,y (X1 [t], X2 (t), Â·), Î¸(t, X1 [t], X2 [t])i2 | X1 ]
= EP n
Î“(y, X1 (t), Î½1 (t))
= Ïƒ 2 + Trace(AX1 Aâˆ—X1 )
a.s., by (6.12) and Lemma 6.7. The result follows on combining above two displays with (6.4),
(6.16), and (6.18).

6.4. Completing the proof of Theorem 4.1. Recall G âˆ— = B(â„¦0 ) âŠ— {âˆ…, â„¦âˆ— }. It follows from
(6.17), Lemma 6.7 and [27, Lemma 1.2] that P0 a.s.






1
1 X1
âˆ—
âˆ—
I (F ) G = exp
Trace(AX1 AX1 ) .
EP0 âŠ—Î»âˆ— exp
2 2
2
Therefore


1
1 X1
âˆ—
I (F ) âˆ’ Trace(AX1 AX1 )
= 1.
EP0 âŠ—Î»âˆ— exp
2 2
2
It then follows from Lemma 6.6 that




EP0 âŠ—Î»âˆ— [exp (J)] = 1.
Also, recall that
EP n [exp (J n (T ))] = 1.
It then follows from Lemma 6.8 âˆš
(with Ï• â‰¡ 0) and Scheffeâ€™s lemma that {exp (J n (T ))} is
uniformly integrable. Since | exp âˆ’1Î·Ì„ n (Ï•) | = 1,
âˆš

{exp âˆ’1Î·Ì„ n (Ï•) + J n (T ) }

MEAN FIELD INTERACTION ON DYNAMIC RANDOM GRAPHS

41

is also uniformly integrable. Hence from Lemma 6.8 we have
âˆš


âˆ’1Î·Ì„ n (Ï•) + J n (T )

âˆš
âˆ’1I1X1 (Ï•) + J
= EP0 âŠ—Î»âˆ— exp




âˆš
1
1 2
1 X1
X1
âˆ—
= EP0 âŠ—Î»âˆ— exp
âˆ’1I1 (Ï•) + I2 (F ) âˆ’ Trace(AX1 AX1 ) EP0 âŠ—Î»âˆ— exp Z âˆ’ Ïƒ
2
2
2





âˆš
1
1
= EP0 âŠ—Î»âˆ— EP0 âŠ—Î»âˆ— exp
âˆ’1I1X1 (Ï•) + I2X1 (F ) âˆ’ Trace(AX1 Aâˆ—X1 ) G âˆ—
2
2


Z
1
(6.19)
exp âˆ’ (ÏƒÏ‰Ï•0 )2 P0 (dÏ‰0 ),
=
2
â„¦0
lim EP n exp

nâ†’âˆ

where the last line is a consequence of Lemma 6.7 and [27, Lemma 1.3]. Thus we have proved
(6.2) which completes the proof of Theorem 4.1.
6.5. Completing the proof of Theorem 4.2. Clearly the function Ï• âˆˆ Ax can be viewed
as an element (abusing notation) Ï• of A defined by Ï•(x, Î¾) := Ï•(x), x, Î¾ âˆˆ D([0, T ] : Z)2 , and
Î¦Ï‰0 = Î¦ for P0 a.s. Ï‰0 . Note that h(Ï‰1 , Ï‰2 ) depends on Ï‰2 only through Xâˆ— (Ï‰2 ). It then
follows from the definition of AÏ‰0 and A that (I âˆ’ A)âˆ’1 Î¦(Ï‰) = (I âˆ’ AÏ‰0 )âˆ’1 Î¦Ï‰0 (Ï‰) for P0
a.s. Ï‰0 and the dependence on Ï‰ is actually only through Xâˆ— (Ï‰). It then follows from the
definition of ÏƒÏ‰Ï•0 that
ÏƒÏ‰Ï•0 = k(I âˆ’ A)âˆ’1 Î¦kH ,

P0 a.s. Ï‰0 .

Therefore from (6.19) we have
lim EP n exp

nâ†’âˆ



1
exp âˆ’ k(I âˆ’ A)âˆ’1 Î¦k2H P0 (dÏ‰0 )
2
â„¦0


1
âˆ’1
2
= exp âˆ’ k(I âˆ’ A) Î¦kH .
2

âˆš

âˆ’1Î·Ì„xn (Ï•) + J n (T ) =

Z

This gives (6.3) and completes the proof of Theorem 4.2.

Appendix A. Proof of Theorem 2.3
Proof of Theorem 2.3. (a) Since the limiting system is McKeanâ€“Vlasov, the proof of existence
and uniqueness is standard (cf. [29, Chapter 1], see also [15, Theorem 2.1]) and hence omitted.
(b) Now we show (2.11). For each fixed i âˆˆ [n] and t âˆˆ [0, T ], we have
EkXin âˆ’ Xi kâˆ—,t
Z
|y| 1[0,Î“(y,Xin (sâˆ’),Î½in (sâˆ’))] (z) âˆ’ 1[0,Î“(y,Xi (sâˆ’),Î½(sâˆ’))] (z) Ni (ds dy dz)
â‰¤E
Xt
Z
|y| |Î“(y, Xin (s), Î½in (s)) âˆ’ Î“(y, Xi (s), Î½(s))| ds Ï(dy).
=E
[0,t]Ã—Z

42

BAYRAKTAR AND WU

Fixing y âˆˆ Z and s âˆˆ [0, T ], we have

E |Î“(y, Xin (s), Î½in (s)) âˆ’ Î“(y, Xi (s), Î½(s))|
Z
n
1X
n
n
e Î½(s)(de
e
Î³(y, Xi (s), x
e, Î¾)
x dÎ¾)
Î³(y, Xi (s), Xj (s), Î¾ij (s)) âˆ’
=E
n
Z2
j=1
n

n

j=1

j=1

1X
1X
Î³(y, Xin (s), Xjn (s), Î¾ij (s)) âˆ’
Î³(y, Xi (s), Xj (s), Î¾ij (s))
â‰¤E
n
n
n

+E

1X
Î³(y, Xi (s), Xj (s), Î¾ij (s)) âˆ’
n
j=1

Z

Z2

e Î½(s)(de
e .
Î³(y, Xi (s), x
e, Î¾)
x dÎ¾)

From Condition 2.2, Remark 2.1(b) and the exchangeability of {(Xjn , Xj ) : j âˆˆ [n]} we have
n

n

j=1

j=1

1X
1X
Î³(y, Xin (s), Xjn (s), Î¾ij (s)) âˆ’
Î³(y, Xi (s), Xj (s), Î¾ij (s))
E
n
n
â‰¤

1
n

n
X
j=1


Î³y E|Xin (s) âˆ’ Xi (s)| + E|Xjn (s) âˆ’ Xj (s)| = 2Î³y E|Xin (s) âˆ’ Xi (s)|.

Since {(Xj , Î¾ij ) : j âˆˆ [n]} are independent with common joint law Î½, using Lemma 5.1 we
have
Z
n
1X
1 Î³y
e Î½(s)(de
e â‰¤ Îºâˆš
Î³(y, Xi (s), x
e, Î¾)
x dÎ¾)
Î³(y, Xi (s), Xj (s), Î¾ij (s)) âˆ’
.
E
n
n
Z2
j=1

Combining above four displays gives


Z
Îº1 Î³y
n
n
âˆš
ds Ï(dy)
|y| 2Î³y E|Xi (s) âˆ’ Xi (s)| +
EkXi âˆ’ Xi kâˆ—,t â‰¤
n
[0,t]Ã—Z
Z t
Îº1 CÎ³
EkXin âˆ’ Xi kâˆ—,s ds + âˆš .
â‰¤ 2CÎ³
n
0

It then follows from Gronwallâ€™s inequality that

Îº2
EkXin âˆ’ Xi kâˆ—,T â‰¤ âˆš
n
for some Îº2 < âˆ. This gives (2.11).
(c) Using (b), the independence of {Xi } and a standard argument (see [29, Chapter 1]) one
has (2.12).
(d) Using (b) and a standard argument (see [29, Chapter 1] and [4, Appendix A]) one has
(2.13) and (2.14). The last two statements (2.15) and (2.16) follow immediately from (2.13)
and (2.14), respectively.

References
[1] D. J. Aldous, Exchangeability and related topics, EÌcole dâ€™eÌteÌ de probabiliteÌs de saint-flour xiii â€” 1983,
1985, pp. 1â€“198.
[2] W. J. Anderson, Strong and exponential ergodicity, Continuous-time markov chains: An applicationsoriented approach, 1991, pp. 204â€“232.

MEAN FIELD INTERACTION ON DYNAMIC RANDOM GRAPHS

43

[3] J. Baladron, D. Fasoli, O. Faugeras, and J. Touboul, Mean-field description and propagation of chaos in
networks of Hodgkinâ€“Huxley and FitzHughâ€“Nagumo neurons, The Journal of Mathematical Neuroscience
2 (2012), no. 1, 10.
[4] S. Bhamidi, A. Budhiraja, and R. Wu, Weakly interacting particle systems on inhomogeneous random
graphs, Stochastic Processes and their Applications 129 (2019), no. 6, 2174â€“2206.
[5] P. Billingsley, Probability and Measure, Wiley series in probability and mathematical statistics: Probability
and mathematical statistics, John Wiley & Sons, New York, 1995.
[6] A. Budhiraja, P. Dupuis, M. Fischer, and K. Ramanan, Limits of relative entropies associated with weakly
interacting particle systems, Electronic Journal of Probability 20 (2015), no. 80, 1â€“22.
[7] A. Budhiraja, P. Dupuis, and A. Ganguly, Large deviations for small noise diffusions in a fast markovian
environment, Electron. J. Probab. 23 (2018), 33 pp.
[8] A. Budhiraja, D. Mukherjee, and R. Wu, Supermarket model on graphs, Ann. Appl. Probab. 29 (201906),
no. 3, 1740â€“1777.
[9] P. E Caines and M. Huang, Graphon mean field games and the GMFG equations, 2018 IEEE Conference
on Decision and Control (CDC), 2018, pp. 4129â€“4134.
[10] R. Carmona, D. Cooney, C. Graves, and M. Lauriere, Stochastic graphon games: I. The static case, arXiv
preprint arXiv:1911.10664 (2019).
[11] F. Coppini, H. Dietert, and G. Giacomin, A law of large numbers and large deviations for interacting diffusions on ErdoÌ‹sâ€“ReÌnyi graphs, Stochastics and Dynamics 0 (2019), no. 0, 2050010, available at
https://doi.org/10.1142/S0219493720500100.
[12] F. Delarue, Mean field games: A toy model on an ErdoÌˆs-Renyi graph., ESAIM: Proceedings and Surveys
60 (2017), 1â€“26.
[13] S. Delattre, G. Giacomin, and E. LucÌ§on, A note on dynamical models on random graphs and Fokkerâ€“Planck
equations, Journal of Statistical Physics 165 (2016), no. 4, 785â€“798.
[14] E. B. Dynkin and A. Mandelbaum, Symmetric statistics, Poisson point processes, and multiple Wiener
integrals, The Annals of Statistics 11 (1983), no. 3, 739â€“745.
[15] C. Graham, Mckean-Vlasov Ito-Skorohod equations, and nonlinear diffusions with discrete jump sets, Stochastic Processes and their Applications 40 (1992), no. 1, 69 â€“82.
[16] N. Ikeda and S. Watanabe, Stochastic Differential Equations and Diffusion Processes, North-Holland
Mathematical Library, vol. 24, Elsevier, 1981.
[17] V. N. Kolokoltsov, Nonlinear Markov Processes and Kinetic Equations, Cambridge Tracts in Mathematics,
vol. 182, Cambridge University Press, 2010.
[18] P. M. Kotelenez and T. G. Kurtz, Macroscopic limits for stochastic partial differential equations of
McKeanâ€“Vlasov type, Probability Theory and Related Fields 146 (2008Dec), no. 1, 189â€“222.
[19] T. G. Kurtz, Approximation of Population Processes, CBMS-NSF Regional Conference Series in Applied
Mathematics, vol. 36, SIAM, 1981.
[20] T. G. Kurtz and J. Xiong, Particle representations for a class of nonlinear SPDEs, Stochastic Processes
and their Applications 83 (1999), no. 1, 103â€“126.
[21] T. G. Kurtz and P. E. Protter, Weak convergence of stochastic integrals and differential equations II:
Infinite dimensional case, Probabilistic Models for Nonlinear Partial Differential Equations, 1996, pp. 197â€“
285.
[22] S. MeÌleÌard, Asymptotic behaviour of some interacting particle systems; Mckean-Vlasov and Boltzmann
models, Probabilistic Models for Nonlinear Partial Differential Equations, 1996, pp. 42â€“95.
[23] S. L Nguyen, G. Yin, and T. A Hoang, On laws of large numbers for systems with mean-field interactions
and Markovian switching, Stochastic Processes and their Applications (2019).
[24] R. I. Oliveira and G. H. Reis, Interacting diffusions on random graphs with diverging average degrees:
Hydrodynamics and large deviations, Journal of Statistical Physics (2019Jul).
[25] F. Parise and A. E Ozdaglar, Graphon games: A statistical framework for network games and interventions,
Available at SSRN: https://ssrn.com/abstract=3437293 (2019).
[26] D. Shah, Gossip algorithms, Foundations and Trends in Networking 3 (2009), no. 1, 1â€“125.
[27] T. Shiga and H. Tanaka, Central limit theorem for a system of Markovian particles with mean field interactions, Probability Theory and Related Fields 69 (1985), no. 3, 439â€“459.
[28] A-S. Sznitman, Nonlinear reflecting diffusion process, and the propagation of chaos and fluctuations associated, Journal of Functional Analysis 56 (1984), no. 3, 311â€“336.
[29] A-S. Sznitman, Topics in propagation of chaos, Ecole dâ€™EteÌ de ProbabiliteÌs de Saint-Flour XIXâ€”1989,
1991, pp. 165â€“251.

44

BAYRAKTAR AND WU

[30] J. Touboul, Propagation of chaos in neural fields, The Annals of Applied Probability 24 (2014), no. 3,
1298â€“1328.
[31] R. L. Tweedie, Criteria for ergodicity, exponential ergodicity and strong ergodicity of markov processes,
Journal of Applied Probability 18 (1981), no. 1, 122â€“130.
[32] G G. Yin and C. Zhu, Hybrid Switching Diffusions: Properties and Applications, Stochastic Modelling and
Applied Probability, vol. 63, Springer-Verlag New York, 2010.
Department of Mathematics, University of Michigan, 530 Church Street, Ann Arbor, MI
48109
Department of Mathematics, Iowa State University, 411 Morrill Road, Ames, IA 50011
E-mail address: erhan@umich.edu, ruoyu@iastate.edu

