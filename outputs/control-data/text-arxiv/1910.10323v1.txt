Region Based Adversarial Synthesis of Facial
Action Units
Zhilei Liu? , Diyi Liu, and Yunpeng Wu

arXiv:1910.10323v1 [cs.CV] 23 Oct 2019

College of Intelligence and Computing, Tianjin University
{zhileiliu, liudiyi, wuyupeng}@tju.edu.cn

Abstract. Facial expression synthesis or editing has recently received
increasing attention in the field of affective computing and facial expression modeling. However, most existing facial expression synthesis
works are limited in paired training data, low resolution, identity information damaging, and so on. To address those limitations, this paper
introduces a novel Action Unit (AU) level facial expression synthesis
method called Local Attentive Conditional Generative Adversarial Network (LAC-GAN) based on face action units annotations. Given desired
AU labels, LAC-GAN utilizes local AU regional rules to control the status
of each AU and attentive mechanism to combine several of them into the
whole photo-realistic facial expressions or arbitrary facial expressions. In
addition, unpaired training data is utilized in our proposed method to
train the manipulation module with the corresponding AU labels, which
learns a mapping between a facial expression manifold. Extensive qualitative and quantitative evaluations are conducted on commonly used
BP4D dataset to verify the effectiveness of our proposed AU synthesis
method.
Keywords: Facial action unit Â· Facial expression synthesis/editing Â·
Conditional generative adversarial network.

1

Introduction

Recently, identity preserving facial expression generation or synthesis from a
single facial image has attracted continuously increasing attention in the field
of affective computing and computer vision [27,18,3]. Ekman and Friesen [4]
developed the Facial Action Coding System (FACS) for defining facial expressions with some basic facial action units (AUs), each of which represents a basic
facial muscle movement or expression change. However, with the challenges of
time-consuming AU annotation and imbalanced databases, FACS based facial
expression analysis with large-scaled data dependent deep learning methods is
not widely conducted. To address those issues, facial expression synthesis combined and coordinated by a certain association of AUs has recently received
increased attention in the method of facial editing and transformation [27,18].
?

Corresponding author.

2

Zhilei Liu, Diyi Liu, and Yunpeng Wu

The generated facial expressions corresponding to desired AU labels can be applied to create a facial expression dataset with multiple diverse AU labels.
Most recent existing studies working on facial expression synthesis based on
distinct AU states pay more attention on the global face and some of them
train with the paired data. Zhou et al. [27] proposed a conditional difference
adversarial autoencoder (CDAAE) to transfer AUs from absence to presence on
the global face. This method divides the database into two groups according
to absence or presence of AU labels and trains with paired images of the same
subject, which increases the complexity of training and the difficulties of preprocess. Moreover, CDAAE employs the low resolution images, which can lose facial
details vital for AU synthesis. GANimation proposed by Pumarola et al. [18],
transfers AUs on the whole face which can produce a co-generated phenomenon
between different AUs. It is difficult for the method to synthesize a single AU
respectively without keeping the other AU untouched. Liu et al. [13] uses 3D
Morphable Model (3DMM) to achieve AU synthesis, where the transformation
from a image to 3D model damages the texture details of the original images.
In this paper, aiming at building a model for facial action unit synthesis with
more local texture details, we propose a facial action unit synthesis framework
named LAC-GAN by integrating local AU regions with conditional generative
adversarial network. With personal identity information well preserved, our proposed LAC-GAN manipulates AUs between different states, which learns a mapping between a facial manifold related to AU manipulation same as CAAE proposed by Zhang et al. [26]. Specifically, our AU manipulation model is trained
on unpaired samples and the corresponding AU labels without grouping the
database by different states of labels. In addition, the key point of our method
is to make the manipulation module focus only on the synthesis of local AU
region without touching the rest identity information and the other AUs. For
this purpose, AU region of interest(ROI) localization mechanism without cogenerated phenomenon between different AUs is applied to synthesize a single
AU respectively and correctly. Finally, to evaluate the performance of proposed
method, quantitative analyses on our synthesized facial images are conducted
with state-of-the-art facial action unit detectors.
In summary, the contributions of our work are as follows: (1) The proposed
AU manipulation module learns a mapping between the facial expression manifold with unpaired samples and corresponding AU labels. (2) Specific AU related
local attention is considered to manipulate AU features locally. (3) The proposed
AU synthesis framework LAC-GAN enables facial expression synthesis given any
desired AU combination of AU combinations. To demonstrate the effectiveness
of our proposed framework, both qualitative and quantitative evaluations are
performed.

2

Related Work

Our proposed framework is closely related to existing studies on GAN based
image generation and facial expression synthesis.

Region Based Adversarial Synthesis of Facial Action Units

2.1

3

Conditional Generative Adversarial Network

Generative Adversarial Network (GAN) [5] and Deep Convoluntional GAN (DCGAN) [19] are powerful class of generative models based on game theory. The
typical GAN optimization consists in simultaneously training a generator network to produce realistic fake samples and a discriminator network to distinguish
between real and fake data. This idea is embedded by the so-called adversarial
loss.
An active area of research is designing CGAN[15] models extended by GAN
that incorporate conditions and constraints into the generation process. Prior
studies have explored combining several conditions on transformation task, such
as class information [16], particularly, interesting for which are those methods
exploring image based conditioning as in image super-resolution [10], image
in-painting [17], image-to-image translation [8] and face age progression or
regression [26,23].Facial expression editing mostly focus on using the methods
of CGAN, similar to the above transferring work.Based on face age progression method [26], we proposed a conditional model to learn the face manifold
conditioned on AUs.
2.2

Facial Expression Synthesis

In recent years, the study on facial expression editing/synthesis has been actively
investigated in computer vision. Recently, Yeh et al. [24] proposed to edit the
facial expression by image warping with appearance flow. Although the model
can generate high-resolution images, paired samples as well as the labeled query
image are required. GANimation [18] proposed a method of virtual avatar animation on the whole facial images while training a cycle conditional generative
adversarial network to achieve the transformation of expression. Recently, Zhou
et al. [27] proposed a conditional difference adversarial autoencoder (CDAAE)
for facial expression synthesis that considers AU labels. However, the resolution
of its generated facial image is only 32 Ã— 32, and the generated facial images
with AU labels are not well quantitatively evaluated. By integrating with the
3D Morphable Model (3DMM), Liu et al. [13] proposed an 3D AU synthesis
framework to transfer AUs in the range of intensities, however, some texture
details are lost for certain AUs because of the limitations of 3D face model. In
addition, while generating a single AU, all these above mentioned methods can
damage the other AU without keeping the identity information untouched.
It is a certain reference for AU synthesis that most works on AU detection
make full use of landmark-based geometry to improve the performance. Li et
al. [12] proposed a deep learning based approach named EAC-Net for facial action unit detection by enhancing and cropping the AU regions of interest(ROI)
with roughly extracted facial landmark information.Therefore, in this paper, imitating EAC-Net, our proposed framework separates the facial image into multiple local AU regions which are integrated in modern deep learning model.In
addition, attention-GAN [1] proposed adopts attention mechanism to focus on
generating objects of interests without touching the background region. Simi-

4

Zhilei Liu, Diyi Liu, and Yunpeng Wu

lar to them, we proposed AU ROI localization module to maintain the other
information except the concerned AU.

3

LAC-GAN for Facial Action Unit Synthesis

In this section, the overall framework of our proposed LAC-GAN for AU synthesis as shown in Fig. 1 is presented at first. The details of the AU manipulation
module with conditional AU region generator (CARG) for single AU synthesis
as shown in Fig. 2 are introduced in the next.

Encoder

ğ‘¹ğ’

ğ‘¹ğŸ

ğ‘¹ğŸ

Encoder

Decoder

ğŸ/ğŸ

VGG_Face

ğ‘³ğ’—ğ’ˆğ’ˆ

Decoder
ğ´ğ‘ˆğ‘–

ğ‘¹ğŸ‘

ğ‘ªğ‘¨ğ‘¹ğ‘® ğ’

ğ‘¨ğ‘¼ğŸ

ğ‘ªğ‘¨ğ‘¹ğ‘® ğŸ‘
ğ‘ªğ‘¨ğ‘¹ğ‘® ğŸ
ğ‘ªğ‘¨ğ‘¹ğ‘® ğŸ

Conditional AU Region Generators

AU ROI localization module

D_globel

-1
-1 1
-1 1
-1 1

ğ‘¨ğ‘¼ğŸ

1
ğ‘¨ğ‘¼ğ’

ğ‘¨ğ‘¼ğŸ‘

AU labels

ğ‘¨ğ‘¹ğ‘® + ğ‘¿ğ‘º Ã— (ğŸ âˆ’ ğ‘¨)

ğ‘¨ğ‘¼ğŸ

ğ‘¿ğ‘º Ã— ğ‘¨

AU manipulation module

Global AU discriminator module

Fig. 1. The Overall Framework of LAC-GAN

3.1

The Overall Framework of LAC-GAN

AS shown in Fig. 1, our proposed LAC-GAN consists of three modules: AU
ROI localization module with fixed local attention, AU manipulation module,
and global AU discriminator module. In the AU ROI localization module, AU
specific ROIs are defined as local regions of the original facial image X s âˆˆ
RHÃ—W Ã—3 . Given the input image X s and the target one-hot AU label vector
N
Ã—2
lAUi âˆˆ {âˆ’1, +1} AU , the AU manipulation module focuses on transforming
the AU ROI from the source state to the target state. The resulting image is
therefore a combination of all the changed AU regions with the rest regions of
original facial image. Finally, the global AU discriminator aims to distinguish
the real images and the generated images.
AU ROI localization module: AUs appear in specific facial regions that are
not limited to facial landmark points. Instead of directly using facial landmarks,
AU center rules are adopted by us to localize AU specific ROIs. Similar to EACnet, AU local attention map containing several AUs are designed here for the

Region Based Adversarial Synthesis of Facial Action Units

5

purpose of generating AU ROIs. In addition, we capture the AU local attention
map by applying the Manhattan distance to the AU center. The local AU ROI
is defined as:
X AUi = AAUi Ã— X s ,

(1)

where X AUi is the pixel-level weight of the AUi with the local attention map
HÃ—W
AAUi âˆˆ {0, ..., 1}
.
In order to preserve the original facial information outside of the AU ROIs,
an local AU attention localization is considered in this module. Concretely, the
whole face is divided into multiple AU regions based on the local AU attention
mapping rules. After obtaining AU ROIs, the desired AU regions with target
AU labels will be generated by CARGs introduced in section 3.2. The generated
target image can be obtained as:
Xt =

N
AU
X

GAUi (X AUi , lAUi ) + (1 âˆ’

i=1

N
AU
X

AAUi ) Ã— X s ,

(2)

i=1

where GAUi (Â·) is the output of the generator G conditioned on target AU
label lAUi and the input ROI X AUi of AUi computed in Eq. 1.
Global AU Discriminator Module: In order to improve the photo-realism
of the generated images, an adversarial loss between the generated image X t
and the real image X r is defined as:
r
t
min max Limg
adv = EXâˆ¼Pdata (X) [log Dimg (X )] + EXâˆ¼Pdata (X) [log(1 âˆ’ Dimg (X ))],

GAU Dimg

(3)
where X t is synthetic output obtained by Equ. 2.
To preserve more identity information between X t and X r , a pre-trained
VGG-face model is leveraged to enforce the similarity in the feature space with
following loss:
X
min Lid =
Î±l L1 (Ï†l (X t ), Ï†l (X r )),
(4)
GAU

l

where Ï†l (Â·) is the feature map of the lth layer of the VGG-face network, and
Î±l is the corresponding weight. Similar to [22], the activations at the conv1 2,
conv2 2, conv3 2, conv4 2 and conv5 2 layer of the VGG-face model are used.
3.2 AU Manipulation Module:
As shown in Fig. 2, the proposed AU manipulation module contains multiple
local conditional AU regional generators (CARGs) for single facial action unit
synthesis. Base on the face manifold assumption demonstrated experimentally
in CAAE, the inputs of our proposed CARG are the source ROI of X AUi and
the target label lAUi . On the other hand, additional local critic discriminator is
used to evaluate the quality of the generated regional image.

6

Zhilei Liu, Diyi Liu, and Yunpeng Wu

AU label layer

Batchnorm layer

P(z) uniform

0/1

ğ‘³ğ’ğ’„ğ’‚ğ’ ğ‘«ğ’Šğ’”ğ’„ğ’“ğ’Šğ’ğ’Šğ’ğ’‚ğ’•ğ’ğ’“ ğ’ğ’ ğ‘¹ğ‘¶ğ‘°

Convolution layer

ğ‘«ğ’Šğ’”ğ’„ğ’“ğ’Šğ’ğ’Šğ’ğ’‚ğ’•ğ’ğ’“ ğ’ğ’ ğ’›

Reshape

Reshape

Real or Fake

ğ‘ â„ğ‘ğ‘Ÿğ‘’ ğ‘¤ğ‘’ğ‘–ğ‘”â„ğ‘¡ğ‘  ğ‘Šğ·
ğ‘¨ğ‘¼ğ’Š
-1

ğ‘¬ğ’ğ’„ğ’ğ’…ğ’†ğ’“_ğ‘¨ğ‘¼

ğ‘«ğ’†ğ’„ğ’ğ’…ğ’†ğ’“_ğ‘¨ğ‘¼

+1

ğ‘¨ğ‘¼ğ’Š _ğ‘¹ğ’†ğ’”ğ’‰ğ’‚ğ’‘ğ’†

ğ‘´ğ’‚ğ’ğ’Šğ’‘ğ’–ğ’ğ’‚ğ’•ğ’Šğ’ğ’ ğ‘´ğ’ğ’…ğ’–ğ’ğ’†

ğ‘³ğ’ğ’„ğ’‚ğ’ ğ‘¨ğ‘¼ ğ‘¹ğ’†ğ’ˆğ’“ğ’†ğ’”ğ’”ğ’Šğ’ğ’ ğ’ğ’ ğ‘¹ğ‘¶ğ‘°

Fig. 2. Proposed Conditional AU Regional Generator of AU Manipulation Module

Local Conditional AU Regional Generators: Given the specific AU ROI
X AUi of the source image and the desired AU label, the generator network
AUi
i
GAUi = (GAU
en , Gde ) with encoder-decoder structure is proposed. At first, the
AUi
i
into a latent feature vector zAUi , Nz is the
encoder GAU
en maps the input X
dimension of the latent feature vector zAUi . By concatenating the obtained zAUi
i
and the desired AU labels lAUi together, the decoder GAU
de is designed to generAU
AU
AU
i
Ë†
AU
i
i
), lAUi ) with original identity
ate the target AU ROI X i = Gde (Gen (X
information and target AU information. A pixel-wise image reconstruction loss
is defined as:

min

AU
AU
Gen i ,Gde i

AUi
AUi
i
), lAUi ) âˆ’ X AUi k1 ],
Lipixel = EX AUi âˆ¼Pdata (X AUi ) [kGAU
de (Gen (X

(5)
in which, l1 -norm is adopted to capture low-level structure information. In
preliminary experiments, we also tried replacing l1 -norm with the other perceptual loss, although we did not observe significant improved performance.

Discriminator Dz on Latent Feature: The fact is demonstrably authenticated that face images lie on a manifold [6,11]. For the purpose of maintaining
the AU regions generated by the face manifold [26], we employ the uniform distribution to zAUi through the discriminator DzAUi ,which imposes zAUi on the
uniform distribution without â€holesâ€. The adversarial training process is of Dz
is defined as:

Region Based Adversarial Synthesis of Facial Action Units

min max Liadv
AUi

Gen

AUi

z

7

=Ezâˆ¼P (z) [log DzAUi (z)]+

Dz

(6)
AUi
i
EX AUi âˆ¼Pdata (X AUi ) [log(1 âˆ’ DzAUi (GAU
))].
en (X

Local AU Discriminator: Similar to recent studies conducted by Huang et
al. [7] and Liu et al. [21], an regional adversarial loss between the real AU ROI
Ë† i = GAUi (GAUi (X AUi ), lAU ) is defined.
X AUi and the generated AU ROI X AU
en
i
de
While reducing the global image adversarial loss, the local AU discriminator
must also reduce the error, which is defined with two components: a local AU
adversarial loss is used to distinguish the real and fake AU regions which learns
to render realistic samples; and an AUs regression loss of AU regions is used to
learn the regression layer on top of local discriminator, which satisfies the target
facial expression encoded by lAUi . Those loss can therefore be defined as:
max Liadv

min

AU
AU
Gen i ,Gde i ,RAUi DAUi

=EX AUi ,lAUi âˆ¼Pdata (X,l) [log DAUi (X AUi , lAUi )]+

AU

Ë† i , lAU ))]
EX AUi ,lAUi âˆ¼Pdata (X,l) [log(1 âˆ’ DAUi (X AU
i
+ Î»AU Lilabel ,
(7)
where Î»AU Llabel is a trade-off parameter of the AU regression loss which
keeps the facial region with a specific AU state generated by manipulating the
corresponding expression code. This loss is completed as follow:
Ë† i , lAU ) âˆ’ lAU k2 ]
min Lilabel = EX AUi ,lAUi âˆ¼Pdata (X,l) [kRAUi (X AU
i
i 2

RAUi

(8)

Overall Objective Function: To generate the target face with desired AU
label vector, following loss function LCARG is defined by linearly combining all
previous partial losses:

AU

min

max

AU

AUi

Gen i ,Gde i ,RAUi DAUi ,Dz

LiCARG = Lipixel + Î»1 Liadv z + Î»2 Liadv

AU .

(9)

The final training loss function of LAC-GAN is a weighted sum of all these
losses defined above:

min

max

GAU ,RAU DAU ,DzAU

LLACâˆ’GAN =

N
AU
X

Î²i LiCARG + Î»3 Lid + Î»4 Ltv .

(10)

i=1

where Î»j (j = 1, 2, 3, 4) is a trade-off parameter,Î²i (j = 1, ..., NAU ) is the
corresponding weight of CARGi loss.The total variation regularization Ltv [14]is
adopt on the reconstructed image to reduce spike artifacts.

8

4
4.1

Zhilei Liu, Diyi Liu, and Yunpeng Wu

Experimental Evaluation
Experimental Dataset and Implementation Details

Dataset: Our LAC-GAN is evaluated on widely used database for facial AU detection, named BP4D[25]. BP4D contains 41 participants with 23 female and 18
male, each of which is involved in 8 sessions captured both 2D and 3D videos for
each participant. Each video frame is manually coded with an intensity for each
of the 12 AUs, namely AU1, AU2, AU4, AU6, AU7, AU10, AU12, AU14, AU15,
AU17, AU23, and AU24, with AU labels of occurrence or absence according to
the FACS. In this work, all annotated video frames with successful face registration (100,767 frames) of 31 subjects are selected as the training set, and all
annotated video frames (39,233 frames) for the remaining 10 subjects are used
as the testing set.The data partition rule is the same as the AU detection model
JAA-Net [20] with two folds for training and the remaining one for testing.
Implementation Details: For each facial image, similarity transformations including rotation, uniform scaling, translation, and normalization are performed
to obtain a 200 Ã— 200 Ã— 3 color facial image.The kernel size of the multiple convolution and de-convolution layers in our proposed framework is set to be 5Ã—5. The
encoder consists of 5 convolutional layers and a reshaped fully connection layer,
while the decoder consists of the other reshaped layer and 5 de-convolutional
layers. The dimensions of z and AU labels are set to be 60. Discriminator Dz
employs four fully connection layers. The batch normalization is adopted on local
AU discriminator to make the framework more stable. Simultaneously. local AU
Regression shares the weight of 4 convolution layers with local AU discriminator.
All models are trained using Tensorflow using the Adam optimizer [9], with
learning rate of 0.0002, Î±1 = 0.5, Î±2 = 0.9, Î±3 = 0.75, Î±4 = Î±5 = 1. The weight
coefficients for the loss terms in Eq. 9 and Eq. 10 are set to Î»1 = 0.01, Î»2 =
0.1, Î»3 = 1, Î»4 = 0.001, Î»AU = 10. During training, we set Î²i = 1, where i is
corresponded with original AU labels.
4.2 Qualitative Analysis of AU Synthesis Performance
Synthetic Results of Single AU: We firstly evaluate our modelâ€™s ability to
activate the status of AUs by transforming the neutral face to the absence or
the presence of specific AU while preserving the personâ€™s identity. Fig. 3 shows a
subset of 12 AUs individually manipulated at different status during the testing
session.
For AU1 (inner brow raiser), AU2 (outer brow raiser), and AU4 (inner brow
lowerer) located in the brow region, the changes of synthesis face between absence and presence of AU can be obviously perceived. For AU6 (cheek Raiser)
located in the cheek region, the movement of cheek muscle can be observed.
For AU10 (Upper Lip Raiser), AU12 (Lip Corner Puller), AU14 (Dimpler), and
AU15 (Lip Corner Depressor) located in the mouth region, obvious movements
of AU muscles can be observed in the synthetic images. For the other AUs corresponding to the movements in the mouth and chin regions, AU23 (Lip Tightener)
can be observed like pouting and AU24 (Lip Pressor) seem like closing mouse

Region Based Adversarial Synthesis of Facial Action Units

9

Fig. 3. Synthetic Results of Single AU.

obviously. However, Subtle variation of AU7 (Lid Tightener) and AU17 (Chin
Raiser) which accounts for minor wrinkling in the synthetic faces.

Fig. 4. Qualitative comparison with GANimation [18].

Qualitative comparison with GANimation: We compare our approach
again the baseline GANimation [18]. For a fair comparison, we adopt the same
dataset and experimental details with training GANimation.Fig. 4 shows our
method performs better than GANimation on synthetic of single AUs, with
keeping identity information and the other AUs untouched.Owing to the dataset
BP4D with poor diversity and unbalanced labels distribution, the generated
results we trained is not as well as in their paper. However, ours model can be
trained to generate the better results with the same dataset.
Synthetic Results of AU combinations: Our LAC-GAN can synthesize
spontaneous facial expression incorporated by multiple AUs, such as happy and
angry. Fig. 5 shows the generated happy (AU2, AU6, AU7 and AU12) and angry
(AU4 and AU23), which are substantially similar to the ground-truth expres-

10

Zhilei Liu, Diyi Liu, and Yunpeng Wu

sions. The slight difference is probably caused by the different intensity of the
presence of AUs.

Fig. 5. Synthetic Results of Common AU combinations.

4.3

Quantitative Analysis of the Synthetic Results

To assess whether the synthetic images are capable of being accurately detected
by the AU detection methods, we evaluate our synthetic images with state-ofthe-art AU detection models. We firstly generated the augmentation dataset
by our synthesis model of which the personâ€™s subject and corresponding AUs
labels distribution is same as the ground-truth test dataset .Afterwards, the real
images and synthesis images are estimated by two AU detectors respectively
which are OpenFace AU classifier [2] and JAA-Net [20].These two models are
demonstrated state-of-the-art results in the task of AU detection. F1 score and
accuracy are adopted as metrics. In the following section, all the results are
simplified without %.
Table 1 shows the quantitative analysis results of synthesis images augmentation. â€™Gapsâ€™ is regarded as the different value between the detection results of
â€™Realâ€™ dataset and â€™Synthesisâ€™dataset. â€™âˆ’â€™ means there are no results of AU24
with OpenFace classifier.Specifically, the results of synthesis images approaches
to the real images, between which the gaps is approximately 6% âˆ¼ 7% with
average metrics which means our synthesis images are highly homologous with
the real images on AU detection level. While JAA-Net used as the detector,
the gaps between Real and Synthesis is less than while OpenFace is used as
the detector, on account of the local AU attention localization JAA-Net adopts
similar with our model. AU1, AU4, AU12, AU14, AU23 generated by our model
is demonstrably close to the the ground truth images with both detectors, especially for AU1 and AU14. However,the performance of generated face on AU17
is poor, which accounts the occurrence of AU17 is lower than the other AUs in
the dataset. Substantially, the images generated by our model is effective to be
detected by the state-of-the-art AU detection models.

Region Based Adversarial Synthesis of Facial Action Units

11

Table 1. Quantitative Analysis Of Our Synthetic Results.

AU
AU1
AU2
AU4
AU6
AU7
AU10
AU12
AU14
AU15
AU17
AU23
AU24
Avg

5

Real
F1 Acc
64.1 76.1
44.3 66.2
71.9 87.4
84.3 83.5
77.6 76.8
88.2 85.0
73.6 68.7
91.0 89.2
44.3 85.7
69.6 78.8
53.6 80.3
69.3 79.8

OpenFace
Synthesis Gaps
F1 Acc F1 Acc
75.8 88.7 -11.7 -12.6
33.0 51.3 11.3 14.9
67.7 77.6 4.2 9.8
73.1 75.2 11.2 8.3
69.5 69.4 8.1 7.4
76.8 72.3 11.4 12.7
72.8 78.0 0.8 -9.3
85.1 84.8 5.9 4.4
35.7 76.7 6.8
9
40.4 51.2 29.3 27.6
47.5 74.9 6.1 5.4
61.7 72.7 7.6 7.1

Real
F1 Acc
46.2 69.7
48.7 78.4
56.4 84.7
80.4 80.2
73.5 72.9
84.9 80.4
88.4 85.7
59.4 60.2
45.6 84.7
61.1 73.3
36.2 81.0
37.4 83.9
59.9 77.9

JAA-Net
Synthesis Gaps
F1 Acc F1 Acc
59.5 76.5 -13.3 -6.8
33.7 67.1 15 11.3
52.6 78.7 3.8 6
72.3 75.9 8.1 4.3
67.2 65.8 6.3 7.1
74.3 68.5 10.6 11.9
83.6 80.1 4.8 5.6
63.7 65.1 -4.3 -4.9
34.1 76.4 11.5 8.3
35.8 54.7 25.3 18.6
31.7 74.9 4.5 6.1
30.6 76.2 6.8 - 7.7
53.3 71.7 6.6 6.3

Conclusion

In this paper, we presents LAC-GAN for facial action units synthesis, which is
incorporated into the photo-realistic facial expressions, without destroying the
other facial information except the AU. The generator can manipulate the AU
in different status, such as transferring from absence to presence. The local discriminator and AU classifier in CARGs guarantees the consistency between the
desired faces and the corresponding target AU. The AU regional localization
is proposed to combine all the AU regions to an integral face with keep the
background untouched. We further conducted extensive experimental quantitative analysis to evaluate our synthesis model. Our future work will explore how
to apply LAC-GAN to other larger and more unconstrained facial expression
dataset with intensity-level generation.

Acknowledgements
This work is supported by the National Natural Science Foundation of China
under Grants of 41806116 and 61503277. We gratefully acknowledge the support
of NVIDIA Corporation with the donation of the Titan V GPU used for this
research.

References
1. Chen, X., Xu, C., Yang, X.: Attention-gan for object transfiguration in wild images.
arXiv preprint arXiv:1803.06798 (2018)
2. Cmusatyalab: Openface: facial action units recognition with deep neural networks.
http://cmusatyalab.github.io/openface/
3. Ding, H., Sricharan, K., Chellappa, R.: Exprgan: Facial expression editing with
controllable expression intensity. arXiv preprint arXiv:1709.03842 (2017)

12

Zhilei Liu, Diyi Liu, and Yunpeng Wu

4. Ekman, P., Rosenberg, E.L.: What the face reveals: Basic and applied studies of
spontaneous expression using the Facial Action Coding System (FACS). Oxford
University Press, USA (1997)
5. Goodfellow, I.J., Pouget-Abadie, J.: Generative adversarial nets. In: Advances in
Neural Information Processing Systems. pp. 2672â€“2680 (2014)
6. He, X., Yan, S., Hu, Y.: Face recognition using laplacianfaces. IEEE TPAMI 27(3),
328â€“340 (2005)
7. Huang, R., Zhang, S., Li, T., He, R.: Beyond face rotation: Global and local perception gan for photorealistic and identity preserving frontal view synthesis. In:
ICCV (2017)
8. Isola, P., Zhu, J.Y., Zhou, T.: Image-to-image translation with conditional adversarial networks. In: CVPR 2017 (2017)
9. Kingma, D., Ba, J.: Adam: A method for stochastic optimization. In: ICLR (2014)
10. Ledig, C., Theis, L., Huszar, F.: Photo-realistic single image super- resolution using
a generative adversarial network. In: CVPR 2017 (2017)
11. Lee, K.C., Ho, J., Yang, M.H.: Video-based face recognition using probabilistic
appearance manifolds. CVPR 1(1) (2003)
12. Li, W., Abtahi, F., Zhu, Z.: Eac-net: A region-based deep enhancing and cropping
approach for facial action unit detection. IEEE Conference on Computer Vision
and Pattern Recognition Workshop pp. 103â€“110 (2017)
13. Liu, Z., Song, G.: Conditional adversarial synthesis of 3d facial action units. arXiv
preprint arXiv:1802.07421 (2018)
14. Mahendran, A., Vedaldi, A.: Understanding deep image representations by inverting them. In: CVPR. pp. 5188â€“5196 (2015)
15. Mehdi Mirza, S.O.: Conditional generative adversarial nets. arXiv preprint
arXiv:1411.1784 (2014)
16. Odena, A., Olah, C., Shlens, J.: Conditional image synthesis with auxiliary classifier
gans. In: ICML 2017 (2017)
17. Pathak, D., Krahenbuhl, P., Donahue, J.: Context encoders: Feature learning by
inpainting. In: CVPR 2016 (2016)
18. Pumarola1, A., Agudo, A.: Ganimation: Anatomically-aware facial animation from
a single image. In: ECCV 2018 (2018)
19. Radford, A., Metz, L.: Unsupervised representation learning with deep convolutional generative adversarial networks. In: ICLR 2016 (2016)
20. Shao, Z., Liu, Z., Cai, J.: Deep adaptive attention for joint facial action unit detection and face alignment. In: ECCV 2018 (2018)
21. Tran, L., Yin, X., Liu, X.: Disentangled representation learning gan for poseinvariant face recognition. CVPR 4(7) (2017)
22. Wang, M., Deng, W.: Deep face recognition: A survey. BMVC 1(6) (2015)
23. Wang, Z., Tang, X.: Face aging with identity-preserved conditional generative adversarial networks. In: CVPR 2018 (2018)
24. Yeh, R., Liu, Z., Goldman, D.B.: Semantic facial expression editing using autoencoded flow. arXiv preprint arXiv:1611.09961 (2016)
25. Zhang, X., Yin, L., Cohn, J.F.: Bp4d-spontaneous: a high-resolution spontaneous
3d dynamic facial expression database. Image and Vision Computing 32(10), 692â€“
706 (2014)
26. Zhang, Z., Song, Y.: Age progression/regression by conditional adversarial autoencoder. In: CVPR 2017 (2017)
27. Zhou, Y., Shi, B.E.: Photorealistic facial expression synthesis by the conditional
difference adversarial autoencoder. In: ACII 2017 (2017)

