1

Federated Variance-Reduced Stochastic Gradient
Descent with Robustness to Byzantine Attacks

arXiv:1912.12716v2 [cs.LG] 3 Feb 2021

Zhaoxian Wu, Qing Ling, Tianyi Chen, and Georgios B. Giannakis

Abstractâ€”This paper deals with distributed finite-sum optimization for learning over networks in the presence of malicious
Byzantine attacks. To cope with such attacks, most resilient
approaches so far combine stochastic gradient descent (SGD)
with different robust aggregation rules. However, the sizeable
SGD-induced stochastic gradient noise makes it challenging to
distinguish malicious messages sent by the Byzantine attackers
from noisy stochastic gradients sent by the â€˜honestâ€™ workers.
This motivates us to reduce the variance of stochastic gradients
as a means of robustifying SGD in the presence of Byzantine
attacks. To this end, the present work puts forth a Byzantine
attack resilient distributed (Byrd-) SAGA approach for learning
tasks involving finite-sum optimization over networks. Rather
than the mean employed by distributed SAGA, the novel ByrdSAGA relies on the geometric median to aggregate the corrected
stochastic gradients sent by the workers. When less than half of
the workers are Byzantine attackers, the robustness of geometric
median to outliers enables Byrd-SAGA to attain provably linear
convergence to a neighborhood of the optimal solution, with
the asymptotic learning error determined by the number of
Byzantine workers. Numerical tests corroborate the robustness
to various Byzantine attacks, as well as the merits of ByrdSAGA over Byzantine attack resilient distributed SGD.
Index Termsâ€”Distributed finite-sum optimization, Byzantine
attacks, gradient noise, variance reduction

I. I NTRODUCTION
With the rapid development of information technologies,
the volume of distributed data increases explosively. Every
day, numerous distributed devices including sensors, cellphones, computers, and vehicles, generate huge amounts of
data, which are often forwarded to datacenters for further
processing and learning tasks. However, collecting data from
distributed devices and storing them in datacenters raise major privacy concerns [1]â€“[3]. Accounting for these concerns,
Zhaoxian Wu and Qing Ling are with School of Data and Computer
Science and Guangdong Province Key Laboratory of Computational Science,
Sun Yat-Sen University, Guangzhou, Guangdong 510006, China. Tianyi
Chen is with Department of Electrical, Computer, and Systems Engineering,
Rensselaer Polytechnic Institute, Troy, New York 12180, USA. Georgios
B. Giannakis is with Department of Electrical and Computer Engineering
and Digital Technology Center, University of Minnesota, Minneapolis,
Minnesota 55455, USA. Qing Ling is supported in part by NSF China Grants
61573331 and 61973324, and Fundamental Research Funds for the Central
Universities. Georgios B. Giannakis is supported in part by NSF Grants
1509040, 1508993, 1711471, and 1901134. A short version of this paper
has been submitted to IEEE International Conference on Acoustics, Speech,
and Signal Processing, Barcelona, Spain, May 4-8, 2020. Corresponding
Email: lingqing556@mail.sysu.edu.cn.

federated learning has been advocated to provide a privacypreserving, decentralized data processing and machine learning framework [4]. Data in federated learning are kept private,
and local computations are carried at the distributed devices.
Updates of local variables (such as stochastic gradients,
corrected stochastic gradients, and model parameters) are
found using per-device private data, while the datacenter
aggregates local variables and disseminates the aggregated
result to the distributed devices.
Even though privacy is preserved, the distributed nature
of federated learning makes it vulnerable to errors and
adversarial attacks. Devices can then become unreliable in
either computing or communicating, or, they can even be
hacked by adversaries. As a result, compromised devices may
send malicious messages to the datacenter, thus misleading
the learning process [5]â€“[7]. We will henceforth focus on the
class of malicious attacks known as Byzantine attacks [8].
Robustifying federated learning against Byzantine attacks is
of paramount importance for secure processing and learning.
To cope with Byzantine attacks in federated learning, several robust aggregation rules have been developed in recent
years, mainly towards improving the distributed stochastic
gradient descent (SGD) solver of the underlying optimization task. Through aggregating stochastic gradients with the
geometric median [9], [10], median [11], trimmed mean
[12], or iterative filtering [13], stochastic algorithms have
been able to tolerate a small number of devices attacked
by Byzantine adversaries. Other aggregation rules include
Krum [14], that selects a stochastic gradient having the
minimal cumulative squared distance from a given number of
nearest stochastic gradients, and RSA [15] which aggregates
models other than stochastic gradients through penalizing the
differences between the local and global model parameters.
Related works also include adversarial learning in distributed
principal component analysis [16], escaping from saddle
points in non-convex distributed learning under Byzantine
attacks [17], and leveraging redundant gradients to improve
robustness [18], [19].
Although robust SGD iterates can ensure convergence
to a neighborhood of the attack-free optimal solution, this
neighborhood size can be large when Byzantine attacks are
carefully crafted [20]. Essentially, SGD suffers from the
sizeable approximation error (noise) associated with stochastic gradients. This leads to the challenge of distinguishing
malicious messages sent by Byzantine attackers from the

2

noisy stochastic gradients sent by â€˜honestâ€™ devices.
In the face of this challenge, we posed the following
question: Is it possible to better distinguish the malicious
messages from the stochastic gradients through reducing
the stochastic gradient-induced noise? Our answer will turn
out to be in the affirmative. Intuitively, if the stochastic
gradient noise is small, the malicious messages should be
easy to identify; see also the illustrative example in Section
II-D. This intuition suggests combining variance reduction
techniques with robust aggregation rules to handle Byzantine
attacks in federated learning.
Existing variance reduction techniques in stochastic optimization include mini-batch [21], and abbreviated ones as
SAG [22], SVRG [23], SAGA [24], SDCA [25], SARAH
[26], Katyusha [27], to list a few. Among these, we are particularly interested in SAGA, which has been proven effective
in finite-sum optimization. SAGA can also be implemented
in a distributed manner [28]â€“[30], and hence it fits well the
federated learning applications, where each device deals with
a finite number of data samples.
Our proposed novel Byzantine attack resilient distributed
(Byrd-) SAGA combines SAGAâ€™s variance reduction with
robust aggregation to deal with the malicious attacks in
federated finite-sum optimization setups. Instead of the mean
employed by distributed SAGA, the datacenter in ByrdSAGA relies on the geometric median to aggregate the
corrected stochastic gradients sent by distributed devices.
Through reducing the stochastic gradient-induced noise,
Byrd-SAGA turns out to outperform the Byzantine attack
resilient distributed SGD. When less than half of the workers
are Byzantine attackers, the robustness of geometric median
to outliers enables Byrd-SAGA to achieve provably linear
convergence to a neighborhood of the optimal solution, and
the asymptotic learning error is solely determined by the
number of Byzantine workers. Numerical tests demonstrate
the robustness of Byrd-SAGA to various Byzantine attacks.
II. P ROBLEM S TATEMENT
We start this section by specifying the federated finite-sum
optimization problem in the presence of Byzantine attacks.
We then elaborate on the limitations of Byzantine attack
resilient distributed SGD algorithms, which motivate our
subsequent development of Byrd-SAGA.
A. Federated finite-sum optimization in the presence of
Byzantine attacks
Consider a network with one master node (datacenter) and
W workers (devices), among which B workers are Byzantine
attackers with their identities unknown to the master node.
Let W be the set of all workers, and B that of Byzantine
attackers with respective cardinalities |W| = W and |B| =
B. The data samples are evenly distributed across the honest
workers w âˆˆ
/ B. Each honest worker has J data samples, and

fw,j (x) denotes the loss of the j-th data sample at the honest
worker w with respect to the model parameter x âˆˆ Rp . We
are interested in the finite-sum optimization problem
X
1
fw (x)
(1)
xâˆ— = arg min f (x) :=
x
W âˆ’B
wâˆˆB
/

where
fw (x) :=

J
1X
fw,j (x).
J j=1

(2)

The main challenge of solving (1) is that the Byzantine
attackers can collude and send arbitrary malicious messages
to the master node so as to bias the optimization process. We
aspire to develop a robust distributed stochastic algorithm to
address this issue. Intuitively, when a majority of workers
are Byzantine attackers, it is difficult to obtain a reasonable
approximate solution to (1). For this reason, we will assume
B< W
2 throughout, and prove that the proposed Byzantine
attack resilient algorithm is able to tolerate attacks from up
to half of the workers.
B. Sensitivity of distributed SGD to Byzantine attacks
When all workers are honest, a popular solver of (1) is
SGD [31]. At time slot (iteration) k, the master node broadcasts xk to workers. Upon receiving xk , worker w uniformly
at random chooses a local data sample with index ikw to obtain
0
k
the stochastic gradient fw,i
k (x ) that then communicates
w
back to the master node. Upon collecting stochastic gradients
from all workers, the master node updates the model as
xk+1 = xk âˆ’ Î³ k Â·

W
1 X 0
f k (xk )
W w=1 w,iw

(3)

where Î³ k is the non-negative step size. Note that the distributed SGD can be extended to its mini-batch version;
whereby, each worker uniformly at random chooses a minibatch of data samples per iteration, and communicates the
averaged stochastic gradient back to the master node.
While the honest workers send true stochastic gradients
to the master node, the Byzantine ones can send arbitrary
malicious messages to the master node in order to perturb
(bias) the optimization process. Let mkw denote the message
worker w sends to the master node at slot k, given by
(
0
k
fw,i
wâˆˆ
/ B,
k (x ),
k
w
mw =
(4)
âˆ—,
wâˆˆB
where âˆ— denotes an arbitrary p Ã— 1 vector. Then, the distributed SGD update (3) becomes
xk+1 = xk âˆ’ Î³ k Â·

W
1 X k
m .
W w=1 w

(5)

Even when only one Byzantine attacker is present, the
distributed SGD may fail. Consider that a Byzantine attacker

3

gradients with large variance

gradients with small variance

honest gradient
Byzantine gradient

geometric median
true gradient

Fig. 1. Impact of stochastic gradient noise on geometric median-based
robust aggregation. Blue dots denote stochastic gradients sent by the honest
workers. Red dots denote malicious messages sent by the Byzantine workers.
Plus signs denote the outputs of geometric median-based robust aggregation.
Pentagrams denote the means of the stochastic gradients sent by the honest
workers. Variance of the stochastic gradients from the honest workers is
large in left and small in right.

P
wb sends to the master node mkwb = âˆ’ w6=wb mkw , which
yields xk+1 = xk . In practice, Byzantine attackers can send
more sophisticated messages to fool the master node, and
thus bias the optimization process.
C. Byzantine attack resilient distributed SGD
Recent works often robustify the distributed SGD by
incorporating robust aggregation rules when the master node
receives messages from the workers. Here, we will adopt and
analyze the geometric median, even though alternative robust
aggregation rules are also viable [9], [10].
With {z, z âˆˆ Z} denoting a subset in a normed space, the
geometric median of {z, z âˆˆ Z} is
X
geomed{z} := arg min
ky âˆ’ zk.
(6)
y

zâˆˆZ

zâˆˆZ

Using (6), the distributed SGD in (5) can be modified to its
Byzantine attack resilient form as
x

k+1

k

k

=x âˆ’Î³ Â·

geomed{mkw }.
wâˆˆW

(7)

In essence, the geometric median chooses a reliable vector
to represent the received messages {mkw } through majority
voting. When the number of Byzantine workers B < W
2 , the
geometric median approximates reasonably well the mean of
{mkw , w âˆˆ
/ B}. This property enables the Byzantine attack
resilient distributed SGD to converge to a neighborhood of
the optimal solution [9], [10].
D. Impact of stochastic gradient noise on robust aggregation
In distributed SGD, the stochastic gradients evaluated by
honest workers are noisy because of the randomness in
choosing data samples. Due to the stochastic gradient noise
however, it is not always easy to distinguish the malicious

messages from the stochastic gradients using just the robust aggregation rules, e.g. the geometric median. Several
existing works have recognized this issue. With carefully
crafted Byzantine attacks, outputs of several Byzantine attack
resilient SGD algorithms can be far away from the optimal
solution [20]. In [10] and [18], the workers are divided into
several groups, with averages taken within groups and the geometric median obtained across groups. This approach leads
to reduced variance and thus enhanced ability to distinguish
malicious messages. In [14], it is explicitly assumed that the
ratio of the variance of stochastic gradients to the distance
between iterate and optimal solution is upper-bounded.
Fig. 1 shows the impact of stochastic gradient noise
on geometric median-based robust aggregation. When the
stochastic gradients sent by honest workers have small variance, the gap between the true mean and the aggregated value
is also small; that is, the same Byzantine attacks are less
effective. We will quantify this statement in our analysis of
Section IV-A.
Prompted by this observation, our key idea is to reduce
the variance of stochastic gradients in order to enhance
robustness to Byzantine attacks. In the Byzantine attack-free
case, an effective approach to alleviating stochastic gradient
noise in SGD is through variance reduction. By compensating
for stochastic gradient noise, variance reduction techniques
lead to faster convergence than SGD. For specificity, we will
focus on SAGA, which reduces stochastic gradient noise for
finite-sum optimization [24], and we will show how SAGA
can also aid robust aggregation against Byzantine attacks.
III. A LGORITHM D EVELOPMENT
In this section, we first introduce distributed SAGA with
mean aggregation. Then, we propose Byrd-SAGA, which
replaces mean aggregation by geometric median-based robust
aggregation.
A. Distributed SAGA with mean aggregation
In distributed SAGA, each worker maintains a table of
stochastic gradients for all of its local data samples [28],
[29]. As in distributed SGD, the master node at slot k
sends xk to the workers, and every worker w uniformly at
random chooses a local data sample with index ikw to find the
0
k
stochastic gradient fw,i
k (x ). However, worker w does not
w
0
k
send back fw,ik (x ) to the master node. Instead, it corrects
w
0
k
fw,i
k (x ) by first subtracting the previously stored stochastic
w
gradient of the ikw -th data sample, and then adding the average
of the stored stochastic gradients across local data samples.
Then, worker w sends such a corrected stochastic gradient
0
k
to the master node, and stores fw,i
k (x ) as the stochastic
w
gradient of the ikw -th data sample in the table. After collecting
the corrected stochastic gradients from all workers, the master
node updates the model xk+1 .

4

Fig. 2. Illustration of Byzantine attack resilient distributed SAGA. For
the ease of illustration, the honest workers are from 1 to W âˆ’ B while
the Byzantine attackers are from W âˆ’ B + 1 to W . But in practice, the
identities of Byzantine attackers are unknown to the master node.

To better describe distributed SAGA, let
(
Ï†kw,j ,
j 6= ikw
k+1
Ï†w,j =
xk ,
j = ikw

(8)

0
where Ï†k+1
w,j is the iterate at which the most recent fw,j is
0
evaluated when slot k ends. Then, fw,j
(Ï†kw,j ) refers to the
previously stored stochastic gradient of the j-th data sample
prior to slot k on worker w, and
k
0
k
0
k
gw
:= fw,i
k (x ) âˆ’ fw,ik (Ï†w,ik ) +
w
w
w

J
1X 0
f (Ï†k )
J j=1 w,j w,j

is the corrected stochastic gradient of worker i at slot k. The
model update of SAGA is hence
xk+1 = xk âˆ’ Î³ Â·

W
1 X k
g
W w=1 w

(9)

where Î³ > 0 is the constant step size.
B. Distributed SAGA with geometric median aggregation
Here, it is useful to recall that Byzantine workers may
send to the master node malicious messages, other than the
corrected stochastic gradient. To account for this, the message
sent from worker w to the master node at slot k is expressed
as
(
k
gw
, wâˆˆ
/ B,
k
mw =
(10)
âˆ—,
wâˆˆB
where âˆ— denotes an arbitrary p Ã— 1 vector. Similar to distributed SGD, distributed SAGA is also sensitive to Byzantine
attacks. Our robust aggregation rule here is the geometric
median. This leads to the proposed Byzantine attack resilient
distributed (Byrd) form of SAGA in (9), that is given by
xk+1 = xk âˆ’ Î³ Â· geomed{mkw }.
wâˆˆW

(11)

Algorithm 1 Byzantine Attack Resilient Distributed SAGA
Require: step size Î³; number of workers W ; number of data
samples J on every honest worker w
Master node and honest workers initialize x0
for all honest worker w do
for j âˆˆ {1, . . . , J} do
0
0
Initializes gradient storage fw,j
(Ï†w,j ) = fw,j
(x0 )
end for
PJ
0
1
(x0 )
Initializes average gradient gÌ„w
= J1 j=1 fw,j
1
Sends gÌ„w to master node
end for
1
Master node updates x1 = x0 âˆ’ Î³ Â· geomedwâˆˆW {gÌ„w
}
for all k = 1, 2, Â· Â· Â· do
Master node broadcasts xk to all workers
for all honest worker node w do
Samples ikw from {1, Â· Â· Â· , J} uniformly at random
0
k
0
k
Updates mkw = fw,i
k (x ) âˆ’ fw,ik (Ï†w,ik ) + gÌ„w
w
w
w
Sends mkw to master node
0
k
0
k
k+1
+ J1 (fw,i
= gÌ„w
Updates gÌ„w
k (x ) âˆ’ fw,ik (Ï†w,ik ))
w
w
w
0
0
Stores gradient fw,ik (Ï†w,ikw ) = fw,ik (xk )
w
w
end for
k+1
k
Master node updates x
= x âˆ’Î³ Â·geomedwâˆˆW {mkw }
end for

The proposed Byzantine attack resilient distributed SAGA,
abbreviated as Byrd-SAGA, is listed step-by-step under
Algorithm 1, and illustrated in Fig. 2. There are various
implementations of the distributed SAGA. For example, [29]
proposed to store the tables of stochastic gradients in the
master node. The workers only need to upload the stochastic
gradients and their indexes, while the master node performs
the aggregation. This setup is also vulnerable to Byzantine
attacks, since the Byzantine attackers may upload incorrect
stochastic gradients. The proposed robust aggregation rule
can also be applied therein.
Robust aggregations other than the geometric median are
available, including the median [11], Krum [14], marginal
trimmed mean [12], and iterative filtering [13]. In the median
for instance, the aggregation outputs the element-wise median
of {mkw }; while in the Krum, the aggregation outputs
X
Krum{mkw } = mwâˆ— , wâˆ— = arg min
kmkw âˆ’ mkw0 k2
wâˆˆW

wâˆˆW

wâ†’w0

where w â†’ w0 (w 6= w0 ) selects the indexes w0 of the
W âˆ’ B âˆ’ 2 nearest neighbors of mkw in {mkw0 }. Note that
Krum needs to know B, the number of Byzantine attackers,
in advance. In addition, other variance reduction techniques,
such as mini-batch [21], SAG [22], SVRG [23], SAGA
[24], SDCA [25], SARAH [26] and Katyusha [27], are also
available to alleviate the gradient noise. Here we opted for the
combination of geometric median and SAGA. Extending the
current work to other robust aggregation rules and variance
reduction techniques, is in our future research agenda.

5

Remark 1. Computing the geometric median involves solving an optimization problem in the form of (6). Since it is
costly to obtain the exact geometric median, one is typically
satisfied with an -approximate value [32]. We say that zâˆ— is
an -approximate geometric median of {z, z âˆˆ Z} if
X
X
kzâˆ— âˆ’ zk â‰¤ inf
ky âˆ’ zk + .
(12)
y

zâˆˆZ

zâˆˆZ

We shall show that the -approximation only slightly affects
the convergence of Byrd-SAGA.
IV. T HEORETICAL A NALYSIS
In this section, we theoretically justify the intuitive idea
that reducing stochastic gradient noise helps identify malicious messages in robust aggregation, specifically to the
geometric median in this paper. We prove that our ByrdSAGA converges to a neighborhood of the optimal solution
at a linear rate under Byzantine attacks, and the asymptotic
learning error is determined by the number of Byzantine
attackers. Due to the page limit, proofs are delegated to the
full version of this paper1 .
A. Importance of reducing stochastic gradient noise
Here, we quantify the role of stochastic gradient noise on
the geometric median aggregation. Towards this objective,
consider the set of messages Z sent by all workers in W,
and the set Z 0 of malicious messages sent by the Byzantine
attackers in B. Further, let zÌ„ denote the true gradient given
by the ensemble average of stochastic gradients. Using these
definitions, the ensuing lemma bounds the mean-square error
of the geometric median relative to the true gradient.
Lemma 1. (Concentration property) Let {z, z âˆˆ Z} be a
subset of random vectors distributed in a normed vector
space. If Z 0 âŠ† Z and |Z 0 | < |Z|
2 , then it holds that
2

Ekgeomed{z} âˆ’ zÌ„k
(13)
zâˆˆZ
P
P
2
2
/ 0 Ekz âˆ’ Ezk
z âˆˆZ
/ 0 kEz âˆ’ zÌ„k
2
â‰¤CÎ±2 zâˆˆZ
+
C
Î±
0
0
|Z| âˆ’ |Z |
|Z| âˆ’ |Z |
where
P
zÌ„ :=
while CÎ± :=

2âˆ’2Î±
1âˆ’2Î± ,

z âˆˆZ
/ 0 Ez
|Z| âˆ’ |Z 0 |

and Î± :=

|Z 0 |
|Z| .

The left-hand side of (13) is the mean-square error of
the geometric median relative to the true gradient, while
the right-hand side is the sum of two terms. The first is
determined by the variances of the local stochastic gradients
sent by the honest workers (inner variation), while the second
term is determined by the variations of the local gradients at
the honest workers with respect to the true gradient (outer
1 https://github.com/MrFive5555/Byrd-SAGA/blob/master/Full.pdf

variation). In the Byzantine attack resilient SGD, the upper
bound can be large due to the large stochastic gradient noise
of SGD. Through reducing the stochastic gradient noise in
terms of either inner variation or outer variation, we are able
to attain improved accuracy under malicious attacks.
B. Convergence of Byrd-SAGA and comparison with Byzantine attack resilient SGD
Here, we establish convergence of Byrd-SAGA, and theoretically justify that, through reducing the impact of inner
variation, Byrd-SAGA enjoys superior robustness to Byzantine attacks. We begin with several needed assumptions on
the functions {fw,j }.
Assumption 1. (Strong convexity and Lipschitz continiuty of
gradients) The function f is Âµ-strongly convex and has LLipschitz continuous gradients, which amounts to requiring
that for any x, y âˆˆ Rp , it holds that
Âµ
(14)
f (x) â‰¥ f (y) + hf 0 (y), x âˆ’ yi + kx âˆ’ yk2
2
and
kf 0 (x) âˆ’ f 0 (y)k â‰¤ Lkx âˆ’ yk.

(15)

Assumption 2. (Bounded outer variation) For any x âˆˆ Rp ,
variation of the aggregated gradients at the honest workers
with respect to the overall gradient is upper-bounded by
0
0
2
2
EwâˆˆB
/ kfw (x) âˆ’ f (x)k â‰¤ Î´ .

(16)

Assumption 3. (Bounded inner variation) For every honest
worker w and any x âˆˆ Rp , the variation of its stochastic
gradients with respect to its aggregated gradient is upperbounded by
0
0
2
2
Eikw kfw,i
k (x) âˆ’ fw (x)k â‰¤ Ïƒ ,
w

âˆ€w âˆˆ
/ B.

(17)

Assumption 1 is standard in convex analysis. Assumptions
2 and 3 bound the variation of gradients and the variation of
stochastic gradients within the honest workers, respectively
[33]. For instance, most of the existing Byzantine attack
resilient SGD algorihtms assume that the stochastic gradients
at the honest workers are independently and identically
distributed (i.i.d.) with finite variance, such that the outer
variation Î´ 2 in Assumption 2 is proportional to 1/J and the
inner variation Ïƒ 2 in Assumption 3 is finite. In the analysis
of Byzantine attack resilient SGD, both outer and inner
variations must be bounded. Interestingly, inner variation will
turn out not to impact Byrd-SAGA, and Assumption 3 will
no longer be necessary in its analysis.
To simplify notation, we will henceforth use E to represent
the expectation with respect to all random variables ikw .
The presence of geometric median makes Byrd-SAGA
analysis challenging. Specifically, for every honest worker
wâˆˆ
/ B, mki is an unbiased estimate of fw0 (xk ), meaning
Emki = fw0 (xk ).

(18)

6

Averaging (18) over all honest workers w âˆˆ
/ B, we have
X
X
1
1
Emki =
fw0 (xk ) = f 0 (xk ). (19)
W âˆ’B
W âˆ’B
wâˆˆB
/

wâˆˆB
/

From (19), we observe that the mean of {mki } over all the
honest workers w âˆˆ
/ B is an unbiased estimate of f 0 (xk ).
Nevertheless, the geometric median of {mki }, even only over
all the honest workers w âˆˆ
/ B and calculated accurately, is
a biased estimate of f 0 (xk ). This is the main challenge in
adapting the proof of SAGA to that of Byrd-SAGA.
The following theorem asserts that Byrd-SAGA converges
to a neighborhood of the optimal solution xâˆ— at a linear rate,
with the asymptotic learning error determined by the number
of Byzantine attackers.
Theorem 1. Under Assumptions 1 and 2, if the number
of Byzantine attackers satisfies B < W
2 and the step size
satisfies
Âµ
Î³â‰¤ âˆš
4 5J 2 CÎ± L2
then for Byrd-SAGA with -approximate geometric median
aggregation, it holds that
Î³Âµ k
) âˆ†1 + âˆ† 2
Ekxk âˆ’ xâˆ— k2 â‰¤ (1 âˆ’
2

(20)

âˆ†1 := kx0 âˆ’ xâˆ— k2 âˆ’ âˆ†2

(21)

where

âˆ†2 :=

10
Âµ2


CÎ±2 Î´ 2 +

2
(W âˆ’ 2B)2


.

(22)

Theorem 2. Under Assumptions 1, 2 and 3, if the number
of Byzantine attackers is B < W
2 and the step size satisfies
Âµ
Î³<
2L2
then for Byzantine attack resilient SGD with -approximate
geometric median aggregation, it holds that

which is close to 1 when J (the number of data samples at
each worker) and L
Âµ (the condition number of functions) are
large. Observe that CÎ± is monotonically increasing when the
portion of Byzantine attackers Î± increases. Therefore, (20)
shows that Byrd-SAGA converges slower as the number of
Byzantine attackers grows. Correspondingly, the theoretical
upper bound of step size Î³ is small when J and CÎ± are
large. The asymptotic learning error âˆ†2 in (22) is also
monotonically increasing when CÎ± (and hence the number
of Byzantine attackers) increases.
To demonstrate the superior robustness of Byrd-SAGA, we
also establish the convergence of Byzantine attack resilient
SGD with constant step size as a benchmark. As in Theorem
1, the convergence of Byzantine attack resilient SGD is in the
mean-square error sense. This is different from [10], where
convergence is asserted in the high probability sense.

(23)

âˆ†01 := kx0 âˆ’ xâˆ— k2 âˆ’ âˆ†02

(24)

where

âˆ†02 :=

4
Âµ2



CÎ±2 Ïƒ 2 + CÎ±2 Î´ 2 +

2
(W âˆ’ 2B)2


.

(25)

.
Let us ignore the approximation error in computing geometric median by setting  = 0, and compare the two
asymptotic learning errors âˆ†2 and âˆ†02 . Therefore, we deduce
that
 2

 2 
CÎ± 2
CÎ± 2
0
2
Î´
and
âˆ†
=
O
(Ïƒ
+
Î´
)
.
âˆ†2 = O
2
Âµ2
Âµ2
Observe that âˆ†02 , the asymptotic learning error of Byzantine
attack resilient SGD, is proportional to the sum of inner
and outer variations. With all honest workers having the
same data sample, we have Ïƒ 2 = Î´ 2 = 0. In this case, the
asymptotic learning error âˆ†02 vanishes because the geometric
median aggregation takes effect and attains the true gradient.
However, when each honest worker has the same set of
distinct data samples, the inner variation Ïƒ 2 is no longer
zero and the asymptotic learning error âˆ†02 can be large. In
contrast, Byrd-SAGA effectively reduces the impact of inner
variation, and is able to achieve smaller learning error.

In (20), the constant of convergence rate is given by
Î³Âµ
1
1âˆ’
â‰¥1âˆ’ âˆš
2
2
4 5J 2 CÎ± L
Âµ2

Ekxk âˆ’ xâˆ— k2 â‰¤ (1 âˆ’ Î³Âµ)k âˆ†01 + âˆ†02

V. N UMERICAL E XPERIMENTS
Here we present numerical experiments on convex and
nonconvex learning problems2 . For each problem, we evenly
distribute the dataset into W âˆ’ B = 50 honest workers
unless indicated otherwise. To account for malicious attacks,
we additionally launch B = 20 Byzantine workers. We
test the performance of the proposed Byrd-SAGA under
three typical Byzantine attacks: Gaussian, sign-flipping and
zero-gradient attacks [15], [34]. For a Gaussian attack, a
k
Byzantine attacker w âˆˆ B draws
P its mkw from a Gaussian
1
distribution with mean W âˆ’B w0 âˆˆB
/ mw0 and variance 30.
For a sign-flipping attack, a Byzantine
w âˆˆ B sets
P attacker
1
k
its message as mkw = u Â· W âˆ’B
w0 âˆˆB
/ mw0 , where the
magnitude u = âˆ’3 is used in the numerical experiments.
And for a zero-gradient
a Byzantine attacker w âˆˆ B
P attack,
k
sends mkw = âˆ’ B1 w0 âˆˆB
/ mw0 so that the messages at the
master sum up to zero. We use the algorithm in [32] to obtain
the -approximate geometric median with  = 1 Ã— 10âˆ’5 .
2 The

codes are available at https://github.com/MrFive5555/Byrd-SAGA

0

2
4
6
8
iteration k / 4000

101

f( x k ) âˆ’ f( x * )

102

10âˆ’1
10âˆ’2
10âˆ’3
0

variance (without attack)

10âˆ’1
0

2
4
6
8 10
iteration k / 4000
(ariance (sign-flipping attack)

Dwâˆ‰B[mwk ]

Dwâˆ‰B[mwk ]

Dwâˆ‰B[mwk ]

10âˆ’2

10âˆ’6

10âˆ’3
2

4
6
8
iteration k / 4000

0

10

2

4
6
8
iteration k / 4000

SGD ean
SGD geo ed

10âˆ’1

10âˆ’2

2
4
6
8 10
iteration k / 4000
variance (Gaussian attack)

10âˆ’1

0

100

100

100

10âˆ’1

10âˆ’1

10âˆ’2
10âˆ’3

10

0

100

10âˆ’4

10âˆ’5

optimalit* gap (zero-gradient attack)

10âˆ’2

10âˆ’4

10

opti alit* gap (sign-flipping attack)

f( x k ) âˆ’ f( x * )

opti alit* gap (Gaussian attack)
100

Dwâˆ‰B[mwk ]

opti alit* gap ()ithout attack)

100
10âˆ’1
10âˆ’2
10âˆ’3
10âˆ’4
10âˆ’5
10âˆ’6
10âˆ’7

f( x k ) âˆ’ f( x * )

f( x k ) âˆ’ f( x * )

7

0

2

BSGD ean
BSGD geo ed

4
6
8
iteration k / 4000

2
4
6
8
iteration k / 4000

10

variance (zero-gradient attack)

10âˆ’2

10âˆ’3

10

0

2

4
6
8
iteration k / 4000

10

SAGA ean
SAGA geo ed

Fig. 3. Performance of the distributed SGD, mini-batch (B)SGD and SAGA, with mean and geometric median (geomed) aggregation rules on IJCNN1
dataset. The step sizes are 0.02, 0.01 and 0.02, respectively. SAGA geomed stands for the proposed Byrd-SAGA. From top to bottom: optimality gap and
variance of honest messages. From left to right: without attack, Gaussian attack, sign-flipping attack, and zero-gradient attack.

ptimalit( gap (with ut attack)

100

ptimalit( gap (Gaussian attack)

100

102

ptimalit( gap (sign-flipping attack)

optimalit( gap (zero-gradient attack)

10âˆ’3

10âˆ’1

10âˆ’2

10âˆ’4
0

2
4
6
8
iteration k / 4000

0

10

variance (with ut attack)

2
4
6
8
iterati n k / 4000

f( x k ) âˆ’ f( x * )

10âˆ’2

f( x k ) âˆ’ f( x * )

f( x k ) âˆ’ f( x * )

f( x k ) âˆ’ f( x * )

10âˆ’1
101
100

10

0

variance (Gaussian attack)

2
4
6
8
iterati n k / 4000

10

2
4
6
8
iteration k / 4000

10

variance (zer -gradient attack)
2 Ã— 100

100

Dwâˆ‰B[mwk ]

Dwâˆ‰B[mwk ]

Dwâˆ‰B[mwk ]

Dwâˆ‰B[mwk ]

100

6 Ã— 10âˆ’1

0

2

4
6
8
iteration k / 4000

10

10âˆ’1

3 Ã— 10âˆ’1

0

2

4
6
8
iterati n k / 4000

SGD mean
SGD ge med

10

BSGD mean
BSGD ge med

100
6 Ã— 10âˆ’1

4 Ã— 10âˆ’1

10âˆ’1

0

variance (sign-flipping attack)
2 Ã— 100

100

100

4 Ã— 10âˆ’1

0

2

4
6
8
iterati n k / 4000

10

0

2

4
6
8
iteration k / 4000

10

SAGA mean
SAGA ge med

Fig. 4. Performance of the distributed SGD, mini-batch (B)SGD and SAGA, with mean and geometric median (geomed) aggregation rules on COVTYPE
dataset. The step sizes are 0.01, 0.005 and 0.01, respectively. SAGA geomed stands for the proposed Byrd-SAGA. From top to bottom: optimality gap
and variance of honest messages. From left to right: without attack, Gaussian attack, sign-flipping attack, and zero-gradient attack.

8

A. `2 -regularized logistic regression
Consider the `2 -regularized logistic regression cost, where
each summand fw,j (x) is given by
Ï
fw,i (x) = ln (1 + exp (âˆ’bw,i haw,i , xi)) + ||x||2
2
with aw,j âˆˆ Rp being the feature vector, bw,j âˆˆ {âˆ’1, 1}
the label, and Ï = 0.01 a constant. We use the IJCNN1 and
COVTYPE datasets3 . IJCNN1 contains 49,990 training data
samples of p = 22 dimensions. COVTYPE contains 581,012
training data samples of p = 54 dimensions.
We first compare SGD, mini-batch (B)SGD with batch size
50 and SAGA, using mean and geometric median aggregation
rules. Compared to SGD, BSGD enjoys smaller stochastic
gradient noise, but incurs higher computational cost. In
comparison, SAGA also reduces stochastic gradient noise,
but its computational cost is in the same order as that of SGD.
For each algorithm, we adopt a constant step size, which is
tuned to achieve the best optimality gap f (xk )âˆ’f (xâˆ— ) in the
Byzantine-free scenario. The performance of these algorithms
on the IJCNN1 and COVTYPE datasets is depicted in Fig. 3
and Fig. 4, respectively. With Byzantine attacks, all three
algorithms using mean aggregation fail. Among the three
using geometric median aggregation, Byrd-SAGA markedly
outperforms the other two, while BSGD is better than SGD.
This demonstrates the importance of variance reduction to
handling Byzantine attacks. Regarding the variance of honest messages in particular, Byrd-SAGA, Byzantine attack
resilient BSGD and Byzantine attack resilient SGD are in
the order of 10âˆ’3 , 10âˆ’2 and 10âˆ’1 , respectively, for the
IJCNN1 dataset. For the COVTYPE dataset, Byrd-SAGA
and Byzantine attack resilient BSGD have the same order of
variance with respect to honest messages. In this case, ByrdSAGA achieves similar optimality gap as Byzantine attack
resilient BSGD, but converges faster because it is able to use
a larger step size.
Theorem 1 establishes that when the outer variation Î´ 2 =
0, the asymptotic learning error of Byrd-SAGA is zero, no
matter how large the inner variation Ïƒ 2 is. In contrast, according to Theorem 2, the asymptotic learning error of Byzantine
attack resilient SGD is still proportional to the inner variation
Ïƒ 2 . To validate these theoretical results, we conducted a
second set of numerical experiments, where every honest
worker has the whole IJCNN1 dataset. Therefore, Î´ 2 = 0 and
Ïƒ 2 remains the same as that in the first set of experiments.
We compare SGD, BSGD with batch size 50 and SAGA,
all using the geometric median aggregation rule. The results
depicted in Fig. 5 corroborate the theoretical findings â€“ the
asymptotic learning error of Byrd-SAGA vanishes, while
those of Byzantine attack resilient SGD and BSGD are the
same as those shown in Fig. 3.
In the third set of numerical experiments, we compare the
use of different aggregation rules in distributed SAGA: mean,
3 https://www.csie.ntu.edu.tw/âˆ¼cjlin/libsvmtools/datasets

TABLE I
ACCURACY OF SGD, MINI - BATCH (B)SGD AND SAGA, WITH MEAN
AND GEOMETRIC MEDIAN ( GEOMED ) AGGREGATION RULES . SAGA
GEOMED STANDS FOR THE PROPOSED B YRD -SAGA.
attack
without

Gaussian

sign-flipping

zero-gradient

algorithm
SGD
BSGD
SAGA
SGD
BSGD
SAGA
SGD
BSGD
SAGA
SGD
BSGD
SAGA

mean acc (%)
97.0
98.6
96.5
36.3
36.3
14.5
0.11
0.16
0.12
9.94
9.89
9.88

geomed acc (%)
92.3
98.0
96.3
92.5
98.0
96.4
0.03
90.3
86.4
26.2
81.5
92.4

geometric median, median, and Krum. As shown in Fig.
6, distributed SAGA using mean aggregation is the best in
terms of the optimality gap f (xk ) âˆ’ f (xâˆ— ) when there are no
Byzantine attacks. However, it fails under all kinds of attacks.
With Gaussian attacks, Byrd-SAGA using geometric median
achieves the best performance. With sign-flipping and zerogradient attacks, Byrd-SAGA using Krum is the best, while
that using geometric median also performs well. Note that
Krum has to know the exact number of Byzantine attackers
in advance, while geometric median and median do not need
this prior knowledge.
B. Neural network training
Here we test training a neural network with one hidden
layer of 50 neurons and â€œtanhâ€ activation function, for
multi-class classification on the MNIST dataset4 comprising
60, 000 data samples, each with dimension p = 784. We
compare SGD with step size 0.1, BSGD with step size 0.5
and batch size 50, and SAGA with step size 0.1. We run the
algorithms for 15, 000 iterations, and report the final accuracy
in Table 1. With mean aggregation, all algorithms yield low
accuracy in the presence of Byzantine attacks. With the help
of geometric median aggregation, BSGD and SAGA are both
robust and outperform SGD. Note that Byrd-SAGA exhibits
a much lower per-iteration computational cost relative to
Byzantine attack resilient BSGD.
VI. C ONCLUSIONS
The present paper developed a novel Byzantine attack
resilient distributed (Byrd-) SAGA approach to federated
finite-sum optimization in the presence of Byzantine attacks.
On par with SAGA, Byrd-SAGA corrects stochastic gradients
through variance reduction. Per iteration, distributed workers
obtain their corrected stochastic gradients before uploading
to the master node. Different from SAGA though, the master
node in Byrd-SAGA aggregates the received messages using
4 http://yann.lecun.com/exdb/mnist

9

optima ity gap (Ga(ssian attack)

10âˆ’3

10âˆ’3

10âˆ’5
10âˆ’7
10âˆ’9

10âˆ’1

10âˆ’5
10âˆ’7

2
4
6
8
iteration k / 200000

10

0

variance (witho(t attack)

10âˆ’3
10âˆ’5

2
4
6
8
iteration k / 200000

10âˆ’3
10âˆ’5
10âˆ’7

10

0

)ariance (Ga(ssian attack)

2
4
6
8
iteration k / 200000

10

0

)ariance (sign-flipping attack)
10âˆ’1

10âˆ’3

10âˆ’3

10âˆ’3

10âˆ’3

10âˆ’5

10âˆ’5
10âˆ’7

10âˆ’9

10âˆ’9

10âˆ’11

10âˆ’11
0

2
4
6
8
iteration k / 200000

10âˆ’5
10âˆ’7
10âˆ’9

0

10

Dwâˆ’B[mwk ]

10âˆ’1

Dwâˆ’B[mwk ]

10âˆ’1

10âˆ’7

2
4
6
8
iteration k / 200000
SGD geomed

10

2
4
6
8
iteration k / 200000

10

)ariance (zero-gradient attack)

10âˆ’1

Dwâˆ’B[mwk ]

Dwâˆ’B[mwk ]

10âˆ’1

10âˆ’7

10âˆ’9
0

optima ity gap (zero-gradient attack)

f( x k ) âˆ’ f( x * )

10âˆ’1

optima ity gap (sign-flipping attack)

f( x k ) âˆ’ f( x * )

10âˆ’1
f( x k ) âˆ’ f( x * )

f( x k ) âˆ’ f( x * )

optima ity gap (*itho(t attack)

10âˆ’5
10âˆ’7
10âˆ’9

0

BSGD geomed

2
4
6
8
iteration k / 200000

10

0

2
4
6
8
iteration k / 200000

10

SAGA geomed

Fig. 5. Performance of the distributed SGD, mini-batch (B)SGD and SAGA, with geometric median (geomed) aggregation rule. Every honest worker has
the whole IJCNN1 dataset. The step sizes are 0.0004, 0.0002 and 0.0004, respectively. SAGA geomed stands for the proposed Byrd-SAGA. From top to
bottom: optimality gap and variance of honest messages. From left to right: without attack, Gaussian attack, sign-flipping attack, and zero-gradient attack.

Ga*(( an a))ack (IJCNN1)

101

10âˆ’8
0

2
4
6
8
iteration k / 4000

w )ho*) a))ack (COVTYPE)

100

100
10âˆ’1

10âˆ’2

10âˆ’2
0

2

4
6
8
)era) on k / 4000

10

0

Ga*(( an a))ack (COVTYPE)

100

102

10âˆ’1

f x

k

10âˆ’3
10âˆ’4

10

f x

k

10âˆ’6

10âˆ’2

zero-gradient attack (IJCNN1)
100
( k ) âˆ’ f( x * )

10âˆ’1

10âˆ’4

( ) âˆ’ f( x * )

10âˆ’2

( ) âˆ’ f( x * )

102

f x

f x

( gn-flipping attack (IJCNN1)

100

k

( ) âˆ’ f( x * )

w )ho*) a))ack (IJCNN1)
100

2

4
6
8
)era) on k / 4000

10

0

( gn-flipping attack (COVTYPE)

2
4
6
8
iteration k / 4000

10

zero-gradient attack (COVTYPE)

10âˆ’2

10âˆ’4
0

2
4
6
8
iteration k / 4000

10

101

( k ) âˆ’ f( x * )

k

10âˆ’3

f x

k

10âˆ’1

100

f x

( ) âˆ’ f( x * )

( ) âˆ’ f( x * )

10âˆ’2

f x

f x

k

( ) âˆ’ f( x * )

10âˆ’1

10âˆ’1

10âˆ’1
0

2

mean

4
6
8
)era) on k / 4000

10

geome)r c med an

0
Kr*m

2

4
6
8
)era) on k / 4000

10

0

2
4
6
8
iteration k / 4000

10

med an

Fig. 6. Optimality gaps of distributed SAGA with different aggregation rules: mean, geometric median, median and Krum. The step sizes are 0.02 and
0.01 for the IJCNN1 and COVTYPE datasets, respectively. Curves of geometric median correspond to the proposed Byrd-SAGA. From top to bottom: on
IJCNN1 dataset and on COVTYPE dataset. From left to right: without attacks, with Gaussian attacks, with sign-flipping attacks, and with zero-gradient
attacks.

10

the geometric median rather than the mean. This robust
aggregation markedly enhances robustness of Byrd-SAGA
in the presence of Byzantine attacks. It was established
that Byrd-SAGA converges linearly to a neighborhood of
the optimal solution, with the asymptotic learning error
determined solely by the number of Byzantine workers.
As confirmed by numerical tests, combinations with other
robust aggregation rules also exhibit satisfactory robustness.
Our future research agenda includes their analysis, as well
as the development and analysis of Byzantine attack resilient
algorithms over fully decentralized networks [35], [36].
R EFERENCES
[1] R. Agrawal and R. Srikant, â€œPrivacy-preserving data mining,â€ Proceedings of SIGMOD, Pordland, Oregan, USA, May 2000.
[2] J. Duchi, M. J. Wainwright, and M. I. Jordan, â€œLocal privacy and
minimax bounds: Sharp rates for probability estimation,â€ Proceedings
of NIPS, Stateline, Nevada, USA, Dec. 2013.
[3] L. Zhou, K. Yeh, G. Hancke, Z. Liu, and C. Su, â€œSecurity and privacy
for the industrial Internet of Things: An overview of approaches to
safeguard endpoints,â€ IEEE Signal Processing Magazine, vol. 35, no.
5, pp. 76â€“87, Sep. 2018.
[4] J. Konecny, H. B. McMahan, D. Ramage, and P. Richtarik, â€œFederated
optimization: Distributed machine learning for on-device intelligence,â€
arXiv Preprint arXiv:1610.02527, Oct. 2016.
[5] A. Vempaty, L. Tong, and P. K. Varshney, â€œDistributed inference with
Byzantine data: State-of-the-art review on data falsification attacks,â€
IEEE Signal Processing Magazine, vol. 30, no. 5, pp. 65â€“75, Aug.
2013.
[6] Y. Chen, S. Kar, and J. M. F. Moura, â€œThe Internet of Things: Secure
distributed inference,â€ IEEE Signal Processing Magazine, vol. 35, no.
5, pp. 64â€“75, Sep. 2018.
[7] Z. Yang, A. Gang, and W. U. Bajwa, â€œAdversary-resilient inference and
machine learning: From distributed to decentralized,â€ arXiv Preprint
arXiv:1908.08649, Aug. 2019.
[8] L. Lamport, R. Shostak, and M. Pease, â€œThe Byzantine generals problem,â€ ACM Transactions on Programming Languages and Systems,
vol. 4, no. 3, pp. 382â€“401, Jul. 1982.
[9] S. Minsker, â€œGeometric median and robust estimation in Banach
spaces,â€ Bernoulli, vol. 21, no. 4, pp. 2308-2335, Nov. 2015.
[10] Y. Chen, L. Su, and J. Xu, â€œDistributed statistical machine learning
in adversarial settings: Byzantine gradient descent,â€ Proceedings of
SIGMETRICS, Phonenix, Arizona, USA, Jun. 2019.
[11] C. Xie, O. Koyejo, and I. Gupta, â€œGeneralized Byzantine-tolerant
SGD,â€ arXiv Preprint arXiv:1802.10116, Feb. 2018.
[12] D. Yin, Y. Chen, K. Ramchandran, and P. Bartlett, â€œByzantine-robust
distributed learning: Towards optimal statistical rates,â€ Proceedings of
ICML, Stockholm, Sweden, Jul. 2018.
[13] L. Su and J. Xu, â€œSecuring distributed machine learning in high
dimensions,â€ arxiv Preprint arXiv:1804.10140, Apr. 2018.
[14] P. Blanchard, E. M. El Mhamdi, R. Guerraoui, and J. Stainer, â€œMachine learning with adversaries: Byzantine tolerant gradient descent,â€
Proceedings of NIPS, Long Beach, California, USA, Dec. 2017.
[15] L. Li, W. Xu, T. Chen, G. B. Giannakis, and Q. Ling, â€œRSA: Byzantinerobust stochastic aggregation methods for distributed learning from
heterogeneous datasets,â€ Proceedings of AAAI, Honolulu, Hawaii,
USA, Jan. 2019.
[16] J. Feng, H. Xu, and S. Mannor, â€œDistributed robust learning,â€ arXiv
Preprint arXiv:1409.5937, Sep. 2014.
[17] D. Yin, Y. Chen, K. Ramchandran, and P. Bartlett, â€œDefending against
saddle point attack in Byzantine-robust distributed learning,â€ arXiv
Preprint arXiv:1806.05358, Jun. 2018.
[18] L. Chen, H. Wang, Z. Charles, and D. Papailiopoulos, â€œDRACO:
Byzantine-resilient distributed training via redundant gradients,â€ arXiv
Preprint arXiv:1803.09877, Mar. 2018.

[19] S. Rajput, H. Wang, Z. Charles, and D. Papailiopoulos, â€œDETOX:
A redundancy-based framework for faster and more robust gradient
aggregation,â€ arXiv Preprint arXiv:1907.12205, Jul. 2019.
[20] C. Xie, O. Koyejo, and I. Gupta, â€œFall of empires: Breaking
Byzantine-tolerant SGD by inner product manipulation,â€ arXiv Preprint
arXiv:1903.03936, Mar. 2019.
[21] P. Goyal, P. Dollar, R. Girshick, P. Noordhuis, L. Wesolowski, A.
Kyrola, A. Tulloch, Y. Jia, and K. He, â€œAccurate, large minibatch SGD:
Training imagenet in 1 hour,â€ arXiv Preprint arXiv:1706.02677, Jun.
2017.
[22] M. W. Schmidt, N. Le Roux, and F. R. Bach, â€œMinimizing finite sums
with the stochastic average gradient,â€ Mathematical Programming, vol.
162, no. 1â€“2, pp. 83â€“112, Mar. 2017.
[23] R. Johnson and T. Zhang, â€œAccelerating stochastic gradient descent
using predictive variance reduction,â€ Proceedings of NIPS, Stateline,
Nevada, USA, Dec. 2013.
[24] A. Defazio, F. R. Bach, and S. Lacoste-Julien, â€œSAGA: A fast
incremental gradient method with support for non-strongly convex
composite objectives,â€ Proceedings of NIPS, Montreal, Canada, Dec.
2014.
[25] S. Shalev-Shwartz and T. Zhang, â€œStochastic dual coordinate ascent
methods for regularized loss minimization,â€ Journal of Machine Learning Research, vol. 14, no. 2, pp. 567â€“599, Feb. 2013.
[26] L. M. Nguyen, J. Liu, K. Scheinberg, and M. Takac, â€œSARAH: A
novel method for machine learning problems using stochastic recursive
gradient,â€ Proceedings of ICML, Sydney, Australia, Aug. 2017.
[27] Z. Allen-Zhu, â€œKatyusha: The first direct acceleration of stochastic
gradient methods,â€ Journal of Machine Learning Research, vol. 18,
no. 1, pp. 8194â€“8244, Jun. 2017.
[28] C. Calauzenes and N. Le Roux, â€œDistributed SAGA: Maintaining
linear convergence rate with limited communication,â€ arXiv Preprint
arXiv:1705.10405, May 2017.
[29] S. De and T. Goldstein, â€œEfficient distributed SGD with variance
reduction,â€ Proceedings of ICDM, Barcelona, Spain, Dec. 2016.
[30] S. J. Reddi, A. Hefny, S. Sra, B. Poczos, and A. J. Smola, â€œOn variance
reduction in stochastic gradient descent and its asynchronous variants,â€
Proceedings of NIPS, Barcelona, Spain, Dec. 2015.
[31] L. Bottou, â€œLarge-scale machine learning with stochastic gradient
descent,â€ Proceedings of COMPSTAT, Paris, France, Aug. 2010.
[32] E. Weiszfeld and F. Plastria, â€œOn the point for which the sum of
the distances to n given points is minimum,â€ Annals of Operations
Research, vol. 167, no. 1, pp. 7â€“41, Mar. 2009.
[33] H. Tang, X. Lian, M. Yan, C. Zhang, and J. Liu, â€œD2: Decentralized
training over decentralized data,â€ Proceedings of ICML, Stockholm,
Sweden, Jul. 2018.
[34] F. Lin, Q. Ling, and Z. Xiong, â€œByzantine-resilient distributed largescale matrix completion,â€ Proceedings of ICASSP, Brighton, UK, May
2019.
[35] W. Ben-Ameur, P. Bianchi, and J. Jakubowicz, â€œRobust distributed
consensus using total variation,â€ IEEE Transactions on Automatic
Control, vol. 61, no. 6, pp. 1550â€“1564, Jun. 2016.
[36] Z. Yang and W. U. Bajwa, â€œBRIDGE: Byzantine-resilient decentralized
gradient descent,â€ arXiv Preprint arXiv:1908.08098, Aug. 2019.
[37] Y. Nesterov, Introductory Lectures on Convex Optimization: A Basic
Course, Springer, 2013.

11

A PPENDIX A
P ROOF OF L EMMA 1
The proof of Lemma 1 relies on the following lemma.
Lemma 2. Let {z : z âˆˆ Z} be a subset of random vectors distributed in a normed vector space. If Z 0 âŠ† Z and |Z 0 | < |Z|
2 ,
then it holds that
P
2
/ 0 Ekzk
2
2
(26)
Ekgeomed{z}k â‰¤ CÎ± zâˆˆZ
|Z| âˆ’ |Z 0 |
zâˆˆZ
where CÎ± :=

2âˆ’2Î±
1âˆ’2Î±

and Î± :=

|Z 0 |
|Z| .

Proof: With z âˆ— = geomedzâˆˆZ {z} and z âˆˆ Z 0 , it holds that kz âˆ— âˆ’ zk â‰¥ kzk âˆ’ kz âˆ— k; and for all z âˆˆ
/ Z 0 , we have
âˆ—
âˆ—
kz âˆ’ zk â‰¥ kz k âˆ’ kzk. Then, summing up kz âˆ’ zk over all z âˆˆ Z yields
X
X
X
kz âˆ— âˆ’ zk â‰¥
kzk + (|Z| âˆ’ 2|Z 0 |)kz âˆ— k âˆ’ 2
kzk.
(27)
âˆ—

zâˆˆZ

z âˆˆZ
/ 0

zâˆˆZ

According to the definition of geometric median, it holds that
X
X
X
kz âˆ— âˆ’ zk = inf
ky âˆ’ zk â‰¤
kzk.
y

zâˆˆZ

zâˆˆZ

(28)

zâˆˆZ

Combining the two inequalities, we arrive at
P
P
P
2 zâˆˆZ
2|Z| âˆ’ 2|Z 0 | zâˆˆZ
/ 0 kzk
/ 0 kzk
/ 0 kzk
âˆ—
=
= CÎ± zâˆˆZ
kz k â‰¤
0
0
0
|Z| âˆ’ 2|Z |
|Z| âˆ’ 2|Z | |Z| âˆ’ |Z |
|Z| âˆ’ |Z 0 |

(29)

and upon squaring both sides of the latter, we find
âˆ— 2

kz k â‰¤

P
2
kzk)2
z âˆˆZ
/ 0 kzk
2
â‰¤
C
.
Î±
(|Z| âˆ’ |Z 0 |)2
|Z| âˆ’ |Z 0 |

(
CÎ±2

P

z âˆˆZ
/ 0

(30)

Then taking expectations on both sides, yields (26), and completes the proof.
With Lemma 2, the proof of Lemma 1 is straightforward.
Proof: It follows readily from Lemma 2 that
2

2

Ekgeomed{z} âˆ’ zÌ„k = Ekgeomed{z âˆ’ zÌ„}k â‰¤
zâˆˆZ

zâˆˆZ

CÎ±2

Ekz âˆ’ zÌ„k2
.
|Z| âˆ’ |Z 0 |

P

z âˆˆZ
/ 0

(31)

Applying the inequality of
Ekz âˆ’ zÌ„k2 = kz âˆ’ Ezk2 + 2Ehz âˆ’ Ez, Ez âˆ’ zÌ„i + kEz âˆ’ zÌ„k2 = kz âˆ’ Ezk2 + kEz âˆ’ zÌ„k2

(32)

to (31), yields
2

Ekgeomed{z} âˆ’ zÌ„k â‰¤
zâˆˆZ

CÎ±2

P

Ekz âˆ’ Ezk2
+ CÎ±2
|Z| âˆ’ |Z 0 |

z âˆˆZ
/ 0

P

EkEz âˆ’ zÌ„k2
|Z| âˆ’ |Z 0 |

z âˆˆZ
/ 0

(33)

which completes the proof.
A PPENDIX B
L EMMA 3 AND I TS P ROOF
Since computing the accurate geometric median is difficult, we consider the -approximate geometric median in this paper.
The following lemma is the -approximate counterpart of Lemma 2.
Lemma 3. Let {z : z âˆˆ Z} be a subset of random vectors distributed in a normed vector space. If Z 0 âŠ† Z and |Z 0 | < |Z|
2 ,
it holds that
P
2
22
/ 0 Ekzk
Ekzâˆ— k2 â‰¤ 2CÎ±2 zâˆˆZ
+
(34)
0
|Z| âˆ’ |Z |
(|Z| âˆ’ 2|Z 0 |)2
where CÎ± :=

2âˆ’2Î±
1âˆ’2Î± ,

Î± :=

|Z 0 |
|Z| ,

and zâˆ— is an -approximate geometric median of Z.

12

Proof: Because zâˆ— is an -approximate geometric median, it follows that
X
X
X
kzâˆ— âˆ’ zk â‰¤ inf
ky âˆ’ zk +  â‰¤
kzk + .
y

zâˆˆZ

zâˆˆZ

(35)

zâˆˆZ

Notice that (27) remains valid here. Hence, we have
kzâˆ— k â‰¤ CÎ±

P


z âˆˆZ
/ 0 kzk
+
.
|Z| âˆ’ |Z 0 |
|Z| âˆ’ 2|Z 0 |

(36)

Squaring both sides of (36), leads to
kzâˆ— k2

2
kzk

+
â‰¤ CÎ±
|Z| âˆ’ |Z 0 |
|Z| âˆ’ 2|Z 0 |
P

2
22
z âˆˆZ
/ 0 kzk
â‰¤ 2CÎ±2
+
0
|Z| âˆ’ |Z |
(|Z| âˆ’ 2|Z 0 |)2
P
2
22
/ 0 kzk
.
â‰¤ 2CÎ±2 zâˆˆZ
+
0
|Z| âˆ’ |Z |
(|Z| âˆ’ 2|Z 0 |)2
P



z âˆˆZ
/ 0

(37)
(38)
(39)

Then taking expectations on both sides, yields (34), and completes the proof.
A PPENDIX C
L EMMA 4 AND ITS PROOF
As we have indicated in Section IV-B, the main challenge in the proof of Byrd-SAGA is that the geometric median of
{mki } is a biased estimate of the gradient f 0 (xk ). To handle the bias, the following lemma characterizes the error between
an -approximate geometric median of {mkw } and f 0 (xk ) per slot k.
Lemma 4. Consider Byrd-SAGA with -approximate geometric median aggregation. Under Assumptions 1 and 2, if the
âˆ—
k
number of Byzantine attackers satisfies B < W
2 , then an -approximate geometric median of {mw }, denoted by z , satisfies
Ekzâˆ— âˆ’ f 0 (xk )k2 â‰¤ 2CÎ±2 L2 S k + 2CÎ±2 Î´ 2 +

22
(W âˆ’ 2B)2

(40)

where
CÎ± :=

2 âˆ’ 2Î±
1 âˆ’ 2Î±

and

Î± :=

B
W

(41)

while S k is defined as
S k :=

J
X 1X
1
kxk âˆ’ Ï†kw,j k2 .
W âˆ’B
J j=1

(42)

wâˆˆB
/

Proof: We begin with upper bounding the mean-square error Ekmkw âˆ’ fw0 (xk )k2 , where w âˆˆ
/ B. Using the definition
of mkw in (10), we have for any w âˆˆ
/ B that
Ekmkw âˆ’ fw0 (xk )k2
0
k
0
k
=Ekfw,i
k (x ) âˆ’ fw,ik (Ï†w,ik ) +
w
w
w

(43)
1
J

J
X

0
fw,j
(Ï†kw,j ) âˆ’ fw0 (xk )k2

j=1

0
k
0
k
2
0
k
=Ekfw,i
k (x ) âˆ’ fw,ik (Ï†w,ik )k âˆ’ kfw (x ) âˆ’
w
w
w
0
k
0
k
2
â‰¤Ekfw,i
k (x ) âˆ’ fw,ik (Ï†w,ik )k
w
w
w

â‰¤L2 Ekxk âˆ’ Ï†kw,ikw k2
=L2

J
1X k
kx âˆ’ Ï†kw,j k2
J j=1

J
1X 0
f (Ï†k )k2
J j=1 w,j w,j

13

0
k
where the second equality is due to variance decomposition Eka âˆ’ Eak2 = Ekak2 âˆ’ kEak2 with a = fw,i
k (x ) âˆ’
w
PJ
1
0
k
0
k
0
k
fw,ik (Ï†w,ik ), and Ea = fw (x ) âˆ’ J j=1 fw,j (Ï†w,j ); while the last inequality comes from Assumption 1.
w
w
To further upper bound the mean-square error Ekmkw âˆ’ f 0 (xk )k2 , we have that

Ekmkw âˆ’ f 0 (xk )k2

(44)

=Ekmkw âˆ’ fw0 (xk ) + fw0 (xk ) âˆ’ f 0 (xk )k2
=Ekmkw âˆ’ fw0 (xk )k2 + 2E mkw âˆ’ fw0 (xk ), fw0 (xk ) âˆ’ f 0 (xk ) + kfw0 (xk ) âˆ’ f 0 (xk )k2
=Ekmkw âˆ’ fw0 (xk )k2 + kfw0 (xk ) âˆ’ f 0 (xk )k2
â‰¤L2

J
1X k
kx âˆ’ Ï†kw,j k2 + Î´ 2 .
J j=1

where the last inequality relies on (43) and Assumption 2.
Next, we will derive an upper bound on Ekzâˆ— âˆ’ f 0 (xk )k2 . According to (34) in Lemma 3 and (44), it holds that
Ekzâˆ— âˆ’ f 0 (xk )k2
X
22
1
Ekmkw âˆ’ f 0 (xk )k2 +
â‰¤2CÎ±2
W âˆ’B
(W âˆ’ 2B)2
wâˆˆB
/
ï£¶
ï£«
J
X
X
22
1
1
ï£­L2
kxk âˆ’ Ï†kw,j k2 + Î´ 2 ï£¸ +
â‰¤2CÎ±2
W âˆ’B
J j=1
(W âˆ’ 2B)2

(45)

wâˆˆB
/

which completes the proof.
A PPENDIX D
L EMMA 5 AND ITS PROOF
In Lemma 4, the upper bound of Ekzâˆ— âˆ’ f 0 (xk )k2 contains a time-varying term S k . The following lemma characterizes
the evolution of S k .
Lemma 5. Consider Byrd-SAGA with -approximate geometric median aggregation. Under Assumptions 1, it holds that
ES k+1 â‰¤ 4J Â· Ekxk+1 âˆ’ xk + Î³f 0 (xk )k2 + 4JÎ³ 2 L2 kxk âˆ’ xâˆ— k2 + (1 âˆ’

1 k
)S
J2

(46)

where S k is defined in (42).
Proof: For the expectation of ES k+1 , we have that
ï£«
ï£¶
J
X 1X
1
2ï£¸
ES k+1 =E ï£­
kxk+1 âˆ’ Ï†k+1
w,j k
W âˆ’B
J j=1
wâˆˆB
/
ï£¶
ï£«
J
X
X

1
1
2 ï£¸
(1 + Î² âˆ’1 )kxk+1 âˆ’ xk k2 + (1 + Î²)kxk âˆ’ Ï†k+1
â‰¤E ï£­
w,j k
W âˆ’B
J j=1
wâˆˆB
/
ï£«
ï£¶
J
X
X
1
1
2ï£¸
=(1 + Î² âˆ’1 ) Â· Ekxk+1 âˆ’ xk k2 + (1 + Î²) ï£­
Ekxk âˆ’ Ï†k+1
w,j k
W âˆ’B
J j=1
wâˆˆB
/

=(1 + Î² âˆ’1 ) Â· Ekxk+1 âˆ’ xk k2 + (1 + Î²)(1 âˆ’

J
X 1X
1
1
)
kxk âˆ’ Ï†kw,j k2
J W âˆ’B
J j=1
wâˆˆB
/

1
=(1 + Î² âˆ’1 ) Â· Ekxk+1 âˆ’ xk k2 + (1 + Î²)(1 âˆ’ )S k
J

(47)

where the inequality comes from ka + bk2 â‰¤ (1 + Î² âˆ’1 )kak2 + (1 + Î²)kbk2 for any Î² > 0, and the third equality holds
because at slot k, honest worker w uniformly at random chooses one out of J data samples. For the chosen data sample j,
k+1
k
k
Ï†k+1
w,j = x ; otherwise, Ï†w,j = Ï†w,j .

14

Using the fact that f 0 (xâˆ— ) = 0, the first term in the right-hand side of (47) can be bounded as
kxk+1 âˆ’ xk k2
=kxk+1 âˆ’ xk + Î³f 0 (xk ) âˆ’ Î³f 0 (xk ) + Î³f 0 (xâˆ— )k2
â‰¤2kxk+1 âˆ’ xk + Î³f 0 (xk )k2 + 2Î³ 2 kf 0 (xk ) âˆ’ f 0 (xâˆ— )k2
â‰¤2kxk+1 âˆ’ xk + Î³f 0 (xk )k2 + 2Î³ 2 L2 kxk âˆ’ xâˆ— k2
where the first inequality comes from ka + bk2 â‰¤ 2kak2 + 2kbk2 , and the last inequality comes from Assumption 1.
Substituting (48) into (47), and choosing Î² = 1/J, we have
ES k+1 â‰¤(1 + J) Â· Ekxk+1 âˆ’ xk k2 + (1 âˆ’
â‰¤2J Â· Ekxk+1 âˆ’ xk k2 + (1 âˆ’

1 k
)S
J2

(48)

1 k
)S
J2

â‰¤4J Â· Ekxk+1 âˆ’ xk + Î³f 0 (xk )k2 + 4JÎ³ 2 L2 kxk âˆ’ xâˆ— k2 + (1 âˆ’

1 k
)S
J2

which completes the proof.
A PPENDIX E
P ROOF OF T HEOREM 1
Proof: Let zâˆ— be the -approximate geometric median of {mkw }. We begin by manipulating Ekxk+1 âˆ’ xâˆ— k2 as
Ekxk+1 âˆ’ xâˆ— k2
k

0

(49)
âˆ—

k

k+1

k

0

k

2

=Ekx âˆ’ Î³f (x ) âˆ’ x + x
âˆ’ x + Î³f (x )k
1
1
â‰¤
kxk âˆ’ Î³f 0 (xk ) âˆ’ xâˆ— k2 + Ekxk+1 âˆ’ xk + Î³f 0 (xk )k2 ,
1âˆ’Î·
Î·
1
where 0 < Î· < 1, and the inequality comes from ka + bk2 â‰¤ Î·1 kak2 + 1âˆ’Î·
kbk2 .
To bound the first term in the right-hand side of (49), we use that fw,ikw is Âµ-strongly convex and has L-Lipschitz
continuous gradients. Using also the fact that f 0 (xâˆ— ) = 0, we obtain

kxk âˆ’ Î³f 0 (xk ) âˆ’ xâˆ— k2

(50)

=kxk âˆ’ Î³(f 0 (xk ) âˆ’ f 0 (xâˆ— )) âˆ’ xâˆ— k2
=kxk âˆ’ xâˆ— k2 âˆ’ 2Î³hf 0 (xk ) âˆ’ f 0 (xâˆ— ), xk âˆ’ xâˆ— i + Î³ 2 kf 0 (xk ) âˆ’ f 0 (xâˆ— )k2
â‰¤kxk âˆ’ xâˆ— k2 âˆ’ 2Î³Âµkxk âˆ’ xâˆ— k2 + Î³ 2 L2 kxk âˆ’ xâˆ— k2
=(1 âˆ’ 2Î³Âµ + Î³ 2 L2 )kxk âˆ’ xâˆ— k2 .
Here hf 0 (xk ) âˆ’ f 0 (xâˆ— ), xk âˆ’ xâˆ— i â‰¥ Âµkxk âˆ’ xâˆ— k2 because f is Âµ-strongly convex; see Theorem 2.1.9 in [37]. Further, because
f has L-Lipschitz continuous gradients, it holds that kf 0 (xk ) âˆ’ f 0 (xâˆ— )k2 â‰¤ L2 kxk âˆ’ xâˆ— k2 .
Substituting (50) into (49) yields
Ekxk+1 âˆ’ xâˆ— k2 â‰¤

1 âˆ’ 2Î³Âµ + Î³ 2 L2 k
1
kx âˆ’ xâˆ— k2 + Ekxk+1 âˆ’ xk + Î³f 0 (xk )k2 .
1âˆ’Î·
Î·

(51)

With Î· = Î³Âµ/2, as long as
Î³ 2 L2 â‰¤

Î³Âµ
2

(52)

it follows that
1 âˆ’ 2Î³Âµ + Î³ 2 L2
â‰¤ 1 âˆ’ Î³Âµ.
1âˆ’Î·
Therefore, (51) can be rewritten as
Ekxk+1 âˆ’ xâˆ— k2 â‰¤(1 âˆ’ Î³Âµ)kxk âˆ’ xâˆ— k2 +

2
Ekxk+1 âˆ’ xk + Î³f 0 (xk )k2 .
Î³Âµ

(53)

15

Then, we construct a Lyapunov function T k as
T k := kxk âˆ’ xâˆ— k2 + cS k

(54)

where c is any positive constant. According to the definition in (42), we know S k is non-negative. Therefore, T k is also
non-negative.
Substituting (46) and (53) into (54), it follows that


1
2
(55)
+ 4cJ Ekxk+1 âˆ’ xk + Î³f 0 (xk )k2 + (1 âˆ’ 2 )cS k .
ET k+1 â‰¤(1 âˆ’ Î³Âµ + 4cJÎ³ 2 L2 )kxk âˆ’ xâˆ— k2 +
Î³Âµ
J
According to Lemma 4, the second term on the right-hand side (55) can be bounded as

k+1
k
0 k 2
2
âˆ—
0 k 2
2
Ekx
âˆ’ x + Î³f (x )k = Î³ Ekz âˆ’ f (x )k â‰¤ Î³ 2CÎ±2 L2 S k + 2CÎ±2 Î´ 2 +

22
(W âˆ’ 2B)2


.

(56)

Hence, we have





1
2
ET k+1 â‰¤(1 âˆ’ Î³Âµ + 4cJÎ³ 2 L2 )kxk âˆ’ xâˆ— k2 +
1âˆ’ 2 c+
+ 4cJ 2CÎ±2 Î³ 2 L2 S k
J
Î³Âµ



2
2
2
+ 4cJ
2CÎ±2 Î´ 2 +
.
+ Î³2
Î³Âµ
(W âˆ’ 2B)2

(57)

If we constrain the step size Î³ as
4cJÎ³ 2 L2 â‰¤

Î³Âµ
2

(58)

the coefficient in front of kxk âˆ’ xâˆ— k2 satisfies
1 âˆ’ Î³Âµ + 4cJÎ³ 2 L2 â‰¤ 1 âˆ’
and the factor

2
Î³Âµ

Î³Âµ
,
2

+ 4cJ satisfies
5
2
2
Âµ
+ 4cJ â‰¤
+
â‰¤
.
Î³Âµ
Î³Âµ 2Î³L2
2Î³Âµ

Similarly, if Î³ and c are chosen such that
Î³Âµ
1
<
2
2J 2

(59)

and
c=

10J 2 CÎ±2 Î³L2
5CÎ±2 Î³L2
â‰¥
Âµ
Âµ(1/J 2 âˆ’ Î³Âµ/2)

the coefficient in front of S k satisfies






1
2
1
5
Î³Âµ
2 2 2
1âˆ’ 2 c+
+ 4cJ 2CÎ± Î³ L â‰¤ 1 âˆ’ 2 c + CÎ±2 Î³L2 â‰¤ (1 âˆ’
)c.
J
Î³Âµ
J
Âµ
2
Therefore, (57) becomes


Î³Âµ
Î³Âµ
5Î³
22
)kxk âˆ’ xâˆ— k2 + (1 âˆ’
)cS k +
2CÎ±2 Î´ 2 +
2
2
2Âµ
(W âˆ’ 2B)2


Î³Âµ k 5Î³
2
=(1 âˆ’
)T +
CÎ±2 Î´ 2 +
.
2
Âµ
(W âˆ’ 2B)2

ET k+1 â‰¤(1 âˆ’

(60)

For simplicity, let also
Ëœ 2 := 5Î³
âˆ†
Âµ



CÎ±2 Î´ 2

2
+
(W âˆ’ 2B)2


.

Using telescopic cancellation on (60) from slot 1 to slot k , we arrive at


Î³Âµ k 0
2 Ëœ
2 Ëœ
ET k â‰¤ (1 âˆ’
) T âˆ’
âˆ†2 +
âˆ†2 .
2
Î³Âµ
Î³Âµ

(61)

(62)

16

Here and thereafter, the expectation is taken over itw for all workers w âˆˆ
/ B and slots t â‰¤ k âˆ’ 1.
The definition of the Lyapunov function in (54), implies that
Î³Âµ k
Ekxk âˆ’ xâˆ— k2 â‰¤ ET k â‰¤ (1 âˆ’
) âˆ†1 + âˆ† 2
2
where the constants âˆ†1 and âˆ†2 are defined as

(63)

âˆ†1 := kx0 âˆ’ xâˆ— k2 âˆ’ âˆ†2
10
2 Ëœ
âˆ†2 = 2
âˆ†2 :=
Î³Âµ
Âµ



CÎ±2 Î´ 2

2
+
(W âˆ’ 2B)2

(64)

.

(65)

In our derivation so far, the step size Î³ must satisfy (52), (58) and (59), meaning that


Âµ
Âµ
1
Î³ â‰¤ min
, âˆš
,
.
2L2 4 5J 3/2 CÎ± L2 J 2 Âµ
Therefore, we simply choose
Âµ
Î³â‰¤ âˆš
4 5J 2 CÎ± L2
and the proof is complete.
A PPENDIX F
P ROOF OF T HEOREM 2
0
k
/ B, and arbitrary otherwise.
Let zâˆ— denote the -approximate geometric median of {mkw }, where mk = fw,i
k (x ) for w âˆˆ
w
k+1
âˆ—
Similar to the proof of Theorem 1, we first derive an upper bound on Ekx
âˆ’x k. Inequality (53) is still true for Byzantine
attack resilient SGD with Î³ < Âµ/(2L2 ), and the only difference is that Ekxk+1 âˆ’ xk + Î³f 0 (xk )k2 becomes

Ekxk+1 âˆ’ xk + Î³f 0 (xk )k2
=Î³ 2 Ekzâˆ— âˆ’ f 0 (xk )k2
!
P
0
k
0 k 2
2
Ekfw,i
k (x ) âˆ’ f (x )k
2
w
âˆˆB
/
w
â‰¤Î³ 2 2CÎ±2
+
W âˆ’B
(W âˆ’ 2B)2

X
22
1
0
k
0
k 2
0
k
0 k 2
=Î³
Ekfw,i
+
k (x ) âˆ’ fw (x )k + kfw (x ) âˆ’ f (x )k
w
W âˆ’B
(W âˆ’ 2B)2
wâˆˆB
/


22
â‰¤Î³ 2 2CÎ±2 Ïƒ 2 + 2CÎ±2 Î´ 2 +
(W âˆ’ 2B)2
2

!

2CÎ±2

where the first inequality and the second equality are analogous to those in (45), while the last inequality comes from
Assumptions 2 and 3. Therefore, for Byzantine attack resilient SGD, we have
2
Ekxk+1 âˆ’ xâˆ— k2 â‰¤(1 âˆ’ Î³Âµ)kxk âˆ’ xâˆ— k2 +
Ekxk+1 âˆ’ xk + Î³f 0 (xk )k2
(66)
Î³Âµ


4Î³
2
â‰¤(1 âˆ’ Î³Âµ)kxk âˆ’ xâˆ— k2 +
CÎ±2 Ïƒ 2 + CÎ±2 Î´ 2 +
.
Âµ
(W âˆ’ 2B)2
Here and thereafter, the expectation is taken over itw for all workers w âˆˆ
/ B, and slots t â‰¤ k âˆ’ 1.
Using telescopic cancellation on (66) from slot 1 to slot k, we deduce that
Ekxk+1 âˆ’ xâˆ— k2 â‰¤ (1 âˆ’ Î³Âµ)k âˆ†01 + âˆ†02

(67)

âˆ†01 := kx0 âˆ’ xâˆ— k2 âˆ’ âˆ†02

(68)

where âˆ†01 and âˆ†02 are defined as

âˆ†02
and the proof is complete.

4
:= 2
Âµ



CÎ±2 Ïƒ 2

+

CÎ±2 Î´ 2

2
+
(W âˆ’ 2B)2


(69)

