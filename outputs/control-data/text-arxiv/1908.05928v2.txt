arXiv:1908.05928v2 [cs.IR] 23 Sep 2020

Recommendation with Attribute-aware Product Networks: A
Representation Learning Model
Guannan Liu‚àó

Liang Zhang

Beihang University
liugn@buaa.edu.cn

Beihang University
zhangliang2017@buaa.edu.cn

Junjie Wu

Xiao Fang

Beihang University
wujj@buaa.edu.cn

University of Delaware

ABSTRACT
With the prosperity of business intelligence, recommender systems have evolved into a new stage that we not only care about
what to recommend, but why it is recommended. Explainability of
recommendations thus emerges as a focal point of research and
becomes extremely desired in e-commerce. Existent studies along
this line often exploit item attributes and correlations from different
perspectives, but they yet lack an effective way to combine both
types of information for deep learning of personalized interests.
In light of this, we propose a novel graph structure, attribute network, based on both items‚Äô co-purchase network and important
attributes. A novel neural model called eRAN is then proposed to
generate recommendations from attribute networks with explainability and cold-start capability. Specifically, eRAN first maps items
connected in attribute networks to low-dimensional embedding
vectors through a deep autoencoder, and then an attention mechanism is applied to model the attractions of attributes to users, from
which personalized item representation can be derived. Moreover,
a pairwise ranking loss is constructed into eRAN to improve recommendations, with the assumption that item pairs co-purchased by
a user should be more similar than those non-paired with negative
sampling in personalized view. Experiments on real-world datasets
demonstrate the effectiveness of our method compared with some
state-of-the-art competitors. In particular, eRAN shows its unique
abilities in recommending cold-start items with higher accuracy, as
well as in understanding user preferences underlying complicated
co-purchasing behaviors.

CCS CONCEPTS
‚Ä¢ Information systems ‚Üí Data mining; Network data models; ‚Ä¢ Computing methodologies ‚Üí Dimensionality reduction and manifold learning;
‚àó Corresponding

author

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
Conference‚Äô17, July 2017, Washington, DC, USA
¬© 2020 Association for Computing Machinery.
ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00
https://doi.org/10.1145/nnnnnnn.nnnnnnn

KEYWORDS
Recommendation, Explainable, Attribute network, Attention
ACM Reference Format:
Guannan Liu, Liang Zhang, Junjie Wu, and Xiao Fang. 2020. Recommendation with Attribute-aware Product Networks: A Representation Learning
Model. In Proceedings of ACM Conference (Conference‚Äô17). ACM, New York,
NY, USA, 10 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn

1

INTRODUCTION

Recommender systems play indispensable roles in contemporary
e-commerce by empowering consumers to reach their preferred
products more efficiently [1]. Traditionally, recommendation methods rely on the similarity between items and recommend the most
‚Äúsimilar‚Äù items in terms of users‚Äô historical purchasing records. Particularly, in item-based collaborative filtering (CF) [10, 20], the similarity between items can arise from the co-purchase relationships
between items, i.e., two items would be regarded similar if they
have been co-purchased by many other users in history.
The underlying factors that drive the co-purchases of items,
however, may not be uniform for different users, and thus such copurchases would not necessarily reveal users‚Äô genuine preferences.
In reality, users may have their own desired feature aspects when
considering to buy an item. For example, two movies may be cowatched by users, but some users may only be driven by the same
leading actor, while others may be in favor of the same director. To
infer users‚Äô underlying individual preferences from co-purchased
items, however, have not been well addressed in prior studies. This
is also closely related to the concept ofexplainable recommendations,
which has absorbed great research interests and become extremely
desired in e-commerce [27, 30].
In this paper, we aim at integrating items‚Äô co-purchase information with items‚Äô attribute information to deep learn users‚Äô personalized interests. Based on the item correlations formed in copurchases, we propose a novel item network structure called attribute network. An attribute network is essentially a customized
sub-network of the item co-purchase network, formed by keeping
only the connections between two items that have the given item
attribute in both, e.g., two movies with a same director. By decomposing a co-purchase network into various attribute networks, we
can specify the diverse driving forces underlying the co-purchase
network, which can then be used to characterize a user‚Äôs individual
preference and explain which aspects of a recommended item that
he/she would desire most, i.e., generate explainable recommendations. For those items that have never been purchased and thus

C
B

A

Same Actor

Same Director

C
A

B

F

D

D

E
B

E

G

Co-purchased Network

A

Actor Network

F
Director Network

G

Attribute Network

Figure 1: Toy example for attribute network.

cannot enter the co-purchase network, they can still enter attribute
networks as long as they have attribute information, which sheds
light on cold-start recommendations.
We then propose a novel method for Explainable Recommendation based on Attribute Networks (eRAN ). In eRAN, items in an
attribute network is represented by low-dimensional embedding
vectors through a deep autoencoder to account for the nonlinearity
and higher-order proximity in the network. Meanwhile, with users
mapped to embedding vectors, an attention mechanism in adopted
in eRAN to model user preferences toward different attributes. Then,
a personalized item representation can be constructed by taking
a weighted average from the node embeddings of each attribute
network. Under the assumption that a user‚Äôs co-purchased items
should be more similar than others, eRAN optimizes towards the
contrastive similarity to derive personalized similarity between
item pairs, which is different from traditional item-to-item methods
that directly factorize the item ratings through aggregate similarity [10]. With the learned parameters, recommendation score of
each item based on the users‚Äô embeddings and personalized item
representations can be obtained and the most similar items in the
lens of individual users would be recommended. Meanwhile, eRAN
can provide fine-grained explanations on users‚Äô desired attributes
for a particular recommended item.
We conduct experiments on three real-world datasets including
movies, books, and music to validate the effectiveness of the proposed methods, where the attributes can directly influence users‚Äô
experiences towards these items. Experimental results demonstrate
the superiority of the proposed methods to the state-of-the-art recommendation methods in terms of accuracy. Also, considering the
incorporated auxiliary item attributes, the method is capable of
coping with cold-start items when the attributes are given, and we
further implement experiments to show that our method can better
predict which users would be interested in these new items. Last
but not least, we showcase a scenario how the attention weights
and user embeddings inferred from the model can guide the explanations for recommendations.

2

ATTRIBUTE NETWORK

Items can be inter-connected with each other from various perspectives. For example, different items can appear in a same user‚Äôs
purchasing history, and the connections between the paired items
can be driven by users‚Äô tastes and preferences. Also, connections
can be established between items with similar attributes since they

may both satisfy users‚Äô specific needs. Such relationships can indeed
be utilized to recommend items to individual users in a personalized
manner with explanations on particular aspects.
One prominent strategy in constructing item relationships is
to learn item similarity by factoring user‚Äôs rating for an item as
an aggregation of its similarity with previously rated items, or
can be referred to as neighborhood items [10]. The relationships
derived from item similarity can provide possible explanations for
recommendations, e.g., ‚Äúitem A is recommended because it is similar
to the previously bought item B‚Äù. Such relationships remain the same
across users, i.e., every user would regard the similarity between
each pair of items with no differences, which however, may not
always hold and refrain the relationships to distinguish fine-grained
user preferences.
For example, the items are connected differently with regards to
the graph structure as shown in Fig. 1, where A has more connections with C, D, and E that have similar actors, while B interacts
more with F and G due to their common directors. Two users X
and Y may have watched both movies A and B, but the underlying
reasons may be different. User X may only prefer the actors, while
Y may be driven by the directors. Thus, the relationships between A
and B should be decomposed in accordance with users‚Äô preferences
on particular aspects, i.e., the similarity between items should not
be treated uniformly for every user. In particular, for users such as
X that favors the movies with specific actors, they may be more
likely to accept the recommended item C or D, rather than F or G
with the following explanations, ‚ÄúWe recommend C because it has
similar actors with the watched movie A‚Äù.
Obviously, item relationships can be decomposed by incorporating auxiliary information such as item attributes. With the attributes
attached to each item, users‚Äô personalized preferences toward particular relationships can be explicitly explained. If the items with
similar attributes are connected as shown in Fig. 1, users‚Äô preferences can propagate along the connections driven by particular
attributes. For example, user X‚Äôs preferences toward actors can manifest in the local connections driven by actor-network, while Y‚Äôs
favor for directors can be disclosed through the director-network.
In addition, traditional content-based methods generally treat
the item attributes independently and compute the similarity between items, which however, would lose higher-order relationships
between them. Take the items again in Fig. 1, item A and item D
share no common actors, and the similarity based on the attribute
of actors would be scored 0 in traditional sense. However, we can
observe that A has a same actor with B, and B also has a same
actress with C, which can indicate proximity between A and C.
Therefore, in order to capture users‚Äô preferences toward attributes and also account for and the higher-order relationships
arise from particular attribute space, we propose a novel network
structure, namely attribute network. We firstly construct a co-purchased
network from the rating/purchasing history denoted by G =<
V , E >, where each item i ‚àà V is regarded as a node, and ‚Äúalso
buy item j with i‚Äù is termed as a link between nodes i and j, ei j ‚àà E.
In particular, item i has a K-dimensional attribute vector gi ‚àà RK ,
and for each type of attribute k, we only reserve the edges in G that
share the same attribute values between the pairwise nodes, with

the subset of links being E k = {eikj |–¥ik = –¥jk && ei j ‚àà E} ‚äÜ E,
which gives an induced subgraph of G, i.e., k-attribute network G k .
We assume that the reason why items are co-purchased can be
attributed to one or several attributes. This assumption is indeed
more applicable for experience products such as movies, music, etc.,
where users would show stable personalized preferences toward
the attributes and the attributes are deemed to influence their experiences for the products greatly. Given the K attribute networks
derived from the induced subgraphs of co-purchase networks, users‚Äô
personalized preferences toward items can be decomposed as multiattribute item relationships, and meanwhile the explanations for
recommendations can be derived with regards to both item-based
and content-based methods.

3

METHODOLOGY

The overview of the modeling framework is shown in Fig. 2. Firstly
each item in the attribute network is represented by K-attribute
embedding vectors with deep autoencoders. Then, users are mapped
to an embedding layer and the personalized preferences toward
attributes are captured by an attention mechanism, so as to compute
the personalized item similarity. Finally a personalized ranking loss
is constructed to account for the individual item pairs and also
apply negative sampling for non-paired items.

3.1

where T represents the number of layers for encoder, and W (t ) , b t
denotes corresponding parameters of the layer. Particularly, y(t )
can be regarded as the hidden representation for x when t = T . In
similar to the encoder part, decoder also applies several non-linear
functions to mapping the representation vectors to the reconstruction space, and obtain the reconstructed output xÃÇ. By minimizing
the reconstruction error between the input and the output, we can
derive the representation, and the loss function can be formulated
as follows.
L=

i=1

y(1) = œÉ (W (1) x + b (1) )
y(t ) = œÉ (W (t ) y(t ‚àí1) + b (t ) ), t = 2, ...,T ,

(1)

||xi ‚àí xÀÜi ||22 .

(2)

We treat the adjacency vector sv of the node v in the k-attribute
as input and feed it into the autoencoder to derive the hidden
representations for the item in the k-attribute network.
However, the network may be sparse and the adjacent matrix
would be filled with many zeros. Thus, traditional autoencoder may
be more likely to reconstruct zeros and hence cannot capture the
local connectivity of the network structure. In order to tackle this
issue, we impose a larger penalty on the reconstruction error of the
non-zero elements [24] by incorporating a regularizer b = {bi j }.
When si j = 0, bi j is set to be a small value Œ≤, and the loss can be
revised as follows.

Embedding Attribute Networks

Attribute network provides a new perspective in probing users‚Äô
preferences by mapping items to manifold space with different
types of connections, which would overcome the limitations when
handling raw item attributes in Euclidean space. Therefore, it is
naturally appealing to firstly map items to low-dimensional vectors
to encode the items in attribute space. For each node v in attribute
network G k , we can learn a mapping function f k (¬∑), to obtain a
d-dimensional vector for the node, i.e., hvk ‚àà Rd , where d ‚â™ |V |.
As discussed previously, higher-order proximity in the attribute
network can disclose the item similarity more accurately, and meanwhile users‚Äô preferences toward a specific attribute would propagate
along the network path. For example, in Fig. 1, node A and node C
share a common neighbor B, and they would be regarded as similar in the connections when second-order similarity is taken into
account. Specifically, second-order proximity can be defined as the
similarity of the neighborhood structure between a pair of nodes,
thus we can represent a node by its adjacency vector svk ‚àà S k . S k
is the adjacency matrix constructed from the k-attribute network,
and the entry si j = 1 when there exists a link between node i and j.
Considering the high nonlinearity of the network structure, we
propose to represent the adjacency vector of each node in the attribute network via a deep autoencoder. Deep autoencoder is a
typical deep learning model to handle nonlinearity, which generally consists of two parts including encoder and decoder, with
both containing multi-layer nonlinear functions. The feedforward
process of the encoder maps the input data x to the representation
space as follows,

N
√ï

Lnet =

N
√ï
i=1

||(xi ‚àí xÀÜi ) ‚äô bi ||22

(3)

where ‚äô means the Hadamard product.
It is worth addressing that this attribute-network representation
framework is capable of handling cold-start items with attributes
given. Though a newly released item has no prior co-purchase
records, the attributes provide clues to connect it with existing attribute networks. Specifically, we can regard the item as a new node
v ‚Ä≤ with attribute vector gv ‚Ä≤ . Then for the k-attribute network, the
edges can be connected to those existing nodes that have the same
attribute value with –¥vk ‚Ä≤ . With the derived parameters of the autoencoder structure, we can further obtain the hidden representation
hvk ‚Ä≤ of the new item.

3.2

Personalized Item Similarity Based on
Attribute Networks

Item-to-item CF is a typical method by employing the neighborhood items to compute the recommendation score for an item. In
this approach, the item similarity is computed by the inner dot
between the latent factors of the item pairs, which is generally in
the following form according to [10].
sim(i, j) = pj qi‚ä§ ,
(4)
where pj and qi denote the latent factors of items respectively. The
similarity between item i and j obtained through the inner dot is
indeed uniform across distinct users, which remain to be a major
limitation for these methods. Therefore, it is desirable to take an
individual view to account for the item similarity.
With the derived representations from attribute networks, we
can replace the item latent factors with the node embeddings to
devise a neural model for item-based recommendation. Moreover,

User Embedding

‡∑ù
ùíô

‚Ä¶

‚Ä¶

‚Ä¶

‚Ä¶

‚Ä¶

‚Ä¶

‚Ä¶

‚Ä¶

‚Ä¶

‚Ä¶

Attention Module

Inner Pd
Softmax

‡∑ç

‡∑ç

Personalized Item
Representation

‚Ä¶

‚Ä¶

‚Ä¶

‚Ä¶

‚Ä¶

‚Ä¶

‚Ä¶

‚Ä¶

‚Ä¶

‚Ä¶

Personalized Item Similarity

ùíô
Vertex ùëñ

Vertex ùëó

Ranking Loss
Negative Item Pair

Attribute Network

Attribute Network
Co-purchased Network

Figure 2: The architecture of eRAN.
we can decompose the item-to-item similarity by taking users‚Äô preferences toward each field of attribute into account. Each user u can
firstly be mapped to an embedding vector zu , and then the user‚Äôs
preferences toward a particular attribute of an item can be captured through an attention mechanism. Attention mechanisms are
generally introduced in NLP, computer vision, and recommender
systems to track the attractions of different components. Specifically, in scoring the attention weights of user u for item i on a
particular attribute k, we simply take the inner dot between the
user embedding vectors and the node embeddings hki from the deep
autoencoder of the k-attribute as follows,
k
au,i
= hki zu‚ä§
(5)
We can further apply softmax to normalize user‚Äôs attention
scores for an item on each attribute k,
k )
exp(au,i
k
aÃÉu,i
= √çK
k‚Ä≤ )
exp(au,i
k ‚Ä≤ =1

(6)

The attention weights can be explained as the extent to which
the user desire for a particular attribute of an item, and thus can
be exploited to provide explanations for the attribute-aware recommendation.
Then, we can proceed to derive each individual user‚Äô affinity
for an item through the node embeddings from all the attribute
network, which can be viewed as a weighted average of the node
embeddings from each type of attribute network.
hu,i =

K
√ï
k =1

k
aÃÉu,i
hki

(7)

Motivated by the general idea of item-to-item methods, we can
employ the item representations to compute the personalized similarity in the neighborhood, which can be approximated by,
simu (i, j) = ‚àí||hu,i ‚àí hu, j ||22 ,
(8)
Different from prior item-based methods, we replace the inner
dot with L2-norm distance metric to measure the item relationships
with the embedding vectors. As proved in [9], inner dot violates

the triangle inequality, which may lead to suboptimal solution.
Moreover, the personalized item similarity indeed decomposes the
relationships towards attributes, which can provide fine-grained
explanations for recommendations.

3.3

Loss Function and Optimization

Since we obtain the personalized similarity for each pair of items,
we can use it to guide the learning of both user embeddings and
item representations, as well as the attention weights on different
attributes. An underlying assumption is that users would remain
stable in their preferences for the items and therefore the neighborhood item tend to be similar in view of the users. Specifically,
given a particular user u and one of the purchased item i ‚àà Ru+ ,
it should be similar to the neighborhood items j ‚àà Ru+ . Thus, the
representations can be learned by maximizing the aggregate personalized similarity, and can be written as a loss function by taking
a negative value based on Equation (8),
L=

√ï

√ï

u ‚ààU i, j ‚ààRu+

log œÉ (||hu,i ‚àí hu, j ||22 ),

(9)

where U represents all the users. However, this loss function is
likely to get trapped to a trivial solution when all the items are
approximated by the same representation. Thus, similarly to the
optimization techniques proposed in BPR [19] that assume users
would prefer items they have bought than those that they have not,
we also introduce a negative sampling strategy to avoid the issues.
Specifically, given a user u, we can sample an item n < Ru+ as
a negative sample. Then for each item i and a co-purchased item
j by u, it is natural that the similarity should be higher than that
between the non-paired items i and n, which would satisfy the
following inequality.
simu (i, j) >u simu (i, n).

(10)

Then the loss function with negative sampling can be revised as,
√ï √ï
Lr ank =
log œÉ (||hu,i ‚àí hu, j ||22 ‚àí ||hu,i ‚àí hu,n ||22 ). (11)
u ‚ààU i, j ‚ààRu+

To preserve the attribute network structures and learn personalized item presentations tailored for recommendation, we combine
the loss functions in Equation (3) and Equation (11) with a weighting
parameter Œ± to jointly minimizes the following objective function:
Lr ec = Lnet + Œ± Lr ank .

(12)

We adopt Adaptive Moment Estimation (Adam) to optimize the
objective function in Equation (12). In each iteration, we sample a
mini-batch of users and item pairs with its corresponding k adjacency matrix to update the parameters.

3.4

Recommendation Score

Similar to traditional item-to-item CF method, when evaluating
the recommendation score of user u for item i given the learned
representations, we need to revisit the relationships between item
i and each item j that has ever been purchased by u, which can be
approximated by,
rÀÜu,i =

√ï
j ‚ààRu+ \{i }

‚àí||hu,i ‚àí hu, j ||22 ,

(13)

where Ru+ \{i} represents the set of items that have been rated by
the user u except for i, i.e., the neighborhood of item i.
In particular, for a new item x, since it does not have any prior
co-purchase records, we can only connect it to the existing nodes
in the attribute networks. According to the learned parameters and
representations, we can also construct the individual representation
hu,x for user u. However, item x never appears together with any
of neighborhood items, thus we relax the restriction and derive
the recommendation score with regards to the minimum similarity
with the neighborhood items.
rÀÜu,x = min+ ‚àí||hu,x ‚àí hu, j ||22
j ‚ààRu

(14)

When generating recommendations for u, we simply need to
rank candidate items according to the recommendation score and
select the ones with highest scores as recommended items. In this
recommendation framework, we can easily interpret the recommendations with both the personalized item similarity and the users‚Äô
attention weights on attributes of each item. Specifically, when the
item i is to be recommended to user u, we can obtain the attention
k according to Equation (5) and (6), to identify which
weights aÃÉu,i
attributes of i that attract the user; meanwhile, we can also position
the item j in the neighborhood that are most similar to i. Therefore,
we can recommend i to u with the following explanations: ‚ÄúWe
recommend i because it is similar to j on the attribute k.

4

EXPERIMENTAL SETUP

We validate the effectiveness of the proposed methods on three
real-world datasets. Eight state-of-the-art (SOTA) baseline methods
are included for a thorough comparative study.

4.1

Data Sets

We first briefly introduce the datasets used in our experiments, with
the statistics listed in Table 1.

Table 1: Data statistics.

Kaggle-Movie
Goodreads-Potery
Amazon-Music

# users
663
39540
11697

# items
6850
24052
7100

# actions
61088
449401
65950

# features
3
4
2

Kaggle-Movie: This dataset is extracted from the Kaggle1 Challenge Dataset. We use the directors, genres and top five actors as
attributes.
Goodreads-Potery: This dataset is collected by Wan et al. [23]
from a popular online book review website named Good-reads2 .
Several attributes are used including authors, number of pages,
publication-year, and top three user-generated shelf names.
Amazon-Music: Each top-level product category on the Amazon3 are constructed as a separate datasets by McAuley et al. [13].
We choose the dataset constructed from the music category, and
extract the top three genres as well as the price as attributes.
In this paper, we remove items with missing values, treat ratings
larger than 3 as positive feedbacks, and retain users whose history
length larger than 5, 5 and 3 for Kaggle-Movie, Goodreads-Potery
and Amazon-Music respectively.

4.2

Baseline Methods

The following SOTA methods are applied as baselines in our experiments.
NMF [15]: NMF is a widely used collaborative filtering approach,
which factorizes the interaction binary matrix.
BPR-MF [19]: BPR-MF is a well-known top-N recommendation
method to cope with implicit matrix, which uses the Bayesian
personalized ranking optimization criterion.
FM [18]: FM is a successful feature-based recommendation method,
which is effective on sparse data.
DeepFM [7]: DeepFM is a deep variant of FM which imposes a
factorization machines as "wide" module to extract shallow feature
interactions.
PNN [16]: PNN is another deep variant of FM which introduces
a product layer after embedding layer to capture high-order feature
interactions.
AFM [26]: AFM extends FM by using attention mechanism to
distinguish the different importance of second-order combinatorial
features.
SVDFeature [3]: SVDFeature is an effective toolkit for featurebased matrix factorization.
FISM [10]: FISM is a state-of-the-art item-based CF method
which learns global item similarities from user-item interactions.
eRAN-L1: eRAN-L1 is a submodel which only optimizes the
ranking loss.
eRAN-L2: eRAN-L2 is another submodel which only optimizes
the reconstruction loss with Œ± = 0. In this submodel, we fix user
embeddings to 1.0 during training.

1 https://www.kaggle.com/rounakbanik/the-movies-dataset/
2 https://www.goodreads.com
3 https://www.amazon.com

4.3

0.70

Parameter Settings

0.65

For our method, we set the mini-batch size, the learning rate of
the Adam, the hyper-parameters of Œ± and Œ≤ to be 2000, 0.001, 1500
and 0.2 respectively. We keep the same structure of autoencoder
with varying datasets. Specifically, the dimensions of hidden states
are 1024, 256 and 32 for y(2) , y(3) and y(4) respectively according
to Equation (1). As for the baseline methods, we apply default
parameters except for the embedding size, which is fixed to be 32
for all the methods.

0.60

Recall

0.55
0.50
0.45

FM
DeepFM
AFM
PNN
SVDFeature
ERAN

0.40
0.35
0.30

50 60 70 80 90 100 110 120 130 140 150

K

5 EXPERIMENTAL RESULTS
5.1 Recommendation Accuracy
We first conduct a comparative study to validate the superiority
of our model to the introduced baseline methods in terms of recommendation accuracy. In this task, we adopt the leave-one-out
evaluation strategy, that is, for each user, we hold-out one purchased item as test set and the remaining is used for training. Since
it is too time-consuming to rank all the items for every user during evaluation, we follow the experimental settings in [8] which
randomly samples 100 negative items and rank the recommendation score among the 100 items. Given the top-K ranked items, we
apply Precison@K and nDCG@K as evaluations measures. The
comparative results of the three datasets are in Table 2 and Table 3.
The proposed model is consistently better than all the baselines
on the three datasets, while in contrast, the second best is relatively
unstable, showing that our methods are more robust. In addition,
we find that FISM outperformed other baselines in many cases in
nDCG, while SVDFeature and PNN performed better in Precision.
This results indicate that eRAN can not only accurately recognize
the items that users really prefer, but also tend to rank them at top
positions simultaneously.
Moreover, we find that most attribute-based methods perform
well on Goodreads-Potery particularly. A possible reason is that the
attribute user-generated shelf names and authors may have great
influences on user preferences, and our method can effectively infer
users‚Äô preferences toward attributes. Also, it is notable that eRAN
achieves the greatest improvement on Amazon-Music with the most
sparse ratings among the three datasets. It might be due to the
attribute network simultaneously model the first-order relationship
and the high-order relationship from attribute space, which can
handle the data sparsity.
It is also notable that AFM doesn‚Äôt perform well on the three
datasets, even worse than FM, which may be due to the fact that
it is unable to learn effective attention weights in feature interaction space when features are scarce. While on the contrary, eRAN
can leverage attention mechanism to model user‚Äôs fine-grained
preferences in the attribute space.
Considering the two variants of eRAN, the submodel eRANL2 can be seen as a kind of network embedding method which
lacks optimization tailored for recommendation, which achieves
the worst performances. Meanwhile, eRNA outperforms eRANL1 significantly, which validates the effectiveness of leveraging
attribute information in improving performances.

Figure 3: Prediction results for cold-start items.

5.2

Cold Start Item Recommendation

In this task, we evaluate the effectiveness of our model in handling
cold start items with attributes given. To simulate the scenarios
for cold-start items, we randomly hold out 40 items and regard
them as new items with no purchasing records. We can treat the
item as a new node and connect it with existing attribute networks
according to Section 3.1. Afterwards, we can obtain the adjacency
matrix of the new node in each attribute network and feed into the
deep autoencoder to derive respective node embeddings.
Then, for each ‚Äònew‚Äô item, we can rank all the users according to
the recommendation score in Equation (14) to predict which users
are most likely to purchase the items. We can use the measure Recall
to evaluate the effectiveness of prediction, i.e., how many users are
accurately predicted to buy the new item among all the users that
have purchased in reality. The methods NMF, BPR, and FISM cannot
be applied for this setting because the new items would not appear
in the rating matrix. Thus, we only implement this experiment on
the attribute-based recommendation methods, in which we remove
the corresponding new items in the training phase and evaluate
the results on prediction with the same setting of our method.
The results on Kaggle-Movie are illustrated in Figure 3. As we can
see that our methods consistently outperform other attribute-based
recommendation methods. Among the baselines, FM, DeepFM and
AFM achieve similar performances in this experiment, PNN performs the second best when K is small, while SVDFeature shows
competitive results when K is large. The results show the superiority of the proposed method in coping with cold-start items, which
also illustrate that eRAN well capture fine-grained user preferences
toward attributes.

5.3

Explanation and Visualization

One of the pervasive advantages our model is that we can obtain
insights into the underlying reasons for recommendation. Thus,
we explore the learned user embeddings and attention weights
on attributes from both quantitative and qualitative perspectives
to explain the recommendation. Take the Kaggle-Movie dataset as
an example, for each user, we regard the users‚Äô average attention
scores for all the items that they have interacted with as the general
description of their preferences. Correspondingly, each user in the

Table 2: Precision@K of the three datasets.

Kaggle-Movie

Method

Goodreads-Potery

Amazon-Music

P@5

P@10

P@15

P@5

P@10

P@15

P@5

P@10

P@15

NMF
BPR-MF
FISM

0.1201
0.1210
0.1217

0.0746
0.0742
0.0736

0.0548
0.0547
0.0536

0.1386
0.1412
0.1528

0.0779
0.0801
0.0827

0.0525
0.0565
0.0576

0.0902
0.0806
0.0951

0.0602
0.0516
0.0636

0.0454
0.0392
0.0465

FM
DeepFM
PNN
AFM
SVDFeature

0.1168
0.1183
0.1195
0.1154
0.1219

0.0725
0.0726
0.0719
0.0721
0.0751

0.0531
0.0542
0.0537
0.0528
0.0556

0.1524
0.1540
0.1557
0.1376
0.1547

0.0844
0.0849
0.0842
0.0783
0.0848

0.0578
0.0590
0.0590
0.0550
0.0588

0.0844
0.0874
0.0875
0.0739
0.0943

0.0530
0.0559
0.0571
0.0497
0.0637

0.0386
0.0419
0.0428
0.0387
0.0480

eRAN-L1
eRAN-L2
eRAN

0.1161
0.0237
0.1289

0.0733
0.0190
0.0789

0.0532
0.0161
0.0570

0.1485
0.0305
0.1626

0.0836
0.0218
0.0875

0.0572
0.0163
0.0604

0.0707
0.0541
0.1104

0.0470
0.0385
0.0691

0.0369
0.0305
0.0508

Table 3: nDCG@K of the three datasets.

Kaggle-Movie

Method

Goodreads-Potery

Amazon-Music

n@5

n@10

n@15

n@5

n@10

n@15

n@5

n@10

n@15

NMF
BPR-MF
FISM

0.4451
0.4554
0.4593

0.4986
0.5002
0.5017

0.5213
0.5206
0.5195

0.5837
0.6044
0.6579

0.5988
0.6256
0.6783

0.6149
0.6379
0.6923

0.3280
0.2807
0.3672

0.3697
0.3173
0.4075

0.3941
0.3362
0.4253

FM
DeepFM
PNN
AFM
SVDFeature

0.3639
0.3782
0.3822
0.3727
0.4272

0.4200
0.4267
0.4341
0.4289
0.4755

0.4419
0.4503
0.4586
0.4527
0.4964

0.6222
0.6370
0.6527
0.5552
0.6464

0.6508
0.6627
0.6747
0.5857
0.6716

0.6595
0.6731
0.6827
0.5968
0.6873

0.2971
0.2969
0.3015
0.2437
0.3370

0.3324
0.3363
0.3431
0.2867
0.3888

0.3454
0.3547
0.3618
0.3079
0.4122

eRAN-L1
eRAN-L2
eRAN

0.3709
0.0684
0.4702

0.4312
0.0916
0.5167

0.4491
0.1064
0.5340

0.6174
0.0852
0.6858

0.6581
0.1019
0.7073

0.6693
0.1467
0.7154

0.2164
0.1805
0.4026

0.2622
0.2173
0.4482

0.2844
0.2367
0.4671

movie dataset can be described with attentions scores on actor,
director, and genre. Larger attention score on an attribute means
the user may prefer the corresponding aspect more.
We firstly validate whether the attention mechanism actually
play a role for identifying users‚Äô preferences. Specifically, according to the learned parameters previously, we select 40 users with
the largest actor-attention score, 40 users with the smallest actorattention and another 40 random users as three separate test groups,
and we denote them as Max, Min, and Random respectively. We
then remove the actor network to train a new model, and other
settings remain the same. We can then test the recommendation
performances for the three test user groups, with the results shown
in Fig. 4a and Fig. 4b. We can see the lack of actor network affects
differently on the three test groups. The overall performance order
is as follows: Min > Random > Max, showing that Max group is
severely influenced, which confirms our analysis that these users
are more concerned about actors. Also, the derived user embeddings projected by t-SNE [22] are also illustrated in Fig. 4c. It‚Äôs not

hard to find out that Max and Min are clearly separated apart from
each other, which demonstrates the effectiveness of the learned
user embeddings in distinguishing users with different preferences.
In addition, we pick two cases to explain the attention scores
output by eRAN in Table 4. We can see user 255 and user 639 both
watch the movie Fear and Loathing in Las Vegas. However, the model
can distinguish that user 255 is driven by the same actor, while user
639 is driven by the same director according to attention scores.
Meanwhile we can compute the most similar movies to them and
find that user 225 is keen on the actor Johnny Depp while user 639
like the director Terry Gilliam. Therefore, we can easily use eRAN
to provide the explanations like "A is similar to B and C, especially
with the same actors".

5.4

Parameter Sensitivity

In this subsection, we examine the sensitivity of two parameters,
i.e., the embedding size and the weighting parameter Œ± in the loss
function.

Table 4: Two comparative case studies for explainable recommendation.
Movie

User Actor Attention Score Director Attention Score

Most Similar Movies

Explanation

Fear and Loathing in Las Vegas

255
639

0.6006
0.3404

0.3021
0.5159

Edward Scissorhands, A Nightmare on Elm Stree
The Meaning of Life, Monty Python and the Holy Grail

The same actor Johnny Depp
The same director Terry Gilliam

Pulp Fiction

467
129

0.4941
0.3091

0.2874
0.4708

Django Unchained, Jurassic Park
Kill Bill, Reservoir Dogs

The same actor Samuel L. Jackson
The same director Quentin Tarantino

0.14
0.12
0.10
0.08
0.06
0.04
0.02
0.00

0.60
0.55
0.50
0.45
0.40
0.35
0.30
0.25

Max
Random
Min

Min
Random
Max

NDCG

Precision

Min
Random
Max

@5

@10

@15

@5

(a) Precision

@10

@15

(b) nDCG

(c) User Embedding

Figure 4: Recommendation results and user embeddings for different user groups.
0.51
Kaggle-Movie
Amazon-Music

0.47
0.45

32

64

Embedding Size

128

0.41

0.070

0.41

0.065

0.38

0.060

0.43
16

0.44

NDCG@10

NDCG@10

Kaggle-Movie
Amazon-Music

0.47

0.075

0.49

Precision@10

Precision@10

0.078
0.076
0.074
0.072
0.070
0.068
0.066

16

32

64

Embedding Size

128

(a) Embedding Size

0.055

Kaggle-Movie
Amazon-Music

10

10e2

Œ±

10e3

10e4

0.35
0.32

Kaggle-Movie
Amazon-Music

10

10e2

Œ±

10e3

10e4

(b) Œ±

Figure 5: Impact of hyper-parameters on ranking performance.
Embedding size. Figure 5a demonstrates the impact of embedding sizes on the results. It‚Äôs easy to find that 32 is the best embedding size for both Kaggle-Movie and Amazon-Music as measured by
Precision and nDCG. Moreover, the performances remain stable in
all the settings, which shows the robustness of our method.
The weighting parameter Œ±. From the results shown in Figure 5b, we can see the performance first increases along with Œ±,
and then begins to drop when Œ± > 1000. It is worth mentioning that
our model would reduce to eRAN-L2 when Œ± is infinitesimal, and
to eRAN-L1 when Œ± is infinity. Their performances are consistent
with the trend indicated by the sensitivity analysis.

6

RELATED WORK

Our work is related to the following streams of recommendation
methods including item-based, attribute based, as well as explainable recommendation.
The idea of item-based CF methods is that the prediction of a
user on a target item depends on the similarity of this item to all

items the user has interacted with in the past. Traditional itembased CF methods often predefine some similarity measures such
as cosine similarity and Pearson coefficient [20]. Another common
approach is to employ random walks on the user-item bipartite
graph [11]. However, such heuristic similarity measurement lacks
optimization tailored for different datasets, and thus may yield
suboptimal results. Recently, Ning et al. has proposed a method
SLIM which learns item similarity directly from data [14]. The idea
is to reconstruct the original user-item interaction matrix by the
item-based CF model. Afterwards, Kabbur et al. further proposes
FISM to explore the low-rank property of the learned similarity
matrix to handle data sparsity problem [10]. While FISM is shown
to outperform recommendation approaches, it has the limitation
in estimating only a single global metric for all users. To that end,
GLSLIM clusters the users and estimates an independent SLIM
model for every user subset [6], whereas the number of clusters
is difficult to determine, and thus the modeling of personalized
preferences is coarse-grained.

In addition to user-item interactions, many researchers attempt
to leverage additional information for recommendation, such as
user-item attributes and context information [2, 29]. FM is an earlier
general feature-based framework for recommendation, which is
suitable for sparse structured data [18], and it is recognized as the
most effective linear embedding method. Due to the recent huge
success of deep learning in many fields, some deep variants of
FM have been proposed to enhance the model‚Äôs representation
capacity, including AMF [26], DeepFM [7] and PNN [16]. In these
methods, the feature weights are the same for all the users and they
cannot capture the fine-grained user preferences, which are not
explainable.
Recently employing auxiliary information to help understand
user behaviors and provide explainable recommendations have become prevailing in the research field. Zhang et al. propose EFM [28],
where the basic idea is to align each latent dimension in matrix factorization with a particular explicit feature, and recommend items
that performs well on the features that users care about. Chen
et al. further extended the EFM model to tensor factorization afterwards [4]. On the other hand, McAuley and Leskovec propose
HFT to understand the hidden factors in latent factor models based
on the hidden topics extracted from textual reviews [12]. After
that, many probabilistic graphic model based methods have been
proposed for explainable recommendation [17, 25]. Recently, deep
learning and attention mechanism have attracted much attention in
the recommendation field, and they have also been wildly applied
for explainable recommendations. For example, Seo et al. leverage
attention mechanisms upon the user/item reviews to explore the
usefulness of reviews and with the learned attention weights, the
model can indicate which part is more important [21]. Chen et al.
propose VER which can highlight the image regions that a user
may be interested in as explanations [5]. Our work follows this
thread but mainly focuses on learning explanations through user
behavior data rather than text data.

7

CONCLUSIONS

In this paper, we propose a personalized item to item recommendation method eRAN. By formulating the co-purchased relationships
and item attributes as multiple attribute networks, eRAN combines
both views of recommendations. By plugging an attention mechanism in obtaining personalized item representation, eRAN gains
ability to derive the attractions of attributes to users and personalized item similarity simultaneously. Experiments on real-world
datasets demonstrate the superiority of our methods for recommendation tasks and cold-start items. Moreover, the learned user
embeddings and attention weights capture the fine-grained user
preferences on attribute level and guide the explanations for recommendations. Future work includes integrating multi-item relationships such as complementation and substitution into our model,
and seeking the influence of other different attention mechanisms.

REFERENCES
[1] Gediminas Adomavicius and Alexander Tuzhilin. 2005. Toward the next generation of recommender systems: A survey of the state-of-the-art and possible
extensions. IEEE Transactions on Knowledge & Data Engineering 6 (2005), 734‚Äì749.
[2] Linas Baltrunas, Bernd Ludwig, and Francesco Ricci. 2011. Matrix factorization
techniques for context aware recommendation. In Proceedings of the fifth ACM
conference on Recommender systems. ACM, 301‚Äì304.

[3] Tianqi Chen, Weinan Zhang, Qiuxia Lu, Kailong Chen, Zhao Zheng, and Yong
Yu. 2012. SVDFeature: a toolkit for feature-based collaborative filtering. Journal
of Machine Learning Research 13, Dec (2012), 3619‚Äì3622.
[4] Xu Chen, Zheng Qin, Yongfeng Zhang, and Tao Xu. 2016. Learning to rank
features for recommendation over multiple categories. In Proceedings of the 39th
International ACM SIGIR conference on Research and Development in Information
Retrieval. ACM, 305‚Äì314.
[5] Xu Chen, Yongfeng Zhang, Hongteng Xu, Yixin Cao, Zheng Qin, and Hongyuan
Zha. 2018. Visually Explainable Recommendation. arXiv preprint arXiv:1801.10288
(2018).
[6] Evangelia Christakopoulou and George Karypis. 2016. Local item-item models for top-n recommendation. In Proceedings of the 10th ACM Conference on
Recommender Systems. ACM, 67‚Äì74.
[7] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017.
Deepfm: a factorization-machine based neural network for ctr prediction. arXiv
preprint arXiv:1703.04247 (2017).
[8] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng
Chua. 2017. Neural collaborative filtering. In Proceedings of the 26th International
Conference on World Wide Web. International World Wide Web Conferences
Steering Committee, 173‚Äì182.
[9] Cheng Kang Hsieh, Longqi Yang, Yin Cui, Tsung Yi Lin, Serge Belongie, and
Deborah Estrin. 2017. Collaborative Metric Learning.
[10] Santosh Kabbur, Xia Ning, and George Karypis. 2013. Fism: factored item similarity models for top-n recommender systems. In Proceedings of the 19th ACM
SIGKDD international conference on Knowledge discovery and data mining. ACM,
659‚Äì667.
[11] David C Liu, Stephanie Rogers, Raymond Shiau, Dmitry Kislyuk, Kevin C Ma,
Zhigang Zhong, Jenny Liu, and Yushi Jing. 2017. Related pins at pinterest:
The evolution of a real-world recommender system. In Proceedings of the 26th
International Conference on World Wide Web Companion. International World
Wide Web Conferences Steering Committee, 583‚Äì592.
[12] Julian McAuley and Jure Leskovec. 2013. Hidden factors and hidden topics:
understanding rating dimensions with review text. In Proceedings of the 7th ACM
conference on Recommender systems. ACM, 165‚Äì172.
[13] Julian McAuley, Christopher Targett, Qinfeng Shi, and Anton Van Den Hengel.
2015. Image-based recommendations on styles and substitutes. In Proceedings
of the 38th International ACM SIGIR Conference on Research and Development in
Information Retrieval. ACM, 43‚Äì52.
[14] Xia Ning and George Karypis. 2011. Slim: Sparse linear methods for top-n
recommender systems. In 2011 11th IEEE International Conference on Data Mining.
IEEE, 497‚Äì506.
[15] Pentti Paatero and Unto Tapper. 2010. Positive matrix factorization: A nonnegative factor model with optimal utilization of error estimates of data values.
Environmetrics 5, 2 (2010), 111‚Äì126.
[16] Yanru Qu, Han Cai, Kan Ren, Weinan Zhang, Yong Yu, Ying Wen, and Jun Wang.
2016. Product-based neural networks for user response prediction. In Data Mining
(ICDM), 2016 IEEE 16th International Conference on. IEEE, 1149‚Äì1154.
[17] Zhaochun Ren, Shangsong Liang, Piji Li, Shuaiqiang Wang, and Maarten de Rijke.
2017. Social collaborative viewpoint regression with explainable recommendations. In Proceedings of the tenth ACM international conference on web search and
data mining. ACM, 485‚Äì494.
[18] Steffen Rendle. 2010. Factorization machines. In Data Mining (ICDM), 2010 IEEE
10th International Conference on. IEEE, 995‚Äì1000.
[19] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme.
2009. BPR: Bayesian personalized ranking from implicit feedback. In Proceedings
of the twenty-fifth conference on uncertainty in artificial intelligence. AUAI Press,
452‚Äì461.
[20] Badrul Sarwar, George Karypis, Joseph Konstan, and John Riedl. 2001. Item-based
collaborative filtering recommendation algorithms. In Proceedings of the 10th
international conference on World Wide Web. ACM, 285‚Äì295.
[21] Sungyong Seo, Jing Huang, Hao Yang, and Yan Liu. 2017. Interpretable convolutional neural networks with dual local and global attention for review rating
prediction. In Proceedings of the Eleventh ACM Conference on Recommender Systems. ACM, 297‚Äì305.
[22] Laurens van der Maaten and Geoffrey E. Hinton. 2008. Visualizing HighDimensional Data Using t-SNE. JMLR 9 (2008), 2579‚Äì2605.
[23] Mengting Wan and Julian McAuley. 2018. Item recommendation on monotonic
behavior chains. In Proceedings of the 12th ACM Conference on Recommender
Systems. ACM, 86‚Äì94.
[24] Daixin Wang, Peng Cui, and Wenwu Zhu. 2016. Structural deep network embedding. In Proceedings of the 22nd ACM SIGKDD international conference on
Knowledge discovery and data mining. ACM, 1225‚Äì1234.
[25] Yao Wu and Martin Ester. 2015. Flame: A probabilistic model combining aspect
based opinion mining and collaborative filtering. In Proceedings of the Eighth
ACM International Conference on Web Search and Data Mining. ACM, 199‚Äì208.
[26] Jun Xiao, Hao Ye, Xiangnan He, Hanwang Zhang, Fei Wu, and Tat-Seng Chua.
2017. Attentional factorization machines: Learning the weight of feature interactions via attention networks. arXiv preprint arXiv:1708.04617 (2017).

[27] Yongfeng Zhang and Xu Chen. 2018. Explainable Recommendation: A Survey
and New Perspectives. arXiv preprint arXiv:1804.11192 (2018).
[28] Yongfeng Zhang, Guokun Lai, Min Zhang, Yi Zhang, Yiqun Liu, and Shaoping
Ma. 2014. Explicit factor models for explainable recommendation based on
phrase-level sentiment analysis. In Proceedings of the 37th international ACM
SIGIR conference on Research & development in information retrieval. ACM, 83‚Äì92.
[29] Wayne Xin Zhao, Sui Li, Yulan He, Edward Y Chang, Ji-Rong Wen, and Xiaoming
Li. 2016. Connecting social media to e-commerce: Cold-start product recommendation using microblogging information. IEEE Transactions on Knowledge and
Data Engineering 28, 5 (2016), 1147‚Äì1159.
[30] Xin Wayne Zhao, Yanwei Guo, Yulan He, Han Jiang, Yuexin Wu, and Xiaoming
Li. 2014. We know what you want to buy: a demographic-based system for
product recommendation on microblogs. In Proceedings of the 20th ACM SIGKDD
international conference on Knowledge discovery and data mining. ACM, 1935‚Äì
1944.

