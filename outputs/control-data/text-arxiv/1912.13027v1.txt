All-or-Nothing Phenomena:
From Single-Letter to High Dimensions

arXiv:1912.13027v1 [math.ST] 30 Dec 2019

Galen Reeves

âˆ—

Jiaming Xuâ€ 

Ilias Zadik

â€¡

January 1, 2020

Abstract
We consider the linear regression problem of estimating a p-dimensional vector Î² from n observai.i.d.
tions Y = XÎ² + W , where Î²j âˆ¼ Ï€ for a real-valued distribution Ï€ with zero mean and unit variance,
i.i.d.
i.i.d.
Xij âˆ¼ N (0, 1), and Wi âˆ¼ N (0, Ïƒ 2 ). In the asymptotic regime where n/p â†’ Î´ and p/Ïƒ 2 â†’ snr for
two fixed constants Î´, snr âˆˆ (0, âˆž) as p â†’ âˆž, the limiting (normalized) minimum mean-squared error
(MMSE) has been characterized by the MMSE of an associated single-letter (additive Gaussian scalar)
channel.
In this paper, we show that if the MMSE function of the single-letter channel converges to a step
function, then the limiting MMSE of estimating Î² in the linear regression problem converges to a step
function which jumps from 1 to 0 at a critical threshold. Moreover, we establish that the limiting meansquared error of the (MSE-optimal) approximate message passing algorithm also converges to a step
function with a larger threshold, providing evidence for the presence of a computational-statistical gap
between the two thresholds.

1

Introduction

Consider the classical linear regression model
Y = XÎ² + W

(1)

i.i.d.

i.i.d.

where X âˆˆ RnÃ—p with Xij âˆ¼ N (0, 1), Î² âˆˆ Rp with Î²j âˆ¼ Ï€ for a distribution Ï€ with zero mean and unit
i.i.d.

variance, and W âˆˆ Rn with Wi âˆ¼ N (0, Ïƒ 2 ). We are interested in estimating Î² from observation of (X, Y ).
b
For a given estimator Î²(X,
Y ), the normalized mean squared-error of estimating Î² is given by


 
2
1
MSE Î²b := E Î² âˆ’ Î²b .
p
 
b or equivalently,
Let MMSE denote the minimum of MSE Î²b among all possible estimators Î²,
MMSE :=

i
1 h
2
E kÎ² âˆ’ E[Î² | X, Y ]k .
p

(2)

âˆ— G. Reeves is with the Department of Electrical and Computer Engineering and the Department of Statistical Science, Duke
University, Durham, NC 27708 USA; e-mail: galen.reeves@duke.edu. G. Reeves is is supported by NSF Grants CCF-1718494
and CCF-1750362.
â€  J. Xu is with the Fuqua School of Business, Duke University, Durham, NC 27708 USA; e-mail: jiamingxu.868@duke.edu.
J. Xu is is supported by NSF Grants IIS-1932630, CCF-1850743, and CCF-1856424.
â€¡ I. Zadik is with the Center for Data Science, New York University, New York, NY 10011 USA; e-mail: zadik@nyu.edu. I.
Zadik is supported by a CDS-Moore-Sloan postdoctoral fellowship.
This paper was presented at the 2019 IEEE International Workshop on Computational Advances in Multi-Sensor Adaptive
Processing (CAMSAP), Guadeloupe, French West Indies.

1

In this paper, we focus on the asymptotic regime:
n
â†’Î´
p

and

p
â†’ snr,
Ïƒ2

as p â†’ âˆž,

(3)

for two fixed constants
Î´, snr âˆˆ

 (0, âˆž).
 Note that Î´ is the under-sampling ratio and snr is the signal-to-noise
ratio in view of E kXÎ²k2 /E kW k2 = p/Ïƒ 2 .
Recent work [1â€“3] proves that under certain structural assumptions in terms of (Ï€, Î´, snr), the limiting
MMSE in the asymptotic regime (3) is characterized by the replica-symmetric (RS) formula through a single
letter channel
âˆš
(4)
y = sÎ²0 + N,
where s > 0, Î²0 âˆ¼ Ï€ and N âˆ¼ N (0, 1) are independent. However, often the RS formula is too complicated
to extract structural behavior of the limiting MMSE.
In this work, we propose generic conditions under which the limiting MMSE exhibits an all-or-nothing
phenomena. More precisely, consider a family (Ï€Ç« , Î´Ç« , snrÇ« ) indexed by a positive parameter Ç« where Ï€Ç« has
finite entropy HÇ« := H(Ï€Ç« ). We show that if the MMSE of the single letter channel (4) as a function of
s converges to a step function as Ç« â†’ 0, then the limiting MMSE of the linear regression model (1) also
converges to a step function, which jumps from 1 to 0 at a critical threshold Î´Ç« = Î´Ç«,MMSE , where
Î´Ç«,MMSE :=

2HÇ«
.
log(1 + snrÇ« )

(5)

In other words, an all-or-nothing phenomena in the single letter channel implies an all-or-nothing phenomena
in the high-dimensional linear regression model. Moreover, we establish that the limiting MSE of the (MSEoptimal) approximate message passing (AMP) algorithm also converges to a step function, which jumps from
1 to 0 at a larger threshold Î´Ç« = Î´Ç«,AMP , where
Î´Ç«,AMP :=

2HÇ« (1 + snrÇ« )
.
snrÇ«

(6)
i.i.d.

An important application of our general result is the binary linear regression model where Î²j âˆ¼ Bern(Ç«).
In this case, we show that the MMSE function of the single letter channel converges to a step function as
the sparsity Ç« â†’ 0. Then we obtain from our general result that the limiting MMSE of the binary linear
2Ç« log(1/Ç«)
.
regression model converges to a step function which jumps from 1 to 0 at the critical threshold Î´Ç« = log(1+snr
Ç«)
This coincides with the all-or-nothing phenomenon established in [4] for the binary linear regression model
where Î² is chosen uniformly at random from the set of binary k-sparse vectors, in the highly sparse and high
âˆš
signal-to-noise ratio regime where k/ p â†’ 0 and k/Ïƒ 2 is above a sufficiently large constant. Furthermore,
we deduce from our general result that the limiting MSE of the (MSE-optimal) AMP converges to a step
Ç«)
. This coincides with the
function which jumps from 1 to 0 at the critical threshold Î´Ç« = 2Ç« log(1/Ç«)(1+snr
snrÇ«
computational threshold for a number of computationally efficient methods in the literature such as LASSO
or Orthogonal Matching Pursuit. In particular, our result adds to the existing evidence for the presence of a
computational-statistical gap between the two thresholds (see [5,6] for an extended discussion and literature
review on the presence of this computational-statistical gap).

2
2.1

Preliminaries
The Replica Symmetric Formulas

To describe the RS formulas, we first define the mutual information and MMSE functions for the single letter
channel (4):
âˆš
(7)
I(s) := I(Î²0 ; sÎ²0 + N ), s > 0
2

M (s) := mmse(Î²0 |

âˆš
sÎ²0 + N ),

s>0

(8)

where Î²0 âˆ¼ Ï€ and N âˆ¼ N (0, 1) are independent. Both of these functions are non-negative and the unit
variance assumption on Ï€ means that for any s > 0, (see [7] for details)
1
s
log(1 + s) â‰¤ ,
2
2
1
M (s) â‰¤
â‰¤ 1.
1+s
I(s) â‰¤

(9)
(10)

Moreover, the I-MMSE relation for the single-letter channel [7] states the derivative of the mutual information
is one half the MMSE, that is I â€² (s) = 12 M (s).
Next, we define the potential function F : [0, âˆž) â†’ [0, âˆž) according to
Î´  s 
F (s) := I(s) + Ï†
,
2 Î´ snr

(11)

where Ï†(x) = x âˆ’ log x âˆ’ 1, and Î´, snr are respectively the undersampling ratio and the signal-to-noise ratio
of our original model. Note that Ï†(x) is convex and non-negative on (0, âˆž).
Lemma 1. All stationary points of F (s) lie on the open interval between Î´ snr /(1 + snr) and Î´ snr.
Proof. By differentiation with respect to s and the I-MMSE relation for the single-letter channel [7], we have
that for any s > 0
F â€² (s) âˆ M (s) + 1/ snr âˆ’Î´/s.
The fact that M (s) > 0 for all s implies that F â€² (s) is strictly positive for all s â‰¥ Î´ snr, and thus F (s) is
strictly increasing on [Î´ snr, âˆž). Alternatively, the fact that M (s) < 1 for all s > 0 implies that F â€² (s) is
strictly negative for all s â‰¤ Î´ snr /(1 + snr), and thus F (s) is strictly decreasing on (0, Î´ snr /(1 + snr)].
In view of Lemma 1, the minimum of the potential function and the smallest and largest minimizers can
be defined as follows:
F âˆ— := min F (s),

(12)

s

âˆ—

âˆ—

(13)

âˆ—

âˆ—

(14)

s := min{s : F (s) = F },

s := max{s : F (s) = F }.

Note that sâˆ— = sâˆ— if and only if the minimum is attained at a unique point.
Proposition 2 (RS MMSE [2,3,8]). For any (Î´, snr, Ï€) for which (snr, Ï€) satisfies the single-crossing property
[2] and Ï€ has finite fourth moment 1 , the mutual information and MMSE satisfy
1
I(Î²; X, Y ) = F âˆ— ,
p
i
1 h
2
lim sup E kÎ² âˆ’ E[Î² | y, X]k â‰¤ M (sâˆ— ),
pâ†’âˆž p
i
1 h
2
lim inf E kÎ² âˆ’ E[Î² | y, X]k â‰¥ M (sâˆ— ),
pâ†’âˆž p
lim

pâ†’âˆž

(15)
(16)
(17)

where the limits are taken as (n = np , p, Ïƒ 2 = Ïƒp2 ) scale to infinity with p â†’ +âˆž, n/p â†’ Î´ and p/Ïƒ 2 â†’ snr.
1A

different set of assumptions on (Î´, snr, Ï€) for which the Proposition 2 holds can be found in [3, 8]

3

Next, we turn to the family of approximate message passing (AMP) [9, 10] algorithms and specifically
to the case of MMSE-AMP which is proven in to be optimal among AMP algorithms in minimizing the
MSE of the recovery problem of interest [11]. For simplicity from now on when we say AMP we refer to the
MMSE-AMP algorithm. It turns out that a related formula to the one given in Proposition 2 describes the
asymptotic MSE associated with AMP.
The smallest stationary point is defined as
sAMP := inf{s : F â€² (s) = 0}.

(18)

It is rather straightforward to check that sAMP is attained by some positive value s and therefore it is a
stationary point of F (s). In particular, by Lemma 1 we have sAMP âˆˆ (Î´ snr /(1 + snr), Î´ snr).
For the next result, for T âˆˆ N let Î²bAMP,T (Y, X) be the output of the AMP estimator [11, Section II.C]
with input data (Y, X) after T iterations.

Proposition 3 (AMP, [10, 11]). For any (Î´, snr, Ï€) where Ï€ has a finite fourth moment, AMP satisfies


2
1
b
lim lim
E Î² âˆ’ Î²AMP,T (Y, X)
= M (sAMP )
(19)
T â†’+âˆž pâ†’+âˆž p

where the inner limit is taken as (n = np , p, Ïƒ 2 = Ïƒp2 ) scale to infinity with p â†’ +âˆž, n/p â†’ Î´ and p/Ïƒ 2 â†’ snr.
Remark 1. The results stated above imply that AMP is optimal whenever sâˆ— = sâˆ— = sAMP .
Remark 2. For a proof of Proposition 3 we refer the reader to the statement and proof of [11, Theorem 6].

3

Main Results

Let us consider now a family of coefficient distributions (Ï€Ç« )Ç«>0 indexed by a positive-valued parameter Ç« > 0.
We assume throughout the section that for each Ç« > 0 the distributions Ï€Ç« has zero mean, unit variance and
finite entropy HÇ« . Our results are all based on the following assumption on the family Ï€Ç« .
Assumption 1. Let (Ï€Ç« )Ç«>0 be a family of distributions with unit variance and finite entropy HÇ« . The
MMSE function MÇ« (s) of the single letter channel, as defined in (8), for Ï€Ç« coefficient distribution is assumed
to converge pointwise to a step function as Ç« â†’ 0 in the following sense
(
1, t âˆˆ [0, 1)
lim MÇ« (2HÇ« t) =
(20)
Ç«â†’0
0, t âˆˆ (1, âˆž).
Remark 3. It can be straightforwardly checked using the I-MMSE relation for the single-letter channel [7]
that the rescaling in the argument of MÇ« by twice the entropy term, i.e. by 2HÇ« , is necessary for the
convergence of MÇ« to the
R âˆžstep function. To see why, observe that MÇ« (s), using the I-MMSE relation, satisfies
the integral constraint 0 MÇ« (2HÇ« t) dt = 1. Thus, convergence to a step function at a threshold other than
2HÇ« would violate this constraint.
Remark 4. As we establish later in the section, Assumption 1 is satisfied for the family of (normalized)
Bernoulli distributions with probability Ç«. See Fig. 1 for a graphical illustration. We expect the assumption
to hold under greater generality.
We now present our two main results assuming the family of distributions (Ï€Ç« )Ç«>0 satisfies Assumption
1.
Theorem 4. Let (Ï€Ç« )Ç«>0 satisfy Assumption 1. Given a number r âˆˆ (0, 1) âˆª (1, âˆž), let (Î´Ç« , snrÇ« , Ï€Ç« )Ç«>0 be a
family of triplets such that
lim

Ç«â†’0

Î´Ç«
Î´Ç«,MMSE
4

= r.

(21)

IÇ« (2HÇ« t)/HÇ«

MÇ« (2HÇ« t)
Ç« = 1e-01
Ç« = 1e-02

1

1

Ç« = 1e-04
Ç« = 1e-08
Ç« = 1e-16
Ç« = 1e-32

Ç« = 1e-01
Ç« = 1e-02
Ç« = 1e-04
Ç« = 1e-08
Ç« = 1e-16
Ç« = 1e-32

0

0

0

1

0

1

t

t

Figure 1: Single letter mutual information and MMSE functions corresponding to the Bernoulli(Ç«) distribution normalized to unit variance.
Then, the minimizers of the RS potential function exhibit the all-or-nothing behavior in the small-Ç« limit
depending on whether r is greater than or less than one:
r âˆˆ (0, 1)

=â‡’

r âˆˆ (1, âˆž)

=â‡’

lim MÇ« (sâˆ—Ç« ) = 1

(22)

lim MÇ« (sâˆ—Ç« ) = 0.

(23)

Ç«â†’0

Ç«â†’0

Combining Theorem 4 with Proposition 2 we obtain the following Corollary.
Corollary 5 (All-or-nothing MMSE behavior). Let r âˆˆ (0, 1)âˆª(1, âˆž). For any family of triplets (Î´Ç« , snrÇ« , Ï€Ç« )Ç«>0 ,
suppose that for any Ç« > 0, (snrÇ« , Ï€Ç« ) satisfies the single-crossing property, Ï€Ç« has finite fourth moment,
(Ï€Ç« )Ç«>0 satisfies Assumption 1, and (21) holds. Then it holds that
(
i
1, r âˆˆ [0, 1)
1 h
2
lim lim E kÎ² âˆ’ E[Î² | X, Y ]k =
(24)
Ç«â†’0 pâ†’âˆž p
0, r âˆˆ (1, âˆž).
where the inner limits are taken as (n = np , p, Ïƒ 2 = Ïƒp2 ) scale to infinity with p â†’ +âˆž, n/p â†’ Î´Ç« and
p/Ïƒ 2 â†’ snrÇ« .
We next present our second main result.
Theorem 6. Let (Ï€Ç« )Ç«>0 satisfy Assumption 1. Given a number r âˆˆ (0, 1) âˆª (1, âˆž), let (Î´Ç« , snrÇ« , Ï€Ç« ) be such
that
lim

Ç«â†’0

Î´Ç«
Î´Ç«,AMP

= r.

(25)

Then, the smallest stationary point sAMP exhibits the all-or-nothing behavior in the small-Ç« limit depending
on whether r is greater than or less than one:

r âˆˆ (0, 1)
=â‡’
lim MÇ« sAMP
=1
(26)
Ç«
Ç«â†’0

= 0.
(27)
r âˆˆ (1, âˆž)
=â‡’
lim MÇ« sAMP
Ç«
Ç«â†’0

5

For the result, for T âˆˆ N let Î²bAMP,T (Y, X) the output of the AMP estimator [11, Section II.C] with input
data (X, Y ) after T iterations. Combining Theorem 6 with Proposition 3 we obtain the following Corollary
on the performance of AMP.
Corollary 7 (All-or-nothing AMP behavior). Let r âˆˆ (0, 1)âˆª(1, âˆž). For any family of triplets (Î´Ç« , snrÇ« , Ï€Ç« )Ç«>0 ,
suppose that each Ï€Ç« has a finite fourth moment, the family (Ï€Ç« )Ç«>0 satisfies Assumption 1, and (25) holds.
Then it holds that

 (
2
1
1, r âˆˆ [0, 1)
b
lim lim lim E Î² âˆ’ Î²AMP,T (X, Y )
=
(28)
pâ†’âˆž
Ç«â†’0 T â†’+âˆž
p
0, r âˆˆ (1, âˆž).
where the inner limits are taken as (n = np , p, Ïƒ 2 = Ïƒp2 ) scale to infinity with p â†’ +âˆž, n/p â†’ Î´Ç« and
p/Ïƒ 2 â†’ snrÇ« .

3.1

Illustration of Theorems 4 and 6 via normalized potential function

To provide insight into the behavior described by Theorems 4 and 6, we consider the scaling Î´Ç« = 2rHÇ« / log(1+
snr) where the pair (snr, r) is considered fixed with respect to Ç« > 0. Note that in this scaling, Î´Ç« = rÎ´Ç«,MMSE .
Define the normalized potential function


F (2HÇ« t)
IÇ« (2HÇ« t)
r
t log(1 + snr)
FeÇ«,r (t) :=
.
(29)
=
+
Ï†
HÇ«
HÇ«
log(1 + snr)
r snr
Note that only the first term depends on Ç«. Under Assumption 1, using the I-MMSE relation we have
IÇ« (2HÇ« t)/HÇ« â†’ 1 âˆ§ t as Ç« â†’ 0. Thus for all t > 0,
lim FeÇ«,r (t) = Fe0,r (t),

Ç«â†’0

where

Fe0,r (t) := 1 âˆ§ t +



t log(1 + snr)
r
.
Ï†
log(1 + snr)
r snr

(30)

(31)

For r âˆˆ (0, 1) âˆª (1, âˆž), the function Fe0,r (t) has a unique global minimizer given by
tâˆ— =

ï£±
r snr
ï£´
ï£² (1 + snr) log(1 + snr) ,
ï£´
ï£³

r snr
log(1 + snr)

r âˆˆ (0, 1)

(32)

r âˆˆ (1, âˆž).

Importantly, using the elementary inequality x/(x + 1) â‰¤ log(1 + x) â‰¤ x for all x > âˆ’1, we deduce
r âˆˆ (0, 1)
r âˆˆ (1, âˆž)

=â‡’
=â‡’

tâˆ— âˆˆ (0, 1)
tâˆ— âˆˆ (1, âˆž).

This dichotomy together with Assumption 1 and equality (30) suggest the following all-or-nothing behavior;
as Ç« â†’ 0, when r < 1 the MMSE converges to one and when r > 1 the MMSE converges to zero. This is the
same all-or-nothing behavior described and rigorously established in Theorem 4. Furthermore, the smallest
stationary point tAMP is given by
ï£±
r snr
AMP
ï£´
)
ï£² (1 + snr) log(1 + snr) , r âˆˆ (0, r
AMP
t
=
(33)
r snr
ï£´
ï£³
,
r âˆˆ (rAMP , âˆž),
log(1 + snr)
6

where rAMP := Î´Ç«,AMP /Î´Ç«,MMSE = (1 + 1/ snr) log(1 + snr). Similar to above, we deduce
r âˆˆ (0, rAMP )

r âˆˆ (rAMP , âˆž)

=â‡’
=â‡’

tAMP âˆˆ (0, 1)

tAMP âˆˆ (1, âˆž).

This dichotomy together with Assumption 1 and equality (30), suggest the following all-or-nothing behavior;
as Ç« â†’ 0, when r < rAMP the MSE of the AMP converges to one and when r > rAMP the MSE of the
AMP converges to zero. This is the same all-or-nothing behavior described and rigorously established in
Theorem 6.
This behavior of the normalized potential function is illustrated graphically in Figure 2 where both FeÇ«,r (t)
and Fe0,r (t) are plotted as a function of t for various r.

4

Application: Sparse binary regression

i.i.d.

We now present our main application of our two results to sparse binary regression, where Î²i âˆ¼ Bern(Ç«). To
this end, we first consider the case where Î²i is i.i.d. drawn from the following two-point distribution:
Ï€Ç« = (1 âˆ’ Ç«) Î´Âµ1 + Ç« Î´Âµ2 ,

(34)

p
p
where Î´x denotes a Dirac distribution with mass at x âˆˆ R, and Âµ1 = âˆ’ Ç«/(1 âˆ’ Ç«) and Âµ2 = (1 âˆ’ Ç«)/Ç« are
chosen such that Ï€Ç« has zero mean and unit variance. The following Lemma holds for the family of MMSE
functions (MÇ« (s))Ç«>0 :
Lemma 8. The distribution Ï€Ç« in (34) has entropy HÇ« = âˆ’Ç« log Ç« âˆ’ (1 âˆ’ Ç«) log(1 âˆ’ Ç«) and MMSE function
ï£¹
ï£®
ï£¯
MÇ« (s) = Eï£¯
ï£°

ï£º
1
ï£º

q
ï£»,
s
s
1 âˆ’ Ç« + Ç« exp 2Ç«(1âˆ’Ç«) + Ç«(1âˆ’Ç«) N

(35)

where N âˆ¼ N (0, 1). Furthermore, the distribution Ï€Ç« satisfies the single-crossing condition [2] for all snr > 0
and

where Q(z) =

Râˆž
z

lim HÇ« /(Ç« log 1/Ç«) = 1
Ç«â†’0


s âˆ’ 2Ç« log(1/Ç«)
âˆš
lim sup MÇ« (s) âˆ’ Q
= 0,
Ç«â†’0 s>0
2 sÇ«

(36)
(37)

(2Ï€)âˆ’1/2 exp(âˆ’t2 /2) dt.

An immediate implication of the result is that the family of distributions (Ï€Ç« )Ç«>0 satisfies Assumption 1
as well as the conditions of Corollaries 5 and 7. Hence, all-or-nothing phase transitions hold for the limiting
MMSE around Î´Ç«,MMSE and for the MSE of the AMP around Î´Ç«,AMP . Using that limÇ«â†’0 HÇ« /(Ç« log 1/Ç«) = 1,
we can further simplify the phase transition points given in (5), (6) by observing


2Ç« log(1/Ç«)
=1
(38)
lim Î´Ç«,MMSE /
Ç«â†’0
log(1 + snrÇ« )
and


2(1 + snrÇ« )Ç« log(1/Ç«)
lim Î´Ç«,AMP /
Ç«â†’0
snrÇ«

7



= 1.

(39)

normalized potential function FeÇ«,r (t) with snr = 5 and Ç« = 10âˆ’16

2.5

2

1.5

1

0.5
global minimizer
smallest stationary point
0

0

1

2

3

4

5
t

6

7

8

9

10

normalized potential function Fe0,r (t) with snr = 5

2.5

2

1.5

1

0.5
global minimizer
smallest stationary point
0

0

1

2

3

4

5
t

6

7

8

9

10

Figure 2: Illustration of the normalized potential functions, FeÇ«,r (t) for Ç« = 10âˆ’16 (top plot) and
Fe0,r (t)(bottom plot), where snr = 5 and r varies in (0, +âˆž). Black-colored curves correspond to r âˆˆ (0, 1),
magenta-colored curves correspond to r âˆˆ (1, rAMP ) and cyan-colored curves correspond to r âˆˆ (rAMP , âˆž).
Note that the global minimizer of Fe0,r (t) transitions from being less than one to bigger than one exactly at
r = 1, while the smallest stationary point transitions from being less than one to bigger than one exactly at
r = rAMP . A similar approximate behavior takes place for FeÇ«,r (t) with Ç« = 10âˆ’16 .

8

i.i.d.

Next, we extend the above results to the sparse binary regression problem of interest, where Î²i âˆ¼ Bern(Ç«).
We denote by k = Ç«p the (expected) number of non-zero coordinates of Î². Define
Î² âˆ’ E[Î²]
Î²e = p
.
Ç«(1 âˆ’ Ç«)

i.i.d.
Then Î²ei âˆ¼ Ï€Ç« as given in (34). Moreover, define

Y âˆ’ XE[Î²]
Ye = p
,
Ç«(1 âˆ’ Ç«)

f= p W
W
.
Ç«(1 âˆ’ Ç«)

f . Since W
fi i.i.d.
âˆ¼ N (0, Ïƒ
e2 ) with Ïƒ
e = Ïƒ/
Then it follows that Ye = X Î²e + W
snrÇ« =

pÇ«(1 âˆ’ Ç«)
p
=
.
Ïƒ
e2
Ïƒ2

p
Ç«(1 âˆ’ Ç«), it follows that

(40)

Hence, according to Corollary 5, (38), (40), we obtain that the limiting MMSE exhibits an all-or-nothing
behavior at
Î´Ç«,MMSE = 2Ç« log(1/Ç«)/ log(1 + Ç«(1 âˆ’ Ç«)p/Ïƒ 2 ),
which using k = Ç«p as Ç« â†’ 0 simplifies with negligible multiplicative error to
Î´Ç«,MMSE = 2(k/p) log(p/k)/ log(1 + k/Ïƒ 2 ).
Note that this is the exact information-theoretic threshold for which an all-or-nothing phenomenon has been
proven to hold when lim supp log k/ log p < 0.5 in [4].
Similarly, according to Corollary 7, (39), and (40), the limiting MSE of the AMP exhibits an all-or-nothing
behavior at:


Ïƒ2
Î´Ç«,AMP = 2 1 +
Ç« log(1/Ç«),
pÇ«(1 âˆ’ Ç«)

which using k = Ç«p as Ç« â†’ 0 simplifies with negligible multiplicative error to

Î´Ç«,AMP = 2 k + Ïƒ 2 log(p/k)/p.

Note that this is the exact computational threshold for a number of computationally efficient methods in
the literature such as LASSO or Orthogonal Matching Pursuit (see [5, 6] for references). Our result suggests
that the threshold corresponds to a barrier also for AMP in a strong sense.

References
[1] Dongning Guo and S. Verdu, â€œRandomly spread cdma: asymptotics via statistical physics,â€ IEEE
Transactions on Information Theory, vol. 51, no. 6, pp. 1983â€“2010, June 2005.
[2] G. Reeves and H. D. Pfister, â€œThe replica-symmetric prediction for random linear estimation with
Gaussian matrices is exact,â€ IEEE Trans. Inform. Theory, vol. 65, no. 4, pp. 2252â€“2283, Apr. 2019.
[3] J. Barbier, M. Dia, N. Macris, and F. Krzakala, â€œThe mutual information in random linear estimation,â€
in Proc. Annual Allerton Conf. on Commun., Control, and Comp., Monticello, IL, 2016.
[4] G. Reeves, J. Xu, and I. Zadik, â€œThe all-or-nothing phenomenon in sparse linear regression,â€ in Conference On Learning Theory (COLT), 2019, [Online]. Available https://arxiv.org/abs/1903.05046.
[5] D. Gamarnik and I. Zadik, â€œHigh dimensional regression with binary coefficients. estimating squared
error and a phase transtition,â€ in COLT, 2017.
9

[6] â€”â€”, â€œSparse high-dimensional linear regression. algorithmic barriers and a local search algorithm,â€
2017, [Online]. Available https://arxiv.org/abs/1711.04952.
[7] Dongning Guo, S. Shamai, and S. Verdu, â€œMutual information and mmse in gaussian channels,â€ in
International Symposium onInformation Theory, 2004. ISIT 2004. Proceedings., June 2004, pp. 349â€“
349.
[8] J. Barbier, F. Krzakala, N. Macris, L. Miolane, and L. ZdeborovaÌ, â€œOptimal errors and phase transitions
in high-dimensional generalized linear models,â€ Proceedings of the National Academy of Sciences, vol.
116, no. 12, pp. 5451â€“5460, 2019. [Online]. Available: https://www.pnas.org/content/116/12/5451
[9] D. L. Donoho, A. Maleki, and A. Montanari, â€œMessage-passing algorithms for compressed sensing,â€
Proceedings of the National Academy of Sciences, vol. 106, no. 45, pp. 18 914â€“18 919, Nov. 2009.
[10] M. Bayati and A. Montanari, â€œThe dynamics of message passing on dense graphs, with applications to
compressed sensing,â€ IEEE Trans. Inform. Theory, vol. 57, no. 2, pp. 764â€“785, Feb. 2011.
[11] G. Reeves and M. Gastpar, â€œThe sampling rate-distortion tradeoff for sparsity pattern recovery in
compressed sensing,â€ IEEE Trans. Inf. Theor., vol. 58, no. 5, pp. 3065â€“3092, May 2012. [Online].
Available: http://dx.doi.org/10.1109/TIT.2012.2184848

A

Properties of RS and AMP Formulas under finite entropy

This section describes some properties of the potential function F (s) defined in (11), such as its minimum
value F âˆ— , the upper and lower minimizers (sâˆ— , sâˆ— ), and the smallest stationary point sAMP , as a function
of the problem parameters (Î´, snr, Ï€). Throughout, we make the additional assumption that Ï€ is a discrete
distribution with finite entropy H = H(Ï€).
Lemma 9. The mutual information function I(s) of the single-letter channel under input distribution Ï€
defined in (7) satisfies
ns o
ns o
(41)
min , H âˆ’ L â‰¤ I(s) â‰¤ min , H
2
2
for all s âˆˆ [0, âˆž), where

L := H âˆ’ I(2H).

(42)

Proof. Define A(s) := s/2 âˆ’ I(s). By the I-MMSE relation and the assumption of the unit variance, the
derivative Aâ€² (s) = (1 âˆ’ M (s))/2 is nonnegative, and thus 0 â‰¤ s â‰¤ 2H implies that A(0) â‰¤ A(s) â‰¤ A(2H).
Noting that A(0) = 0 and A(2H) = L yields 0 â‰¤ s/2 âˆ’ I(s) â‰¤ L for all s âˆˆ [0, 2H]. Meanwhile, the mutual
information also satisfies the upper bound I(s) â‰¤ H for all s. Finally, because I(s) is non-decreasing, s â‰¥ 2H
implies that I(s) â‰¥ I(2H) = H âˆ’ L. Combining these inequalities gives the stated result.

A.1

Minimizer of potential function

We now consider upper and lower bounds on the minimizers of the potential function F (s) defined in (11).
The basic idea behind our approach is to use the following simple relations for the minimizers of F (s):
min F (s) > min F (s)

=â‡’

sâˆ— > t

(43)

min F (s) < min F (s)

=â‡’

sâˆ— < t.

(44)

sâˆˆ(0,t]
sâˆˆ(0,âˆž)

sâˆˆ(0,âˆž)

sâˆˆ[t,âˆž)

Lemma 10. For any t âˆˆ [2H, âˆž),
Î´>

t + 2L
log(1 + snr)
10

=â‡’

sâˆ— > t.

(45)

Proof. Observe that Ï†(x) is convex and thus Ï†(y) â‰¥ Ï†(x) + (y âˆ’ x)Ï†â€² (x) for all x, y âˆˆ (0, âˆž). Noting that
Ï†â€² (x) = 1 âˆ’ 1/x and evaluating with y = s/(Î´ snr) and x = 1/(1 + snr) leads to
 s 
s
â‰¥ log(1 + snr) âˆ’ .
(46)
Ï†
Î´ snr
Î´
Using this inequality to lower bound the potential function, we have


Î´
s
min F (s) â‰¥ min I(s) + log(1 + snr) âˆ’
sâˆˆ[0,t)
sâˆˆ[0,t)
2
2
Î´
t
â‰¥ H âˆ’ L + log(1 + snr) âˆ’ > H,
2
2

(47)
(48)

where the second step follows from Lemma 9 and the assumption t â‰¥ 2H, and the last step follows from the
assumption on Î´. Meanwhile, using the upper bound I(s) â‰¤ H, we have


Î´  s 
= H.
(49)
min F (s) â‰¤ min H + Ï†
2 Î´ snr
sâˆˆ(0,âˆž)
sâˆˆ(0,âˆž)
Thus, we can conclude that the minimum is not attained in the interval (0, t].
Lemma 11. For any t âˆˆ (0, 2H],
Î´<

t âˆ’ 2L
log(1 + snr)

sâˆ— < t.

=â‡’

(50)

Proof. Noting that Ï†(x) â‰¥ 0, we have
min F (s) â‰¥ min I(s) â‰¥ I(t) â‰¥

sâˆˆ[t,âˆž)

sâˆˆ[t,âˆž)

t
Î´
âˆ’ L > log(1 + snr),
2
2

where the third step follows from Lemma 9 for s = t and the assumption t â‰¤ 2H, and the last step follows
from the assumption on Î´. Meanwhile, using the upper bound I(s) â‰¤ s/2, we have


Î´
s Î´  s 
= log(1 + snr).
+ Ï†
(51)
min F (s) â‰¤ min
2 Î´ snr
2
sâˆˆ(0,âˆž)
sâˆˆ(0,âˆž] 2

Thus, we can conclude that the minimum is not attained in the interval [t, âˆž).

A.2

Smallest stationary point of the potential function

Lemma 12. For any t âˆˆ (0, âˆž),



1
Î´ < sup s M (s) +
snr
sâˆˆ(0,t]


1
Î´ > sup s M (s) +
snr
sâˆˆ(0,t]

=â‡’

sAMP < t

(52)

=â‡’

sAMP > t.

(53)

Proof. By differentiation and the I-MMSE relation, one finds that every stationary point of F (s) satisfies
the fixed-point equation M (s) âˆ’ Î´/s + 1/ snr = 0. Rearranging and solving for Î´, we see that s is a stationary
point if and only if Î´FP (s) = Î´ where


1
.
(54)
Î´FP (s) := s M (s) +
snr
The function Î´FP (s) is continuous with Î´FP (0) = 0. Therefore, if Î´ < supsâˆˆ(0,t] Î´FP (s) then the equation Î´FP (s) = Î´ has at least one solution on [0, t), and this implies that sAMP < t. Conversely, if Î´ >
supsâˆˆ(0,t] Î´FP (s) then there is no solution on [0, t) and this implies that sAMP > t.
11

B
B.1

Proofs of Main Results
Proof of Theorem 4

First we show that Assumption 1 implies that LÇ« /HÇ« â†’ 0, where we recall that LÇ« = HÇ« âˆ’ IÇ« (2HÇ« ). To see
why, observe that
1
LÇ« =
2

Z

0

2HÇ«

(1 âˆ’ M (s)) ds = HÇ«

Z

0

1

(1 âˆ’ M (2HÇ« t)) dt.

(55)

For any Î· âˆˆ (0, 1), the integral on the right can be upper bounded as
Z

0

1

(1 âˆ’ M (2HÇ« t)) dt â‰¤

Z

1âˆ’Î·

0

(1 âˆ’ M (2HÇ« t)) dt + Î·.

(56)

By Assumption 1, the first term on the right-hand side converges to zero in the small-Ç« limit. Noting that Î·
can be chosen arbitrarily small establishes that LÇ« /HÇ« â†’ 0.
We are now ready to consider the case r âˆˆ (0, 1). Fix Î· such that r < 1 âˆ’ Î· < 1 and let tÇ« = (1 âˆ’ Î·/2)2HÇ«.
For all Ç« sufficiently small, we have Î·HÇ« > 2LÇ« and thus tÇ« âˆ’ 2LÇ« â‰¥ (1 âˆ’ Î·)2HÇ« > r2HÇ« . Under the assumed
scaling in (21), this means that Î´Ç« â‰¤ (tÇ« âˆ’ 2LÇ« )/ log(1 + snrÇ« ) for all Ç« small enough. By Lemma 11, this
implies that sâˆ— < (1 âˆ’ Î·/2)2HÇ« and by Assumption 1, it follows that MÇ« (sâˆ—Ç« ) â†’ 1. The case r âˆˆ (1, âˆž)
follows from a similar argument and is omitted.

B.2

Proof of Theorem 6

Consider the case r âˆˆ (0, 1). Fix Î· such that r < (1 âˆ’ Î·)2 < 1 and let tÇ« = (1 âˆ’ Î·)2HÇ« . By (20), for all Ç«
sufficiently small, we have MÇ« (s) â‰¥ 1 âˆ’ Î· for all s âˆˆ [0, tÇ« ], and thus




1
1
â‰¥ sup s 1 âˆ’ Î· +
(57)
sup s MÇ« (s) +
snrÇ«
snrÇ«
sâˆˆ[0,tÇ« ]
sâˆˆ[0,tÇ« ]


1
= (1 âˆ’ Î·)2HÇ« 1 âˆ’ Î· +
(58)
snrÇ«


1
.
(59)
> (1 âˆ’ Î·)2 2HÇ« 1 +
snrÇ«
The scaling (25) combined with the assumption r < (1 âˆ’ Î·)2 means that, for all Ç« small enough,


1
.
Î´Ç« < sup s MÇ« (s) +
snrÇ«
sâˆˆ[0,tÇ« ]

(60)

By Lemma 12, this implies that sAMP
< tÇ« and by (20), this means that MÇ« (sAMP
) â†’ 1. The case r âˆˆ (1, âˆž)
Ç«
Ç«
follows from a similar argument and is omitted.

12

