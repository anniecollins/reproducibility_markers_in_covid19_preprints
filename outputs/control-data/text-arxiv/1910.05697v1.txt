Generalization Bounds for Neural Networks via Approximate
Description Length
Amit Danielyâˆ—

Elad Granotâ€ 

arXiv:1910.05697v1 [cs.LG] 13 Oct 2019

October 15, 2019

Abstract
We investigate the sample complexity of networks with bounds on the magnitude of its
weights. In particular, we consider the class
N = {Wt â—¦ Ï â—¦ Wtâˆ’1 â—¦ Ï . . . â—¦ Ï â—¦ W1 : W1 , . . . , Wtâˆ’1 âˆˆ MdÃ—d , Wt âˆˆ M1,d }
where the spectral norm of each Wi is bounded by O(1), the Frobenius norm is bounded by
ex
x
R, and Ï is the sigmoid function 1+e
x or the smoothened ReLU function ln (1 + e ). We show
 2
that for any depth t, if the inputs are in [âˆ’1, 1]d , the sample complexity of N is OÌƒ dR
. This
2
bound
up to log-factors, and substantially improves over the previous state of the art
 is2 optimal

2
of OÌƒ d R
,
that
was established in a recent line of work [7, 4, 8, 5, 2, 9].
2
We furthermore show that this bound remains valid if instead of considering the magnitude
of the Wi â€™s, we consider the magnitude of Wi âˆ’ Wi0 , where Wi0 are some reference matrices,
with spectral norm of O(1). By taking the Wi0 to be the matrices at the onset of the training
process, we get sample complexity bounds that are sub-linear in the number of parameters, in
many typical regimes of parameters.
To establish our results we develop a new technique to analyze the sample complexity of
families H of predictors. We start by defining a new notion of a randomized approximate
description of functions f : X â†’ Rd . We then show that if there is a way to approximately
describe functions in a class H using d bits, then d2 examples suffices to guarantee uniform
convergence. Namely, that the empirical loss of all the functions in the class is -close to the
true loss. Finally, we develop a set of tools for calculating the approximate description length
of classes of functions that can be presented as a composition of linear function classes and
non-linear functions.

âˆ—
â€ 

School of Computer Science and Engineering, The Hebrew University, Jerusalem, and Google Research, Tel-Aviv.
School of Computer Science and Engineering, The Hebrew University, Jerusalem

Contents
1 Introduction

1

2 Preliminaries
2.1 Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2 Uniform Convergence and Covering Numbers . . . . . . . . . . . . . . . . . . . . . .
2.3 Basic Inequalities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2
2
2
4

3 Simplified Approximate Description Length
3.1 Linear Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.2 Simplified Depth 2 Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4
6
8

4 Approximation Description Length
11
4.1 Linear Operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
4.2 Non-Linear Operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
5 Sample Complexity of Neural Networks
17
5.1 Proof of Theorem 5.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
6 Proof of theorem 5.3
18
6.1 Shattering with Quadratic Activation . . . . . . . . . . . . . . . . . . . . . . . . . . 18
6.2 Shattering with other Activations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
7 Future Work

25

1

Introduction

We analyze the sample complexity of networks with bounds on the magnitude of their weights. Let
us consider a prototypical case, where the input space is X = [âˆ’1, 1]d , the output space is R, the
number of layers is t, all hidden layers has d neurons, and the activation function is Ï : R â†’ R.
The class of functions computed by such an architecture is
N = {Wt â—¦ Ï â—¦ Wtâˆ’1 â—¦ Ï . . . â—¦ Ï â—¦ W1 : W1 , . . . , Wtâˆ’1 âˆˆ MdÃ—d , Wt âˆˆ M1,d }
As the class N is defined by (t âˆ’ 1)d2 + d = O(d2 ) parameters, classical results (e.g. [1]) tell us that
order of d2 examples are sufficient and necessary in order to learn a function from N (in a standard
worst case analysis). However, modern networks often succeed to learn with substantially less
examples. One way to provide alternative results, and a potential explanation to the phenomena,
is to take into account the magnitude of the weights. This approach was a success story in the days
of SVM [3] and Boosting [10], provided a nice explanation to generalization with sub-linear (in the
number of parameters) number of examples, and was even the deriving force behind algorithmic
progress. It seems just natural to adopt this approach in the context of modern networks. For
instance, it is natural to consider the class
NR = {Wt â—¦ Ï â—¦ Wtâˆ’1 â—¦ Ï . . . â—¦ Ï â—¦ W1 : âˆ€i, kWi kF â‰¤ R, kWi k â‰¤ O(1)}
qP
d
2
where kW k = maxkxk=1 kW xk is the spectral norm and kW kF =
i,j=1 Wij is the Frobenius
norm. This class has been analyzed
 2 2 in several recent works [7, 4, 8, 5, 2, 9]. Best known results
show a sample complexity of OÌƒ d R
(for the sake of simplicity, in the introduction, we ignore the
2
dependence on the depth
in the big-O notation). In this paper we prove, for various activations, a
 2
dR
stronger bound of OÌƒ 2 , which is optimal, up to log factors, for constant depth networks.
How good is this bound? Does it finally provide sub-linear bound in typical regimes of the
parameters? To answer this question, we need to ask how large R is. While this question of course
donâ€™t have a definite answer, empirical studies (e.g. [13]) show that it is usually the case that the
norm (spectral, Frobenius, and others) of the weight matrices is at the same order of magnitude as
the norm of the matrix in the onset of the training process. In most standard training methods,
the initial matrices are random matrices with independent (or almost independent) entries,
with
âˆš
mean zero and variance of order d1 . The Frobenius norm of such
d.
Hence,
a
matrix
is
of
order

âˆš
2
the magnitude of R is of order d. Going back to our OÌƒ dR
bound, we get a sample complexity
2

 2
of OÌƒ d2 , which is unfortunately still linear in the number of parameters.
Since our bound is almost optimal, we can ask whether this is the end of the story? Should we
abandon the aforementioned approach to network sample complexity? A more refined examination
of the training process suggests another hope for this approach. Indeed, the training process
doesnâ€™t start from the zero matrix, but rather form a random initialization matrix. Thus, it stands
to reason that instead of considering the magnitude of the weight matrices Wi , we should consider
the magnitude of Wi âˆ’ Wi0 , where Wi0 is the initial weight matrix. Indeed, empirical studies [6]
show that the Frobenius norm of Wi âˆ’ Wi0 is often order of magnitude smaller than the Frobenius
norm of Wi . Following this perspective, it is natural to consider the class

NR (W10 , . . . , Wt0 ) = Wt â—¦ Ï â—¦ Wtâˆ’1 â—¦ Ï . . . â—¦ Ï â—¦ W1 : kWi âˆ’ Wi0 k â‰¤ O(1), kWi âˆ’ Wi0 kF â‰¤ R
For some fixed matrices, W10 , . . . , Wt0 of spectral norm1 O(1). It is natural to expect that considering balls around the initial Wi0 â€™s instead of zero, shouldnâ€™t change the sample complexity of the
1

The bound of O(1) on the spectral norm of the Wi0 â€™s and Wi âˆ’ Wi0 is again motivated by the practice of neural

1

class at hand. Inother
words, we can expect that the sample complexity of NR (W10 , . . . , Wt0 ) is

2
â€“ the sample complexity of NR . Such a bound would finally be sub-linear,
approximately OÌƒ dR
2
as in practice, it is often the case that R2  d.
This approach was pioneered by Bartlett et al. [4] who considered the class

NR2,1 (W10 , . . . , Wt0 ) = Wt â—¦ Ï â—¦ Wtâˆ’1 â—¦ Ï . . . â—¦ Ï â—¦ W1 : kWi âˆ’ Wi0 k â‰¤ O(1), kWi âˆ’ Wi0 k2,1 â‰¤ R
Pd

qP
d

2
where kW k2,1 =
i=1
j=1 Wij . For this class they proved a sample complexity bound of
 2
 2 2
âˆš
dR
OÌƒ 2 . Since, kW k2,1 â‰¤ dkW kF , this implies a sample complexity bound of OÌƒ d R
on
2
âˆš
0
0
NR (W1 , . . . , Wt ), which is still not sublinear. We note that kW k2,1 = Î˜( d) even if W is a
random matrix with variance that is calibrated so that kW kF = Î˜(1) (namely, each 
entryhas
2
1
variance d2 ). In this paper we finally prove a sub-linear sample complexity bound of OÌƒ dR
on
2

NR (W10 , . . . , Wt0 ).
To prove our results, we develop a new technique for bounding the sample complexity of function
classes. Roughly speaking, we define a notion of approximate description of a function, and count
how many bits are required in order to give an approximate description for the functions in the class
under study. We then show that this number, called the approximate description length (ADL),
gives an upper bound on the sample complexity. The advantage of our method over existing
techniques is that it behaves nicely with compositions. That is, once we know the approximate
description length of a class H of functions from X to Rd , we can also bound the ADL of Ï â—¦ H,
as well as L â—¦ H, where L is a class of linear functions. This allows us to utilize the compositional
structure of neural networks.

2
2.1

Preliminaries
Notation

We denote by med(x1 , . . . , xk ) the median of x1 , . . . , xk âˆˆ R. For vectors x1 , . . . , xk âˆˆ Rd we denote
med(x1 , . . . , xk ) = med(x11 , . . . , xk1 ), . . . , med(x1d , . . . , xkd ) . We use log to denote log2 , and ln to
denote loge An expression of the form f (n) . g(n) means that there is a universal constant
c > 0 for
1 P
which f (n) â‰¤ cg(n). For a finite set A and f : A â†’ R we let ExâˆˆA f = ExâˆˆA f (a) = |A|
aâˆˆA f (a).
dâˆ’1 = {x âˆˆ Rd :
We denote BdM = {x âˆˆ Rd : kxk â‰¤ M } and Bd = Bd1 . Likewise, we denote SP
2
kxk = 1}. We denote the Frobenius norm of a matrix W by kW kF = hW, W i = ij Wij2 , while the
spectral norm is denoted by kW k = maxkxk=1 kW xk. For a pair of vectors x, y âˆˆ Rd we denote by
xy âˆˆ Rd their point-wise product xy = (x1 y1 , . . . , xd yd ). For a scalar a we denote by ~a âˆˆ Rd the
vector whose all coordinates are a. Let V be a finite dimensional inner product space. A standard
Gaussian in V is a centered Gaussian vector X âˆˆ V such that VAR(hu, Xi) = kuk2 for any u âˆˆ V .
For a subspace U âŠ‚ V we denote by PU the orthogonal projection on U .

2.2

Uniform Convergence and Covering Numbers

Fix an instance space X , a label space Y and a loss ` : Rd Ã— Y â†’ [0, âˆ). We say that ` is Lipschitz
/ Bounded / etc. if for any y âˆˆ Y, `(Â·, y) is. Fix a class H from X to Rd . For a distribution D and
networks â€“ the spectral norm of Wi0 , with standard initializations, is O(1), and empirical studies [6, 13] show that
the spectral norm of Wi âˆ’ Wi0 is usually very small.

2

a sample S âˆˆ (X Ã— Y)m we define the representativeness of S as
m

repD (S, H) = sup `D (h) âˆ’ `S (h) where `D (h) =
hâˆˆH

E

(x,y)âˆ¼D

`(h(x), y) and `S (h) =

1 X
`(h(xi ), yi )
m
i=1

We note that if repD (S, H) â‰¤  then any algorithm that is guaranteed to return a function hÌ‚ âˆˆ H
will enjoy a generalization bound `D (h) â‰¤ `S (h) + . In particular, the ERM algorithm will return a
function whose loss is optimal, up to an additive factor of . We will focus on bounds on repD (S, H)
when S âˆ¼ Dm . To this end, we will rely on the connection between representativeness and the
covering numbers of H.
Definition 2.1. Fix a class H of functions from X to Rd , an integer m,  > 0 and 1 â‰¤ p â‰¤ âˆ.
We define Np (H, m, ) as the minimal integer for which the following holds. For every A âŠ‚ X of
X
size â‰¤ m there exists HÌƒ âŠ‚ Rd
such that HÌƒ â‰¤ Np (H, m, ) and for any h âˆˆ H there is hÌƒ âˆˆ HÌƒ
1

p p
with ExâˆˆA h(x) âˆ’ hÌƒ(x)
â‰¤ . For p = 2, we denote N (H, m, ) = N2 (H, m, )
âˆ

Lemma 2.2. [11] Let ` : Rd Ã— Y â†’ R be B-bounded. Then
M
q
12B X âˆ’k
E m repD (S, H) â‰¤ B2âˆ’M +1 + âˆš
2
ln (N (` â—¦ H, m, B2âˆ’k ))
Sâˆ¼D
m
k=1

Furthermore, with probability at least 1 âˆ’ Î´,
repD (S, H) â‰¤ B2

âˆ’M +1

M
q
12B X âˆ’k
2
+âˆš
ln (N (` â—¦ H, m, B2âˆ’k )) + B
m
k=1

r

2 ln (2/Î´)
m

We conclude with a special case of the above lemma, which will be useful in this paper.
Lemma 2.3. Let ` : Rd Ã— Y â†’ R be L-Lipschitz w.r.t. k Â· kâˆ and B-bounded. Assume that for any
0 <  â‰¤ 1, log (N (H, m, )) â‰¤ n2 . Then
âˆš
(L + B) n
âˆš
E repD (S, H) .
log(m)
Sâˆ¼Dm
m
Furthermore, with probability at least 1 âˆ’ Î´,
r
âˆš
(L + B) n
2 ln (2/Î´)
âˆš
repD (S, H) .
log(m) + B
m
m
Proof. Denote
M
q
12B X âˆ’k
A = B2âˆ’M +1 + âˆš
2
ln (N (` â—¦ H, m, B2âˆ’k ))
m
k=1

âˆš

We will show that A .


(L+B) n
âˆš
m

log(m). We have, if

ln N (` â—¦ H, m, B2

âˆ’k

B2âˆ’k
L

â‰¤ 1,

 


nL2 22k
B âˆ’k
) â‰¤ ln N H, m, 2
â‰¤
+n
L
B2

3

Hence,
A â‰¤ B2

âˆ’M +1

âˆš
M âˆš
M
12(LM + B) n
12B X nL 12B X âˆ’k âˆš
âˆ’M +1
âˆš
n â‰¤ B2
+
+âˆš
+âˆš
2
B
m
m
m
k=1

Choosing M = log

pm
n

we get,
Aâ‰¤

2.3

k=1

12(L log

âˆš
âˆš
+ B) n + B n
âˆš
m

pm
n

Basic Inequalities

Lemma 2.4. Let X1 , . . . , Xn be independent r.v. with that that are Ïƒ-estimators to Âµ. Then
 n
2
Pr (|med(X1 , . . . , Xn ) âˆ’ Âµ| > kÏƒ) <
k
Proof. We have that Pr(|Xi âˆ’Âµ| > kÏƒ) â‰¤ k12 . It follows that the probability that â‰¥
fall outside of the segment (Âµ âˆ’ kÏƒ, Âµ + kÏƒ) is bounded by


3

n
dn/2e



1
k2

dn/2e

n

<2



1
k2

dn/2e

n
2

of X1 , . . . , Xn

 n
2
â‰¤
k

Simplified Approximate Description Length

To give a soft introduction to our techniques, we first consider a simplified version of it. We next
define the approximate description length of a class H of functions from X to Rd , which quantifies
the number of bits it takes to approximately describe a function from H. We will use the following
notion of approximation
Definition 3.1. A random vector X âˆˆ Rd is a Ïƒ-estimator to x âˆˆ Rd if
E X = x and âˆ€u âˆˆ Sdâˆ’1 , VAR(hu, Xi) = E hu, X âˆ’ xi2 â‰¤ Ïƒ 2
A random function fË† : X â†’ Rd is a Ïƒ-estimator to f : X â†’ Rd if for any x âˆˆ X , fË†(x) is a
Ïƒ-estimator to f (x).
A (Ïƒ, n)-compressor C for a class H takes as input a function h âˆˆ H, and outputs a (random)
function Ch such that (i) Ch is a Ïƒ-estimator of h and (ii) it takes n bits to describe Ch. Formally,
Definition 3.2. A (Ïƒ, n)-compressor for H is a triplet (C, â„¦, Âµ) where Âµ is a probability measure
X
on â„¦, and C is a function C : â„¦ Ã— H â†’ Rd
such that
1. For any h âˆˆ H and x âˆˆ X , (CÏ‰ h)(x), Ï‰ âˆ¼ Âµ is a Ïƒ-estimator of h(x).
X
2. There are functions E : â„¦ Ã— H â†’ {Â±1}n and D : {Â±1}n â†’ Rd
for which C = D â—¦ E

4

Definition 3.3. We say that a class H of functions from X to Rd has approximate description
length n if there exists a (1, n)-compressor for H
It is not hard to see that if (C, â„¦, Âµ) is a (Ïƒ, n)-compressor for H, then
Pk
(CÏ‰1 ,...,Ï‰k h)(x) :=


âˆšÏƒ , kn
k

i=1 (CÏ‰i h)(x)

k



-compressor for H. Hence, if the approximate description length of H is n, then for

any 1 â‰¥  > 0 there exists an , ndâˆ’2 e -compressor for H.
We next connect the approximate description length, to covering numbers and representativeness. We separate it into two lemmas, one for d = 1 and one for general d, as for d = 1 we can
prove a slightly stronger bound.
is a

Lemma 3.4. Fix a class H of functions from X to R with approximate description length n. Then,
 
log (N (H, m, )) â‰¤ n âˆ’2
Hence, if ` : Rd Ã— Y â†’ R is L-Lipschitz and B-bounded, then for any distribution D on X Ã— Y
âˆš
(L + B) n
âˆš
E repD (S, H) .
log(m)
Sâˆ¼Dm
m
Furthermore, with probability at least 1 âˆ’ Î´,
r
âˆš
(L + B) n
2 ln (2/Î´)
âˆš
repD (S, H) .
log(m) + B
m
m
  
Proof. Fix a set A âŠ‚ X . Let (C, â„¦, Âµ) be a n âˆ’2 ,  -compressor for H. Let HÌƒ be the range
âˆ’2
of C. Note that HÌƒ â‰¤ 2nd e . Fix h âˆˆ H. It is enough to show that there is hÌƒ âˆˆ HÌƒ with

2
ExâˆˆA h(x) âˆ’ hÌƒ(x) â‰¤ 2 . Indeed,
E

E (h(x) âˆ’ (CÏ‰ h)(x))2 = E

Ï‰âˆ¼Âµ xâˆˆA

E (h(x) âˆ’ (CÏ‰ h)(x))2 â‰¤ 2 .

xâˆˆA Ï‰âˆ¼Âµ


2
Hence, there exists hÌƒ âˆˆ HÌƒ for which ExâˆˆA h(x) âˆ’ hÌƒ(x) â‰¤ 2
Lemma 3.5. Fix a class H of functions from X to Rd with approximate description length n.
Then,


log (Nâˆ (H, m, )) â‰¤ log (N (H, m, )) â‰¤ n 16âˆ’2 dlog(dm)e
Hence, if ` : Rd Ã— Y â†’ R is L-Lipschitz w.r.t. k Â· kâˆ and B-bounded, then for any distribution D
on X Ã— Y
p
(L + B) n log(dm)
âˆš
E repD (S, H) .
log(m)
Sâˆ¼Dm
m
Furthermore, with probability at least 1 âˆ’ Î´,
r
p
(L + B) n log(dm)
2 ln (2/Î´)
âˆš
repD (S, H) .
log(m) + B
m
m
5


 
Proof. Denote k = dlog(dm)e. Fix a set A âŠ‚ X . Let C be a n 16âˆ’2 , 4 -compressor for H.
Define
(CÏ‰0 1 ,...,Ï‰k h)(x) = med ((CÏ‰1 h)(x), . . . , (CÏ‰k f )(x))
âˆ’2
Let HÌƒ be the range of C 0 . Note that HÌƒ â‰¤ 2knd16 e . Fix h âˆˆ H. It is enough to show that there

is hÌƒ âˆˆ HÌƒ with maxxâˆˆA h(x) âˆ’ hÌƒ(x)
Pr

Ï‰1 ,...,Ï‰k âˆ¼Âµ

âˆ

â‰¤ . By lemma 2.4 we have that


âˆƒx âˆˆ A, (CÏ‰0 1 ,...,Ï‰k h)(x) âˆ’ h(x) >  < dm2âˆ’k â‰¤ 1

In particular, there exists hÌƒ âˆˆ HÌƒ for which maxxâˆˆA h(x) âˆ’ hÌƒ(x)

3.1

âˆ

â‰¤

Linear Functions

We next bound the approximate description length of linear functions with bounded Frobenius
norm.

Theorem 3.6. Let class Ld1 ,d2 ,M = x âˆˆ Bd1 7â†’ W x : W is d2 Ã— d1 matrix with kW kF â‰¤ M has
approximate description length


1
nâ‰¤
+ 2M 2 2 dlog (2d1 d2 (M + 1))e
4
Hence, if ` : Rd2 Ã— Y â†’ R is L-Lipschitz w.r.t. k Â· kâˆ and B-bounded, then for any distribution D
on X Ã— Y
p
(L + B) M 2 log(d1 d2 M ) log(d2 m)
âˆš
E repD (S, Ld1 ,d2 ,M ) .
log(m)
Sâˆ¼Dm
m
Furthermore, with probability at least 1 âˆ’ Î´,
r
p
(L + B) M 2 log(d1 d2 M ) log(d2 m)
2 ln (2/Î´)
âˆš
repD (S, Ld1 ,d2 ,M ) .
log(m) + B
m
m
We remark that the above bounds on the representativeness coincides with standard bounds
([11] for instance), up to log factors. The advantage of these bound is that they remain valid for
any output dimension d2 .
In order to prove theorem 3.6 we will use a randomized sketch of a matrix.
Definition 3.7. Let w âˆˆ Rd be a vector.2 A random sketch of w isj a krandom vector wÌ‚ that is
wi
wi
wi
1
samples as follows. Choose i w.p. pi = 2kwk
let b = 1 and otherwise
2 + 2d . Then, w.p. p âˆ’
pi
i
j k

wi
b = 0. Finally, let wÌ‚ =
pi + b ei . A random k-sketch of w is an average of k-independent
random sketches of w. A random sketch and a random k-sketch of a matrix is defined similarly,
with the standard matrix basis instead of the standard vector basis.
q
The following useful lemma shows that an sketch w is a 14 + 2kwk2 -estimator of w.
Lemma 3.8. Let wÌ‚ be a random sketch of w âˆˆ Rd . Then,
1. E wÌ‚ = w
6

2. For any u âˆˆ Sdâˆ’1 , E (hu, wÌ‚i âˆ’ hu, wi)2 â‰¤ E hu, wÌ‚i2 â‰¤

1
4

+ 2kwk2

Proof. Items 1. is straight forward. To see item 2. note that
E (hu, wÌ‚i âˆ’ hu, wi)2 â‰¤ E hu, wi2
   
2 
   2 !
X
wi
wi
wi
wi
+1 + 1âˆ’
u2i
=
pi
pi
pi
pi
pi
i
 2
     !
X
wi
wi
wi
wi
=
pi
+
u2i
+2
pi
pi
pi
pi
i
   2    2 !
X
wi
wi
wi
wi
+
âˆ’
=
pi
+
u2i
pi
pi
pi
pi
i
 2   
 !
X
wi
wi
wi
=
pi
+
1âˆ’
u2i
pi
pi
pi
i
!
 2
X
1
wi
â‰¤
u2i
pi
+
pi
4
i

â‰¤

X w2 u2
1
i i
kuk2âˆ +
4
pi

â‰¤

1 X wi2 u2i
+
4
pi

i

i

Now, since pi =

wi2
2kwk2

+

1
2d

we have

X w2 u2
i

i

pi

i

â‰¤

X w2 u2
i i
i

wi2

= 2kwk2

X

u2i = 2kwk2

i

2kwk2

Proof. (of theorem 3.6) We construct a compressor for Ld1 ,d2 ,M as follows. Given W , we will


sample a k-sketch WÌ‚ of W for k = 41 + 2M 2 , and will return the function x 7â†’ WÌ‚ x. We claim
that that W 7â†’ WÌ‚ is a (1, 2k dlog(2d1 d2 (M + 1))e)-compressor for Ld1 ,d2 ,M . Indeed, to specify a
sketch of W we need dlog(d1 d2 )e bits to describe the chosen index, as well as log (2d1 d2 M + 2)
bits to describe the value in that index. Hence, 2k dlog(2d1 d2 (M + 1))e bits suffices to specify a
k-sketch. It remains to show that for x âˆˆ Bd1 , WÌ‚ x is a 1-estimator of W x. Indeed, by lemma 3.8,
E WÌ‚ = W and therefore
E WÌ‚ x = W x
Likewise, for u âˆˆ Sd2 âˆ’1 . We have
E

D
E
2
D
E
2
u, WÌ‚ x âˆ’ hu, W xi = E WÌ‚ , xuT âˆ’ W, xuT
â‰¤

7

1
4

+ 2M 2
â‰¤1
k

3.2

Simplified Depth 2 Networks

To demonstrate our techniques, we consider the following class of functions. We let the domain X
to be Bd . We fix an activation function Ï : R â†’ R that is assumed to be a polynomial
Ï(x) =

k
X

ai xi

i=0

with

Pn

n=1 |an |

= 1. For any W âˆˆ Md,d we define
d

1 X
hW (x) = âˆš
Ï(hwi , xi)
d i=1
Finally, we let

H=

hW

1
: âˆ€i, kwi k â‰¤
2



In order to build compressors for classes of networks, we will utilize to compositional structure of
the classes. Specifically, we have that
H=Î›â—¦Ïâ—¦F
Where
F = {x 7â†’ W x : W is d Ã— d matrix with kwi k â‰¤ 1 for all i}
and

d

1 X
Î›(x) = âˆš
xi
d i=1
As F is a subset of Ld,d,âˆšd , we know that there exists a (1, O (d log(d)))-compressor for it. We will
use this compressor to build a compressor to Ï â—¦ F, and then to Î› â—¦ Ï â—¦ F. We will start with the
latter, linear case, which is simpler
Lemma 3.9. Let X be a Ïƒ-estimator to x âˆˆ Rd1 . Let A âˆˆ Md2 ,d1 be a matrix of spectral norm
â‰¤ r. Then, AX is a (rÏƒ)-estimator to Ax. In particular, if C is a (1, n)-compressor to a class H
of functions from X to Rd . Then
CÏ‰0 (Î› â—¦ h) = Î› â—¦ CÏ‰ h
is a (1, n)-compressor to Î› â—¦ H
Proof. We have E AX = A E X = Ax. Furthermore, for any u âˆˆ Sd2 âˆ’1 ,
E hu, AX âˆ’ Axi2 = E AT u, X âˆ’ x

2

â‰¤ kAT uk2 Ïƒ 2 â‰¤ r2 Ïƒ 2

We next consider the composition of F with the non-linear Ï. As opposed to composition
with a linear function, we cannot just generate a compression version using Fâ€™s compressor and
then compose with Ï. Indeed, if X is a Ïƒ-estimator to x, it is not true in general that Ï(X) is
an estimator of Ï(x). For instance, consider the case that Ï(x) = x2 , and X = (X1 , . . . , Xd ) is
a vector of independent standard Gaussians. X is a 1-estimator of 0 âˆˆ Rd . On the other hand,
Ï(X) = (X12 , . . . , Xn2 ) is not an estimator of 0 = Ï(0). We will therefore take a different approach.

8

Given f âˆˆ F, we will sample k independent estimators {CÏ‰i f }ki=1 from Fâ€™s compressor, and define
the compressed version of Ïƒ â—¦ h as
CÏ‰0 1 ,...,Ï‰k f =

d
X
i=0

ai

i
Y

C Ï‰i f

j=0

This construction is analyzed in the following lemma
d


Lemma 3.10. If C is a 21 , n -compressor of a class H of functions from X to âˆ’ 12 , 12 . Then C 0
is a (1, n)-compressor of Ï â—¦ H
Combining theorem 3.6 and lemmas 3.9, 3.10 we have:
Theorem 3.11. H has approximation length . d log(d). Hence, if ` : R Ã— Y â†’ R is L-Lipschitz
and B-bounded, then for any distribution D on X Ã— Y
p
(L + B) d log(d)
âˆš
E repD (S, H) .
log(m)
Sâˆ¼Dm
m
Furthermore, with probability at least 1 âˆ’ Î´,
r
p
(L + B) d log(d)
2 ln (2/Î´)
âˆš
log(m) + B
repD (S, H) .
m
m
Lemma 3.10 is implied by the following useful lemma:
Lemma 3.12.

1. If X is a Ïƒ-estimator of x then aX is a (|a|Ïƒ)-estimator of aX

d Assume furthermore that
2. P
Suppose that for
Pâˆn = 1, 2, 3, . . . Xn is a Ïƒdn -estimator of xn âˆˆ R . P
âˆ
âˆ
n=1 Xn is a Ïƒ-estimator
n=1 Ïƒn converge to x âˆˆ R and Ïƒ âˆˆ [0, âˆ). Then,
n=1 xn and
of x
Q
3. Suppose that {Xi }ki=1 are independent
Ïƒi -estimators
of xi âˆˆ Rd . Then ki=1 Xi is a Ïƒ 0 

Q
Q
Q
estimator of ki=1 xi for Ïƒ 02 = ki=1 Ïƒi2 + kxi k2âˆ âˆ’ ki=1 kxi k2âˆ

We note that the bounds in the above lemma are all tight. Specifically, (3) is tight in the case
that {Xi }ki=1 are independent Gaussians with means {xi }ki=1 and co-variance matrices {Ïƒi2 I}ki=1 .
Proof. 1. and 2. are straight forward. We next prove 3. By replacing each Xi with

9

Xi
Ïƒi

we can

assume w.l.o.g. that Ïƒ1 = . . . = Ïƒk = 1. We have
*
E

X1 ,...,Xk

u,

k
Y

Xi âˆ’

i=1

k
Y

+2

k
Y

*

xi

=

u,

E

X1 ,...,Xk

i=1

((Xi âˆ’ xi ) + xi ) âˆ’

i=1

+2

k
Y

xi

i=1
k
Y

*
=

X Y

u,

E

X1 ,...,Xk

Y

(Xi âˆ’ xi )

xi âˆ’

iâˆˆAc

AâŠ‚[k] iâˆˆA

=

E

X1 ,...,Xk

+

ï£­ u,

Y

(Xi âˆ’ xi )

âˆ’

xi

=

E

X1 ,...,Xk

+*

X

E

k
Y

u,

xi

+
Y

u,

iâˆˆAc

xi

Y

(Xi âˆ’ xi )

xi

iâˆˆB c

iâˆˆB

+*

+
Y

u,

i=1

AâŠ‚[k]

Y

Y

(Xi âˆ’ xi )

xi

iâˆˆAc

iâˆˆA

+2

k
Y

u,

(Xi âˆ’ xi )

iâˆˆA

AâŠ‚[k] BâŠ‚[k]

X1 ,...,Xk

*
+

Y

u,

*
âˆ’2

u,

+ï£¶2
xi ï£¸

i=1

*
X

k
Y

*

iâˆˆAc

AâŠ‚[k] iâˆˆA

X

xi

i=1

ï£«*
X Y

+2

xi

i=1

+2

*

(1)

=

X

E

X1 ,...,Xk

Y

u,

*
âˆ’

xi

u,

iâˆˆAc

iâˆˆA

AâŠ‚[k]

Y

(Xi âˆ’ xi )

k
Y

+2
xi

i=1

2
(2)

X

â‰¤

Y

u

xi

iâˆˆAc

AâŠ‚[k],A6=[k]

2

X

=

X

â‰¤

xi

iâˆˆA

AâŠ‚[k],A6=âˆ…
(3)

Y

u

Y

kxi k2âˆ

AâŠ‚[k],A6=âˆ… iâˆˆA
k 
Y

=

k
 Y
1 + kxi k2âˆ âˆ’
kxi k2âˆ

i=1

i=1

(1) If A 6= B, then w.l.o.g. k âˆˆ A \ B. In this case we have
*
E

X1 ,...,Xk

+*
Y

u,

Y

(Xi âˆ’ xi )

xi

+

iâˆˆAc

iâˆˆA

E

E

X1 ,...,Xkâˆ’1 Xk

Y

u,

Y

(Xi âˆ’ xi )

X1 ,...,Xkâˆ’1

Y

(Xi âˆ’ xi )

Y

xi

iâˆˆB c

iâˆˆB

*
=
=

E

X1 ,...,Xkâˆ’1

xi

+
u,

Y

(Xi âˆ’ xi )

Y

(Xi âˆ’ xi )

Y

xi
+

*
E

Xk

xi

u,

Y

(Xi âˆ’ xi )

Y

xi

iâˆˆAc

iâˆˆA

=0

}|
{ Y
z
xi
(Xi âˆ’ xi ) E (Xk âˆ’ xk )

Y

u,

iâˆˆB c

iâˆˆB

Y
iâˆˆB c

iâˆˆB

+*
u,

xi

iâˆˆAc

iâˆˆAc

iâˆˆA

+
u,

E

Y

+*

*
=

(Xi âˆ’ xi )

iâˆˆB

*
=

Y

u,

Xk

iâˆˆA\[k]

+

iâˆˆAc

0

Similarly, if A 6= âˆ…, then w.l.o.g. k âˆˆ A. In this case we have
*
E

X1 ,...,Xk

u,

k
Y
i=1

+*
xi

+
u,

Y
iâˆˆA

(Xi âˆ’ xi )

Y

xi

k
Y

*
=

iâˆˆAc

E

E

X1 ,...,Xkâˆ’1 Xk

*
=

E

X1 ,...,Xkâˆ’1

10

u,

u,

+*
xi

u,

i=1
k
Y
i=1

+
Y
iâˆˆA

xi

iâˆˆAc
=0

+*
xi

(Xi âˆ’ xi )

Y

u,

Y
iâˆˆA\[k]

z
}|
{ Y
(Xi âˆ’ xi ) E (Xk âˆ’ xk )
xi
Xk

iâˆˆAc

+

(2) Fix a set A that is w.l.o.g. A = {1, . . . , k 0 }. We note that if X âˆˆ Rd is a 1-estimator to 0,
then for any vector z âˆˆ Rd
E kzXk2 =

d
X

X

zi2 E Xi2 =

i=1

X

d
X

d
X

zi2 E hei , Xi2 â‰¤
X

i=1

zi2 = kzk2

i=1

It follows that

E

X1 ,...,Xk0 âˆ’1

k
Y

u

xi

i=k0 +1

0 âˆ’1
kY

2

(Xi âˆ’ xi )

=

i=1

â‰¤

E

u

E

X1 ,...,Xk0 âˆ’2 Xk0 âˆ’1

xi

i=k0 +1

k
Y

u

E

X1 ,...,Xk0 âˆ’2

k
Y

xi

0 âˆ’2
kY

i=k0 +1

0 âˆ’1
kY

2

(Xi âˆ’ xi )

i=1
2

(Xi âˆ’ xi )

i=1

..
.
â‰¤

2

k
Y

u

xi

i=k0 +1
2

=

u

Y

xi

iâˆˆAc

Hence,
+2

*
E

X1 ,...,Xk

Y

u,

(Xi âˆ’ xi )

iâˆˆA

Y

k
Y

*

xi

=

E

E

X1 ,...,Xk0 âˆ’1 Xk0

iâˆˆAc
Xk0 is 1-estimator of xk

â‰¤

u

E

X1 ,...,Xk0 âˆ’1

u

xi

i=k0 +1
k
Y

xi

i=k0 +1

0
kY
âˆ’1

0
kY
âˆ’1

+2
(Xi âˆ’ xi ) , (Xk0 âˆ’ xk0 )

i=1
2

(Xi âˆ’ xi )

i=1

2

â‰¤

u

Y

xi

iâˆˆAc

(3) If z = u

Q

iâˆˆA xi

then for any j âˆˆ [d], |zj | â‰¤ |uj |
2

kzk â‰¤

Y

kxi kâˆ

d
X

iâˆˆA kxi kâˆ .

u2j =

j=1

iâˆˆA

4

Q

Y

Hence,

kxi kâˆ

iâˆˆA

Approximation Description Length

In this section we refine the definition of approximate description length that were given in section
3. We start with the encoding of the compressed version of the functions. Instead of standard
strings, we will use what we call bracketed string. The reason for that often, in order to create
a compressed version of a function, we concatenate compressed versions of other functions. This
results with strings with a nested structure. For instance, consider the case that a function h is
encoded by the concatenation of h1 and h2 . Furthermore, assume that h1 is encoded by the string
11

01, while h2 is encoded by the concatenation of h3 , h4 and h5 that are in turn encoded by the
strings 101, 0101 and 1110. The encoding of h will then be
[[01][[101][0101][1110]]]
We note that in section 3 we could avoid this issue since the length of the strings and the recursive
structure were fixed, and did not depend on the function we try to compress. Formally, we define
Definition 4.1. A bracketed string is a rooted tree S, such that (i) the children of each edge are
ordered, (ii) there are no nodes with a singe child, and (iii) the leaves are labeled by {0, 1}. The
length, len(S) of S is the number of its leaves.
Let S be a bracketed string. There is a linear order on its leaves that is defined as follows.
Fix a pair of leaves, v1 and v2 , and let u be their LCA. Let u1 (resp. u2 ) be the child of u that
lie on the path to v1 (resp. v2 ). We define v1 < v2 if u1 < u2 and v1 > v2 otherwise (note that
necessarily u1 6= u2 ). Let v1 , . . . , vn be the leaves of T , ordered according to the above order, and
let b1 , . . . , bn be the corresponding bits. The string associated with T is s = b1 . . . bn . We denote
by Sn the collection of bracketed strings of length â‰¤ n, and by S = âˆªâˆ
n=1 Sn the collection of all
bracketed strings.
The following lemma shows that in log-scale, the number of bracketed strings of length â‰¤ n
differ from standard strings of length â‰¤ n by only a constant factor
Lemma 4.2. |Sn | â‰¤ 32n
Proof. By adding a pair of brackets around each bit, each bracketed string can be described by
2n âˆ’ 1 correctly matched pairs of brackets, and a string of length â‰¤ n. As the
of ways
 number
1 2k
2k , we have,
to correctly match k pairs of brackets is the Catalan number Ck = k+1
â‰¤
2
k
|Sn | â‰¤ 24nâˆ’2 2n+1
We next revisit the definition of a compressor for a class H. The definition of compressor will
now have a third parameter, ns , in addition to Ïƒ and n. We will make three changes in the definition.
The first, which is only for the sake of convenience, is that we will use bracketed strings rather than
standard strings. The second change, is that the length of the encoding string will be bounded
only in expectation. The final change is that the compressor can now output a seed. That is, given
a function h âˆˆ H that we want to compress, the compressor can generate both a non-random seed
Es (h) âˆˆ Sns and a random encoding E(Ï‰, h) âˆˆ S with EÏ‰âˆ¼Âµ len(E(Ï‰, h)) â‰¤ n. Together, Es (h)
X
and E(Ï‰, h) encode a Ïƒ-estimator. Namely, there is a function D : Sns Ã— S â†’ Rd
such that
D(Es (h), E(Ï‰, h)), Ï‰ âˆ¼ Âµ is a Ïƒ-estimator of h. The advantage of using seeds is that it will allow
us to generate many independent estimators, at a lower cost. In the case that n  ns , the cost of
generating k independent estimators of h âˆˆ H is ns + kn bits (in expectation) instead of k(ns + n)
bits. Indeed, we can encode k estimators by a single seed Es (h) and k independent â€œregularâ€
encodings E(Ï‰k , h), . . . , E(Ï‰k , h). The formal definition is given next.
Definition 4.3. A (Ïƒ, ns , n)-compressor for H is a 5-tuple C = (Es , E, D, â„¦, Âµ) where Âµ is a
probability measure on â„¦, and Es , E, D are functions Es : H â†’ T ns , E : â„¦ Ã— H â†’ T , and
X
such that for any h âˆˆ H and x âˆˆ X
D : T ns Ã— T â†’ Rd
1. D(Es (h), E(Ï‰, h)), Ï‰ âˆ¼ Âµ is a Ïƒ-estimator of h.
2. EÏ‰âˆ¼Âµ len(E(Ï‰, h)) â‰¤ n

12

We finally revisit the definition of approximate description length. We will add an additional
parameter, to accommodate the use of seeds. Likewise, the approximate description length will
now be a function of m â€“ we will say that H has approximate description length (ns (m), n(m)) if
there is a (1, ns (m), n(m))-compressor for the restriction of H to any set A âŠ‚ X of size at most m.
Formally:
Definition 4.4. We say that a class H of functions from X to Rd has approximate description
length (ns (m), n(m)) if for any set A âŠ‚ X of size â‰¤ m there exists a (1, ns (m), n(m))-compressor
for H|A
It is not hard to see that if H has approximate description length (ns (m),
 n(m)), then for any
âˆ’2
1 â‰¥  > 0 and a set A âŠ‚ X of size â‰¤ m, there exists an , ns (m), n(m)d e -compressor for H|A .
We next connect the approximate description length, to covering numbers and representativeness.
The proofs are similar the the proofs of lemmas 3.4 and 3.5.
Lemma 4.5. Fix a class H of functions from X to R with approximate description length (ns (m), n(m)).
Then,
n(m)
log (N (H, m, )) . ns (m) + 2

d
Hence, if ` : R Ã— Y â†’ R is L-Lipschitz and B-bounded, then for any distribution D on X Ã— Y
p
(L + B) ns (m) + n(m)
âˆš
E repD (S, H) .
log(m)
Sâˆ¼Dm
m
Furthermore, with probability at least 1 âˆ’ Î´,
r
p
(L + B) ns (m) + n(m)
2 ln (2/Î´)
âˆš
log(m) + B
repD (S, H) .
m
m
Lemma 4.6. Fix a class H of functions from X to Rd with approximate description length (ns (m), n(m)).
Then,
n(m) log(dm)
log (N (H, m, )) â‰¤ log (Nâˆ (H, m, )) . ns (m) +
2
Hence, if ` : Rd Ã— Y â†’ R is L-Lipschitz w.r.t. k Â· kâˆ and B-bounded, then for any distribution D
on X Ã— Y
p
(L + B) ns (m) + n(m) log(dm)
âˆš
E repD (S, H) .
log(m)
Sâˆ¼Dm
m
Furthermore, with probability at least 1 âˆ’ Î´,
r
p
(L + B) ns (m) + n(m) log(dm)
2 ln (2/Î´)
âˆš
log(m) + B
repD (S, H) .
m
m

4.1

Linear Operations

Lemma 4.7. Let H1 , H2 be classes of functions from X to Rd with approximate description length of
(n1s (m), n1 (m)) and (n2s (m), n2 (m)). Then H1 +H2 has approximate description length of (n1s (m)+
n2s (m), 2n1 (m) + 2n2 (m))
Lemma 4.8. Let H be a class of functions from X to Rd with approximate description length
of (ns (m),
Let
 n(m)).

 A be d2 Ã— d1 matrix. Then A â—¦ H1 has approximate description length
2
ns (m), kAk n(m)
13

Definition 4.9. Denote by Ld1 ,d2 ,r,R the class of all d2 Ã— d1 matrices of spectral norm at most r
and Frobenius norm at most R.
Lemma 4.10. Let H be a class of functions from X to Rd1 with approximate description length
(ns (m), n(m)). Assume furthermore that for any x âˆˆ X and h âˆˆ H we have that kh(x)k â‰¤ B.
Then, Ld1 ,d2 ,r,R â—¦ H has approximate description length

ns (m), n(m)O(r2 + 1) + O (d1 + B 2 )(R2 + 1) log(Rd1 d2 + 1)
Proof. Fix as set A âŠ‚ X of size m. We will construct a compressor to Ld1 ,d2 ,r,R â—¦ H as follows.
Given h âˆˆ H and W âˆˆ Ld1 ,d2 ,r,R we q
first pay a seed cost ns (m) to use Hâ€™s compressor. Then,
we use Hâ€™s compressor to generate a

1
k1 -estimator

hÌ‚ of h, at the cost of k1 n(m) bits. Then, we

take WÌ‚ to be a k2 -sketch of W , at the costs of k2 O (log (d1 d2 R + 1)) bits. Finally, we output the
estimator hÌ‚ â—¦ WÌ‚ . Fix a âˆˆ A. We must show that WÌ‚ X := WÌ‚ hÌ‚(a) is a 1-estimator of x = h(a).
Indeed, for u âˆˆ Sd2 âˆ’1 we have,
D
E2
E E u, WÌ‚ X âˆ’ W x

X WÌ‚

D
E2
D
E
E E u, WÌ‚ X âˆ’ W X + 2 u, WÌ‚ X âˆ’ W X hu, W X âˆ’ W xi + hu, W X âˆ’ W xi2

=

X WÌ‚

=0

D

E2

D

E2

E E u, WÌ‚ X âˆ’ W X

=

X WÌ‚

E E u, WÌ‚ X âˆ’ W X

=

X WÌ‚

*
+2E

X

+
z h }|
i{
u, E WÌ‚ âˆ’ W X hu, W X âˆ’ W xi + E E hu, W X âˆ’ W xi2
WÌ‚

X WÌ‚

+ hu, W X âˆ’ W xi2

E2
D
E2 D
+ W T u, X âˆ’ x
E E WÌ‚ âˆ’ W, XuT

=

X WÌ‚

Lemma 3.8

â‰¤

(1)

â‰¤
(2)

â‰¤
â‰¤

2kW k2F + 1
1
E kXk2 +
kW uk2
X
k2
k1


2kW k2F + 1
1
E kX âˆ’ xk2 + kxk2 +
kW k2
X
k2
k1


2kW k2F + 1 1
1
d1 + kxk2 +
kW k2
k2
k1
k1


2R2 + 1 1
1 2
d1 + B 2 +
r
k2
k1
k1

(1) We have
E kX âˆ’ xk2 = E kXk2 âˆ’ 2 hX, xi + kxk2

X

X

= E kXk2 âˆ’ 2 hE X, xi + kxk2
X

= E kXk2 âˆ’ kxk2
X

(2) We have
E kX âˆ’ xk2 =

X

d1
X
i=1

=

â‰¤

d1
X
i=1
d1
X
i=1



Finally, by choosing k1 = 2r


2

E(Xi âˆ’ xi )2
E hX âˆ’ x, ei i2
1
d1
=
k1
k1

+ 1 and k2 = 2(d1 + B 2 )(2R2 + 1) we get the result.
14

4.2

Non-Linear Operations

Lemma
4.11. Suppose that
{Xn }âˆ
Ïƒ-estimators to x âˆˆ Rd . Let Ï(t) =
n=1 are independent
P
Q
Pâˆ
âˆ
n
an
n
n=1 aÌ‚n Yn where Yn = q
i=1 Xi and aÌ‚n = p1 w.p. pi and 0 othern=0 an t . Let U = a0 +
P
kan k2âˆ
2
2 n
2n
wise. Then U is Ïƒâ€™-estimator of Ï(x) with Ïƒ 0 = âˆ
n=1
pn ((Ïƒ + kxkâˆ ) + (1 âˆ’ pn )dkxkâˆ ).
l
m
(
log3 (d)
p
1
n
â‰¤
2
1
Remark 4.12. In particular, if kan kâˆ â‰¤ B n , Ïƒ 2 + kxk2âˆ â‰¤ 6B
and pn =
,
4âˆ’n otherwise
log3 (d)+4
. Indeed,
2
s
s
âˆ
âˆ
X
X
kan k2âˆ 2
kan k2âˆ
n
n
2
2
2n
2
((Ïƒ + kxkâˆ ) + (1 âˆ’ pn )dkxkâˆ ) â‰¤
(Ïƒ + kxkâˆ ) +
(1 âˆ’ pn )dkxk2n
âˆ
pn
pn
n=1
n=1

We have Ïƒ 0 â‰¤ 1 and E max{n : aÌ‚n 6= 0} â‰¤
âˆ
X
n=1

s

kan k2âˆ
pn

â‰¤

âˆ
X

(2B)n

n=1

â‰¤

â‰¤

3

âˆ
X

âˆš
+

d
n=

âˆ  n
X
1
n=1

(2B)n kxkn
âˆ

l
m
log3 (d)
n=
+1
2

âˆ  n
X
1
n=1

âˆ
X

q
âˆš
(Ïƒ 2 + kxk2âˆ )n + d

3

+

l

log3 (d)
2

âˆ  n
X
1
n=1

3

m
+1

 n
1
3

=1

and
âˆ
X




log3 (d)
E max{n : aÌ‚n =
6 0} â‰¤
+
2
n=

l

log3 (d)
2

4
m
+1

âˆ’n


log3 (d)
+1
nâ‰¤
2

Proof. By lemma 3.12 it is enough to show that for all n, aÌ‚n Yn is a
estimator of an xn . Indeed,
VAR (hu, aÌ‚n Yn i)

=
=
=
=
=
lemma 3.12

â‰¤

â‰¤



q

kan k2âˆ
pn

((Ïƒ 2 + kxk2âˆ )n + (1 âˆ’ pn )dkxk2n
âˆ )-

E (hu, aÌ‚n Yn i âˆ’ hu, an xn i)2

2

an
+ (1 âˆ’ pn ) hu, an xn i2
pn E
u,
Yn âˆ’ hu, an xn i
pn
1
E hu, an Yn i2 âˆ’ 2 E hu, an Yn i hu, an xn i + pn hu, an xn i2 + (1 âˆ’ pn ) hu, an xn i2
pn
1
E hu, an Yn i2 âˆ’ hu, an xn i2
pn
 1âˆ’p
1 
n
E han u, Yn i2 âˆ’ han u, xn i2 +
hu, an xn i2
pn
pn
n

kan uk22
Ïƒ 2 + kxk2âˆ + (1 âˆ’ pn )kxn k22
pn
n

kan k2âˆ
Ïƒ 2 + kxk2âˆ + (1 âˆ’ pn )dkxk2n
âˆ
pn

Definition 4.13. A function f : R â†’ R is B-strongly-bounded if for all n â‰¥ 1, kf (n) kâˆ â‰¤ n!B n .
Likewise, f is strongly-bounded if it is B-strongly-bounded for some B
We note that
Lemma 4.14. If f is B-strongly-bounded then f is analytic and its Taylor coefficients around any
point are bounded by B n
15

Figure 1: The functions ln (1 + ex ) and

ex
1+ex

The following lemma gives an example to a strongly bounded sigmoid function, as well as a
strongly bounded smoothened version of the ReLU (see figure 1).
Lemma 4.15. The functions ln (1 + ex ) and

ex
1+ex

are strongly-bounded

z

e
Proof. Consider the complex function f (z) = 1+e
z . It is defined in the strip {z = x + iy : |y| < Ï€}.
By Cauchy integral formula, for any r < Ï€, a âˆˆ R and n â‰¥ 0,
Z
n!
f (z)
f (n) (a) =
2Ï€i |zâˆ’a|=r (z âˆ’ a)n+1

It follows that
f (n) (a) â‰¤

n!
n!
max |f (z)| â‰¤ n
n
r |zâˆ’a|=r
r

max

|f (x + iy)|

x+iy:|y|<r

Now, if |y| < r < Ï€2 , we have
|f (x + iy)| =

ex
ex
ex
1
â‰¤
â‰¤
â‰¤
|1 + eiy ex |
|1 + cos(y)ex |
|1 + cos(r)ex |
cos(r)

x

e
This implies that 1+e
x is strongly bounded. Likewise, since
x
function ln (1 + e ) is strongly bounded as well.

ex
1+ex

is the derivative of ln (1 + ex ), the

Lemma 4.16. Let H be a class of functions from X to Rd with approximate description length of
(ns (m), n(m)). Let Ï : R â†’ R be B-strongly-bounded. Then, Ï â—¦ H has approximate description
length of


ns (m) + O n(m)B 2 log(md) , O n(m)B 2 log(d)
âˆš
1
1
Ïƒ 2 + 2 â‰¤ 6B
. To
Proof. Fix a set A âŠ‚ X of size â‰¤ m. Let 2 = Ïƒ 2 = 72B
2 and note that
generate a 1-estimator to Ï â—¦ h âˆˆ Ï â—¦ H on A we first describe hÌƒ, which forms the seed, such that âˆ€i âˆˆ
[m], khÌƒ(xi ) âˆ’ h(xi )kâˆ â‰¤ . Then, we generate Ïƒ-estimators hÌ‚1 , hÌ‚2 , . . . , to h|A . Finally, we
m
l sample
(
1
n â‰¤ log23 (d)
Bernoulli random variables Z1 , Z2 , . . . where the parameter of Zn is pn =
.
4âˆ’n otherwise
The final estimator is
gÌ‚(x) = Ï(hÌƒ(x)) +

âˆ
X
Ï(n) (hÌƒ(x)) Zn
n=1

n!

pn

Yn where Yn =

n 
Y
i=1

By lemma 4.11 and the following remark, gÌ‚ is 1-estimator of Ï â—¦ h|A .
16

hÌ‚i (x) âˆ’ hÌƒ(x)



How many bits do we need in order to specify gÌ‚? By lemma 4.6 the restriction of H|A has an
-cover, w.r.t. the âˆ-norm, of log-size . ns (m) + n(m) log(md)
. So the generation of the seed hÌƒ costs
2

bits. We also need to specify N := max{n : Zn 6= 0}, Z1 , . . . , ZN and hÌ‚1 , . . . , hÌ‚N .
ns (m)+ n(m) log(md)
2
This can be done by concatenating the descriptions of the pairs (Zn , hÌ‚n ) for n = 1, . . . , N . The bit

cost of this is bounded (in expectation) by log3 (d)+4
d72B 2 en(m) + 1
2

5

Sample Complexity of Neural Networks

We next utilize the tools we developed in order to analyze the sample complexity of networks. For
simplicity we will focus on a standard fully connected architecture. We note that nevertheless the
ADL approach is quite flexible, and can be applied to various other network architectures.
âˆš This isd
however left for future investigation. Fix the instance space X to be the ball of radius d in R
(in particular [âˆ’1, 1]d âŠ‚ X ). Consider the class

Ï
Nr,R
(d0 , . . . , dt ) = Wt â—¦ Ï â—¦ Wtâˆ’1 â—¦ Ï . . . â—¦ Ï â—¦ W1 : Wi âˆˆ Mdiâˆ’1 di kWi k â‰¤ r, kWi kF â‰¤ R
and more generally, for matrices Wi0 âˆˆ Mdi ,diâˆ’1 , i = 1, . . . , t consider

Ï
Nr,R
(W10 , . . . , Wt0 ) = Wt â—¦ Ï â—¦ Wtâˆ’1 â—¦ Ï . . . â—¦ Ï â—¦ W1 : kWi âˆ’ Wi0 k â‰¤ r, kWi âˆ’ Wi0 kF â‰¤ R
Theorem 5.1. Fix a constants2 r > 0, t > 0 and a strongly bounded activation Ïƒ. Then, for every
choice of matrices Wi0 âˆˆ Mdi ,diâˆ’1 , i = 1, . . . , t with d := maxi di and maxi kWi0 k â‰¤ r we have that
Ï
(W10 , . . . , Wt0 ) is
the approximate description length of H = Nr,R

 


dR2 O logt (d) log(md), dR2 O logt+1 (d) = OÌƒ dR2 , OÌƒ dR2
In particular, if ` : Rdt Ã— Y â†’ R is bounded and Lipschitz w.r.t. k Â· kâˆ , then for any distribution
D on X Ã— Y
!
r
dR2
E repD (S, H) â‰¤ OÌƒ
Sâˆ¼Dm
m
Furthermore, with probability at least 1 âˆ’ Î´,
r
repD (S, H) . OÌƒ

dR2
m

!

r
+O

ln (1/Î´)
m

The above theorem shows that the sample complexity H is OÌƒ



!

dR2
2



. We next show a corre-

Ï
sponding lower bound. This lower bound is valid already for the simple case of N1,R
(d, d, 1), where
Ï is the ReLU activation, and will match this aforementioned bound on the sample complexity up
to poly-log factor. However, it will be valid for a family of activations, that is not the family of
strongly-bounded activations, and therefore there is still certain discrepancy between our upper
and lower bounds. The lower bound will be given in the form of shattering.

Definition 5.2. Let H be a class of functions from a domain X to R. We say that H Î³-shatters
a set A âŠ‚ X if for any B âŠ‚ A there is h âˆˆ H such that h|B â‰¥ Î³ while h|A\B â‰¤ âˆ’Î³. The Î³-fat
shattering dimension of H, denoted FatÎ³ (H), is the maximal cardinality of a strongly shaterred set.
We will also denote Fat := Fat1
2

The constant in the big-O notation will depend only on t, r and sigma.

17

It is well known that many losses of interest, such as the large margin loss, ramp loss, the logloss, the
of a class H is lower bounded
 loss, the 0-1 loss and others, the sample complexity
 hinge
âˆš
Fat(H)
. The following theorem shows that for R â‰¤ d, and the ReLU activation Ï(x) =
by â„¦
2


 2

Ï
max(0, x), Fat N1,R
(d, d, 1) = â„¦Ìƒ dR2 , implying that its sample complexity is â„¦Ìƒ dR
.
2
âˆš
Ï
(d, R2 , 1)) =
Theorem 5.3. Let Ï the ReLU activation. Then, for any R â‰¤ d we have that Fat(N1,R


2
â„¦ logdR2 (d)

5.1

Proof of Theorem 5.1

We note that
Ï
Ï
Ï
Nr,R
(W10 , . . . , Wt0 ) = Nr,R
(Wt0 ) â—¦ . . . â—¦ Nr,R
(W10 )

The following lemma analyzes the cost, in terms of approximate description length, when moving
Ï
from a class H to Nr,R
(W 0 ) â—¦ H.
Lemma 5.4. Let H be a class of functions from X to Rd1 with approximate description length
Ï
(Wt0 ) â—¦ H
(ns (m), n(m)) and kh(x)k â‰¤ M for any x âˆˆ X and h âˆˆ H. Fix W 0 âˆˆ Md2 ,d1 . Then, Nr,R
has approximate description length of

ns (m) + n0 (m)B 2 log(md2 ), n0 (m)B 2 log(d2 )
for
n0 (m) = n(m)O(r2 + kW 0 k2 + 1) + O (d1 + M 2 )(R2 + 1) log(Rd1 d2 + 1)



The lemma is follows by combining lemmas 4.7, 4.8, 4.10 and 4.16.
âˆš We note that in the case
âˆš
that d1 , d2 â‰¤ d, M = O( d1 ), B, r, kW 0 k = O(1) (and hence R = O
d ) and R â‰¥ 1 we get that
Ï
Nr,R
(W 0 ) â—¦ H has approximate description length of


ns (m) + O (n(m) log(md)) , O (n(m) log(d)) + O d1 R2 log2 (d)
Theorem 5.1 now follows by simple induction.

6
6.1

Proof of theorem 5.3
Shattering with Quadratic Activation

In this section we will consider the fat shatering dimension of depth two networks with quadratic
activations. We will later use it as a building block for establishing lower bounds on the fat shatering
dimension of networks with other activations. Specifically, for k â‰¤ d and B > 0 let Qd,k,B be the
class of functions from the d-cube {Â±1}d to the reals given by
q(x) =

k
X

Î»i hui , xi2

i=1

Where u1 , . . . , uk are orthonormal, and maxi |Î»i | â‰¤ âˆšBk . We will show that there is a universal


dk
constant B > 0 for which Fat(Qd,k,B ) = â„¦ log(d)
. In fact, we will show a slightly stronger result,
that will be useful later in order to handle other activation functions. We will use the following
notion of shattering
18

Definition 6.1. We say that Qd,k,B nicely-shatters the set A âŠ‚ {Â±1}d if A is 1-shattered by the
sub-class
)
(
k
X
p
Qd,k,B (A) = q(x) =
Î»i hui , xi2 âˆˆ Qd,k,B : âˆ€x âˆˆ A, i âˆˆ [k], | hui , xi | â‰¤ 2 ln(20d|A|)
i=1

Theorem

 6.2. For a universal constant B > 0, Qd,k,B with k â‰¤ d nicely-shatters a set of size
dk
â„¦ log(d)
Denote by Hd,k the space of dÃ—d symmetric matrices W such that Wi,j = 0 whenever min(i, j) â‰¥
k + 1. Denote by Î¨k : {Â±1}d â†’ Hd,k the mapping
ï£±
ï£² âˆš xi xj
min(i, j) â‰¤ k
k(2dâˆ’k)
(Î¨k (x))ij =
ï£³0
otherwise
We say that a subset X of an inner product space H is Î³-shattered by another subset F âŠ‚ H, if X
is Î³-shattered by the function class {x 7â†’ hf , xi : f âˆˆ F }.
Lemma 6.3. Fix x1 , . . . , xD âˆˆ {Â±1}d . Suppose that Î¨k (x1 ), . . . , Î¨k (xD ) are 1-shaterred by
p
âˆš
QÌƒd,k,B (X) = {W âˆˆ Hd,k : kW k â‰¤ dB and |hu, xi i| â‰¤ kuk 2 ln(20dD) for any xi and eigenvector u of W }
Then, x1 , . . . , xD âˆˆ {Â±1}d are nicely shaterred by Qd,min(2k,d),âˆš2B
âˆš
Proof.
p Let h : [D] â†’ {Â±1}. There is W âˆˆ Hd,k such that (1) kW k â‰¤ dB, (2) |hu, xi i| â‰¤
kuk 2 ln(20dD) for any i âˆˆ [D] and eigenvector u of W , and (3) h(i) hW, Î¨k (xi )i â‰¥ 1 for any
i âˆˆ [D]. Let u1 , . . . , uk0 be normalized and orthonormal sequence of eigenvectors of W , that span
the space of spanned by the eigenvectors of W corresponding to non-zero eigenvalues. Such a
sequence exists since W is symmetric (and is unique, up to sign and order in case that W donâ€™t
have eigenvalues of multiplicity > 1). Since W is of rank at most min(2k, d), k 0 â‰¤ min(2k, d).
âˆš
âˆš
âˆš
P 0
0 âˆˆ [âˆ’B
Since kW k â‰¤ B d, there
are
scalars
Î»Ìƒ
,
.
.
.
,
Î»Ìƒ
d,
B
d] such that W = ki=1 Î»Ìƒi ui uTi .
1
k
i
h âˆš
i
h
âˆš
Let Î»i = âˆš Î»Ìƒi
âˆˆ âˆ’ âˆšBk , âˆšBk âŠ‚ âˆ’ âˆš2B0 , âˆš2B0 . We will conclude the proof by showing that
k
k
k(2dâˆ’k)
Pk0
2
q(x) = hW, Î¨k (x)i for the function q âˆˆ Qd,min(2k,d),âˆš2B (X) given by q(x) =
i=1 Î»i hui , xi .
Indeed,
X
1
p
hW, Î¨k (x)i
=
Wij xi xj
k(2d âˆ’ k) min(i,j)â‰¤k
=

d
X
1
p
Wij xi xj
k(2d âˆ’ k) i,j=1

=

1
p
W, xxT
k(2d âˆ’ k)

W âˆˆHd,k

0

=

=

1

k
X

p
k(2d âˆ’ k)

i=1

k0
X

Î»i hui , xi2

i=1

19

Î»Ìƒi ui uTi , xxT

Theorem 6.2 is therefore implied by the following theorem.
Theorem 6.4. For a universal constant B > 0, and any k â‰¤ d there is a choice of D = â„¦



dk
log(d)



points x1 , . . . , xD for which Î¨k (x1 ), . . . , Î¨k (xD ) are 1-shattered by QÌƒd,k,B (X)
The remaining part of this section is devoted to the proof of theorem 6.4. We will first show
a lemma that shows that any â€œlargeâ€ subset W of an inner product space V shatters a contant
fraction of any collection of vectors that are â€œalmost orthogonalâ€. Theorem 6.4 will then follow
dk
by showing that there are â„¦ log(d)
vectors x1 , . . . xD âˆˆ {Â±1}d such that Î¨k (x1 ), . . . , Î¨k (xD ) are
â€œalmost orthonormalâ€ in Hd,k , and that QÌƒd,k,B (X) is a â€œlargeâ€ subset of Hd,k
Let V be an inner product. We say that a sequence of unit vectors x1 , . . . , xD âˆˆ V is b-almostorthonormal if for any i âˆˆ [D], kPViâˆ’1 xi k2 â‰¤ b, where Vi = span{xj }ij=1 (and PVi is the orthogonal
projection on Vi ). In this section we will prove the following lemma:
Lemma
There
are universal constants a, b > 0 for which the following holds. Let x1 , . . . , xD âˆˆ

 6.5.
8
b2
V be 2 log(20D)
-almost-orthonormal and let W âŠ‚ V be a set of measure â‰¥ 10
according to the
standard Gaussian measure on V . Then W a-shatters a set of size â„¦ (D)
Theorem 6.4 therfore follows from the following two lemmas.
Lemma 6.6. Let V âŠ‚ Hd,K be a linear subspace of dimension D. Let X âˆˆ {Â±1}d be a uniform
vector. Then
k + 2D + 2
E kPV Î¨k (X)k2 â‰¤
k(2d âˆ’ k)
Note
thatthe lemma implies that there are D vectors x1 , . . . , xD âˆˆ {Â±1}d for which Î¨k (x
1 ), . . . ,Î¨k (xD )

dk
k+2D
are k(2dâˆ’k) -almost-orthonormal. In particular, for any constant b > 0, there are D = â„¦ log(d)


b2
-almost-orthonormal.
vecotrs x1 , . . . , xD âˆˆ {Â±1}d for which Î¨k (x1 ), . . . , Î¨k (xD ) are 2 log(20D)
Lemma 6.7. For large enough B > 0 and any choice of vectors x1 , . . . , xD âˆˆ {Â±1}d , the Gaussian
8
measure of QÌƒd,k,B (X) is â‰¥ 10
P
Proof. (of lemma 6.6) We first assume that Ik := ki=1 Eii âˆˆ V . Here Eii is the matrix whose all
elements are 0 except the ii entry which is 1. Let E1 , . . âˆš
. , ED be an orthonormal basis to V such
that E1 = âˆš1k Ik . In particular, for all i > 1, tr(Ei ) = k hE1 , Ei i = 0. We note that for every
E âˆˆ Hd,k we have
h
i
E hÎ¨k (X), Ei2 =
=
â‰¤

h
i
1
2
E XX T , E
k(2d âˆ’ k)
P
Pd
2
2
i6=j Eii Ejj + 2Eij +
i=1 Eii
k(2d âˆ’ k)
+ 2kEk2F
k(2d âˆ’ k)

tr2 (E)

20

Hence,
E kPV Î¨k (X)k

2

=

D
X

E hÎ¨k (X), Ei i2

i=1
D

â‰¤

X
1
tr2 (Ei ) + 2kEi k2F
k(2d âˆ’ k)
i=1

=
=

tr2 (E1 ) + 2D
k(2d âˆ’ k)
k + 2D
k(2d âˆ’ k)

In case that Ik âˆˆ
/ V , let VÌƒ be the linear span of V âˆª {Ik }. By what we have shown and the fact
that dim(VÌƒ ) = D + 1 we have
E kPV Î¨k (X)k2 â‰¤ E kPVÌƒ Î¨k (X)k2 =

k + 2D + 2
k(2d âˆ’ k)

Proof. (of lemma 6.7. Sketch) Let W âˆˆ Hk,d be a standard Gaussian, and let u be the kâ€™th
normalized eigenvector of W (with sign determined uniformly at random). It is not hard to
see that the distribution of u is invariant to any diagonal Â±1 matrix U . It follows that given
(u21, . . . , u2d ), u1 x1 , . . . , ud xd are
 independent random variables, and Hoefddingâ€™s bound implies that
p
1
Pr | hu, xi i | â‰¥ 2 ln(20dD) â‰¤ 10dD
. Via a union bound we conclude that the probability that
p
1
. The lemma fol| hu, xi i | â‰¥ 2 ln(20dD) for some i and normalized eigenvector u is at most 10
lows from that, together with the fact (e.g. Corollary 5.35 at [14]) that with probability at least
âˆš
t2
1 âˆ’ 2eâˆ’ 2 , kW k â‰¤ 2d + t
To prove lemma 6.5 we will use Steeleâ€™s generalization [12] of the VC dimension and SauerShelah lemma
Definition 6.8. Let H âŠ‚ Y X . A set A âŠ‚ X is shattered if H|A = Y A . The dimension of H,
denoted dim(H), is the maximal cardinality of a shattered set.
Pdim(H) |X|
|X|âˆ’i
Lemma 6.9 ([12]). For any H âŠ‚ Y X , |H| â‰¤ i=0
i (|Y | âˆ’ 1)
In the sequel we denote for vectors v, x in an
ï£±
ï£´
ï£²1
hv,a (x) := âˆ—
ï£´
ï£³
âˆ’1

inner product space V and a âˆˆ R,
hv, xi â‰¥ a
âˆ’a < hv, xi < a
hv, xi â‰¤ âˆ’a

Lemma 6.10. Let a > 0 be the scalar such that PrXâˆ¼N (0,1) (X âˆˆ (âˆ’a, a)) = 13 . Let b > 0 small
enough such that for any Âµ âˆˆ [âˆ’b, b] and Ïƒ 2 âˆˆ [1 âˆ’ b2 , 1]


1
max
Pr
(X â‰¤ âˆ’a) ,
Pr
(X â‰¥ a) ,
Pr
(âˆ’a < X < a) , â‰¤ + 0
2
2
2
3
Xâˆ¼N (Âµ,Ïƒ )
Xâˆ¼N (Âµ,Ïƒ )
Xâˆ¼N (Âµ,Ïƒ )

21

Fix unit vectors x1 , . . . , xD âˆˆ V such that for any k âˆˆ [D], kPVkâˆ’1 xk k2 â‰¤
span{xi }ki=1 .

b2
2 ln(20D) ,

where Vk =

Fix also h : [D] â†’ {âˆ’1, 1, âˆ—} and a standard Gaussian w âˆˆ V . Then,



Pr âˆ€i âˆˆ [D], hw,a (xi ) = h(i) and w, PViâˆ’1 xi

2

â‰¤ PViâˆ’1 xi

Proof. Let Ak be the event
n
Ak = w : âˆ€i âˆˆ [k], hw,a (xi ) = h(i) and w, PViâˆ’1 xi

2

D
 1
+ 0
2 ln(20D) â‰¤
3

2

â‰¤ PViâˆ’1 xi

2

o
2 ln(20D)

Since Ak âŠ‚ Akâˆ’1 ,
Pr(Ak ) = Pr(Ak |Akâˆ’1 ) Pr(Akâˆ’1 )
Hence,
Pr (AD ) =

D
Y

Pr(Ak |Akâˆ’1 )

k=1

It is therefore enough to show that Pr(Ak |Akâˆ’1 ) â‰¤ 13 + 0 .
To see this, write w = w1 + w2 + w3 where w1 , w2 , w3 are independent standard Gaussians
on Vkâˆ’1 , the orthogonal complement of Vkâˆ’1 in Vk and VkâŠ¥ . Note that w âˆˆ Akâˆ’1 if and only if
w1 âˆˆ Akâˆ’1 . It holds that given that w1 âˆˆ Akâˆ’1 , w âˆˆ Ak only if
hw, xk i = hw1 , xk i + hw2 , xk i âˆˆ Ih(k) , where Iâˆ’1 = (âˆ’âˆ, âˆ’a], I1 = [a, âˆ) and Iâˆ— = (âˆ’a, a)
Now, given w1 âˆˆ Akâˆ’1 , hw, xk i = hw1 , xk i + hw2 , xk i is a Gaussian of variance Ïƒ 2 = 1 âˆ’
kPVkâˆ’1 xk k2 â‰¥ 1 âˆ’ b2 and mean Âµ = hw1 , xk i = PVkâˆ’1 w, xk = w, PVkâˆ’1 xk whose absolute
value satisfies
p
|Âµ| â‰¤ PVkâˆ’1 xk
2 ln(20D) â‰¤ b
It therefore follows that the probability that W, xk xTk âˆˆ Ih(k) is bounded by
Proof. (of lemma 6.5) Let a, b be as in lemma 6.10 with 0 =
Gaussian measure on V . Define
WÌƒ = W âˆ© {w : âˆ€i âˆˆ [D], w, PViâˆ’1 xi

2

1
9

1
3

+ 0 .

and denote by Âµ the standard

â‰¤ PViâˆ’1 xi

2

2 ln(20D)}

t2

Since PrXâˆ¼N (0,1) (|X| â‰¥ t) â‰¤ 2e 2 we have

Âµ(WÌƒ) â‰¥ Âµ(W) âˆ’ Âµ W : âˆƒi âˆˆ [D], w, PViâˆ’1 xi

2

> PViâˆ’1 xi

2


7
2 ln(20D) â‰¥
10

We will show that W a-shatters a set of size â„¦ (D). Let H = {hw,a : w âˆˆ WÌƒ }. For any h : [D] â†’
D
{âˆ’1, 1, âˆ—} define WÌƒh = {w âˆˆ WÌƒ : hw,a = h}. By lemma 6.10, Âµ(WÌƒh ) â‰¤ 94 . On the other hand
X
hâˆˆH

Âµ(WÌƒh ) =

X

Âµ(WÌƒh ) = Âµ(WÌƒ) â‰¥

h:[D]â†’{âˆ’1,1,âˆ—}

It follows that
7
|H| â‰¥
10
22

 D
9
4

7
10

On the other hand, by Steeleâ€™s lemma 6.9
D



|H| â‰¤ 2

D
â‰¤ dim(H)

Hence


eD
dim(H)


â‰¤2

dim(H)

D



eD
dim(H)

dim(H)

 D
9
8

7
â‰¥
10

It follows that
dim(H) = â„¦ (D)

6.2

Shattering with other Activations

Definition 6.11. We say that an activation Ï : R â†’ R is nice. If there is a constant C > 0 and a
distribution Âµ on [âˆ’2, 2] Ã— [âˆ’C, C] such that for any x âˆˆ [âˆ’1, 1] it holds that E(a,b)âˆ¼Âµ bÏ(x âˆ’ a) = x2
Lemma 6.12. The ReLU activation Ï(x) = max(0, x) is nice.
Proof. We first claim that if f : R â†’ R is smooth and compactly supported then f = f 00 âˆ— Ïƒ.
Indeed,
Z âˆ
00
f 00 (t)Ï(x âˆ’ t)dt
(f âˆ— Ï)(x) =
âˆ’âˆ
âˆ

Z
=
=

00

Z

xâˆ’t

f (t)

Ï0 (Ï„ )dÏ„ dt

âˆ’âˆ
âˆ’âˆ
Z âˆ Z xâˆ’t
00

f (t)Ï0 (Ï„ )dÏ„ dt

âˆ’âˆ âˆ’âˆ
âˆ Z xâˆ’Ï„

Z
=

âˆ’âˆ
âˆ

âˆ’âˆ

Z
=

f 00 (t)Ï0 (Ï„ )dtdÏ„

0

Z

xâˆ’Ï„

Ïƒ (Ï„ )

f 00 (t)dtdÏ„

âˆ’âˆ
âˆ’âˆ
âˆ Z xâˆ’Ï„
00

Z
=

f (t)dtdÏ„

âˆ’âˆ

0

Z

âˆ

=
Z0 x

f 0 (x âˆ’ Ï„ )dÏ„
f 0 (Ï„ )dÏ„

=
âˆ’âˆ

= f (x)
Now, let f : R â†’ R be a function that is smooth, coincides with x2 on [âˆ’1, 1] and supported in
[âˆ’2, 2]. For any x âˆˆ [âˆ’1, 1] we have
Z âˆ
Z âˆ


|f 00 (a)|
2
00
00
x = f (x) = f âˆ— Ï (x) =
Ï(x âˆ’ a)f (a)da =
kf 00 k1 sign(f (a)) Ï(x âˆ’ a) 00 da
kf k1
âˆ’âˆ
âˆ’âˆ
The lemma thus holds for the distribution Âµ of the random variable (a, (kf 00 k1 sign(f (a)))) where
00
a is sampled according to the density function |fkf 00(a)|
k1
23

Theorem 5.3 now follows from the following theorem.
Theorem 6.13. Let Ï be a nice activation. Then, for any R â‰¤


2
â„¦ logdR2 (d)

âˆš
Ï
d we have that Fat(N1,R
(d, R2 , 1)) =

Proof. In the proof we will allow neurons to have bias terms. This can be standardly eliminated by
d
adding constant
 dimensions to shattered vectors. Fix k â‰¤ d and let A = {x1 , . . . , xD } âŠ‚ {Â±1} be
dk
D = Î˜ log(d)
vectors that are nicely shattered by Qd,k,B for the universal constant B from theorem
Ï
6.2. We will show that x1 , . . . , xD are 1-shattered by NO(1),O(R)
(d, O(k log(d)), 1). By simple scaling


Ï
dk
R2
. Choosing k = log(d)
arguments it follows that N1,R
(d, k log(d), 1) shatters a set of size Î˜ log(d)
will establish the theorem.
Ï
Fix g : A â†’ {Â±1} it is enough to show that there is f âˆˆ NO(1),O(R)
(d, O(k log(d)), 1) such that

âˆ€x âˆˆ A, f (x)g(x) â‰¥ 1

(1)

Since A is nicely
shattered
by Qd,k,B there are orthogonal unit vectors u1 , . . . , uk and numbers
h
i
B
B
Î»1 , . . . , Î»k âˆˆ âˆ’ âˆšk , âˆšk such that
âˆ€x âˆˆ A, f (x)q(x) â‰¥ 1 for q(x) =

k
X

Î»i hui , xi2

i=1

and
âˆ€i âˆˆ [k] and x âˆˆ A, |hui , xi| â‰¤

p
2 ln(20dD)

We will create a random network with nk hidden neurons, p
where n will be determined later. Denote
by U âˆˆ Mkd the matrix whose iâ€™s row is ui , and let L := 2 ln(20dD). The hidden weight matrix
(without the biases) will be
ï£®
ï£¹ï£¼
U ï£´
ï£´
ï£ºï£´
ï£½
1 ï£¯
ï£¯ U ï£º
ï£¯ . ï£º n times
L ï£° .. ï£»ï£´
ï£´
ï£´
ï£¾
U
To generate the biases and the output weights we will sample nk independent pairs {(ai,j , bi,j )}1â‰¤iâ‰¤k,1â‰¤jâ‰¤n
from the distribution Âµ on [âˆ’2, 2] Ã— [âˆ’C, C] that satisfies E(a,b)âˆ¼Âµ bÏ(x âˆ’ a) = x2 for any x âˆˆ [âˆ’1, 1].
The bias of the (i(k âˆ’ 1) + j)â€™th neuron will be âˆ’ai,j , and the corresponding output weight will be
2Î»i bi,j L
. The network will then calculate the function
n


n X
k
X
2Î»i bi,j L
hui , xi
fa,b (x) =
Ï
âˆ’ ai,j
n
L
j=1 i=1

Now, we have that for any x âˆˆ A,
 E fa,b(x) = 2g(x). Likewise, fa,b (x) is a sum of nk independent

âˆš
random variables, bounded by O log(d)
. Using Hoeffdingâ€™s bound and union bound, we can choose
n k
n = O(log(d)) so that with positive probability âˆ€x âˆˆ A, |fa,b (x) âˆ’ 2q(x)| < 1, implying the (1)
holds. Finally, the spectral norm of the hidden weight matrix is
r
n
= O(1)
2 ln(20dD)

24

Hence, since the rank is at most kn, the Frobenius norm is
âˆš 
p

O
kn = O
k log(d)
As for the output weights, the squared norm is
n
n
k
k
L2 X X 2 2
L2 X X O(1)
= O(1)
Î»
b
â‰¤
i ij
n2
n2
k
j=1 i=1

j=1 i=1

Ï
This implies that fa,b âˆˆ NO(1),O(R)
(d, O(k log(d)), 1)

7

Future Work

As we elaborate next our work leaves many open directions for further research. First, we used
ADL in order to analyze the sample complexity of fully connected neural networks. We believe
however that our approach is quite flexible and can be used to analyze the sample complexity of
many other classes of functions. Natural candidates are convolutional and residual networks, as
well as magnitude bounds in terms of of norms other than the spectral and Euclidean norm. We
also believe that ADL can be useful beyond supervised learning, and can be used to analyze the
sample complexity of sub-space learning (such as PCA and dictionary learning), clustering, and
more. In even more generality, it is interesting to explore the scope ADL in analyzing sample
complexity. Is ADL a â€œcompleteâ€ framework? That is, does learnability implies low ADL?
Second, our current analysis leaves much to be desired. There are many poly-log factors in our
bounds, the activation is required to be strongly bounded (and in particular, the ReLU activation
is not captured), the loss function should be bounded, it is not clear whether the use of seeds in
necessary, etc. Getting over these shortcomings is left for future work, which will hopefully lead to
a cleaner theory.
âˆš
Lastly, we note that our lower bound, theorem 5.3, requires that R â‰¤ d. We believe that this
requirement in unnecessary, and the lower bound should hold for much larger Râ€™s.

Acknowledgements
The authors acknowledge Kunal Talwar for many discussions in early stages of this work. The
authors also acknowledge Haim Kaplan, Aryeh Kontorovich, and Yoram Singer for many useful
comments.

References
[1] Martin Anthony and Peter Bartlet. Neural Network Learning: Theoretical Foundations. Cambridge University Press, 1999.
[2] Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds
for deep nets via a compression approach. In ICML, 2018.
[3] P. L. Bartlett and S. Mendelson. Rademacher and Gaussian complexities: Risk bounds and
structural results. Journal of Machine Learning Research, 3:463â€“482, 2002.

25

[4] Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds
for neural networks. In Advances in Neural Information Processing Systems, pages 6240â€“6249,
2017.
[5] Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity
of neural networks. In COLT, 2018.
[6] Vaishnavh Nagarajan and J Zico Kolter. Generalization in deep networks: The role of distance
from initialization. arXiv preprint arXiv:1901.01672, 2019.
[7] Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in
neural networks. In Conference on Learning Theory, pages 1376â€“1401, 2015.
[8] Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A pac-bayesian approach to
spectrally-normalized margin bounds for neural networks. In ICLR, 2018.
[9] Behnam Neyshabur, Zhiyuan Li, Srinadh Bhojanapalli, Yann LeCun, and Nathan Srebro. The
role of over-parametrization in generalization of neural networks. In ICLR, 2019.
[10] R.E. Schapire, Y. Freund, P. Bartlett, and W.S. Lee. Boosting the margin: A new explanation
for the effectiveness of voting methods. In Machine Learning: Proceedings of the Fourteenth
International Conference, pages 322â€“330, 1997. To appear, The Annals of Statistics.
[11] Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to
algorithms. Cambridge university press, 2014.
[12] J Michael Steele. Existence of submatrices with all possible columns. Journal of Combinatorial
Theory, Series A, 24(1):84â€“88, 1978.
[13] Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initialization and momentum in deep learning. In International conference on machine learning,
pages 1139â€“1147, 2013.
[14] Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv
preprint arXiv:1011.3027, 2010.

26

