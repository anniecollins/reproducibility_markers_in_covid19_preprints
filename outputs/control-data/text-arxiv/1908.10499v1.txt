INFORMS

MATHEMATICS OF OPERATIONS RESEARCH
Vol. 00, No. 0, Xxxxx 0000, pp. 000â€“000
issn 0364-765X | eissn 1526-5471 | 00 | 0000 | 0001

doi 10.1287/xxxx.0000.0000
c 0000 INFORMS

arXiv:1908.10499v1 [math.OC] 28 Aug 2019

On computing the nonlinearity interval in parametric
semidefinite optimization
Jonathan D. Hauenstein
Department of Applied and Computational Mathematics and Statistics, University of Notre Dame, hauenstein@nd.edu

Ali Mohammad-Nezhad
Department of Industrial and Systems Engineering, Lehigh University, mohamm42@purdue.edu

Tingting Tang
Department of Applied and Computational Mathematics and Statistics, University of Notre Dame, ttang@nd.edu

TamaÌs Terlaky
Department of Industrial and Systems Engineering, Lehigh University, terlaky@lehigh.edu

This paper revisits the parametric analysis of semidefinite optimization problems with respect to the perturbation of the objective function along a fixed direction. We review the notions of invariancy set, nonlinearity
interval, and transition point of the optimal partition, and we investigate their characterizations. We show
that the continuity of the optimal set mapping, on the basis of PainleveÌ-Kuratowski set convergence, might
fail on a nonlinearity interval. Furthermore, under a mild assumption, we prove that the set of transition
points and the set of points at which the optimal set mapping is discontinuous are finite. We then present
a methodology, stemming from numerical algebraic geometry, to efficiently compute nonlinearity intervals
and transition points of the optimal partition. Finally, we support the theoretical results by applying our
procedure to some numerical examples.
Key words : Parametric semidefinite optimization; Optimal partition; Nonlinearity interval; Numerical
algebraic geometry
MSC2000 subject classification : Primary: 90C22; Secondary: 90C31, 90C51

1. Introduction Let Sn be the vector space of n Ã— n symmetric matrices. Consider a parametric semidefinite optimization (SDO) problem
(P )
(D )


infn hC + CÌ„, X i : hAi , X i = bi , i = 1, . . . , m, X  0 ,
XâˆˆS


m
X
sup
bT y :
yi Ai + S = C + CÌ„, S  0 ,
(y,S)âˆˆRm Ã—Sn

i=1

where C, Ai âˆˆ Sn for i = 1, . . . , m, b âˆˆ Rm , CÌ„ âˆˆ Sn is a fixed direction, the inner product is defined as
hC, X i := tr(CX), and X  0 means that the matrix X is symmetric and positive semidefinite. Let
v() âˆˆ R âˆª {âˆ’âˆ, âˆ} denote the optimal value of (P ). This yields a function v : R â†’ R âˆª {âˆ’âˆ, âˆ}
which is the so-called optimal value function. Let E := { âˆˆ R : v() > âˆ’âˆ} be the domain of v().
The primal and dual optimal set mappings are defined as
P âˆ— ():= {X : hC + CÌ„, X i = v(), X âˆˆ P ()},
Dâˆ— ():= (y, S) : bT y = v(), (y, S) âˆˆ D() ,

where P () and D() denote the primal and dual feasible set mappings:
1

2

Hauenstein et al.: On computing the nonlinearity interval in parametric semidefinite optimization
Mathematics of Operations Research 00(0), pp. 000â€“000, c 0000 INFORMS

P ():= {X : hAi , X i = bi , i = 1, . . . , m, X  0},


m
X
i
D():= (y, S) :
yi A + S = C + CÌ„, S  0 .
i=1

Note that P âˆ— () or Dâˆ— () might be empty for some  âˆˆ E . To avoid trivialities, we make the following
assumption throughout this paper:
Assumption 1. The interior point
 condition holds for both (P ) and (D ) at  = 0, i.e., there
exists a feasible X â—¦ (0), y â—¦ (0), S â—¦ (0) âˆˆ P âˆ— (0) Ã— Dâˆ— (0) such that X â—¦ (0), S â—¦ (0)  0, where  0 means
positive definite.
Assumption 1 implies that E is nonempty and non-singleton, and that v() is proper and concave
on E [8, Lemma 2.2]. The concavity of v() yields that E is a closed, possibly unbounded, interval [8,
Lemma 2.2] and that v() is continuous on int(E ) [11, Corollary 2.109].
Remark 1. Assumption
1 is equivalent to the existence of a strictly feasible solution

X â—¦ (), y â—¦ (), S â—¦ () at every  âˆˆ int(E ) [19, Lemma 3.1], where int(.) denotes the interior of a
convex set.
Hence, for all  âˆˆ int(E ), Assumption 1 ensures that strong duality holds and that the optimal sets
P âˆ— () and Dâˆ— () are nonempty and compact. In this paper, by strong duality we mean that the
optimal values of (P ) and (D ) are both attained and the duality gap is zero, see e.g., [11, Theorem
5.81]. In particular, the optimality conditions for (P ) and (D ) can be written as
hAi , X i = bi ,
m
X

yi Ai + S = C + CÌ„,

i=1

i = 1, . . . , m,
(1)

XS = 0,
X, S  0,
where XS = 0 denotes the complementarity condition. Furthermore, Assumption 1 guarantees the
existence of a so-called maximally complementary optimal solution for every  âˆˆ int(E ).
Definition 1. An optimal solution X âˆ— (), y âˆ— (), S âˆ— () is called maximally complementary if



X âˆ— () âˆˆ ri P âˆ— () and y âˆ— (), S âˆ— () âˆˆ ri Dâˆ— () ,
where ri(.) denotes the relative
interior of a convex set. A maximally complementary optimal

âˆ—
âˆ—
âˆ—
solution X (), y (), S () is called strictly complementary if X âˆ— () + S âˆ— ()  0.


For any fixed  âˆˆ int(E ), rank X âˆ— () + rank S âˆ— () is maximal on P âˆ— () Ã— Dâˆ— (), see e.g., [15,
Lemma 2.3]. Even though a strictly complementary optimal solution may fail to exist, a maximally
complementary optimal solution always exists under Assumption 1.
In practice, given a fixed , (P ) and (D ) can be efficiently solved in polynomial time using a
primal-dual path-following interior point method (IPM), see [32]. A primal-dual path following
IPM generates a sequence of solutions whose accumulation points are maximally complementary
optimal solutions [21].
1.1. Optimal partition For SDO, the optimal partition information can be leveraged to
establish sensitivity analysis results. The optimal partition provides a characterization of the optimal set, and it is uniquely defined for any instance of an SDO problem which satisfies strong

Hauenstein et al.: On computing the nonlinearity interval in parametric semidefinite optimization
Mathematics of Operations Research 00(0), pp. 000â€“000, c 0000 INFORMS

3



duality [15]. For a fixed  âˆˆ int(E ), let X âˆ— (), y âˆ— (), S âˆ—() âˆˆ ri P âˆ— () Ã— Dâˆ— () be a maximally com
plementary optimal solution, and let B ():= R X âˆ— () , N ():= R S âˆ— () , and T ():= R X âˆ— () +

âŠ¥
R S âˆ— ()
, where R(.) is the column space and âŠ¥ denotes the orthogonal complement of a subspace. Then the triple B
 (), T (), N () is called the optimal partition of (P ) and (D ). Note that
the subspaces R X âˆ— () and R S âˆ— () are orthogonal
by the complementarity condition in (1).

Further, the optimal partition B (), T (), N () is independent of the choice of a maximally complementary optimal solution [15, Lemma 2.3(i)].
1.2. Related works Sensitivity analysis along a fixed direction has been extensively studied
in optimization theory and was originally introduced for linear optimization (LO) and linearly
constrained quadratic optimization (LCQO) problems in [1, 8, 27]. Sensitivity analysis of nonlinear optimization problems was studied by Fiacco [17] and Fiacco and McCormick [16] using the
implicit function theorem [36]. Their analysis was based on linear independence constraint qualification, second-order sufficient condition, and the strict complementarity condition. Furthermore,
Fiacco [17] showed how to compute/approximate the partial derivatives of a locally optimal solution. Robinson [35] removed the reliance on the strict complementarity condition by imposing a
strong second-order sufficient condition. Kojima [29] removed the dependence on the strict complementarity condition by invoking the degree theory of a continuous map, see e.g., [33]. See [18]
for a survey of classical results.
A comprehensive treatment of directional and differential stability of nonlinear SDO problems is
given by Bonnans and Shapiro [10, 11], see also [9, 38]. The study of sensitivity analysis based on
the optimal partition approach was initiated by Adler and Monteiro [1] and Jansen et al. [27] for LO
and then extended for LCQO, SDO, and linear conic optimization by Berkelaar et al. [8], Goldfarb
and Scheinberg [19], and Yildirim [43], respectively. Recently, the second and fourth authors [30]
introduced the concepts of a nonlinearity interval and a transition point for the optimal partition
of (P ) and (D ) to investigate the sensitivity of the optimal partition and the approximation of
the optimal partition with respect to .
1.3. Contributions A parametric SDO problem was initially studied in [19, 30]. Based on
the notion of an invariancy set, which might be either a singleton or an open interval, from [19, 30]
and the notions of nonlinearity interval and transition point from [30], we present a methodology
for the identification of the optimal partitions on the entire int(E ). An invariancy interval is an open
subinterval of int(E ) on which the optimal partition is invariant with respect to . A nonlinearity
interval is an open maximal length subinterval of int(E ) on which the rank of maximally complementary optimal solutions X âˆ— () and S âˆ— () stay constant, while the optimal partition varies with
. A transition point is a singleton invariancy set which does not belong to a nonlinearity interval.
To the best of our knowledge, this is the first comprehensive methodology for the computation of
nonlinearity intervals and transition points in int(E ).
Our main contributions are 1) the study of continuity of optimal set mapping on a nonlinearity
interval, 2) algebraic interpretation of transition points of the optimal partition, and 3) a numerical
procedure for the computation of nonlinearity intervals and transition points. Using continuity
arguments on the basis of PainleveÌ-Kuratowski set convergence, we provide sufficient conditions
under which the set of transition points has empty interior, see Lemma 2, and a nonlinearity
interval exists, see Lemma 3. We analyze the continuity of the optimal set mapping and show that
continuity may fail on a nonlinearity interval, see problem (8). Furthermore, we show that even a
continuous selection [37, Chapter 5(J)] through the relative interior of the optimal sets might fail to
exist, see problem (10). The second part of this paper investigates the computation of nonlinearity
intervals and transition points of the optimal partition. Under a mild assumption, we show that
the set of transition points and the set of points at which the optimal set mapping fails to be

4

Hauenstein et al.: On computing the nonlinearity interval in parametric semidefinite optimization
Mathematics of Operations Research 00(0), pp. 000â€“000, c 0000 INFORMS

continuous relative to int(E ) are finite, see Theorem 2. Using numerical algebraic geometry, we then
present a methodology to partition int(E ) into the finite union of invariancy intervals, nonlinearity
intervals, and transition points, see Algorithms 1 through 4.
Besides sensitivity analysis purposes and their economical interpretations, the identification of a
nonlinearity interval is important from practical perspectives. For example, in order to approximate
the optimal value function on a neighborhood of a given , one needs to utilize samples from the
same nonlinearity interval containing . Cifuentes et al. [13] studied the local stability of SDO
relaxations for polynomial and semi-algebraic optimization problems with emphasis on a notion
similar to a nonlinearity interval.
1.4. Organization of the paper The rest of this paper is organized as follows. In Section 2,
we investigate the continuity of the feasible and optimal set mappings at a given  âˆˆ int(E ) relative
to int(E ). In Section 3, we study the sensitivity of the optimal partition with respect to . Further,
we use continuity arguments to partially characterize nonlinearity intervals and transition points,
and we investigate the continuity of the optimal set mapping on a nonlinearity interval. In Section 4,
we present an algorithm to partition int(E ) into invariancy intervals, nonlinearity intervals, and
transition points of the optimal partition. Our numerical experiments are presented in Section 5.
Finally, we present remarks and topics for future research in Section 6.
Notation Throughout this paper, Sn+ denotes the cone of n Ã— n symmetric positive semidefinite
matrices. Associated with a symmetric matrix, Î»min (X) denotes the smallest eigenvalue of X, Î›(X)
serves as the diagonal matrix of the eigenvalues, and svec(X) denotes a linear mapping stacking
theâˆšupper triangular part of a symmetric matrix, in which the off-diagonal entries are multiplied
by 2, i.e.,
âˆš
âˆš
âˆš
âˆš
T
svec(X):= X11 , 2X12 , . . . , 2X1n , X22 , 2X23 , . . . , 2X2n , . . . , Xnn .
For any two square matrices K1 and K2 and a symmetric matrix H, the symmetric Kronecker
product, denoted by âŠ—s , is defined as
(K1 âŠ—s K2 ) svec(H):=


1
svec K2 HK1T + K1 HK2T ,
2

see e.g., [15] for more details. Finally, for a given  âˆˆ int(E ), a maximally complementary optimal
solution is denoted by X âˆ— (), y âˆ— (), S âˆ— () .
2. Continuity of the feasible set and optimal set mappings This section investigates
the continuity of the primal and dual feasible set mappings and the outer semicontinuity of the
primal and dual optimal set mappings for (P ) and (D ). We adopt the notions and definitions
from [37].
Let Rq and Rl be finite-dimensional Euclidean spaces. A mapping Î¦(Î¾) : Rq â‡’ Rl is called a setvalued mapping if it assigns a subset of Rl to each element of Rq . The domain of a set-valued
mapping Î¦(Î¾) is
dom(Î¦):= {Î¾ : Î¦(Î¾) 6= âˆ…},
and the range of Î¦(Î¾) is defined as
range(Î¦):= {Î½ : âˆƒ Î¾ s.t. Î½ âˆˆ Î¦(Î¾)}.
The following discussion concisely reviews the continuity of a set-valued mapping on the basis of
PainleveÌ-Kuratowski set convergence, see [37, Chapters 4 and 5] for more details. Let N be the set

Hauenstein et al.: On computing the nonlinearity interval in parametric semidefinite optimization
Mathematics of Operations Research 00(0), pp. 000â€“000, c 0000 INFORMS

5

of natural numbers, J denote the collection of subsets J âŠ† N such that N \ J is finite, and Jâˆ be
l
the collection of all infinite subsets of N. For a sequence {Ck }âˆ
k=1 of subsets of R , the outer and
inner limits are defined, respectively, as
o
n
J
lim sup Ck := Î½ : âˆƒ J âˆˆ Jâˆ , âˆƒ Î½k âˆˆ Ck , (k âˆˆ J) with Î½k â†’ Î½ ,
kâ†’âˆ
o
n
J
lim inf Ck := Î½ : âˆƒ J âˆˆ J , âˆƒ Î½k âˆˆ Ck , (k âˆˆ J) with Î½k â†’ Î½ ,
kâ†’âˆ

J
Â¯ A set-valued
where Î½k â†’ Î½ means that limkâˆˆJ Î½k = Î½. Let X be a subset of Rq containing Î¾.
Â¯
Â¯ and inner
mapping Î¦(Î¾) is called outer semicontinuous at Î¾ relative to X if lim sup Î¦(Î¾) âŠ† Î¦(Î¾)
Î¾â†’Î¾Ì„

Â¯ where
semicontinuous at Î¾Â¯ relative to X if lim inf Î¦(Î¾) âŠ‡ Î¦(Î¾),
Î¾â†’Î¾Ì„

n
o
Â¯ âˆƒ Î½k â†’ Î½ with Î½k âˆˆ Î¦(Î¾k ) ,
lim sup Î¦(Î¾):= Î½ : âˆƒ {Î¾k }âˆ
âŠ†
X
with
Î¾
â†’
Î¾,
k
k=1
Î¾â†’Î¾Ì„
n
o
Â¯
lim inf Î¦(Î¾):= Î½ : âˆ€ {Î¾k }âˆ
âŠ†
X
with
Î¾
â†’
Î¾,
âˆƒ
Î½
â†’
Î½
with
Î½
âˆˆ
Î¦(Î¾
)
.
k
k
k
k
k=1

(2)

Î¾â†’Î¾Ì„

Â¯
When X = Rq , we simply call Î¦(Î¾) outer or inner semicontinuous at Î¾.
Definition 2. A set-valued mapping Î¦(Î¾) is PainleveÌ-Kuratowski continuous at Î¾Â¯ relative to
X if it is both outer and inner semicontinuous at Î¾Â¯ relative to X .
In our setting, outer and inner semicontinuity agree with the notions of closedness and openness
of a point-to-set map in [26], see also [37, Theorem 5.7(c)] and [26, Corollary 1.1].
We show the continuity of the feasible set mapping and the outer semicontinuity of the optimal set
mapping relative to int(E ). Trivially, P () : R â‡’ Sn is continuous since it remains invariant with
respect to . Furthermore, the continuity of D() : R â‡’ Rm Ã— Sn relative to int(E ) follows from [26,
Theorems 10 and 12], where D() = âˆ… for every  âˆˆ R \ E . For the sake of completeness, we provide
a proof for our special case here. Let Dy () : R â‡’ Rm be a set-valued mapping defined by


m
X
m
i
D ():= y âˆˆ R : C + CÌ„ âˆ’
yi A  0 .
y

i=1

Now, the following result is in order.
Lemma 1. Under Assumption 1, the set-valued mapping Dy () and thus D() are continuous
relative to int(E ).
Proof Since Sn+ is a closed convex cone and int(E ) âŠ† dom(Dy ), it follows that Dy () is outer
semicontinuous relative to int(E ), see e.g., [37, Example 5.8]. Hence, it only remains to show that
Dy () is inner semicontinuous at every 0 âˆˆ int(E ).
Pm
Pm
Let yÌ„ âˆˆ Dy (0 ) such that C + 0 CÌ„ âˆ’ i=1 yÌ„i Ai  0 and yÌ‚ âˆˆ Dy (0 ) such that SÌ‚ := C + 0 CÌ„ âˆ’ i=1 yÌ‚i Ai
has at least one zero eigenvalue. The case when SÌ‚  0 is trivial. Given a P
sequence {k }âˆ
k=1 with
m
0
i
k â†’  , we will construct a convergent sequence yk â†’ yÌ‚ so that C + k CÌ„ âˆ’ i=1 (yk )i A  0 for all
sufficiently large values of k.
Pm
Define yk = (1 âˆ’ Î±k )yÌ‚ + Î±k yÌ„. Note that C + k CÌ„ âˆ’ i=1 (yk )i Ai  0 holds if
m
m




X
X
(1 âˆ’ Î±k ) C + k CÌ„ âˆ’
yÌ‚i Ai + Î±k C + k CÌ„ âˆ’
yÌ„i Ai  0.
i=1

i=1

(3)

Hauenstein et al.: On computing the nonlinearity interval in parametric semidefinite optimization
Mathematics of Operations Research 00(0), pp. 000â€“000, c 0000 INFORMS

6

If 0 â‰¤ Î±k â‰¤ 1, then (3) holds if


(1 âˆ’ Î±k )Î»min C + k CÌ„ âˆ’

m
X

i

yÌ‚i A





+ Î±k Î»min C + k CÌ„ âˆ’

i=1

m
X


yÌ„i Ai â‰¥ 0,

i=1

which is equivalent to
Î±k â‰¥

Î»min


Pm
âˆ’Î»min C + k CÌ„ âˆ’ i=1 yÌ‚i Ai


Pm
Pm
C + k CÌ„ âˆ’ i=1 yÌ„i Ai âˆ’ Î»min C + k CÌ„ âˆ’ i=1 yÌ‚i Ai

for sufficiently large k, since the denominator has to be positive. Letting Î±k := max{Ïk , 0}, where

Pm
âˆ’Î»min C + k CÌ„ âˆ’ i=1 yÌ‚i Ai
,

Pm
Pm
Ïk :=
Î»min C + k CÌ„ âˆ’ i=1 yÌ„i Ai âˆ’ Î»min C + k CÌ„ âˆ’ i=1 yÌ‚i Ai
we have Ïk â†’ 0 and yk âˆˆ Dy (k ) for sufficiently large k since 0 â‰¤ Î±k â‰¤ 1 and Î±k â†’ 0. This completes
the proof for the continuity
of Dy () relative to int(E ). The continuity of D() is then immediate
Pm
from S = C + CÌ„ âˆ’ i=1 yi Ai . 
As a result of Lemma 1, we can show that P âˆ— () : R â‡’ Sn and Dâˆ— () : R â‡’ Rm Ã— Sn are outer
semicontinuous relative to int(E ), see e.g., [26, Theorem 8]. All this implies that for any 0 âˆˆ int(E )
and any sequence k â†’ 0 we have
lim inf P âˆ— (k ) âŠ† lim sup P âˆ— (k ) âŠ† P âˆ— (0 )
kâ†’âˆ

kâ†’âˆ

and

lim inf Dâˆ— (k ) âŠ† lim sup Dâˆ— (k ) âŠ† Dâˆ— (0 ).
kâ†’âˆ

kâ†’âˆ

However, P âˆ— () and Dâˆ— () are not necessarily inner semicontinuous relative to int(E ) as shown in
Example 1, where the optimal set is multiple-valued at  = 12 but single-valued everywhere else in
a neighborhood of 12 . Nevertheless, the set of points at which P âˆ— () or Dâˆ— () fails to be continuous
relative to int(E ) is of first category in int(E ), i.e., it is the union of countably many nowhere dense
sets in int(E ), see e.g., [31]. This directly follows from the outer semicontinuity of the optimal set
mapping relative to int(E ) and Theorem 5.55 in [37]. All this yields the following result.
Theorem 1. The set of points at which P âˆ— () or Dâˆ— () fails to be continuous relative to int(E )
has empty interior.
Proof Since int(E ) is a Baire subset of R [31, Lemma 48.4], every first category subset of int(E )
has empty interior. 
As a consequence of Theorem 1, every open subset of int(E ) contains a point at which both P âˆ— ()
and Dâˆ— () are continuous relative to int(E ).
3. Sensitivity of the optimal partition We briefly review the notions of an invariancy

interval, nonlinearity interval, and a transition point from [30]. Let Ï€():= B (), T (), N () denote
the subspaces of the optimal partition at , and let
Q := QB() , QT () , QN ()



be an orthonormal basis partitioned according to the subspaces of the optimal partition.
Definition 3 ([19, 30]). An invariancy set is a subset Iinv of int(E ) on which Ï€() is invariant
for all  âˆˆ Iinv . A non-singleton Iinv is called an invariancy interval. Otherwise, Iinv is called a
singleton invariancy set.

Hauenstein et al.: On computing the nonlinearity interval in parametric semidefinite optimization
Mathematics of Operations Research 00(0), pp. 000â€“000, c 0000 INFORMS

7

Indeed, a non-singleton invariancy set is proven to be an open, possibly unbounded, subinterval
of int(E ) [30, Section 3.1]. The boundary points of an invariancy set, containing a given Â¯, can be
efficiently computed by solving a pair of auxiliary SDO problems [19, Lemma 4.1]:
Î±inv (Î²inv ):= inf(sup)
s.t.


m
X

yi Ai + QN (Â¯) US QTN (Â¯) = C + CÌ„,

(4)

i=1

US  0,
where we might have Î±inv = âˆ’âˆ, Î²inv = âˆ, or both. If Î±inv < Â¯ < Î²inv holds, then Â¯ belongs to an
invariancy interval. Otherwise, Â¯ is a singleton invariancy set which either belongs to a nonlinearity
interval, or it is a transition point, as formally defined in Definitions 4 and 5.
Definition 4 (Definition 3.6 in [30]). Anonlinearity interval
is an open maximal subin
terval Inon of int(E ) on which both rank X âˆ— () and rank S âˆ— () are constant while Ï€() varies
with .
Definition 5 (Definition 3.5 in [30]). A singleton invariancy set {Â¯} âŠ‚ int(E ) is called a
transition point if for every Î´ > 0, there exists  âˆˆ (Â¯
 âˆ’ Î´, Â¯ + Î´) such that




rank X âˆ— () 6= rank X âˆ— (Â¯
) or rank S âˆ— () 6= rank S âˆ— (Â¯
) .
(5)
Remark 2. Both the primal and dual optimal sets must vary with  on a nonlinearity interval.
Otherwise, one would get an invariancy interval [30, Lemma 3.3 and Remark 5]. Indeed, a nonlinearity interval can
 be thought of as the union of infinitely many singleton invariancy sets on which
both rank X âˆ— () and rank S âˆ— () stay constant.
Since the domain E may be unbounded, a nonlinearity interval Inon may be unbounded too. Furthermore, a boundary point of an invariancy or a nonlinearity interval must be a transition point,
since (5) always holds at a boundary point, see also [30, Remark 5]. Lemma 2 indicates that under
an extra condition, the converse of this statement is true as well, i.e., a transition point must be a
boundary point of an invariancy or a nonlinearity interval.
Lemma 2. Assume that the set of points at which P âˆ— () or Dâˆ— () fails to be continuous relative
to int(E ) is finite. Then the set of transition points in int(E ) has empty interior.
Proof To reach a contradiction, suppose that there exists a subset I âŠ† int(E ) with int(I ) 6= âˆ…
such that  is a transition point for every  âˆˆ I . By the assumption, there must exist Â¯ âˆˆ int(I ) and
Ï‚ > 0 such that both P âˆ— () and Dâˆ— () are continuous on (Â¯
 âˆ’ Ï‚, Â¯ + Ï‚), and




rank X âˆ— (Â¯
) â‰¤ rank X âˆ— () and rank S âˆ— (Â¯
) â‰¤ rank S âˆ— ()
hold with at least one strict inequality for every  âˆˆ (Â¯
 âˆ’ Ï‚, Â¯ + Ï‚). Then choosing Â¯:=  and applying
this argument infinitely many times with a small enough Ï‚ >
 0 and an âˆ— âˆˆ (Â¯
 âˆ’ Ï‚, Â¯ + Ï‚), we arrive
âˆ—
at a contradiction, since each iteration increases rank X () or rank S () at least by 1. 
Under the condition of Lemma 2, the union of invariancy and nonlinearity intervals is dense in
int(E ). The following example shows the existence of nonlinearity intervals and transition points.
Example 1. Consider the following parametric SDO problem:
ï£«
ï£¶
1 x y 0
(
)
ï£¬x 1 z 0 ï£·
3
ï£·
min (4 âˆ’ 2)x + (2 âˆ’ 4)y âˆ’ 2z : ï£¬
(6)
ï£­y z 1 0 ï£¸  0, (x, y, z) âˆˆ R ,
0 0 0 1âˆ’z

Hauenstein et al.: On computing the nonlinearity interval in parametric semidefinite optimization
Mathematics of Operations Research 00(0), pp. 000â€“000, c 0000 INFORMS

8

Figure 1. The feasible set of the parametric SDO problem (6).

in which the feasible region is the intersection of a 3-elliptope and the inequality constraint z â‰¤ 1,
see Figure 1. Notice that (6) can be cast into the primal form (P ), where X âˆˆ S4 and m = 7.
It is easy to check that for all  âˆˆ (âˆ’ 12 , 23 ) \ { 12 }, see also [30, Example 3.1], the unique strictly
complementary optimal solution of (6) is given by
ï£«
ï£¶
1
1
âˆ’
 âˆ’ 12
0
2
ï£¬1 âˆ’ 
ï£·
1
1 âˆ’ 2( âˆ’ 12 )2
0
2
ï£·,
X âˆ— () = ï£¬
ï£­ âˆ’ 1 1 âˆ’ 2( âˆ’ 1 )2
ï£¸
1
0
2
2
1 2
0
0
0
2( âˆ’ 2 )
ï£«
ï£¶
(7)
(2 âˆ’ 1)2 2 âˆ’ 1 1 âˆ’ 2 0
ï£¬ 2 âˆ’ 1
T
1
âˆ’1 0ï£·
ï£·,
y âˆ— () = âˆ’(2 âˆ’ 1)2 , âˆ’1, âˆ’1, 0, 0, 0, 0 ,
S âˆ— () = ï£¬
ï£­ 1 âˆ’ 2
âˆ’1
1 0ï£¸
0
0
0 0
and the analytic center [15] of the optimal set at  =
ï£«
1
ï£¬
0
X âˆ— ( 21 ) = ï£¬
ï£­0
0

0
1
1
0

0
1
1
0

ï£¶
0
0ï£·
ï£·,
0ï£¸
0

1
2

is given by

y âˆ— ( 12 ) = 0, âˆ’ 12 , âˆ’ 12 , âˆ’1, 0, 0, 0

T

,

ï£¶
ï£«
0 0 0 0
ï£¬0 1 âˆ’ 1 0ï£·
2
2 ï£·
S âˆ— ( 12 ) = ï£¬
ï£­0 âˆ’ 1 1 0ï£¸ .
2
2
0 0 0 1

The eigenvalue decompositions of X âˆ— () and S âˆ— () reveal that
(
(
1 3
1


3

âˆˆ
(
âˆ’
,
)
\
{
}
,
1
2 2
2
rank S âˆ— () =
rank X âˆ— () =
1
2
 = 2,
2

 âˆˆ (âˆ’ 21 , 32 ) \ { 12 },
 = 12 .

Thus, (âˆ’ 21 , 12 ) and ( 21 , 23 ) are nonlinearity intervals and  = 12 is a transition point of the optimal
partition. In fact, for all  âˆˆ (âˆ’ 12 , 32 ) \ { 12 }, the optimal partition is given by
ï£«ï£«

0 âˆš2sgn(2âˆ’1)
0
2
2(2âˆ’1) +4
âˆ’|2âˆ’1|

ï£¬ï£¬
ï£¬ï£¬ âˆš1 âˆš
ï£¬ï£¬
B () = R ï£¬ï£¬ 2 2(2âˆ’1)2 +4
ï£¬ï£¬ âˆš1 âˆš |2âˆ’1|
ï£­ï£­ 2 2(2âˆ’1)2 +4
0
0

ï£¶ï£¶

ï£·ï£·
ï£·
0ï£·
ï£·ï£·
ï£·ï£· ,
ï£·
0ï£·
ï£¸ï£¸
1

ï£«ï£«

âˆš (1âˆ’2)

ï£¶ï£¶

(2âˆ’1)2 +2

T () = {0},

ï£¬ï£¬
ï£·ï£·
ï£¬ï£¬ âˆš âˆ’1 2 ï£·ï£·
ï£¬
ï£¬
ï£·ï£· ,
(2âˆ’1)
+2
N () = R ï£¬ï£¬
ï£·ï£·
1
âˆš
ï£­ï£­ (2âˆ’1)2 +2 ï£¸ï£¸
0

Hauenstein et al.: On computing the nonlinearity interval in parametric semidefinite optimization
Mathematics of Operations Research 00(0), pp. 000â€“000, c 0000 INFORMS

while the optimal partition at  =

1
2

9

is

ï£«ï£«

ï£¶ï£¶
0 1
ï£¬ï£¬ âˆš1 0ï£·ï£·
ï£¬ 2 ï£·ï£·
B ( 12 ) = R ï£¬
ï£­ï£­ âˆš1 0ï£¸ï£¸ ,
2
0 0

ï£«ï£«

T ( 12 ) = {0},

ï£¶ï£¶
0 0
âˆ’1 ï£·ï£·
ï£¬ï£¬ âˆš
ï£¬ 12 0ï£·ï£· ,
N ( 21 ) = R ï£¬
ï£­ï£­ âˆš 0ï£¸ï£¸
2
0 1

where sgn(.) denotes the signum function.
Due to unknown behavior of the optimal set mapping in a parametric SDO problem, see Remark 2,
a general existence condition for a nonlinearity interval or a transition point is still an open question.
Nevertheless, strict complementarity coupled with the continuity of the optimal set mapping at a
given Â¯ relative to int(E ) provide sufficient conditions for the existence of a nonlinearity interval
surrounding Â¯, see also [30, Theorems 3.7 and 3.10].

Lemma 3. Assume that {Â¯} is a singleton invariancy set. If (X âˆ— (Â¯
), y âˆ— (Â¯
), S âˆ— (Â¯
) is a strictly
complementary optimal solution, and both the primal and dual optimal set mappings are continuous
at Â¯ relative to int(E ), then Â¯ belongs to a nonlinearity interval.
Proof The strict complementarity condition yields


rank X âˆ— (Â¯
) + rank S âˆ— (Â¯
) = n.
Continuity of X âˆ— () and S âˆ— () at Â¯, along with the continuity
of the eigenvalues, shows that

âˆ—
âˆ—
âˆ—
âˆ—
rank X (Â¯
) â‰¤ rank X () and rank S (Â¯
) â‰¤ rank S () for all  in a small neighborhood of Â¯,
see also [36, Theorem 3B.2(b)]. Hence, the rank of X âˆ— () and S âˆ— () remain constant on a sufficiently
small neighborhood of Â¯. 
Unfortunately, the converse of Lemma 3 is not necessarily true for a parametric SDO problem. In
fact, the primal or dual optimal set mapping might fail to be continuous on a nonlinearity interval.
This can occur since the lim inf of a sequence of faces is not necessarily a face of the feasible set,
i.e., it might be a subset of the relative interior of a face. A counterexample can be given as
ï£«
ï£¶
(
)
1 x y
min (4 âˆ’ 2)x + (2 âˆ’ 4)y âˆ’ 2z : ï£­x 1 z ï£¸  0, (x, y, z) âˆˆ R3 ,
(8)
y z 1
where the strict complementarity condition holds on a nonlinearity interval (âˆ’ 12 , 32 ), see [30, Example 3.1]. The primal optimal set mapping is single-valued, and thus continuous, everywhere on
(âˆ’ 12 , 12 ) âˆª ( 12 , 32 ). However, P âˆ— () fails to be inner semicontinuous at  = 21 , because
lim inf P âˆ— (k ) âŠ‚ ri(P âˆ— ( 12 ))
kâ†’âˆ

for any sequence k â†’ 12 .
Remark 3. The continuity condition in Lemma 3 can be relaxed by imposing the conditions

lim inf P âˆ— (k ) âˆ© ri P âˆ— (Â¯
) =
6 âˆ… and
kâ†’âˆ


lim inf Dâˆ— (k ) âˆ© ri Dâˆ— (Â¯
) =
6 âˆ…
kâ†’âˆ

(9)

for every sequence k â†’ Â¯, see also [36, Proposition 3A.1], which by (2) and the continuity of
the eigenvalues imply the existence of a nonlinearity interval around Â¯. However, even the weaker

Hauenstein et al.: On computing the nonlinearity interval in parametric semidefinite optimization
Mathematics of Operations Research 00(0), pp. 000â€“000, c 0000 INFORMS

10

Figure 2. The feasible set of the parametric SDO problem (10).

condition (9) may not hold on a nonlinearity interval. For instance, by adding the inequality
constraint x + y + z â‰¤ 1 to (8) we get
ï£«

1
ï£¬x
min (4 âˆ’ 2)x + (2 âˆ’ 4)y âˆ’ 2z : ï£¬
ï£­y
0
(

x
1
z
0

ï£¶
y
0
ï£·
z
0
ï£·  0,
ï£¸
1
0
0 1âˆ’xâˆ’yâˆ’z

)
(x, y, z) âˆˆ R3 ,

(10)


for which X âˆ— (), y âˆ— (), S âˆ— () defined by (7) is still a unique strictly complementary optimal solution for all  âˆˆ (âˆ’ 12 , 32 ) \ { 12 }, see Figure 2. However, for any k â†’ 21 the sequence X âˆ— (k ) converges
to an optimal solution on the boundary of P âˆ— ( 12 ). This example shows that even a continuous
selection [37, Chapter 5(J)] through the relative interior of the optimal sets might fail to exist on
a nonlinearity interval. However, we do not know yet whether (9) could fail at a boundary point
of a nonlinearity interval.
4. Identification of the optimal partitions This section proposes a methodology to identify the optimal partitions on the entire int(E ). By Definitions 3 to 5, the interval int(E ) is the union
of invariancy intervals, nonlinearity intervals, and transition points. An invariancy interval can be
efficiently computed by solving the auxiliary problems (4). In general, however, the identification
of a nonlinearity interval around a given Â¯ is a nontrivial computational task, since the conditions
of Lemma 3 may not be easily checked in practice. One could try to simply solve (P ) and (D ) for
various  in a neighborhood of Â¯ with the aim of finding the desired nonlinearity interval. However,
this could fail due to the fact that the solution of IPMs usually come with numerical inaccuracy,
while an eigenvalue of X âˆ— () or S âˆ— () might be doubly exponentially small [34]. On the other hand,
since the set of transition points might have empty interior, see Lemma 2, the numerical inaccuracy
could lead one to miss a transition point when simply solving (P ) and (D ) at a given set of mesh
points.
In order to compute the nonlinearity intervals, we numerically locate the transition points by
reformulating the optimality conditions (1) as a system of polynomials. We then view the problem
of finding transition points through the lens of numerical algebraic geometry, see [7, 39] for an
overview of results regarding polynomial systems. Together with the auxiliary problems (4), this
approach allows us to identify the optimal partitions on int(E ).

Hauenstein et al.: On computing the nonlinearity interval in parametric semidefinite optimization
Mathematics of Operations Research 00(0), pp. 000â€“000, c 0000 INFORMS

11

T
4.1. Algebraic formulation For A := svec(A1 ), . . . , svec(Am ) , the optimality conditions (1) can be equivalently written as
ï£¶
ï£«
A svec(X) âˆ’ b
(11)
F (V, ):= ï£­AT y + svec(S) âˆ’ svec(C + CÌ„)ï£¸ = 0,
1
svec(XS + SX)
2
X, S  0,

(12)

where V :=(X, y, S) is the vector of variables. Given a particular , the set of solutions satisfying (11)
is denoted by
 
V F (V, ) := V âˆˆ Cm+2n : F (V, ) = 0 .
(13)

Following this notation, a solution in V F (V, ) , an optimal solution, and a maximally complementary optimal solution of (P ) and (D ) are denoted by V (), V (), and V âˆ— (), respectively.
Clearly, V () is not necessarily an optimal solution of (P ) and (D ). As  varies on a nonlinearity
interval Inon , the solutions V âˆ— () for  âˆˆ Inon form a solution sheet of (11).
The Jacobian matrix of (11) is given by
ï£«

ï£¶
A
0
0
AT In(n+1)/2 ï£¸ ,
J(V, ):= ï£­ 0
S âŠ—s In 0 X âŠ—s In
where the symmetric Kronecker product âŠ—s is defined in Section 1.4. If the Jacobian is nonsingular
at (V âˆ— (Â¯
), Â¯), then V âˆ— (Â¯
) is the unique and strictly complementary optimal solution of (PÂ¯) and (DÂ¯).

Lemma 4 (Theorem 3.1 of [2] and [20]). The Jacobian J V âˆ— (Â¯
), Â¯ is nonsingular if and
only if the optimal solution V âˆ— (Â¯
) is unique and strictly complementary.
When the Jacobian is nonsingular, then the implicit function theorem [36] and Lemma 3 describe
the behavior of V âˆ— () in a neighborhood of Â¯ and induce the existence of a nonlinearity interval
around Â¯, see [30, Lemma 3.9] and the subsequent discussion therein. Consequently, transition
points and the points at which P âˆ— () or Dâˆ— () fails to be continuous relative to int(E ) are both
subsets of singular points, i.e., the set of  âˆˆ C such that


âˆƒ V () âˆˆ V F (V, ) where J V (),  is singular,
in which case V () is called a singular solution. This inclusion might be strict as demonstrated by
problem (8), where  = 12 is a singular point but not a transition point. If  is not a singular point,
then it is called a nonsingular point. Here, our goal is to locate the real singular points in int(E )
and then identify the transition points out of the singular points.
Singular points of parameterized systems are well-studied in algebraic geometry, e.g., Sylvesterâ€™s
19th century work in discriminants and resultants [40, 41]. Under a mild assumption, the algebraic
formulation (11) shows that the set of singular points must be an algebraic subset of C, leading to
the following finiteness result.
Theorem 2. Assume that there exists a nonsingular point Â¯ âˆˆ int(E ). Then the set of singular
points in int(E ), and hence the set of transition points and the set of points at which P âˆ— () or Dâˆ— ()
fails to be continuous relative to int(E ), are finite.

Hauenstein et al.: On computing the nonlinearity interval in parametric semidefinite optimization
Mathematics of Operations Research 00(0), pp. 000â€“000, c 0000 INFORMS

12

Proof By definition, the set Î¥ of all (V (), ) with a singular Jacobian satisfies


Î¥:= (V, ) âˆˆ Cm+2n+1 : F (V, ) = 0, det J(V, ) = 0 ,

(14)

where (14) is a basic constructible set [3] in Cm+2n+1 . Since the projection of a constructible set to
C is a constructible subset of C [3, Theorem 1.22], it holds that

 âˆˆ C : âˆƒV âˆˆ Cm+2n s.t. (V, ) âˆˆ Î¥
(15)
is either finite or the complement of a finite subset of C, see e.g., [3, Exercise 1.2]. On the other
hand, it follows from the assumption and the implicit function theorem that the complement of (15)
contains an open neighborhood of Â¯. All this implies that the projection of Î¥ is finite, and thus it is
an algebraic subset of C. The finiteness result naturally holds when we restrict the set of singular
points to R, in which our domain E is defined. Consequently, there are only finitely many real
singular points in int(E ). 
Remark 4. Under the weaker assumption that there exists a singular point  âˆˆ int(E ) such
that J V âˆ— (), ) is nonsingular, Theorem 2 implies the existence of finitely many singular solutions
in the solution sheet which passes through V âˆ— ().
From a computational algebraic geometry viewpoint, the problem of computing singular points for
a parametric SDO problem was studied by the first and third authors in [24] in a more general
context. Here, we present a simplified process to locate the singular points in int(E ). To that end,
we make the following assumption from this point on:
Assumption 2. There exists a nonsingular point Â¯ âˆˆ int(E ).
Remark 5. While the condition of Lemma 2 automatically follows from Assumption 2, Theorem 2 provides a stronger result of finiteness.
With Assumption 2, it follows from Theorem 2 that any two invariancy/nonlinearity intervals are
separated by a transition point. This will enable us to decompose int(E ) into the union of finitely
many open intervals of maximal length and their finitely many singular boundary points. The final
step is to classify the singular points into transition and non-transition points and then form the
nonlinearity intervals from the appropriate open intervals.
Given a nonsingular initial point Â¯ âˆˆ int(E ), the key idea is using Davidenkoâ€™s [14, 28] ordinary
differential equation (ODE)
J(V, )

âˆ‚F (V, )
dV
+
=0
d
âˆ‚

(16)

to track an optimal solution V () from Â¯ to the nearest singular point in each direction. Since
solutions of (16) correspond to level sets of F (V, ), i.e., {(V, ) : F (V, ) = c} for arbitrary constant
c, using the initial condition V (Â¯
) = V âˆ— (Â¯
) yields the set of solutions to (11) and (12) for all  in a
neighborhood of Â¯. Hence, this approach utilizes the local information provided by the Jacobian,
when it is nonsingular, to obtain accurate approximations of the optimal solutions nearby. The
following lemma provides a summary of the solution [24].
Lemma 5 (Theorems 2 and 3 in [24]). Suppose that the Jacobian is nonsingular on an
interval Ireg âŠ† int(E ) and let Â¯ âˆˆ Ireg . Then, V âˆ— () is analytic on Ireg , and it is the unique solution of
dV
âˆ‚F (V, )
= âˆ’J(V, )âˆ’1
,
d
âˆ‚
for every  âˆˆ Ireg .

V (Â¯
) = V âˆ— (Â¯
)

Hauenstein et al.: On computing the nonlinearity interval in parametric semidefinite optimization
Mathematics of Operations Research 00(0), pp. 000â€“000, c 0000 INFORMS

13

Using the results of [22], we can track along Ireg , on which the optimal solution V âˆ— () is analytic,
until we reach the boundary points of Ireg . As the perturbation parameter approaches a singular
point at the boundary of Ireg , ill-conditioning of F (V, ) = 0, or spurious numerical behavior will be
detected numerically. Thus, the singularity of the Jacobian matrix J(V, ) indicates the existence of
a possible transition point. Consequently, the common scenario of jumping over a transition point,
when using just an IPM on discrete mesh points, can be avoided.
At a singular boundary point Ë†, we examine the uniqueness of the corresponding optimal solution
V a (Ë†
), where V a (Ë†
) is an accumulation point of the sequence of unique optimal solutions V âˆ— (),
obtained from (16), as  % Ë† or  & Ë†. An accumulation point exists, by the outer semicontinuity of
P âˆ— () and Dâˆ— () relative to int(E ), and it belongs to P âˆ— (Ë†
) Ã— Dâˆ— (Ë†
). Toward this end, we compute
a
the local dimension of the algebraic set V F (V, Ë†) at V (Ë†
) using a numerical local dimension
test [4, 42]. The
local
dimension
is
defined
as
the
maximum
dimension
of the irreducible components


a
of V F (V, Ë†) which contain V (Ë†
). If V F (V, Ë†) has local dimension zero at V a (Ë†
), then we can
conclude from Lemma 3 that Ë† is a transition point, since V a (Ë†
) turns out to be the unique optimal
solution of (PË†) and (DË†). Otherwise, we need to examine the change of rank at a maximally
âˆ—
complementary
). Such a solution is generic on the irreducible component
 optimal solutiona V (Ë†
of V F (V, Ë†) which contains V (Ë†
), and it can be computed efficiently using numerical algebraic
geometry [7]. See e.g., [39] for a detailed description of algebraic sets and irreducible components.
4.2. Partitioning algorithm Based on the above description, we present the outline of Algorithm 1, a three-part algorithm, i.e., Algorithms 2, 3 and 4, which partitions int(E ) into the finite
union of invariancy intervals, nonlinearity intervals, and transition points. Algorithm 2 computes
the singular boundary points of an invariancy interval, which are indeed the transition points in
int(E ), by solving the auxiliary problems (4). Algorithm 3 locates the singular points in int(E ).
In particular, Algorithm 3 tracks the optimal solution of (P ) and (D ) by solving the ODE system (16) using a predictor-corrector tracking method [12]. This procedure is repeated alongside
Algorithm 2 until all singular points and invariancy intervals in int(E ) are identified. Finally, Algorithm 4 classifies the singular points into transition and non-transition points.
In order to completely cover the interval, the increment change âˆ† can be positive or negative to
allow both left and right movements from the starting point. Furthermore, we assume, for the simplicity of computation, that the domain E is bounded, i.e., E = [Emin , Emax ], where |Emin |, |Emax | < âˆ.
Accordingly, the optimal value of the auxiliary problems (4) is constrained to (Emin , Emax ).
Computation of singular points and invariancy intervals Lemma 5 specifies a systematic way to approximate the boundary points of the nonsingular interval Ireg surrounding the
given Â¯. The numerical detection of singular points is described
in detail in [24]

 with respect to
âˆ—
âˆ—
several singularity criteria, e.g., the derivative of Î»min X () and Î»min S () with respect to ,
or the singularity of the Jacobian of (11). We omit the details here and refer the reader to [24] for
more information on the numerical implementation of the singularity criteria.
Once a singular point is identified, the numerical solution obtained from the ODE system (16) at
the next mesh point is most likely non-optimal, due to the numerical instability or the infeasibility
of the solution. Thus, we invoke a primal-dual IPM in Algorithms 2 and 3 to compute the unique
optimal solution at the first neighboring mesh point in the remaining interval. In order to guarantee
that every singular point is correctly identified, a finer mesh pattern might be needed, and a higher
precision might be required for the computation of singular points, far beyond the double precision
arithmetic.

Hauenstein et al.: On computing the nonlinearity interval in parametric semidefinite optimization
Mathematics of Operations Research 00(0), pp. 000â€“000, c 0000 INFORMS

14

Algorithm 1 Partitioning of int(E )
Input: A, b, C, CÌ„, domain [Emin , Emax ], nonsingular initial point init âˆˆ int(E), a positive increment
change âˆ†
Output: The union Uinv of invariancy intervals in (Emin , Emax ), the union Unon of nonlinearity intervals
in (Emin , Emax ), the set Utran of transition points in (Emin , Emax )
Set  = init , Uinv = âˆ…, Unon = (Emin , Emax ), Utran = âˆ…, and Usin = âˆ…
while  < Emax do
. Track forwards
repeat
. Check the existence of an invariancy interval
Set (Î±inv , Î²inv , , Uinv , Unon , Utran )=Invariancy(A, b, C, CÌ„, âˆ†, , Emin , Emax , Uinv , Unon , Utran )
until Î±inv < Î²inv and  < Emax
if  < Emax then
Set (, Uinv , Unon , Usin , Utran )=Singular(A, b, C, CÌ„, âˆ†, , Uinv , Unon , Usin , Utran )
end if
end while
(
Set 0 =

inf 

âˆˆUinv

âˆ

. Algorithm 3

Uinv 6= âˆ…
Uinv = âˆ…

Set  = min{init , 0 }
if  > Emin then
Set âˆ† = âˆ’âˆ†
if  < init then
Set  =  + âˆ†
end if

. Move past a transition point

while  > Emin do
. Track backwards
repeat
. Check the existence of an invariancy interval
Set (Î±inv , Î²inv , , Uinv , Unon , Utran )=Invariancy(A, b, C, CÌ„, âˆ†, , Emin , Emax , Uinv , Unon , Utran )
until Î±inv < Î²inv and  > Emin
if  > Emin then
Set (, Uinv , Unon , Usin , Utran )=Singular(A, b, C, CÌ„, âˆ†, , Uinv , Unon , Usin , Utran ) . Algorithm 3
end if
end while
end if
if { : (V, ) âˆˆ Usin } \ Utran 6= âˆ… then
Set Utran =Transition(Usin , Utran )
end if

. Classify the singular points which are not already in Utran
. Algorithm 4

Set Unon = Unon \ Utran

Solution sharpening Since the singular points are algebraic numbers, they can be computed
to arbitrary accuracy, see e.g., [23]. The process of increasing the algebraic precision of a singular
point is also known as the sharpening process, see Algorithm 3. More specifically, using a numerical
approximation of a given singular point, which is indeed the nearest mesh point to the singular

Hauenstein et al.: On computing the nonlinearity interval in parametric semidefinite optimization
Mathematics of Operations Research 00(0), pp. 000â€“000, c 0000 INFORMS

15

Algorithm 2 Identification of invariancy intervals
function Invariancy(A, b, C, CÌ„, âˆ†, , Emin , Emax , Uinv , Unon , Utran )
Output: Î±inv , Î²inv , , Uinv , Unon , Utran
Compute the unique optimal solution V âˆ— () using a primal-dual IPM
Compute the orthonormal basis QN () from V âˆ— ()
Using QN () solve the SDO problems (4) restricted to (Emin , Emax ) to obtain Î±inv and Î²inv
if Î±inv <  < Î²inv then
Set Uinv = Uinv âˆª (Î±inv , Î²inv ) and Unon = Unon \ (Î±inv , Î²inv )
if Î±inv > Emin and Î±inv 6âˆˆ Utran then
Set Utran = Utran âˆª {Î±inv }
end if
if Î²inv < Emax and Î²inv 6âˆˆ Utran then
Set Utran = Utran âˆª {Î²inv }
end if
if âˆ† < 0 then
. Move past a transition point
Set  = Î±inv + âˆ†
else
Set  = Î²inv + âˆ†
end if
end if
end function

Algorithm 3 Identification of the singular points
function Singular(A, b, C, CÌ„, âˆ†, Â¯, Uinv , Unon , Usin , Utran )
Output: , Uinv , Unon , Usin , Utran
Set F (V, ) = [A svec(X) âˆ’ b; AT y + svec(S) âˆ’ svec(C + CÌ„); 21 svec(XS + SX)]
Compute the unique optimal solution V âˆ— (Â¯
) using a primal-dual IPM
Set  = Â¯
while  + âˆ† âˆˆ Unon and Jacobian is nonsingular on [,  + âˆ†] do
. Check the singularity
Set  =  + âˆ†
Compute the unique optimal solution V âˆ— () by solving (16) with initial point V âˆ— (Â¯
)
end while
if  + âˆ† âˆˆ Unon and a singular point exists in [,  + âˆ†] then
Compute the singular point Ë† and set Usin = Usin âˆª {(V a (Ë†
), Ë†)}
Set  = Ë† + âˆ†
else
Set  =  + âˆ†
end if
end function

. Compute the singular point
. Move past a singular point

16

Hauenstein et al.: On computing the nonlinearity interval in parametric semidefinite optimization
Mathematics of Operations Research 00(0), pp. 000â€“000, c 0000 INFORMS

point, the theory of isosingular sets [25] allows one to construct a new polynomial system where
Newtonâ€™s method would converge quadratically to the singular point.
Classification of singular points The use of adaptive precision, see e.g., [6], in Bertini [5,
7] ensures that adequate precision is being used for reliable computations near the singular solutions. This method enables one to compute a maximally complementary optimal solution near
V a (Ë†
) to arbitrary accuracy. With the ability to refine the accuracy of a maximally complementary
optimal solution, we can determine if a given singular point is a transition point. This can be done
robustly by examining the rank of X âˆ— () and S âˆ— () using standard numerical rank revealing methods, such as singular value decomposition. More specifically, by computing the eigenvalues of an
approximate maximally complementary optimal solution at various precisions, one can determine
if the least positive eigenvalues of X âˆ— () and S âˆ— () converge to zero as we increase the precision of
computation. This process accurately reveals the rank of X âˆ— () and S âˆ— () at a singular point.
Algorithm 4 Classification of the singular points
function Transition(Usin , Utran )
Output: Utran
for (V, ) âˆˆ Usin do

Calculate the local dimension d of the algebraic set V F (V, ) , defined in (13), at V
if d = 0 then
Set Utran = Utran âˆª {}
else
. Compute a maximally complementary optimal solution V âˆ— ()
Use a polynomial solver to compute V âˆ— () in the irreducible component which contains V
if the rank of X âˆ— () or S âˆ— () changes then
Set Utran = Utran âˆª {}
end if
end if
end for
end function

5. Numerical examples In this section, using the approach described in Section 4 and
outlined by Algorithm 1, we conduct numerical experiments on the computation of nonlinearity
intervals and transition points. Section 5.1 demonstrates the convergence rate of computing the
singular points. Section 5.2 describes a parametric SDO problem where the continuity of the dual
optimal set mapping fails at a transition point. Section 5.3 computes the nonlinearity interval of
the parametric SDO problem (10) where the Jacobian is singular at a non-transition point. All
numerical experiments are conducted on a PC with Intel Core i7-6500U CPU @2.5 GHz.
5.1. Convergence rate

Consider the following parametric SDO problem
min ï£«
âˆ’ 2x1 âˆ’ 2(1 âˆ’ )x2
ï£¶
1 x1 x2 0
0
ï£¬x1 1 0
0
0 ï£·
ï£¬
ï£·
ï£¬
0
0 ï£·
s.t. ï£¬x2 0 1
ï£·  0,
ï£­ 0 0 0 x2 x1 âˆ’ 1ï£¸
0 0 0 x1 âˆ’ 1 x2

(17)

which can be cast into the primal form (P ), where m = 13 and X âˆˆ S5 . The block structure of
the matrix indicates that (17) is indeed an SDO reformulation of a parametric second-order conic

Hauenstein et al.: On computing the nonlinearity interval in parametric semidefinite optimization
Mathematics of Operations Research 00(0), pp. 000â€“000, c 0000 INFORMS

17

optimization problem with E = R, see also Figure 3. For computational purposes, we choose a


bounded domain [âˆ’ 41 , 54 ] and the initial point  = 14 , where rank X âˆ— ( 14 ) = 4 and rank S âˆ— ( 41 ) = 1.

1.5

1

0.5

0

-0.5

-1

-1.5
-2

-1.5

-1

-0.5

0

0.5

1

1.5

2

2.5

Figure 3. The feasible set of problem (17).

Algorithm 2 identifies  =

1
4

as a singleton invariancy set belonging to a nonlinearity interval.

We then invoke Algorithm 3 to track the unique optimal solutions until we locate the singular
points  = 0 and  = 1. Algorithm 3 then computes a sufficiently accurate approximation of the
singular points. Figure 4 demonstrates the exact and numerical approximation of x1 () and the
minimum modulus of the Jacobian eigenvalues versus . In particular, this tracking indicates that
the Jacobian approaches singularity near  = 0 and  = 1.
Restarting at the first mesh point next to the singular points, Algorithm 2 identifies the invariancy
intervals (âˆ’ 14 , 0) and (1, 54 ) and determines that  = 0 and  = 1 are indeed the transition points of
the optimal partition.

We should point
out that while J V âˆ— ( 14 ), 14 is nonsingular, there exists a singular solution V ( 41 ),

and J V âˆ— (),  is singular for every maximally complementary optimal solution V âˆ— () on (âˆ’âˆ, 0)
and (1, âˆ). Nevertheless, Algorithm 1 produces the correct result even with the singular initial
point  = 14 . In this case, by Remark 4, there are finitely many singular solutions in the solution
sheet which passes through V âˆ— ( 41 ).

Hauenstein et al.: On computing the nonlinearity interval in parametric semidefinite optimization
Mathematics of Operations Research 00(0), pp. 000â€“000, c 0000 INFORMS

18

Figure 4. Left: The exact and numerical approximation of x1 () versus . Right: The minimum modulus of the
Jacobian eigenvalues.

Using different patterns of mesh points, we demonstrate the convergence of x1 (), computed by
Algorithm 3, when  approaches the singular points  = 0 and  = 1. To that end, we let initial
âˆ† take values from 0.05 Ã— 2âˆ’j for j = 0, . . . , 5 or 0.03 Ã— 2âˆ’j for j = 0, . . . , 5, and we set  = 41 as
the initial point. Tables 1 and 2 summarize the numerical results, where the L1 error between the
exact and numerical approximation of x1 () on [ 41 , 1) and (0, 14 ], the order of convergence, and the
computation time are reported. The order of convergence is computed by
!
Err(âˆ†j )
Ïj+1 :=log2
,
j = 0, . . . , 4,
Err(âˆ†j+1 )
where Err(âˆ†j ) denotes the L1 error associated with mesh pattern j. Notice the difference between
Ïj and the classical notion of the order of convergence in computational optimization.
Table 1. Convergence of x1 () when  approaches to the singular point  = 1.

j
0
1
2
3
4
5

âˆ†j
Approximate singular point
Err(âˆ†j )
0.05
1.00
4.1597 Ã— 10âˆ’6
0.05 Ã— 2âˆ’1
1.00
2.6520 Ã— 10âˆ’7
0.05 Ã— 2âˆ’2
1.00
1.6707 Ã— 10âˆ’8
âˆ’3
0.05 Ã— 2
1.00
1.0484 Ã— 10âˆ’9
0.05 Ã— 2âˆ’4
1.00
6.5671 Ã— 10âˆ’11
âˆ’5
0.05 Ã— 2
1.00
4.1090 Ã— 10âˆ’12

Ïj
3.971
3.989
3.994
3.997
3.998

CPU(s)
4.05
6.56
12.79
26.14
55.81
125.27

In Table 1, the singular point  = 1 is exactly identified by Algorithm 3, since the singular point
coincides with one of the mesh points. In general, however, it is unlikely that a singular point
belongs to the mesh point set. This can be observed in Table 2, where a fixed increment change
0.03 Ã— 2âˆ’j for j = 0, . . . , 5 is utilized. In this case, the approximate singular point is taken as the last
mesh point before the minimum eigenvalues of X âˆ— () or S âˆ— (), obtained from the ODE system (16),

Hauenstein et al.: On computing the nonlinearity interval in parametric semidefinite optimization
Mathematics of Operations Research 00(0), pp. 000â€“000, c 0000 INFORMS

19

become negative, or the first mesh point at which the minimum modulus of the Jacobian eigenvalues
drops below 10âˆ’5 . As stated in Section 4.2, we can utilize numerical algebraic geometric tools to
compute a singular point to arbitrary accuracy.
Table 2. Convergence of x1 () when  approaches to the singular point  = 0.

j
0
1
2
3
4
5

âˆ†j
Approximate singular point
Err(âˆ†j )
0.03
0.01
2.0415 Ã— 10âˆ’7
0.03 Ã— 2âˆ’1
0.01
1.2917 Ã— 10âˆ’8
âˆ’2
0.03 Ã— 2
0.025
8.2444 Ã— 10âˆ’10
0.03 Ã— 2âˆ’3
0.0025
5.1677 Ã— 10âˆ’11
âˆ’4
0.03 Ã— 2
0.000625
3.2461 Ã— 10âˆ’12
0.03 Ã— 2âˆ’5
0.000625
2.0302 Ã— 10âˆ’13

Ïj
3.982
3.970
3.996
3.993
3.999

CPU(s)
2.85
4.57
8.52
17.73
34.90
72.34

5.2. A transition point with discontinuous dual optimal set mapping
sider the parametric SDO problem
min x
ï£«1 + (1 âˆ’ )x2
1 x1 x2 0
ï£¬x1 1 0 0
ï£¬
ï£¬x2 0 1 0
s.t. ï£¬
ï£¬0 0 0 1
ï£¬
ï£­ 0 0 0 1 x1
2
0 0 0 x2

0
0
0
1
x
2 1
1
0

ï£¶
0
0ï£·
ï£·
0ï£·
ï£·  0,
x2 ï£·
ï£·
0ï£¸
1

We next con-

(18)

in which the feasible set is compact and E = R. It can be verified that the Jacobian is nonsingular
on E \ {0}, rank X âˆ— () = 5, and rank S âˆ— () = 1 at every  âˆˆ E \ {0}. Since both the primal and
dual problems have unique optimal solutions for every  âˆˆ E \ {0}, the dual optimal set mapping
fails to be continuous at  = 0.

1

0.5

0

-0.5

-1
-2

-1.5

-1

-0.5

0

0.5

1

1.5

2

Figure 5. Left: The feasible set of problem (18). Right: The exact and numerical approximation of the optimal value
function for problem (18) on [âˆ’1, 23 ].

For the purpose of numerical experiments, we consider the bounded domain [âˆ’1, 32 ]. When starting
from a nonsingular point  = 12 with a fixed increment change 0.01, Algorithm 3 properly identifies

20

Hauenstein et al.: On computing the nonlinearity interval in parametric semidefinite optimization
Mathematics of Operations Research 00(0), pp. 000â€“000, c 0000 INFORMS

 = 0 as a singular point. One could easily skip over the singular point  = 0 when simply solving
at a finite set of mesh points. Figure 5 demonstrates the exact optimal value function versus its
numerical approximation obtained from Algorithm 3. Upon refining the accuracy of the approximate singular point and obtaining the singular point  = 0, we invoke Bertini solver in Algorithm 4

to compute the dimension of all irreducible components of V F (V, 0) whichcontain V a (0). We
observe that V a (0) lies on a 1-dimensional irreducible
component of

 V F (V, 0) , and there exists a
âˆ—
âˆ—
âˆ—
generic solution V (0) such that rank X (0) = 4 and rank S (0) = 2. All this indicates that the
rank of X âˆ— () and S âˆ— () change at  = 0, and thus  = 0 is a transition point. Consequently, we can
partition (âˆ’1, 23 ) into two nonlinearity intervals (âˆ’1, 0) and (0, 32 ) and the transition point {0}.
5.3. A non-transition point with singular Jacobian Here, we apply Algorithm 1 to
identify the singular points and the transition points of the parametric SDO problem (10) in a
bounded domain [âˆ’1, 2]. We initialize Algorithm 1 with the nonsingular point  = 0, see (7), and the
initial increment change âˆ† = 0.005. While tracking forwards, Algorithm 3 computes the numerical
approximation of the unique optimal solution until it locates the singular points  = 21 and  = 32 .
Then restarting the solution tracking at 23 + âˆ†, Algorithm 2 identifies the invariancy interval ( 32 , 2)
and the transition point  = 32 . In an analogous fashion, while tracking backwards, Algorithm 3 and
Algorithm 2 identify the singular point  = âˆ’ 12 and the invariancy interval (âˆ’1, âˆ’ 21 ), respectively.
Figure 6 illustrates the exact and numerical approximation of the optimal value function, where
the singular/transition points are represented by the dots marks.

Figure 6. The exact and numerical approximation of the optimal value function for problem (10) on [âˆ’1, 2].

Applying Algorithm 4 to the singular point  = 21 , we can observe that V a ( 12 ) is not isolated, and it

belongs to a 1-dimensional irreducible component of V F (V, 12 ) . We then invoke the polynomial

Hauenstein et al.: On computing the nonlinearity interval in parametric semidefinite optimization
Mathematics of Operations Research 00(0), pp. 000â€“000, c 0000 INFORMS

21

solver Bertini to compute a generic solution
ï£«

ï£¶
1
âˆ’0.0449 âˆ’0.0449 0
ï£¬âˆ’0.0449
1
1
0 ï£·
ï£·,
X âˆ— ( 12 ) = ï£¬
ï£­âˆ’0.0449
1
1
0 ï£¸
0
0
0
0.0898
ï£«

ï£¶
0
ï£¬âˆ’1ï£·
ï£¬ ï£·
ï£¬âˆ’1ï£·
ï£¬ ï£·
âˆ— 1
ï£·
y (2) = ï£¬
ï£¬ 0 ï£·,
ï£¬0ï£·
ï£¬ ï£·
ï£­0ï£¸
0

ï£¶
ï£«
0 0 00
ï£¬0 1 âˆ’1 0ï£·
ï£·
S âˆ— ( 12 ) = ï£¬
ï£­0 âˆ’1 1 0ï£¸ ,
0 0 00



in which rank X âˆ— ( 12 ) = 3 and rank S âˆ— ( 12 ) = 1. Given the rank of X âˆ— () and S âˆ— () on (âˆ’ 12 , 12 ) âˆª
( 21 , 23 ), all this implies that the singular point  = 12 belongs to the nonlinearity interval (âˆ’ 12 , 23 ).
Consequently, the domain (âˆ’1, 2) is partitioned as
Uinv = (âˆ’1, âˆ’ 21 ) âˆª ( 32 , 2),

Unon = (âˆ’ 12 , 32 ),

Utran = {âˆ’ 12 , 32 }.

6. Concluding remarks and future research This paper utilized an optimal partition
approach to parametric analysis for SDO problems, where the objective function is perturbed
along a fixed direction. In terms of continuity, we provided sufficient conditions for the existence of
nonlinearity intervals and the emptiness of the interior of the set of transition points. We showed
that the optimal set mapping might fail to be continuous on a nonlinearity interval, and the
sequence of maximally complementary optimal solutions may converge to the boundary of the
optimal set at an  in a nonlinearity interval. Under the assumption of the existence of a nonsingular
point in int(E ), we then proposed a methodology, stemming from numerical algebraic geometry,
to efficiently partition int(E ) into finite union of invariancy intervals, nonlinearity intervals, and
transition points. The computational approach was demonstrated on several examples.
We conjecture that condition (9) could fail at a boundary point of a nonlinearity interval. It is
worth providing a counterexample or sufficient conditions which guarantee the validity of (9) at
a boundary point of a nonlinearity interval. Furthermore, we still do not know about any upper
bound on the number of points at which P âˆ— () or Dâˆ— () fails to be continuous on a nonlinearity
interval, or whether the subspaces B (), T (), N () move continuously on a nonlinearity interval.
These topics are subjects of future research. We note that one could extend Theorem 2 to provide
an upper bound on the number of singular points, and hence on the number of transition points.
However, such bounds would be on the number of complex singular points, which may drastically
overestimate the number of transition points in int(E ).
Acknowledgments. The first and third authors were supported in part by Office of Naval
Research (ONR) grant N00014-16-1-2722 and National Science Foundation (NSF) grant CCF1812746. The second and fourth authors were supported in part by the Air Force Office of Scientific
Research (AFOSR) grant FA9550-15-1-0222.
References
[1] Adler I, Monteiro RDC (1992) A geometric view of parametric linear programming. Algorithmica
8(1):161â€“176.
[2] Alizadeh F, Haeberly JPA, Overton ML (1998) Primal-dual interior-point methods for semidefinite programming: Convergence rates, stability and numerical results. SIAM Journal on Optimization 8(3):746â€“
768.
[3] Basu S, Pollack R, Roy MF (2006) Algorithms in Real Algebraic Geometry (Springer).
[4] Bates DJ, Hauenstein JD, Peterson C, Sommese AJ (2009) A numerical local dimensions test for
points on the solution set of a system of polynomial equations. SIAM Journal on Numerical Analysis
47(5):3608â€“3623.

22

Hauenstein et al.: On computing the nonlinearity interval in parametric semidefinite optimization
Mathematics of Operations Research 00(0), pp. 000â€“000, c 0000 INFORMS

[5] Bates DJ, Hauenstein JD, Sommese AJ, Wampler CW (2006) Bertini: Software for Numerical Algebraic
Geometry. Available at bertini.nd.edu.
[6] Bates DJ, Hauenstein JD, Sommese AJ, Wampler CW II (2008) Adaptive multiprecision path tracking.
SIAM Journal on Numerical Analysis 46(2):722â€“746.
[7] Bates DJ, Sommese AJ, Hauenstein JD, Wampler CW (2013) Numerically Solving Polynomial Systems
with Bertini (PA).
[8] Berkelaar A, Jansen B, Roos K, Terlaky T (1996) Sensitivity analysis in (degenerate) quadratic programming. Technical Report 96-26, Delft University of Technology, Netherlands.
[9] Bonnans JF, RamÄ±Ìrez C H (2005) Perturbation analysis of second-order cone programming problems.
Mathematical Programming 104(2):205â€“227.
[10] Bonnans JF, Shapiro A (1998) Optimization problems with perturbations: A guided tour. SIAM Review
40(2):228â€“264.
[11] Bonnans JF, Shapiro A (2000) Perturbation Analysis of Optimization Problems (Springer).
[12] Butcher JC (2003) Numerical Methods for Ordinary Differential Equations (New York: John Wiley &
Sons).
[13] Cifuentes D, Agarwal S, Parrilo P, Thomas R (2017) On the local stability of semidefinite relaxations.
ArXiv:1710.04287 https://arxiv.org/abs/1710.04287.
[14] Davidenko D (1953) On a new method of numerical solution of systems of nonlinear eqauations. Dokl.
Akad. Nauk SSR 87(4):601â€“602.
[15] de Klerk E (2006) Aspects of Semidefinite Programming: Interior Point Algorithms and Selected Applications, volume 65 of Series Applied Optimization (Springer).
[16] Fiacco A, McCormick G (1990) Nonlinear Programming: Sequential Unconstrained Minimization Techniques, volume 65 of the series Applied Optimization (SIAM).
[17] Fiacco AV (1976) Sensitivity analysis for nonlinear programming using penalty methods. Mathematical
Programming 10(1):287â€“311.
[18] Fiacco AV (1983) Introduction to Sensitivity and Stability Analysis in Nonlinear Programming, volume
165 (Academic Press Inc.).
[19] Goldfarb D, Scheinberg K (1999) On parametric semidefinite programming. Applied Numerical Mathematics 29(3):361â€“377.
[20] Haeberly JP (1998) Remarks on nondegeneracy in mixed semidefinite-quadratic programming. Unpublished memorandum, available from http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.
1.43.7501&rep=rep1&type=pdf.
[21] HalickaÌ M, de Klerk E, Roos C (2002) On the convergence of the central path in semidefinite optimization. SIAM Journal on Optimization 12(4):1090â€“1099.
[22] Hauenstein JD, Haywood I, Liddell AC Jr (2014) An a posteriori certification algorithm for Newton
homotopies. ISSAC 2014â€”Proceedings of the 39th International Symposium on Symbolic and Algebraic
Computation, 248â€“255 (ACM, New York).
[23] Hauenstein JD, Sommese AJ (2017) What is numerical algebraic geometry? [Foreword]. J. Symbolic
Comput. 79(part 3):499â€“507.
[24] Hauenstein JD, Tang T (2018) On semidefinite programming under perturbations with unknown boundaries Available at https://www3.nd.edu/~jhauenst/preprints/htSDPperturb.pdf.
[25] Hauenstein JD, Wampler CW (2013) Isosingular sets and deflation. Foundations of Computational
Mathematics 13(3):371â€“403.
[26] Hogan WW (1973) Point-to-set maps in mathematical programming. SIAM Review 15(3):591â€“603.
[27] Jansen B, Roos K, Terlaky T (1993) An interior point method approach to postoptimal and parametric
analysis in linear programming. Technical Report 92-21, Delft University of Technology, Netherlands.
[28] Kalaba RE, Zagustin E, Holbrow W, Huss R (1977) A modification of Davidenkoâ€™s method for nonlinear
systems. Computers & Mathematics with Applications 3(4):315â€“319.

Hauenstein et al.: On computing the nonlinearity interval in parametric semidefinite optimization
Mathematics of Operations Research 00(0), pp. 000â€“000, c 0000 INFORMS

23

[29] Kojima M (1980) Strongly stable stationary solutions in nonlinear programs. Robinson SM, ed., Analysis
and Computation of Fixed Points, 93 â€“ 138 (Academic Press).
[30] Mohammad-Nezhad A, Terlaky T (2018) Parametric analysis of semidefinite optimization.
ArXiv:1808.00587 https://arxiv.org/abs/1808.00587.
[31] Munkres JR (2000) Topology (Prentice Hall).
[32] Nesterov Y, Nemirovskii A (1994) Interior-Point Polynomial Algorithms in Convex Programming, volume 13 (Society for Industrial and Applied Mathematics).
[33] Ortega J, Rheinboldt W (1970) Iterative Solution of Nonlinear Equations in Several Variables, volume
30 of Classics in Applied Mathematics (SIAM).
[34] Ramana MV (1997) An exact duality theory for semidefinite programming and its complexity implications. Mathematical Programming 77(1):129â€“162.
[35] Robinson SM (1982) Generalized equations and their solutions, part II: Applications to nonlinear programming. Guignard M, ed., Optimality and Stability in Mathematical Programming, 200â€“221 (Berlin,
Heidelberg: Springer).
[36] Rockafellar R, Dontchev A (2014) Implicit Functions and Solution Mappings (Springer).
[37] Rockafellar RT, Wets RJB (2009) Variational Analysis, volume 317 (Springer).
[38] Shapiro A (1997) First and second order analysis of nonlinear semidefinite programs. Mathematical
Programming 77(1):301â€“320.
[39] Sommese AJ, Wampler CW II (2005) The numerical solution of systems of polynomials arising in
engineering and science (World Scientific Publishing Co. Pte. Ltd., Hackensack, NJ).
[40] Sylvester JJ (1851) On a remarkable discovery in the theory of canonical forms and of hyperdeterminants.
Philosophical Magazine 4th series:391â€“410.
[41] Sylvester JJ (1904-1912) The collected mathematical papers of James Joseph Sylvester (Cambridge,
England: Cambridge University press).
[42] Wampler CW, Hauenstein JD, Sommese AJ (2011) Mechanism mobility and a local dimension test.
Mechanism and Machine Theory 46(9):1193â€“1206.
[43] Yildirim E (2004) Unifying optimal partition approach to sensitivity analysis in conic optimization.
Journal of Optimization Theory and Applications 122(2):405â€“423.

