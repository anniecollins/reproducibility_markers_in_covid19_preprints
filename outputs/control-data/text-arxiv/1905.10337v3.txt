What Can ResNet Learn Efficiently, Going Beyond Kernels?
Zeyuan Allen-Zhu
zeyuan@csail.mit.edu
Microsoft Research AI

Yuanzhi Li
yuanzhil@stanford.edu
Stanford University

May 23, 2019

arXiv:1905.10337v3 [cs.LG] 1 Jun 2020

(version 3)âˆ—

Abstract
How can neural networks such as ResNet efficiently learn CIFAR-10 with test accuracy more
than 96%, while other methods, especially kernel methods, fall relatively behind? Can we more
provide theoretical justifications for this gap?
Recently, there is an influential line of work relating neural networks to kernels in the overparameterized regime, proving they can learn certain concept class that is also learnable by
kernels with similar test error. Yet, can neural networks provably learn some concept class
better than kernels?
We answer this positively in the distribution-free setting. We prove neural networks can
efficiently learn a notable class of functions, including those defined by three-layer residual
networks with smooth activations, without any distributional assumption. At the same time,
we prove there are simple functions in this class such that with the same number of training
examples, the test error obtained by neural networks can be much smaller than any kernel
method, including neural tangent kernels (NTK).
The main intuition is that multi-layer neural networks can implicitly perform hierarchical
learning using different layers, which reduces the sample complexity comparing to â€œone-shotâ€
learning algorithms such as kernel methods. In a follow-up work [2], this theory of hierarchical
learning is further strengthened to incorporate the â€œbackward feature correctionâ€ process when
training deep networks.
In the end, we also prove a computation complexity advantage of ResNet with respect to
other learning methods including linear regression over arbitrary feature mappings.

âˆ—

V1 appears on this date, V2 slightly improved the lower bound, V3 strengthens experiments and adds citation to
â€œbackward feature correctionâ€ which is an even stronger form of hierarchical learning [2]. We would like to thank Greg
Yang for many enlightening conversations as well as discussions on neural tangent kernels. A 45-min presentation of
this result at the UC Berkeley Simons Institute can be found at https://youtu.be/NNPCk2gvTnI.

1

Introduction

Frobenius norm

Neural network learning has become a key practical machine learning approach and has achieved
remarkable success in a wide range of real-world domains, such as computer vision, speech recognition, and game playing [19, 20, 23, 35]. On the other hand, from a theoretical standpoint, it is less
understood that how large-scale, non-convex, non-smooth neural networks can be optimized efficiently over the training data and generalize to the test data with relatively few training examples.
There has been a sequence of research trying to address this question, showing that under
certain conditions neural networks can be learned efficiently [3, 9â€“11, 16, 17, 22, 24, 25, 27â€“29, 36â€“
39, 41, 44]. These provable guarantees typically come with strong assumptions and the proofs
heavily rely on them. One common assumption from them is on the input distribution, usually
being random Gaussian or sufficiently close to Gaussian. While providing great insights to the
optimization side of neural networks, it is not clear whether these works emphasizing on Gaussian
inputs can coincide with the neural network learning process in practice. Indeed, in nearly all real
world data where deep learning is applied to, the input distributions are not close to Gaussians;
even worse, there may be no simple model to capture such distributions.
The difficulty of modeling real-world distributions brings us back to the traditional PAC-learning
language which is distribution-free. In this language, one of the most popular, provable learning
methods is the kernel methods, defined with respect to kernel functions K(x, x0 ) over pairs of data
(x, x0 ). The optimization task associated with kernel methods is convex, hence the convergence
rate and the generalization error bound are well-established in theory.
Recently, there is a line of work studying the convergence of neural networks in the PAClearning language, especially for over-parameterized neural networks [1, 4â€“8, 13â€“15, 21, 26, 45],
putting neural network theory back to the distribution-free setting. Most of these works rely on
the so-called Neural Tangent Kernel (NTK) technique [13, 21], by relating the training process of
sufficiently over-parameterized (or even infinite-width) neural networks to the learning process over
a kernel whose features are defined by the randomly initialized weights of the neural network. In
other words, on the same training data set, these works prove that neural networks can efficiently
learn a concept class with as good generalization as kernels, but nothing more is known.1
In contrast, in many practical tasks, neural networks
400
give much better generalization error compared to ker300
nels, although both methods can achieve zero training
200
error. For example, ResNet achieves 96% test accuracy
100
on the CIFAR-10 data set, but NTKs achieve 77% [7]
0
and random feature kernels achieve 85% [33]. This gap
0.004
0.016
0.063
0.25
1.
error
becomes larger on more complicated data sets.
we construct by hand in train/test error
To separate the generalization power of neural netbest found by SGD in train error
best found by SGD in test error
works from kernel methods, the recent work [40] tries to
identify conditions where the solutions found by neural Figure 1: d = 40, N = 5000, after exhaustive
networks provably generalize better than kernels. This
search in network size, learning rate,
weight decay, randomly initialized SGD
approach assumes that the optimization converges to
still cannot find solutions with Frobeminimal complexity solutions (i.e. the ones minimizing
nius norm comparable to what we conthe value of the regularizer, usually the sum of squared
struct by hand. Details and more exFrobenius norms of weight matrices) of the training obperiments in Section 8.2.
1

Technically speaking, the three-layer learning theorem of [4] is beyond NTK, because the learned weights across
different layers interact with each other, while in NTK the learned weights of each layer only interact with random
weights of other layers. However, there exist other kernelsâ€” such as recursive kernels [43] â€” that can more or less
efficiently learn the same concept class proposed in [4].

1

jective. However, for most practical applications, it is unclear how, when training neural networks,
minimal complexity solutions can be found efficiently by local search algorithms such as stochastic
gradient descent. In fact, it is not true even for rather simple problems (see Figure 1).2 Towards
this end, the following fundamental question is largely unsolved:
Can neural networks efficiently and distribution-freely learn a concept class,
with better generalization than kernel methods?
In this paper, we give arguably the first positive answer to this question for neural networks with
ReLU activations. We show without any distributional assumption, a three-layer residual network
(ResNet) can (improperly) learn a concept class that includes three-layer ResNets of smaller size
and smooth activations. This learning process can be efficiently done by stochastic gradient descent
(SGD), and the generalization error is also small if polynomially many training examples are given.
More importantly, we give a provable separation between the generalization error obtained by
neural networks and arbitrary kernel methods. For some Î´ âˆˆ (0, 1), with N = O(Î´ âˆ’2 ) training samples, we prove that neural networks can efficiently achieve generalization error Î´ for this concept
class over any distribution; in contrast, there exists rather simple distributions such that any âˆš
kernel
method (including NTK, recursive kernel, etc) cannot have generalization error better than Î´ for
this class. To the best of our knowledge, this is the first work that gives provable, efficiently achievable separation between neural networks with ReLU activations and kernels in the distribution-free
setting. In the end, we also prove a computation complexity advantage of neural networks with
respect to linear regression over arbitrary feature mappings as well.
Roadmap. We present detailed overview of our positive and negative results in Section 2 and 3.
Then, we introduce notations in Section 4, formally define our concept class in Section 5, and give
proof overviews in Section 6 and 7.

2

Positive Result: The Learnability of Three-Layer ResNet

In this paper, we consider learner networks that are single-skip three-layer ResNet
with ReLU activation, defined as a function out : Rd â†’ Rk :
out(x) = A (Ïƒ (Wx + b1 ) + Ïƒ (UÏƒ (Wx + b1 ) + b2 ))

(2.1)

ğ‘¥

ğ‘Š
ReLU

Here, Ïƒ is the ReLU function, W âˆˆ RmÃ—d and U âˆˆ RmÃ—m are the hidden weights,
A âˆˆ RkÃ—m is the output weight, and b1 , b2 âˆˆ Rm are two bias vectors.
We wish to learn a concept class given by target functions that can be written
as
H(x) = F(x) + Î±G (F(x))

(2.2)

ğ‘ˆ
ReLU

ğ´

where Î± âˆˆ [0, 1) and G : Rk â†’ Rk , F : Rd â†’ Rk are two functions that can be written as two-layer
networks with smooth activations (see Section 5 for the formal definition). Intuitively, the target
function is a mixture of two parts: the base signal F, which is simpler and contributes more to
the target, and the composite signal G (F), which is more complicated but contributes less. As an
analogy, F could capture the signal in which â€œ85%â€ examples in CIFAR-10 can be learned by kernel
2
Consider the class of degree-6
âˆš polynomials over 6 coordinates of the d-dimensional input. There exist twolayer networks with F-norm O( d) implementing this function (thus have near-zero training and testing error).
By Rademacher complexity, O(d) samples suffice to learn if we are able to find a minimal complexity solution.
Unfortunately, due to the non-convexity of the optimization landscape, two-layer networks can not be trained to
match this F-norm even with O(d2 ) samples, see Figure 1.

2

methods, and G (F) could capture the additional â€œ11%â€ examples that are more complicated. The
goal is to use three-layer ResNet (2.1) to improperly learn this concept class (2.2), meaning learning
â€œbothâ€ the base and composite signals, with as few samples as possible. In this paper, we consider
a simple `2 regression task where the features x âˆˆ Rd and labels y âˆˆ Rk are sampled from some
unknown distribution D. Thus, given a network out(x), the population risk is
1
kout (x) âˆ’ yk22 .
(x,y)âˆ¼D 2
E

To illustrate our result, we first assume for simplicity that y = H (x) for some H of the form (2.2)
(so the optimal target has zero regression error). Our main theorem can be sketched as follows.
Let CF and CG respectively be the individual â€œcomplexityâ€ of F and G, which at a high level,
capture the size and smoothness of F and G. This complexity notion shall be formally introduced
in Section 4, and is used by prior works such as [4, 8, 43].

Theorem 1 (ResNet, sketched). For any distribution over x, for every Î´ âˆˆ (Î±CG )4 , 1 , with
probability at least 0.99, SGD efficiently learns a network out(x) in the form (2.1) satisfying
 2
1
e CF samples
E
kout (x) âˆ’ yk22 â‰¤ Î´ using N = O
Î´2
(x,y)âˆ¼D 2
The running time of SGD is polynomial in poly(CG , CF , Î±âˆ’1 ).
In other words, ResNet is capable of achieving population risk Î±4 , or equivalently learning the
output H(x) up to Î±2 error. In our full theorem, we also allow label y to be generated from H(x)
with error, thus our result also holds in the agnostic learning framework.

2.1

Our Contributions

Our main contribution is to obtain time and sample complexity in CF and CG without any dependency on the composed function G(F) as in prior work [4, 43]. We illustrate this crucial difference
with an example. Suppose
I/d), k = 2âˆšand F âˆˆ Rd â†’ R2 consists of two linear function:
 x âˆ¼ N (0,
âˆ—
âˆ—
âˆ—
F(x) = hw1 , xi, hw2 , xi with kw1 k2 , kw2âˆ— k2 = d, and G is degree-10 polynomial with constant
âˆš
e
Theorem 1 implies
coefficient. As we shall see, CF = O( d) and CG = O(1).
e
e 2 ).
â€¢ we need O(d)
samples to efficiently learn H = F + Î±G(F) up to accuracy O(Î±
âˆš 10
e
In contrast, the complexity of G(F) is O((
d) ), so
e 10 ) samples to efficiently learn H up to any accuracy o(Î±),
â€¢ prior works [4, 43] need â„¦(d
even if G(x) is of some simple form such as hw1âˆ— , xi10 âˆ’ hw2âˆ— , xi10 .3
Inductive Bias. Our network is over-parameterized, thus intuitively in the example above, with
only O(d) training examples, the learner network could over-fit to the training data since it has
to decide from a set of d10 many possible coefficients to learn the degree 10 polynomial G. This
is indeed the case if we learn the target function using kernels, or possibly even learn it with a
two-layer network. However, three-layer ResNet posts a completely different inductive bias, and
manages to avoid over-fitting to G(F) with the help from F.
Of course, if one knew a priori the form H(x) = hw1âˆ— , xi10 âˆ’ hw2âˆ— , xi10 , one could also try to solve it directly by
minimizing objective (hw1âˆ— , xi10 âˆ’ hw2âˆ— , xi10 + hw2 , xi10 âˆ’ hw1 , xi10 )2 over w1 , w2 âˆˆ Rd . Unfortunately, the underlying
optimization process is highly non-convex and it remains unclear how to minimize it efficiently. Using matrix
e 5 ).
sensing [29], one can efficiently learn such H(x) in sample complexity O(d
3

3

Implicit Hierarchical Learning using Forward Feature Learning. Since H(x) = F(x) +
Î±G (F(x)), if we only learn F but not G (F), we will have regression error â‰ˆ (Î±CG )2 . Thus, to get
to regression error (Î±CG )4 , Theorem 1 shows that ResNet is also capable of learning G (F) up to
some good accuracy with relatively few training examples. This is also observed in practice, where
with this number of training examples, three-layer fully-connected networks and kernel methods
can indeed fail to learn G (F) up to any non-trivial accuracy, see Figure 2.
Intuitively, there is a hierarchy of the learning process: we would like to first learn F, and
then we could learn G(F) much easier with the help of F using the residual link. In our learner
network (2.1), the first hidden layer serves to learn F and the second hidden layer serves to learn
G with the help of F, which reduces the sample complexity. However, the important message is
that F and G are not given as separate data to the network, rather the learning algorithm has to
disentangle them from the â€œcombinedâ€ function H = F + Î±G(F) automatically during the training
process. Moreover, since we train both layers simultaneously, the learning algorithm also has to
distribute the learning task of F and G onto different layers automatically. We call this process
â€œforward feature learningâ€:
Hierarchical Learning in ResNet: The Forward Feature Learning
During the training process of a residual network, the lower-level layers automatically learn an
approximation of the lower-complexity features/signals in the target function. It then forward
these features to the higher-level layers in the network to further learn the higher-complexity
features/signals in the target function.
We point out forward feature learning is different from layer-wise training. For instance, our
result cannot be obtained by first training the hidden layer close to the input, and then fixing it and
training the hidden layer close to the output. Since it could be the case the first layer incurs some
Î± error (since it cannot learn G(F) directly), then it could be really hard, or perhaps impossible,
for the second layer to fix it only using inputs of the form F(x) Â± Î±. In other words, it is crucial
that the two hidden layers are simultaneously trained. 4
A follow-up work. In a follow-up work [2], this theory of hierarchical learning is strengthened to
further incorporate the backward feature correction step when training deep neural networks.
In the language of this paper, when the two layers trained together, given enough samples, the
accuracy in the first layer can actually be improved from F Â± Î± to arbitrarily close to F during the
training process. As a consequence, the final training and generalization error can be arbitrarily
small as well, as opposite to Î±2 (or equivalently population risk Î±4 ) in this work. The new â€œbackward
feature correctionâ€ is also critical to extend the hierarchical learning process from 3 layers to
arbitrarily number of layers.

3
3.1

Negative Results
Limitation of Kernel Methods

Given (Mercer) kernels K1 , . . . , Kk : RdÃ—d â†’ R and training examples {(x(i) , y (i) )}iâˆˆ[N ] from D, a
kernel method tries to learn a function K : Rd â†’ Rk where each
P
Kj (x) = nâˆˆ[N ] Kj (x, x(n) ) Â· wj,n
(3.1)
4
This does not mean that the error of the first layer can be reduced by its own, since it is still possible for the
first layer to learn F + Î±R(x) Â± Î±2 and the second layer to learn G(F)(x) âˆ’ R(x) Â± Î±, for an arbitrary (bounded)
function R.

4

is parameterized by a weight vector wj âˆˆ RN . Usually, for the `2 regression task, a kernel method
finds the optimal weights w1 , . . . , wk âˆˆ RN by solving the following convex minimization problem
P
(i) 2
1 PN P
(i) (n) )w
+ R(w)
(3.2)
j,n âˆ’ yj
i=1
jâˆˆ[k]
nâˆˆ[N ] Kj (x , x
N
for some convex regularizer R(w).5 In this paper, however, we do not make assumptions about how
K(x) is found as the optimal solution of the training objective. Instead, we focus on any kernel
regression function that can be written in the form (3.1).
Most of the widely-used kernels are Mercer kernels.6 This includes
(1) Gaussian kernel K(x, y) =

2 /h
âˆ’kxâˆ’yk
2
e
; (2) arcsin kernel K(x, y) = arcsin hx, yi/(kxk2 kyk2 ) ; (3) recursive kernel with any recursive function [43]; (4) random feature kernel K(x, y) = Ewâˆ¼W Ï†w (x)Ï†w (y) for any function Ï†w (Â·)
and distribution W; (5) the conjugate kernel defined by the last hidden layer of random initialized neural networks [12]; (6) the neural tangent kernels (NTK) for fully-connected [21] networks,
convolutional networks [7, 42] or more generally for any architectures [42].
Our theorem can be sketched as follows:
Theorem 2 (kernel, sketched). For every constant k â‰¥ 2, for every sufficiently large d â‰¥ 2, there
exist concept classes consisting of functions H(x) = F(x) + Î±G (F(x)) with complexities CF , CG
and Î± âˆˆ (0, C1G ) such that, letting
Nres be the sample complexity from Theorem 1 to achieve Î±3.9 population risk,
then there exists simple distributions D over (x, H(x))
 such that, for at least 99% of the functions
k/2
H in this concept class, even given N = O (Nres )
training samples from D, any function K(x)
of the form (3.1) has to suffer population risk
E(x,y)âˆ¼D

1
2

kK(x) âˆ’ yk22 > Î±2

even if the label y = H(x) has no error.

Contribution and Intuition. Let us compare this to Theorem 1. While both algorithms are
efficient, neural networks (trained by SGD) achieve population risk Î±3.9 using Nres samples for any
distribution over x, while kernel methods cannot achieve any population risk better than Î±2 for some
simple distributions even with N = (Nres )k/2  Nres samples.7 Our two theorems together gives
a provable separation between the generalization error of the solutions found by neural networks
and kernel methods, in the efficiently computable regime.
More specifically, recall CF and CG only depend on individual complexity of G, F, but not on
G(F). In Theorem
âˆš 2, we will construct F as linear functions and G as degree-k polynomials. This
ensures CF = O( d) and CG = O(1) for k being constant, but the combined complexity of G(F) is
as high as â„¦(dk/2 ). Since ResNet can perform hierarchical learning, it only needs sample complexity
Nres = O(d/Î±8 ) instead of paying (square of) the combined complexity â„¦(dk ).
In contrast, a kernel method is not hierarchical: rather than discovering F first and then learning
G(F) with the guidance of it, kernel method tries to learn everything in one shot. This unavoidably
requires the sample complexity to be at least â„¦(dk ). Intuitively, as the kernel method tries to learn
G(F) from scratch, this means that it has to take into account all â„¦(dk ) many possible choices
of G(F) (recall that G is a degree k polynomial over dimension d). On the other hand, a kernel
5

In many cases, R(w) = Î» Â·

P

jâˆˆ[k]

wj> Kj wj is the norm associated with the kernel, for matrix Kj âˆˆ RN Ã—N defined

as [Kj ]i,n = Kj (x(i) , x(n) ).
6
Recall a Mercer kernel K : RdÃ—d â†’ R can be written as K(x, y) = hÏ†(x), Ï†(y)i where Ï† : Rd â†’ V is a feature
mapping to some inner product space V.
7
It is necessary the negative result of kernel methods is distribution dependent, since for trivial distributions where
x is non-zero only on the first constantly many coordinates, both neural networks and kernel methods can learn it
with constantly many samples.

5

method with N samples only has N -degrees of freedom (for each output dimension). This means, if
N  o(dk ), kernel method simply does not have enough degrees of freedom to distinguish between
different G(F), so has to pay â„¦(Î±2 ) in population risk. Choosing for instance Î± = dâˆ’0.1 , we have
the desired negative result for all N â‰¤ O (Nres )k/2  o(dk ).

3.2

Limitation of Linear Regression Over Feature Mappings

Given an arbitrary feature mapping Ï† : Rd â†’ RD , one may consider learning a linear function over
Ï†. Namely, to learn a function F : Rd â†’ Rk where each
Fj (x) = wj> Ï†(x)

(3.3)

is parameterized by a weight vector wj âˆˆ RD . Usually, these weights are determined by minimizing
the following regression objective:8

2

P
1 P
> Ï† x(i) âˆ’ y (i)
w
+ R(w)
j
iâˆˆ[N ]
jâˆˆ[k]
j
N
for some regularizer R(w). In this paper, we do not make assumptions about how the weighted are
found. Instead, we focus on any linear function over such feature mapping in the form (3.3).
Theorem 3 (feature mapping, sketched). For sufficiently large integers d, k, there exist concept
classes consisting of functions H(x) = F(x) + Î±G (F(x)) with complexities CF , CG and Î± âˆˆ (0, C1G )
such that, letting
Tres be the time complexity from Theorem 1 to achieve Î±3.9 population risk,
then for at least 99% of the functions H in this concept class, even with arbitrary D = (Tres )2
dimensional feature mapping, any function F(x) of the form (3.3) has to suffer population risk
E(x,y)âˆ¼D

1
2

kF(x) âˆ’ yk22 > Î±2

even if the label y = H(x) has no error.

Interpretation. Since any algorithm that optimizes linear functions over D-dimensional feature
mapping has to run in time â„¦(D), this proves a time complexity separation between neural networks
(say, for achieving population risk Î±3.9 ) and linear regression over feature mappings (for achieving
even any population risk better than Î±2  Î±3.9 ). Usually, such an algorithm also has to suffer
from â„¦(D) space complexity. If that happens, we also have a space complexity separation. Our
hard instance in proving Theorem 3 is the same as Theorem 2, and the proof is analogous.

4

Notations

We denote by kwk2 and kwkâˆ the Euclidean and infinity norms of vectors w, and kwk0 the number
of non-zeros of w. We also abbreviate kwk = kwk2 when it is clear from the context. We denote
the row `p norm for W âˆˆ RmÃ—d (for p â‰¥ 1) as
P
def
p 1/p
kWk2,p =
.
iâˆˆ[m] kwi k2
By definition, kWk2,2 = kWkF is the Frobenius norm of W. We use kWk2 to denote the matrix
spectral norm. For a diagonal matrix D we use kDk0 to denote its sparsity. For a matrix W âˆˆ
RmÃ—d , we use Wi or wi to denote the i-th row of W.
We use N (Âµ, Ïƒ) to denote Gaussian distribution with mean Âµ and variance Ïƒ; or N (Âµ, Î£) to
denote Gaussian vector with mean Âµ and covariance Î£. We use 1event or 1[event] to denote the
8

If R(w) is the `2 regularizer, then this becomes a kernel method again since the minimizer can be written in the
form (3.1). For other regularizers, this may not be the case.

6

indicator function of whether event is true. We use Ïƒ(Â·) to denote the ReLU function, namely
Ïƒ(x) = max{x, 0} = 1xâ‰¥0 Â· x. Given univariate function f : R â†’ R, we also use f to denote the
same function over vectors: f (x) = (f (x1 ), . . . , f (xm )) if x âˆˆ Rm .
For notation simplicity, throughout this paper â€œwith high probabilityâ€ (or w.h.p.) means with
2
e to hide polylog(m) factors.
probability 1 âˆ’ eâˆ’c log m for a sufficiently large constant c. We use O
Function complexity. The following notions introduced
the complexity of any
Pin [4] measure
i is its Taylor expansion. 9
c
z
infinite-order smooth function Ï† : R â†’ R. Suppose Ï†(z) = âˆ
i=0 i
âˆš


log(1/Îµ) âˆ— i
def P
âˆ— i
âˆš
CÎµ (Ï†) = CÎµ (Ï†, 1) = âˆ
C
|ci |
i=0 (C ) +
i
P
def
Cs (Ï†) = Cs (Ï†, 1) = C âˆ— âˆ
i=0 (i + 1)|ci |
where C âˆ— is a sufficiently large constant (e.g., 104 ).
Example 4.1. If Ï†(z) = ecÂ·z âˆ’ 1, sin(c Â· z), cos(c Â· z) or degree-c polynomial for constant c, then
CÎµ (Ï†, 1) = o(1/Îµ) and Cs (Ï†, 1) = O(1). If Ï†(z) = sigmoid(z) or tanh(z), to get Îµ approximation we
can truncate their Taylor series at degree Î˜(log 1Îµ ). One can verify that CÎµ (Ï†, 1) â‰¤ poly(1/Îµ) by the
fact that (log(1/Îµ)/i)i â‰¤ poly(Îµâˆ’1 ) for every i â‰¥ 1, and Cs (Ï†, 1) â‰¤ O(1).

5

Concept Class

We consider learning some unknown distribution D of data points z = (x, y) âˆˆ Rd Ã— Rk , where
x âˆˆ Rd is the input vector and y is the associated label. Let us consider target functions H : Rd â†’ Rk
coming from the following concept class.
Concept 1. H is given by two smooth functions F, G : Rk â†’ Rk and a value Î± âˆˆ R+ :
H(x) = F(x) + Î±G (F(x)) ,
where for each output coordinate r,
X

âˆ—
Fr (x) =
aâˆ—F ,r,i Â· Fr,i hwr,i
, xi

and

iâˆˆ[pF ]

Gr (h) =

(5.1)
X

âˆ—
aâˆ—G,r,i Â· Gr,i hvr,i
, hi



(5.2)

iâˆˆ[pG ]

âˆ— âˆˆ Rd and v âˆ— âˆˆ Rk . We assume for
for some parameters aâˆ—F ,r,i , aâˆ—G,r,i âˆˆ [âˆ’1, 1] and vectors wr,i
r,i
âˆš
âˆ— k = kv âˆ— k = 1/ 2.10 For simplicity, we assume kxk = 1 and kF(x)k = 1 for
simplicity kwr,i
2
2
2
2
r,i
(x, y) âˆ¼ D and in Appendix A we state a more general Concept 2 without these assumptions.11

We denote by CÎµ (F) = maxr,i {CÎµ (Fr,i )} and Cs (F) = maxr,i {Cs (Fr,i )}. Intuitively, F and G
are both generated by two-layer neural networks with smooth activation functions Fr,i and Gr,i .
Borrowing the agnostic PAC-learning language, our concept class consists of all functions H(x)
in the form of Concept 1 with complexity bounded by tuple (pF , CF , pG , CG ). Let OPT be the
population risk achieved by the best target function in this concept class. Then, our goal is to learn
this concept class with population risk O(OPT)+Îµ using sample and time complexity polynomial in
pF , CF , pG , CG and 1/Îµ. In the remainder of this paper, to simplify notations, we do not explicitly
9
In [4, ver.5], they have used (i + 1)1.75 |ci | instead of (i + 1)|ci |. For the purpose of this paper we have tightened
this complexity measure.
10
âˆ—
âˆ—
For general kw1,i
k2 â‰¤ B, kw2,i
k2 â‰¤ B, |aâˆ—r,i | â‰¤ B, the scaling factor B can be absorbed into the activation
0
function Ï† (x) = Ï†(Bx). Our results then hold by replacing the complexity of Ï† with Ï†0 .
11
Since we use ReLU networks as learners, they are positive homogeneous so to learn general functions F, G which
may not be positive homogenous, it is in some sense necessary that the inputs are scaled properly.

7

define this concept class parameterized by (pF , CF , pG , CG ). Instead, we equivalently state our
theorem with respect to any (unknown) fixed target function H with with population risk OPT:


E(x,y)âˆ¼D 12 kH(x) âˆ’ yk22 â‰¤ OPT .
In the analysis we adopt the following notations. For every (x, y) âˆ¼ D, it satisfies kF(x)k2 â‰¤ BF
and kG(F(x))k2 â‰¤ BF â—¦G . We âˆš
assume G(Â·) is LG -Lipschitz
continuous. It is a simple âˆš
exercise (see
âˆš
Fact A.3) to verify that LG â‰¤ kpG Cs (G), BF â‰¤ kpF Cs (F) and BF â—¦G â‰¤ LG BF + kpG C(G) â‰¤
kpF Cs (F)pG Cs (G).

6

Overview of Theorem 1

We learn the unknown distribution D with three-layer ResNet with ReLU activation (2.1) as learners. For notation simplicity, we absorb the bias vector into weight matrix: that is, given W âˆˆ RmÃ—d
and bias b1 âˆˆ Rm , we rewrite Wx + b as W(x, 1) for a new weight matrix W âˆˆ RmÃ—(d+1) . We
also re-parameterize U as U = VA and we find this parameterization (similar to the â€œbottleneckâ€
structure in ResNet) simplifies the proof and also works well empirically for our concept class. After
such notation simplification and re-parameterization, we can rewrite out(x) : Rd â†’ Rk as


out(W, V; x) = out(x) = out1 (x) + AÏƒ (V(0) + V)(out1 (x), 1)
out1 (W, V; x) = out1 (x) = AÏƒ(W(0) + W)(x, 1) .
Above, A âˆˆ RkÃ—m , V(0) âˆˆ RmÃ—(k+1) , W(0) âˆˆ RmÃ—(d+1) are weight matrices corresponding to random initialization, and W âˆˆ RmÃ—(k+1) , W âˆˆ RmÃ—(d+1) are the additional weights to be learned by
the algorithm. To prove the strongest result, we only train W, V and do not train A.12 We consider
random Gaussian initialization where the entries of A, W(0) , V(0) are independently generated as
follows:



1
2
Ai,j âˆ¼ N 0, m
[W(0) ]i,j âˆ¼ N 0, Ïƒw
[V(0) ]i,j âˆ¼ N 0, Ïƒv2 /m
In this paper we focus on the `2 loss function between H and out, given as:
1
Obj(W, V; (x, y)) = ky âˆ’ out(W, V; x)k22
2
We consider the vanilla SGD algorithm given in Algorithm 1.13

(6.1)

Algorithm 1 SGD
1: Initially W0 , V0 = 0.
2: for t = 0, 1, Â· Â· Â· , T âˆ’ 1 do
3:
Sample (xt , yt ) âˆ¼ D.
4:
Define `2 objective Obj(W, V; (xt , yt )) = 21 kyt âˆ’ out(W, V; xt )k22 .
t ,yt ))
5:
Update Wt+1 â† Wt âˆ’ Î·w âˆ‚Obj(W,V;(x
.
âˆ‚W
W=Wt ,V=Vt
t ,yt ))
Update Vt+1 â† Vt âˆ’ Î·v âˆ‚Obj(W,V;(x
âˆ‚V
7: end for

6:

W=Wt ,V=Vt

12

.

This can be more meaningful than training all the layers together, in which if one is not careful with parameter
choices, the training process can degenerate as if only the last layer is trained [12]. (That is a convex kernel method.)
Of course, as a simple corollary, our result also applies to training all the layers together, with appropriately chosen
random initialization and learning rate.
13
Performing SGD with respect to W(0) + W and V(0) + V is the same as that with respect to W and V; we
introduce W(0) , V(0) notation for analysis purpose. Note also, one can alternatively consider having a training set
and then performing SGD on this training set with multiple passes; similar results can be obtained.

8


1
e
Theorem 1. Under Concept 1 or Concept 2, for every Î± âˆˆ 0, Î˜(
kpG Cs (G) ) and Î´ â‰¥ OPT +

e Î±4 (kpG Cs (G))4 (1 + BF )2 . There exist M = poly(CÎ± (F), CÎ± (G), pF , Î±âˆ’1 ) satisfying that for
Î˜
every m â‰¥ M , with high probability over A, W(0) , V(0) , for a wide range of random initialization
parameters Ïƒw , Ïƒv (see Table 1), choosing




2
Î±pG Cs (G) 2
e
e
e (kpF Cs (F))
Î·
=
Î˜
(min{1,
Î´})
Î·
=
Î·
Â·
Î˜
T =Î˜
w
v
w
min{1, Î´ 2 }
pF Cs (F)
With high probability, the SGD algorithm satisfies
T âˆ’1
1 X
E kH(x) âˆ’ out(Wt , Vt ; x)k22 â‰¤ O(Î´) .
T
(x,y)âˆ¼D
t=0

As a corollary, under Concept 1, we can archive population risk
T âˆ’1

1 X
e Î±4 (kpG Cs (G))4
E kH(x) âˆ’ out(Wt , Vt ; x)k22 â‰¤ O(OPT) + Î˜
T
(x,y)âˆ¼D

using sample complexity T .

t=0

(6.2)
Remark 6.1. Our Theorem 1 is almost in the PAC-learning language, except that the final error
has an additive Î±4 term that can not be arbitrarily small.

6.1

Proof Overview

In the analysis, let us define diagonal matrices
DV(0) ,W = diag{1V(0) (out1 (x),1)â‰¥0 }

DW(0) = diag{1W(0) (x,1)â‰¥0 }
DW = diag{1(W(0) +W)(x,1)â‰¥0 }

DV,W = diag{1(V(0) +V)(out1 (x),1)â‰¥0 }

which satisfy out1 (x) = ADW (W(0) + W)(x, 1) and out(x) = ADV,W (V(0) + V)(out1 (x), 1).
The proof of Theorem 1 can be divided into three simple steps with parameter choices in Table 1.
1
e
In this paper, we assume 0 < Î± â‰¤ O(
kpG Cs (G) ) and choose parameters

and they satisfy

Ïƒw âˆˆ [mâˆ’1/2+0.01 , mâˆ’0.01 ]

Ïƒv âˆˆ [polylog(m), m3/8âˆ’0.01 ]

def
e
Ï„w = Î˜(kp
F Cs (F)) â‰¥ 1

def
e
Ï„v = Î˜(Î±kp
G Cs (G)) â‰¤



1/4
Ï„w âˆˆ m1/8+0.001 Ïƒw , m1/8âˆ’0.001 Ïƒw

1
polylog(m)


Ïƒv
3/8
Ï„v âˆˆ Ïƒv Â· (k/m) ,
polylog(m)

Table 1: Three-layer ResNet parameter choices.


2
Ïƒw , Ïƒv : recall entries of W(0) and V(0) are from N 0, Ïƒw
and N 0, Ïƒv2 /m .
Ï„w , Ï„v : the proofs work with respect to kWk2 â‰¤ Ï„w and kVk2 â‰¤ Ï„v .

In the first step, we prove that for all weight matrices not very far from random initialization
(namely, all kWk2 â‰¤ Ï„w and kVk2 â‰¤ Ï„v ), many good â€œcoupling propertiesâ€ occur. This includes
upper bounds on the number of sign changes (i.e., on kDW(0) âˆ’ DW k0 and DV(0) ,W âˆ’ DV,W )
0

as well as vanishing properties such as ADW W(0) , ADV,W V(0) being negligible. We prove such
properties using techniques from prior works [4, 6]. Details are in Section C.1.
Ï„v
In the second step, we prove the existence of W> , V> with kW> kF â‰¤ Ï„10w and kV> kF â‰¤ 10
satisfying ADW(0) W> (x, 1) â‰ˆ F(x) and ADV(0) ,W V> (out1 (x), 1) â‰ˆ Î±G (out1 (x)). This existential
9

proof relies on an â€œindicator to functionâ€ lemma from [4]; for the purpose of this paper we have to
revise it to include a trainable bias term (or equivalently, to support vectors of the form (x, 1)).
Combining it with the aforementioned vanishing properties, we derive (details are in Section C.2):
ADW W> (x, 1) â‰ˆ F(x)

and ADV,W V> (out1 (x), 1) â‰ˆ Î±G (out1 (x)) .

(6.3)

In the third step, consider iteration t of SGD with sample (xt , yt ) âˆ¼ D. For simplicity we assume
OPT = 0 so yt = H(xt ). One can carefully write down gradient formula, and plug in (6.3) to derive
def

Ît = hâˆ‡W,V Obj(Wt , Vt ; (xt , yt )), (Wt âˆ’ W> , Vt âˆ’ V> ))i
â‰¥ 21 kH(xt ) âˆ’ out(Wt , Vt ; xt )k22 âˆ’ 2kErrt k22



e Î±4 (kpG Cs (G))4 . This quantity Ît is quite famous in classical mirror
with E kErrt k22 â‰¤ Î˜
descent analysis: for appropriately chosen learning rates, Ît must converge to zero.14 In other
words, by concentration, SGD is capable of finding solutions Wt , Vt so that the population risk
kH(xt ) âˆ’ out(Wt , Vt ; xt )k22 is as small as E[kErrt k22 ]. This is why we can obtain population risk

e Î±4 (kpG Cs (G))4 in (6.2). Details are in Section C.3 and C.4.
Î˜

7

Overview of Theorem 2 and 3

 Â±1 d
We construct the following hard instance. The input x âˆˆ âˆš
in on the (scaled) Boolean cube,
d


def
Â±1 d1
and is drawn from distribution x âˆ¼ D = U âˆš
Ã—D2 . That is, the first d1 coordinates are drawn
d
âˆš d
uniformly at random from {Â±1/ d} 1 , and the last d âˆ’ d1 coordinates are drawn from an arbitrary
distribution D2 . Our hard instance works for a wide range of d1 , including for example d1 = d
(uniform distribution over boolean cube) and d1 = o(d) (only a small subset of the coordinates are
uniform). We consider X = {x(1) , . . . , x(N ) } being N i.i.d. samples from D.
Consider the class of target functions H(x) = F(x) + Î±G(F(x)), where

Q
F(x) = Wâˆ— x and G(y) =
(7.1)
jâˆˆ[k] yj iâˆˆ[k]
âˆš
where Wâˆ— = d(ei1 , ei2 , Â· Â· Â· eik ) for
 i1 , i2 , . . . , ik âˆˆ [d1 ] are distinct indices chosen from the first d1
coordinates. There are clearly dk1 many target functions in this class.
Intuitively, e1 , Â· Â· Â· , ed1 represent the directions where the signal possibly lies, where usually
the inputs would have high variance; and ed1 +1 , . . . , ed represent the directions that can be view
as â€œbackground noiseâ€, where the distribution can be arbitrary. For example when d1 â‰¤ d/2,
such distribution D can be very different from Gaussian distribution or uniform distribution over
Boolean cube, yet kernel methods still suffer from high population risk when learning over these
distributions comparing to using neural networks.
We first state the population risk for the three-layer ResNet to learn this concept class: Our
Theorem 1 âˆš
implies the following complexity on learning this concept class (after verifying that
Cs (F) = O( d), pF = 1, Cs (G) = 2O(k) , pG = 2k , see Section D.4).

1
Corollary 7.1. For every d â‰¥ d1 â‰¥ k â‰¥ 2, for every Î± âˆˆ 0, e O(k)
, there exist M =
Î˜(2

)

poly(d, 2k , Î±âˆ’1 ) satisfying that for every m â‰¥ M , for every target functions H(x) in the class (7.1),
with probability at least 0.99 over A, W(0) , V(0) and X , given labels y (n) = H(x(n) ) for n âˆˆ [N ],
P âˆ’1
Indeed, one can show Tt=0
Ît â‰¤ O(Î·w + Î·v ) Â· T +
âˆš
O( T ) ignoring other factors.
14

kW> k2
F
Î·w

10

+

kV> k2
F
Î·v

, and thus the right hand side can be made

SGD finds a network out(x) with population risk
E kH(x) âˆ’

xâˆ¼D

7.1

out(x)k22

e 4 2O(k) )
â‰¤ O(Î±


e
using N = Î˜

k2 d
Î±8


samples .

Kernel Method

We restate Theorem 2 as follows.

d1
1
Theorem 2 (restated). For every integers k, d1 , d, N satisfying 2 â‰¤ k â‰¤ d1 â‰¤ d and N â‰¤ 1000
k ,
for every Î± âˆˆ (0, 1), for every X , for every (Mercer) kernels K1 , . . . , Kk : RdÃ—d â†’ R, the following
holds for at least 99% of the target functions H(x) in the class (7.1). For all kernel regression
functions
P
Ki (x) = nâˆˆ[N ] Ki (x, x(n) ) Â· wi,n for i âˆˆ [k],
where weights wi,n âˆˆ R can depend on Î±, X , K and the training labels {y (1) , Â· Â· Â· , y (N ) }, it must
suffer from population risk
E kH(x) âˆ’ K(x)k22 > Î±2 /16 .

xâˆ¼D

As an example, when k â‰¥ 2 is constant, d = Î˜(d1 ) is sufficiently large, and Î± = Î˜(dâˆ’0.1 ),
â€¢ Corollary 7.1 says that ResNet achieves regression error Î±3.9 on the true distribution, with
e 1.8 ) samples to learn any function in (7.1);
Nres = O(d
â€¢ Theorem 2 says that kernel methods cannot achieve Î±2 /16 error even with N â‰¤ (Nres )k/2 
o(dk ) samples. Hence, to achieve generalization Î±2 /16  Î±3.9 , the sample complexity of any
kernel method is at least N â‰¥ (Nres )k/2  Nres .
Proof Overview. Our proof of Theorem 2 is relatively
simple, and we illustrate the main idea

n
in the case of d = d1 . At a high level, given N  d samples, the kernel regression function only
has
 N -degrees of freedom (each with respect to a sample point). Now, since there are possibly
n
d many target functions, if the kernel regression learns most of these target functions to some
sufficient accuracy, then by some rank counting argument, the degree of freedom is not enough.

7.2

Linear Regression Over Feature Mappings

We restate Theorem 3 as follows.

d1
1
Theorem 3 (restated). For every integers k, d1 , d, D satisfying 2 â‰¤ k â‰¤ d1 â‰¤ d and D â‰¤ 1000
k ,
for every Î± âˆˆ (0, 1), for every feature mapping Ï† : Rd â†’ RD , the following holds for at least 99% of
the target functions H(x) in the class (7.1). For all linear regression functions
Fj (x) = wj> Ï†(x)

for j âˆˆ [k],

where weights wj âˆˆ RD can depend on Î± and Ï†, it must suffer from population risk
E kH(x) âˆ’ F(x)k22 > Î±2 /16 .

xâˆ¼D

As an example, there exists sufficiently large constant c > 1 such that, for every k â‰¥ 4c, for every
d1 â‰¥ d/2, for every d â‰¥ â„¦(2k ), there exists choice Î± = 2âˆ’Î˜(k) Â· dâˆ’0.001 such that
e 4 2O(k) ) â‰¤ Î±3.9 in time Tres =
â€¢ Corollary 7.1 says that ResNet achieves regression error O(Î±
poly(d, 2k , Î±âˆ’1 ) â‰¤ dc to learn any function in (7.1);
11

â€¢ Theorem 3 says that linear
over feature mapping cannot achieve regression error
 regression
d1
2
2c
Î± /16 even if D = â„¦ k â‰¥ d .
In particular, this means linear regression over feature mappings cannot achieve regression error
Î±2 /16 even if D = (Tres )2 . Since a linear regression over RD normally takes at least time/space D
to compute/store, this implies that ResNet is also more time/space efficient than linear regression
over feature mappings as well.
Theorem 3 can be proved in the same way as Theorem 2, using exactly the same hard instance,
since F(x) has exactly D-degrees of freedom.

8

Experiments

8.1

ResNet vs. Kernel Methods vs. Fully-Connected Networks

Consider synthetic data where the feature vectors x âˆˆ {âˆ’1, 1}30 that are uniformly sampled at
random, and labels are generated from a target function H(x) = F(x) + Î±G(F(x)) âˆˆ R15 satisfying
F(x) = (x1 x2 , . . . , x29 x30 ) and Gi (y) = (âˆ’1)i y1 y2 y3 y4 for all i = 1, 2, . . . , 15. In other words, F is a
degree-2 parity function over 30 dimensions, and G is a degree-4 parity function over 15 dimensions.
Neural Networks Algorithms. Recall in our positive result on three-layer ResNet (see Theorem 1
and Footnote 12), to prove the strongest result, we only train hidden weights W and V but not the
output layer A. One can naturally extend this to show that Theorem 1 also holds when W, V, A
are jointly trained. For such reason, we implement both algorithms: 3resnet(hidden) for training
only W, V and 3resnet(all) for training all W, V, A. This is similar for two-layer and three-layer
fully-connected networks, where previously the strongest theoretical work is in terms of training
only hidden weights [4], so we implement both (all) and (hidden) for them.
Kernel Methods. We implement conjugate kernel, which corresponds to training only the last
(output) layer [12]; as well as neural tangent kernel (NTK), in which we train all the layers [21].
Setup. We choose the network width (i.e., parameter m) in the range m âˆˆ {20, 50, 100, 200, . . . }
until the largest possible value m that fits into a 16GB GPU memory. We choose the popular
random initialization: entries of A, V, W (and their corresponding bias terms) are all i.i.d. from
1 15
N (1, m
). We use similar initializations for two and three-layer networks.
We use the default SGD optimizer of pytorch, with momentum 0.9, mini-batch size 50. We
carefully run each algorithm with respect to learning rates and weight decay parameters in the set
{10âˆ’k , 2 Â· 10âˆ’k , 5 Â· 10âˆ’k : k âˆˆ Z}, and present the best one in terms of testing accuracy. In each
parameter setting, we run SGD for 800 epochs, and decrease the learning rate by 10 on epoch 400.
Experiment 1: Performance Comparison. Since it is unfair to compare neural network
training â€œwith respect to hidden weights onlyâ€ vs. â€œwith respect to all weightsâ€, we conduct two
experiments. The first experiment is on training all layers vs. kernel methods, see Figure 2(a); and
the second experiment is on training hidden layers vs. kernel methods, see Figure 2(b). We use
N = 500 training samples for the former case and N = 1000 samples for the latter case, because
training the last layer together gives more power to a neural network.
In both experiments, we choose Î± = 0.3 and k = 15 so that test error kÎ±2 = 1.35 is a threshold
for detecting whether the trained model has successfully learned Î±G(F(x)) or not. If the model
has not learned Î±G(F(x)) to any non-trivial accuracy, then the error is Î± per output coordinate,
totaling to kÎ±2 in regression error.
15

1
This corresponds to choosing the standard deviation as âˆšfan in+1âˆšfan out . Some practitioners also use âˆšfan
as
in
the standard deviation. We have included an experiment with respect to that choice in our V1/V2 of this paper.

12

1

0.1

0.01

3resnet(hidden)
3layer(hidden)
2layer(hidden)
3resnet(last)
3layer(last)
2layer(last)
3resnet(NTK)
3layer(NTK)
2layer(NTK)

5

Test error

Test error

25

3resnet(all)
3layer(all)
2layer(all)
3resnet(last)
3layer(last)
2layer(last)
3resnet(NTK)
3layer(NTK)
2layer(NTK)

10

1

0.2

0.04

m = number of hidden heurons

m = number of hidden heurons

(a) N = 500, train all layers vs. kernel methods

(b) N = 1000, train hidden layers vs. kernel methods

Figure 2: Performance comparison. 3resnet stands for our three-layer ResNet and 3layer/2layer stands for three
and two-layer fully connected networks. (all) stands for training all layers, (hidden) stands for training
only hidden layers, (last) stands for training only the last output layer, and (NTK) stands for training all
layers in the neural tangent kernel [21]. We emphasize that (last) is a kernel method and corresponds to
the conjugate kernel [12]. Experiment setup is in Section 8.1.

10

Test error

0.1
0.01

Test error

10

resnet(Î²=1,all)
resnet(Î²=0.5,all)
resnet(Î²=0.3,all)
resnet(Î²=0.2,all)
resnet(Î²=0,all)

1

resnet(Î²=1,hidden)
resnet(Î²=0.5,hidden)
resnet(Î²=0.3,hidden)
resnet(Î²=0.2,hidden)
resnet(Î²=0,hidden)

1

0.1

0.001
0.0001

0.01

m = number of hidden neurons

m = number of hidden neurons

(a) training all layers of 3resnet

(b) training hidden layers of 3resnet

Figure 3: Sensitivity test on Î±. Using the same choice of F(x) and G(y) from Section 8.1, we choose target function
H(x) = Î²F(x) + Î±G(F(x)) with Î± = 0.3 and varying Î² âˆˆ [0, 1].

From Figure 2, it is clear that for our choice of N , training a three-layer ResNet is the only
method among the ones we compare that can learn Î±G(F(x)) (even only non-trivially). All kernel
methods fall far behind even when the network width m is large.
Experiment 2: Sensitivity on Î±. One key assumption of this paper is to have Î± to be sufficiently
small, so that ResNet can perform hierarchical learning, by first learning the base signal F, which
is simpler and contributes more to the target, and then learning the composite signal Î±G (F), which
is more complicated but contributes less.
In Figure 3, we verify that this assumption is indeed necessary. Instead of varying Î± (which
will change the error magnitude), we define H(x) = Î²F(x) + Î±G(F(x)) and let Î² vary between 0
and 1. As shown in Figure 3, when Î± . Î², the base signal is larger than the composite signal, so
indeed ResNet can perform hierachical learning; in contrast, when Î± & Î², learning the composite
signal becomes practically impossible.
Other Findings. Although this paper proves theoretical separation between three-layer ResNet
and kernel methods (and it is verified by Figure 2), we do not yet have
â€¢ theoretical separation between two/three-layer fully-connected networks and kernel methods;
13

â€¢ theoretical separation between three-layer ResNet and two/three-layer networks.
It seems in practice such separations do exist (as observed in Figure 2). We leave these as future
research directions.

8.2

SGD Does Not Converge To Minimal Norm Solutions

Frobenius norm

Frobenius norm

400
300
200
100

0
0.004

0.016

0.063
0.25
1.
error
we construct by hand in train/test error
best found by SGD in train error
best found by SGD in test error

1200
800

400
0
0.004

0.063
0.25
1.
error
we construct by hand in train/test error
best found by SGD in train error
best found by SGD in test error

(a) d = 40, N = 5000

0.016

(b) d = 100, N = 50000

Figure 4: SGD cannot find solutions with Frobenius norm comparable to what we construct by hand.

We give a simple experiment to show that optimization methods (such as SGD) do not necessarily converge to minimal complexity solutions.
Consider two-layer neural networks F (W ; x) = a> Ïƒ(W x) where W âˆˆ RmÃ—d and a âˆˆ { âˆšÂ±1
}m is
m
an arbitrary vector with exactly m/2 positive and m/2 negative values. For simplicity, we focus
on the case when x is of norm 1 and we only train W keeping a fixed.
Â±1
Consider a simple data distribution where each xi is independently drawn from { âˆš
} for some
d
def

d â‰¥ 6. Consider labels y âˆˆ {âˆ’1, +1} being generated from some target function y = F(x) =
d3 xi1 xi2 xi3 xi4 xi5 xi6 for some distinct indices i1 , i2 , i3 , i4 , i5 , i6 âˆˆ [d].
It is a simple experimental exercise to verify that, for every even m â‰¥ 200 and every d â‰¥ 6,
there exist16
âˆš


â€¢ W âˆ— âˆˆ RmÃ—d with kW âˆ— kF â‰ˆ 9.7 d satisfying E(x,y) |F (W ; x) âˆ’ y|2 â‰¤ 0.12.
âˆš


â€¢ W âˆ— âˆˆ RmÃ—d with kW âˆ— kF â‰ˆ 12.5 d satisfying E(x,y) |F (W ; x) âˆ’ y|2 â‰¤ 0.037.
âˆš


â€¢ W âˆ— âˆˆ RmÃ—d with kW âˆ— kF â‰ˆ 13.8 d satisfying E(x,y) |F (W ; x) âˆ’ y|2 â‰¤ 0.011.
Using simple Rademacher complexity argument,
the above existential statement implies if we
âˆš
focus only on matrices W P
with kW kF â‰¤ 9.7 d, then given N training samples the Rademacher
âˆš2

kWj k2

k
âˆš F .17 This implies, for any m â‰¥ 200 and d â‰¥ 6,
complexity is at most m âˆšN
â‰¤ 2kW
N
if N = O(d) samples are given and if SGD
âˆš finds any close-to-minimal complexity solution (i.e.
with F-norm within some constant times d) that performs well on the training set, then it also
generalizes to give small test error (i.e. test error < 0.3).
Unfortunately, one can experimentally verify that, even for d = 40 and N = 5000, starting from
random initialization, even after searching learning rates and weight decay parameters in the set
{10âˆ’k , 2 Â· 10âˆ’k , 5 Â· 10âˆ’k : k âˆˆ Z}, searching network size m in {200, 500, 1000, 2000, . . . , 100000}:
jâˆˆ[m]

16

This can be done by first considering m = 200 and d = 6. Experimentally one can easily use SGD to train such
two-layer networks to obtain some W âˆ— with such test errors. Then, for general d > 6, one can pad W âˆ— with d âˆ’ 6
zero columns; and for general m > 200, one can duplicate the rows of W âˆ— and re-scale.
17
This can found for instance in [18, 32]. A cleaner one page proof can be found in the lecture notes [30].

14

â€¢ SGD cannot find solution with test error better than 0.69 (see Figure 4(a)), and
â€¢ SGD cannot find solution with small training error and small Frobenius norm (see Figure 4(a)).
Thus, SGD starting from random initialization fails to find the minimal complexity solution.
We also tried d = 100 and N = 50000 (where N is the same comparing to the standard CIFAR10/100 datasets), and this time we choose mini-batch 100 to speed up training. Even after
searching learning rates and weight decay parameters in the set {10âˆ’k , 2 Â· 10âˆ’k , 5 Â· 10âˆ’k : k âˆˆ Z},
searching network size m in {200, 500, 1000, 2000, . . . , 50000}:
â€¢ SGD cannot find solution with test error better than 0.98 (see Figure 4(b)), and
â€¢ SGD cannot find solution with small training error and small Frobenius norm (see Figure 4(b)).

Appendix: Complete Proofs
In Appendix A we give some more information about our concept class and complexity measure.
In Appendix B we review some simple lemmas from probability theory.
In Appendix C we give our full proof to Theorem 1.
In Appendix D we give our full proof to Theorem 2.
In Appendix E we include a variant of the existential lemma from prior work, and include its
proof only for completenessâ€™ sake.

A

Complexity and Concept Class

In this section we introduce an alternative (but bigger) concept class.
Definition A.1. We say F : Rd â†’ Rk has general complexity (p, Cs (F), CÎµ (F)) if for each r âˆˆ [k],
 âˆ—

p
X
hw1,i , (x, 1)i
âˆ—
âˆ—
Fr (x) =
ar,i Â· Fr,i
Â· hw2,i
, (x, 1)i ,
k(x, 1)k2
i=1

aâˆ—r,i

âˆ— , w âˆ— âˆˆ Rd+1 has Euclidean norm 1, each F : R â†’ R is a
where each
âˆˆ [âˆ’1, 1], each w1,i
r,i
2,i
smooth function with only zero-order and odd-order terms in its Taylor expansion at point zero,
and CÎµ (F) = maxr,i {CÎµ (Fr,i )} and Cs (F) = maxr,i {Cs (Fr,i )}.

Concept 2. H is given by two smooth functions F, G : Rk â†’ Rk and a value Î± âˆˆ R+ :
H(x) = F(x) + Î±G (F(x)) ,

(A.1)

where where F and G respectively have general complexity (pF , Cs (F), CÎµ (G)) and (pG , Cs (G), CÎµ (G)).
We further assume kF(x)k2 â‰¤ BF for all (x, y) âˆ¼ D.
We have the following lemma which states that Concept 1 is a special case of Concept 2 (with
constant factor 2 blow up).
Lemma A.2. Under Concept 1, we can construct F 0 , G 0 satisfying Concept 2 with general complexity (2pF , Cs (F), CÎµ (G)) and (2pG , Cs (G), CÎµ (G)) and with BF = 1.

15

Proof of Lemma A.2. Lemma A.2 is a simple corollary of the following claim.
Given any F : Rd â†’ Rk where for each r âˆˆ [k]:
 âˆ—

p
X
hwi , xi
âˆ—
âˆš
Fr (x) =
ar,i Â· Fr,i
,
2
i=1
where each aâˆ—r,i âˆˆ [âˆ’1, 1], each wiâˆ— âˆˆ Rd has Euclidean norm 1, each Fr,i : R â†’ R is a smooth
function. Then, there exists some F 0 : Rd â†’ Rk such that:
â€¢ F(x) = F 0 (x) for all unit vectors x âˆˆ Rd ; and
â€¢ F 0 has general complexity (2p, Cs (F), CÎµ (F)) where CÎµ (F) = maxr,i {CÎµ (Fr,i )} and Cs (F) =
maxr,i {Cs (Fr,i )}.
P
i
Below we prove that the above claim holds. For each Fr,i (Â·) suppose we have Fr,i (z) = âˆ
i=0 ci z
as its Taylor expansion, then we can write
ï£«
ï£¶
ï£«
ï£¶
X
X
def
+
âˆ’
ci z i ï£¸ + z Â· ï£­
ci z iâˆ’1 ï£¸ .
Fr,i (z) = Fr,i
(z) + z Â· Fr,i
(z) = ï£­
i=0,1,3,5,7,...

i=2,4,6,8,...

+
âˆ’
From this expansion we see that both Fr,i
and Fr,i
have only zero-order or odd-order terms in its
0
d
Taylor expansion at zero. We can define F : R â†’ Rk where






p
X
1 âˆ’ h(wiâˆ— , 0), (x, 1)i
h(wiâˆ— , 0), (x, 1)i
+
âˆ—
0
âˆ—
~
Â· h(0, 1), (x, 1)i + âˆš Fr,i
Â· h(wi , 0), (x, 1)i
Fr (x) =
ar,i Â· Fr,i
k(x, 1)k2
k(x, 1)k2
2
i=1

It is a simple exercise to verify that F 0 (x) = F(x) for all unit vectors x.



We also state some simple properties regarding our complexity measure.
Fact A.3. If F : Rdâˆšâ†’ Rk has general complexity (p, Cs (F),âˆšCÎµ (F)), then for every x, y âˆˆ Rd , it
satisfies kF(x)k2 â‰¤ kpCs (F) Â· kxk2 and kF(x) âˆ’ F(y)k2 â‰¤ kpCs (F) Â· kx âˆ’ yk2 .
Proof of Fact A.3. The boundedness
ofkF(x)k2 is trivial so we only focus on kF(x) âˆ’ F(y)k2 . For
 hwâˆ— ,(x,1)i
1,i
âˆ— , (x, 1)i, denoting by w âˆ— as the first d coordinate of
each component g(x) = Fr,i k(x,1)k2
Â· hw2,i
1
âˆ— , and by w âˆ— as the first d coordinates of w âˆ— , we have
w1,i
2,i
2,i
 âˆ—

hw1,i , (x, 1)i
g 0 (x) = Fr,i
Â· w2âˆ—
k(x, 1)k2
 âˆ—

âˆ— , (x, 1)i Â· (x, 1)/k(x, 1)k2
hw1,i , (x, 1)i
w1âˆ— Â· k(x, 1)k2 âˆ’ hw1,i
2
âˆ—
0
+ hw2,i , (x, 1)i Â· Fr,i
Â·
k(x, 1)k2
k(x, 1)k22
This implies
0

kg (x)k2 â‰¤ Fr,i



âˆ— , (x, 1)i
hw1,i
k(x, 1)k2


+2

0
Fr,i



âˆ— , (x, 1)i
hw1,i
k(x, 1)k2

â‰¤ 3Cs (Fr,i ) .



As a result, |Fr (x) âˆ’ Fr (y)| â‰¤ 3pCs (Fr,i ).

B



Probability Theory Review

The following concentration of chi-square distribution is standard.

16

Proposition B.1 (chi-square concentration). If g âˆ¼ N (0, I) is m-dimensional, then for every t â‰¥ 1
âˆš
Pr[kgk22 âˆ’ m â‰¥ 2 mt + 2t] â‰¤ eâˆ’t
The following norm bound on random Gaussian matrix is standard.
Proposition B.2. If M âˆˆ RnÃ—m is a random matrix where Mi,j are i.i.d. from N (0, 1). Then,
âˆš
âˆš
2
â€¢ For any t â‰¥ 1, with probability â‰¥ 1 âˆ’ eâˆ’â„¦(t ) it satisfies kMk2 â‰¤ O( n + m) + t.

âˆš
2
â€¢ If 1 â‰¤ s â‰¤ O logm2 m , then with probability â‰¥ 1 âˆ’ eâˆ’â„¦(n+s log m) it satisfies kMvk2 â‰¤ O( n +
âˆš
s log m) Â· kvk2 for all s-sparse vectors v âˆˆ Rm .
Proof. The first statement can be found for instance in [34, Proposition 2.4]. As for the second
statement, it suffices for us to consider all m
s possible n Ã— s sub-matrices of M, each applying the
first statement, and then taking a union bound.

The following concentration is proved for instance in [4].
Lemma B.3 (Gaussian indicator concentration). Let (n1 , Î±1 , a1,1 , a2,1 ), Â· Â· Â· , (nm , Î±m , a1,m , a2,m ) be
m i.i.d. samples from some distribution, where within a 4-tuples:
â€¢ the marginal distribution of a1,i and a2,i is standard Gaussian N (0, 1);
â€¢ ni and Î±i are not necessarily independent;
â€¢ a1,i and a2,i are independent; and
â€¢ ni and Î±i are independent of a1,i and a2,i .
Suppose h : R â†’ [âˆ’L, L] is a fixed function. Then, for every B â‰¥ 1:
ï£®ï£«
ï£¶
ï£¹
X
âˆš
2
Pr ï£° ï£­
a1,i a2,i 1[ni â‰¥ 0]h(Î±i )ï£¸ â‰¥ BL( m + B)ï£» â‰¤ 4eâˆ’B /8
iâˆˆ[m]

and
ï£®ï£«
Pr ï£° ï£­

ï£¹
âˆš
2
a21,i 1[ni â‰¥ 0]h(Î±i )ï£¸ âˆ’ m E[a21,1 1[n1 â‰¥ 0]h(Î±1 )] â‰¥ BL( m + B)ï£» â‰¤ 4eâˆ’B /8 .
ï£¶

X
iâˆˆ[m]

Proof of Lemma B.3. Let us consider a fixed n1 , Î±1 , Â· Â· Â· , nm , Î±m , then since each |1[ni â‰¥ 0]h(Î±i )| â‰¤
L, by Gaussian chaos variables concentration bound (e.g., Example 2.15 in [31]) we have that
ï£®ï£«
ï£¶
ï£¹
X
âˆš
2
a1,i a2,i 1[ni â‰¥ 0]h(Î±i )ï£¸ â‰¥ BL( m + B) {ni , Î±i }iâˆˆ[m] ï£» â‰¤ 4eâˆ’B /8 .
Pr ï£° ï£­
iâˆˆ[m]

Since this holds for every choice of {ni , Î±i }iâˆˆ[m] we can complete the proof. The second inequality
follows from sub-exponential concentration bounds.

The next proposition at least traces back to [5] and was stated for instance in [6].
I
Proposition B.4. Suppose Î´ âˆˆ [0, 1] and g (0) âˆˆ Rm is a random vector g (0) âˆ¼ N (0, m
). With
2/3
âˆ’â„¦(mÎ´
)
0
m
0
0
mÃ—m
probability at least 1 âˆ’ e
, for all vectors g âˆˆ R with kg k2 â‰¤ Î´, letting D âˆˆ R
be the
diagonal matrix where (D0 )k,k = 1(g(0) +g0 )k â‰¥0 âˆ’ 1(g(0) )k â‰¥0 for each k âˆˆ [m], we have

kD0 k0 â‰¤ O(mÎ´ 2/3 )

and

17

kD0 g (0) k2 â‰¤ kg 0 k2 .

Proof of Proposition B.4. Observe that (D0 )j,j is non-zero for some j âˆˆ [m] only if
|gj0 | > |(g (0) )j | .

(B.1)

Therefore, denoting by x = D0 g (0) , for each j âˆˆ [m] such that xj 6= 0, we must have |xj | = |(g (0) )j | â‰¤
|(g 0 )j | so we have kxk2 â‰¤ kg 0 k2 .
Let Î¾ â‰¤ 2âˆš1m be a constant parameter to be chosen later.
â€¢ We denote by S1 âŠ† [m] the index sets where j satisfies |(g (0) )j | â‰¤ Î¾. Since we know (g (0) )j âˆ¼
âˆš
N (0, 1/m), we have Pr[|(g (0) )j | â‰¤ Î¾] â‰¤ O (Î¾ m) for each j âˆˆ [m]. Using Chernoff bound for
3/2
all j âˆˆ [m], we have with probability at least 1 âˆ’ eâˆ’â„¦(m Î¾) ,
n
o
|S1 | = i âˆˆ [m] : |(g (0) )j | â‰¤ Î¾ â‰¤ O(Î¾m3/2 ) .
â€¢ We denote by S2 âŠ† [m] \ S1 the index set of all j âˆˆ [m] \ S1 where (D0 )j,j 6= 0. Using (B.1),
kg 0 k2

we have for each j âˆˆ S2 it satisfies |(g 0 )j | â‰¥ |(g (0) )j | â‰¥ Î¾ . This means |S2 | â‰¤ Î¾12 2 .
2
Î´ 2/3
From above, we have kD0 k0 â‰¤ |S1 | + |S2 | â‰¤ O Î¾m3/2 + Î´Î¾2 . Choosing Î¾ = 2m
1/2 gives the desired
result.


C

Theorem 1 Proof Details

In the analysis, let us define a diagonal matrices
DW(0) = diag{1W(0) (x,1)â‰¥0 }

DV(0) ,W = diag{1V(0) (out1 (x),1)â‰¥0 }

DW = diag{1(W(0) +W)(x,1)â‰¥0 }

DV,W = diag{1(V(0) +V)(out1 (x),1)â‰¥0 }

which satisfy out1 (x) = ADW (W(0) + W)(x, 1) and out(x) = ADV,W (V(0) + V)(out1 (x), 1).
Throughout the proof, we assume m â‰¥ poly(CÎ± (F), CÎ± (G), pG , pF , k, Î±âˆ’1 ).

C.1

Coupling

In this subsection we present our coupling lemma. It shows that for all weight matrices not very
far from random initialization (namely, all kWk2 â‰¤ Ï„w and kVk2 â‰¤ Ï„v ), many good properties
occur. This includes upper bounds on the number of sign changes (i.e., on kDW(0) âˆ’ DW k0 and
DV(0) ,W âˆ’ DV,W

0

) as well as vanishing properties such as ADW W(0) , ADV,W V(0) being neg-

ligible. We prove such properties using techniques from prior works [4, 6].


1/4 
Lemma C.1 (Coupling). Suppose Ï„w â‰¥ 1, Ï„w âˆˆ m1/8+0.001 Ïƒw , m1/8âˆ’0.001 Ïƒw , and Ï„v âˆˆ Ïƒv Â·

(k/m)3/8 , Ïƒv . Then, for every fixed x, with high probability over A, W(0) , V(0) , we have that for
all W, V satisfying kWk2 â‰¤ Ï„w and kVk2 â‰¤ Ï„v , it holds that
(a) kDW(0) âˆ’ DW k0 â‰¤ O((Ï„w /Ïƒw )2/3 m2/3 )
(b)

ADW W(x, 1) âˆ’ ADW ((W(0) + W)(x, 1))

(c) kout1 (x)k2 = ADW (W(0) + W)(x, 1)

2

2

e
â‰¤O



Ï„w (Ï„w /Ïƒw )1/3
m1/6

â‰¤ O(mâˆ’0.001 )

â‰¤ O (Ï„w )

â‰¤ O((Ï„v /Ïƒv )2/3 m)

(d)

DV(0) ,W âˆ’ DV,W

(e)

ADV,W V(out1 (x), 1) âˆ’ ADV,W (V(0) + V)(out1 (x), 1)

0



18

2


e Ï„v (Ï„v /Ïƒv )1/3 Â·(kout1 (x)k2 +1)
â‰¤O

(f )

ADV,W V(0)

(g)

ADV,W (V(0) + V)(out1 (x), 1)

2

e Ï„v (Ï„v /Ïƒv )1/3
â‰¤O
2


e (Ï„v (kout1 (x)k2 + 1))
â‰¤O

Proof.
(a) Using basic probability argument (appropriately scaling and invoking Proposition B.4) we have


Ï„w 2/3
âˆš
Â· m = O((Ï„w /Ïƒw )2/3 m2/3 ) .
kDW âˆ’ DW(0) k0 â‰¤ O
Ïƒw m
(b) We write
ADW W(x, 1) âˆ’ ADW (W(0) + W)(x, 1) = âˆ’ADW(0) W(0) (x, 1) + A(DW(0) âˆ’ DW )W(0) (x, 1)
âˆš
For the first term, we have DW(0) W(0) (x, 1) 2 â‰¤ W(0) (x, 1) 2 â‰¤ O(Ïƒw m) with high
probability due to concentration of chi-square distribution, and then using the randomness of A
and applying concentration of chi-square distribution again, we have ADW(0) W(0) (x, 1) 2 â‰¤
âˆš
e kÏƒw ) with high probability.
O(
For the second term, invoking Proposition B.4 again, we have
(DW âˆ’ DW(0) )W(0) (x, 1)

2

â‰¤ kW(x, 1)k2 â‰¤ Ï„w
âˆš

e âˆš s ) Â· kyk2 with high probability
Recall for every s-sparse vectors y, it satisfies kAyk2 â‰¤ O(
m
(see Proposition B.2). This implies
âˆš
âˆš
s
s
(0)
e
e
A(DW âˆ’ DW(0) )W (x, 1) â‰¤ O( âˆš ) Â· kW(x, 1)k2 â‰¤ O( âˆš Ï„w )
m
m
2



2/3
w
Â· m . Together, we have
for s = O ÏƒwÏ„âˆš
m
!
1/3
âˆš
Ï„
(Ï„
/Ïƒ
)
w w
w
e
ADW W(x, 1) âˆ’ ADW ((W(0) + W)(x, 1)) â‰¤ O
.
+ kÏƒw
2
m1/6
(c) We use Lemma C.1b together with kADW W(x, 1)k2 â‰¤ kAk2 kWk2 â‰¤ O(Ï„w ), where the property kAk2 â‰¤ O(1) holds with high probability using Proposition B.2.
(d) Recall DV(0) ,W = diag{1V(0) (out1 (x),1)â‰¥0 } and DV,W = diag{1(V(0) +V)(out1 (x),1)â‰¥0 }. Let us
denote by z = (out1 (x), 1). We know that if z âˆˆ Rk+1 is a fixed vector (as opposed to depending
on W(0) and W), then owing to Proposition B.4


Ï„v 2/3
DV,W âˆ’ DV(0) ,W â‰¤ O
Â·m
(C.1)
Ïƒv
0
2/3

with probability at least 1 âˆ’ eâˆ’â„¦(m ) . This means, taking Îµ-net over all possible unit vectors
z âˆˆ Rk+1 , we have (C.1) holds for all such unit vectors z, therefore also for all vectors z âˆˆ
Rk+1 .18 In particular, choosing z = (out1 (x), 1) finishes the proof.
(e) We write
ADV,W V(out1 (x), 1) âˆ’ ADV,W (V(0) + V)(out1 (x), 1)
= âˆ’ADV(0) ,W V(0) (out1 (x), 1) + A(DV(0) ,W âˆ’ DV,W )V(0) (out1 (x), 1)
18
More formally, this requires one to construct a set {z1 , z2 , . . . } âŠ‚ Rk+1 of Îµâˆ’â„¦(k) unit vectors so that each unit
vector is at most Îµ-close to some point in this set with Îµ = 1/poly(m). Then, one can derive that as long as Îµ is
2/3
sufficiently small, for each i, with probability at least 1 âˆ’ eâˆ’â„¦(m ) inequality (C.1) holds for all unit vectors z with
kz âˆ’ zi k2 â‰¤ Îµ. Taking union bond over all zi in this set finishes the argument.

19

Let us denote by z = (out1 (x), 1). Again, suppose for now that z âˆˆ Rk+1 is a fixed vector that
does not depend on W(0) or W.
Then, for the first term, we have ADV(0) ,W V(0) z = AÏƒ(V(0) z) and by by concentration of
chi-square distribution we have kV(0) zk2 â‰¤ O(Ïƒv kzk2 ) with probability at least 1âˆ’eâˆ’â„¦(m) , and
then using the randomness of A and applying chi-square concentration again (see Proposition B.1),
2
we have with probability at least 1 âˆ’ eâˆ’â„¦(k log m) ,
âˆš âˆš
e k/ m) Â· O(Ïƒv kzk2 ) .
ADV(0) ,W V(0) z â‰¤ O(
2

For the second term, invoking Proposition B.4, we have
(DV,W âˆ’ DV(0) ,W )V(0) z

2

â‰¤ kVzk2 â‰¤ Ï„v Â· kzk2
âˆš

e âˆš s ) Â· kyk2 with probability at least
Recall for every s-sparse vectors y, it satisfies kAyk2 â‰¤ O(
m
1 âˆ’ eâˆ’â„¦(s) (see Proposition B.2). This implies
e

A(DV,W âˆ’ DV(0) ,W )V
for s = O




Ï„v 2/3
Ïƒv

(0)

z

2

âˆš
s
e
â‰¤ O( âˆš Ï„v kzk2 )
m



Â· m . Combining the two bounds above, we have for every fixed z âˆˆ Rk+1 ,

ADV,W V(0) z = ADV,W Vz âˆ’ ADV,W (V(0) + V)z
2
2




âˆš
âˆš
1/3
1/3
e (Ï„v (Ï„v /Ïƒv ) + kÏƒv / m)kzk2 â‰¤ O
e kzk2 Ï„v (Ï„v /Ïƒv )
.
â‰¤O
2

with probability at least 1 âˆ’ eâˆ’â„¦(k log m) . Finally, because this confidence is sufficiently small,
one can take an Îµ-net over all possible vectors z âˆˆ Rk+1 and derive the above bound for all
vectors z. In particular, choosing z = (out1 (x), 1) finishes the proof.
(f) This is a byproduct of the proof of Lemma C.1e.
(g) With high probability
kADV,W V(out1 (x), 1)k2 â‰¤ kAk2 Â· kVk2 Â· (1 + kout1 (x)k2 ) â‰¤ O(Ï„v ) Â· (1 + kout1 (x)k2 )
Combining this with Lemma C.1e gives the proof.


C.2

Existantial

Ï„v
In this subsection, we prove the existence of matrices W> , V> with kW> kF â‰¤ Ï„10w and kV> kF â‰¤ 10
satisfying ADW(0) W> (x, 1) â‰ˆ F(x) and ADV(0) ,W V> (out1 (x), 1) â‰ˆ Î±G (out1 (x)).
This existential proof relies on an â€œindicator to functionâ€ lemma that was used in prior work [4];
however, for the purpose of this paper we have to revise it to include a trainable bias term (or
equivalently, to support vectors of the form (x, 1)). We treat that carefully in Appendix E.

Lemma C.2. Suppose Î± âˆˆ (0, 1) and Î±
e=

Î±
k(pF Cs (F )+pG Cs (G)) ,

there exist M = poly(CÎ±e (F), CÎ±e (G), Î±
eâˆ’1 )

satisfying that for every m â‰¥ M , with high probability over A, W(0) , V(0) , one can construct
W> âˆˆ RmÃ—(d+1) and V> âˆˆ RmÃ—(k+1) with
Ï„w def e
Ï„v def e
kW> kF â‰¤
= O(kpF Cs (F)) and kV> kF â‰¤
= O(e
Î±kpG Cs (G))
10
10
satisfying
20

i
h
e4 ;
(a) E(x,y)âˆ¼D kADW(0) W> (x, 1) âˆ’ F(x)k22 â‰¤ Î±
(b) for all x and W, ADV(0) ,W V> (out1 (x), 1) âˆ’ Î±G (out1 (x))

2

â‰¤Î±
e2 Â· k(out1 (x), 1)k2 .

Proof.


P
>
(a) For each r âˆˆ [k], we have ADW(0) W> (x, 1) k =
iâˆˆ[m] ar,i 1hw(0) ,(x,1)iâ‰¥0 hwi , (x, 1)i. By
i

applying Lemma E.1 (which is a simple modification on top of the existential result from [4]),
>
we can construct matrix W> satisfying kAD
e2 Â· k(x, 1)k2 for each
(0) W (x, 1) âˆ’ F(x)k2 â‰¤ Î±
âˆš W
x âˆˆ Rd with probability at least 1 âˆ’ eâˆ’â„¦( m) . This translates to an expected guarantee with
respect to (x, y) âˆ¼ D.
(b) For each r âˆˆ [k], we have
X


ar,i 1hv(0) ,(out
ADV(0) ,W V> (out1 (x), 1) k =
iâˆˆ[m]

1 (x),1)iâ‰¥0

i

hvi> , (out1 (x), 1)i .

Now, applying Lemma E.1 again,
we can construct matrix V> satisfying for each z âˆˆ Rk with
âˆš
probability at least 1 âˆ’ eâˆ’â„¦( m) :
X X

ar,i 1hv(0) ,(z,1)iâ‰¥0 hvi> , (z, 1)i âˆ’ Î±G(z) â‰¤
i

râˆˆ[k] iâˆˆ[m]

Î±
e2
Â· k(z, 1)k2 .
2

By applying a careful Îµ-net
argument and using m â‰¥ poly(k),19 this translates to, with probâˆš
âˆ’â„¦(
m)
ability at least 1 âˆ’ e
, for all vectors z âˆˆ Rk .
X X

ar,i 1hv(0) ,(z,1)iâ‰¥0 hvi> , (z, 1)i âˆ’ Î±G(z) â‰¤ Î±
e2 Â· k(z, 1)k2

râˆˆ[k] iâˆˆ[m]

(C.2)

i

Finally, choosing z = out1 (x) finishes the proof.


Next, we can combine coupling and existential lemmas:
Lemma C.3. Under the assumptions of Lemma C.1 and Lemma C.2, we have
h
i
(a) E(x,y)âˆ¼D kADW W> (x, 1) âˆ’ F(x)k22 â‰¤ O(e
Î±4 )

(b) âˆ€x âˆˆ Rd , kADV,W V> (out1 (x), 1) âˆ’ Î±
eG (out1 (x))k2 â‰¤ Î±
e2 + O(Ï„v (Ï„v /Ïƒv )1/3 ) Â· k(out1 (x), 1)k2
h
i
2
>
(c) E(x,y)âˆ¼D kADW (W âˆ’ W)(x, 1) âˆ’ (F(x) âˆ’ out1 (x))k2 â‰¤ O(e
Î±4 )
Proof.
âˆš

e âˆš s ) Â· kyk2 with high probability (see
(a) For every s-sparse vectors y, it satisfies kAyk2 â‰¤ O(
m
Proposition B.2). We also have kW> (x, 1)k2 â‰¤ O(kW> kF ) â‰¤ O(Ï„w ). Therefore, kA(DW(0) âˆ’
âˆš
âˆš
DW )W> (x, 1)k â‰¤ O( sÏ„w / m) where s is the maximum sparsity of DW(0) âˆ’ DW , which
satisfies s = O((Ï„w /Ïƒw )2/3 m2/3 ) by Lemma C.1a. This, combining with Lemma C.2a gives
h
i
2
E
ADW W> (x, 1) âˆ’ F(x) 2 â‰¤ 2e
Î±2 + O(Ï„w (Ï„w /Ïƒw )1/3 /m1/6 )2 â‰¤ O(e
Î±4 ) .
(x,y)âˆ¼D

19

This is a bit non-trivial to derive, because one has to argue that if z changes a little bit (i.e., by 1/poly(m)),
then according to Lemma C.1d, the number of sign changes in {1hv(0) ,(z,1)iâ‰¥0 }iâˆˆ[m] is o(m), and thus the interested
i

quantity changes by at most 1/poly(m).

21

âˆš

e âˆš s )Â·kyk2 with high probability. We
(b) Again, for every s-sparse vectors y, it satisfies kAyk2 â‰¤ O(
m
also have kV> (out1 (x), 1)k2 â‰¤ O(kV> kF ) Â· k(out1 (x), 1)k2 â‰¤ O(Ï„v ) Â· k(out1 (x), 1)k2 . Therefore,
âˆš
âˆš
kA(DV(0) ,W âˆ’ DV,W )V> (out1 (x), 1)k â‰¤ O( sÏ„v / m) Â· k(out1 (x), 1)k2
where s is the maximum sparsity of DV(0) ,W âˆ’ DV,W , which satisfies s = O((Ï„v /Ïƒv )2/3 m) by
Lemma C.1d. This, combining with Lemma C.2b gives


  2
E
ADW W> (x, 1) âˆ’ F(x) 2 â‰¤ Î±
e + O(Ï„v (Ï„v /Ïƒv )1/3 ) Â· k(out1 (x), 1)k2 .
(x,y)âˆ¼D

(c) This combines Lemma C.1b and Lemma C.3a, together with our sufficiently large choice of m.


C.3

Optimization

In this subsection we give some structural results that shall be later used in the optimization step.
The first fact gives an explicit formula of the gradient.
Fact C.4. When Obj(W, V; (x, y)) = 12 ky âˆ’ out(W, V; x)k22 , we can write its gradient as follows.
hâˆ‡W,V Obj(W, V; (x, y)), (âˆ’W0 , âˆ’V0 )i = hy âˆ’ out(x), f (W0 ; x) + g(V0 ; x)i
where

f (W0 ; x) = ADV,W (V(0) + V) ADW W0 (x, 1) , 0 + ADW W0 (x, 1)
g(V0 ; x) = ADV,W V0 (out1 (x), 1)
The next claim gives simple upper bound on the norm of the gradient.
Claim C.5. For all (x, y) in the support of D, with high probability over A, W(0) , V(0) , we have
that for all W, V satisfying kWkF â‰¤ Ï„w and kVkF â‰¤ Ï„v , it holds that
kâˆ‡W Obj (W, V; (x, y))kF â‰¤ ky âˆ’ out(x)k2 Â· O(Ïƒv + 1)
kâˆ‡V Obj (W, V; (x, y))kF â‰¤ ky âˆ’ out(x)k2 Â· O(Ï„w + 1) .
Proof. For the gradient in W, we derive using the gradient formula Fact C.4 that


kâˆ‡W Obj (W, V; (x, y))kF = (x, 1)(y âˆ’ out(x))> ADV,W (V(0) + V)(ADW , 0) + ADW
F


>
(0)
= k(x, 1)k2 Â· (y âˆ’ out(x)) ADV,W (V + V)(ADW , 0) + ADW
â‰¤ 2 ky âˆ’ out(x)k2 Â· ADV,W (V

(0)

+ V)(ADW , 0) + ADW

2

2

â‰¤ ky âˆ’ out(x)k2 Â· O(Ïƒv + 1) .
Above, the last inequality uses kAk2 â‰¤ O(1) and kV(0) k2 â‰¤ O(Ïƒv ) with high probability (using
random matrix theory, see Proposition B.2), as well as Ï„v â‰¤ Ïƒv . Similarly, using the gradient
formula Fact C.4, we derive that
kâˆ‡V Obj (W, V; (x, y))kF = (out1 (x), 1)(y âˆ’ out(x))> ADV,W

F

>

= k(out1 (x), 1)k2 Â· (y âˆ’ out(x)) ADV,W

2

â‰¤ ky âˆ’ out(x)k2 Â· O(Ï„w + 1) Â· O(1)
where the last inequality uses Lemma C.1c and kAk2 â‰¤ O(1).
22



The next claim gives a careful approximation to f (W> âˆ’W; x)+g(V> âˆ’V; x), which according
to Fact C.4 is related to the correlation between the gradient direction and (W âˆ’ W> , V âˆ’ V> ).
Claim C.6. In the same setting as Lemma C.1 and Lemma C.2, suppose we set parameters according to Table 1. Then, we can write
f (W> âˆ’ W; x) + g(V> âˆ’ V; x) = H(x) âˆ’ out(x) + Err
with

E

(x,y)âˆ¼D

kErrk22 â‰¤ O(Ï„v + Î±LG )2 Â·

E

(x,y)âˆ¼D

kH(x) âˆ’ out(x)k22

+O Î±
e2 + Ï„v2 (1 + BF ) + Î±Ï„v LG (BF + 1)

2

.

and for every (x, y) âˆ¼ D, with high probability kErrk2 â‰¤ O(Ï„w ).
Proof of Claim C.6.
f (W> âˆ’ W; x) + g(V> âˆ’ V; x)

= ADV,W (V(0) + V) ADW (W> âˆ’ W)(x, 1), 0 + ADW (W> âˆ’ W)(x, 1) + ADV,W (V> âˆ’ V)(out1 (x), 1)


= ADV,W (V(0) + V) ADW (W> âˆ’ W)(x, 1), 0 + ADW W> (x, 1) + ADV,W V> (out1 (x), 1)
|
{z
} |
{z
}
â™£

â™ 

âˆ’ (ADW W(x, 1) + ADV,W V(out1 (x), 1))
|
{z
}
â™¦

We treat the three terms separately.
â€¢ For the â™£ term, under expectation over (x, y) âˆ¼ D,
kâ™£k22 â‰¤ kADV,W V(0) k2 + kAk22 kVk22



ADW (W> âˆ’ W)x

â‰¤ O(1) Â· O(Ï„v )2 Â· kF(x) âˆ’ out1 (x)k22 + O(e
Î±2 )

2
2

where the last inequality uses Lemma C.1f and Lemma C.3c, together with Ï„v â‰¤

1
polylog(m) Ïƒv .

â€¢ For the â™  term, under expectation over (x, y) âˆ¼ D,
kâ™  âˆ’ (F(x) + Î±G(F(x))k22 â‰¤ O(e
Î±2 + Ï„v (Ï„v /Ïƒv )1/3 )2 Â· (kout1 (x)k2 + 1)2
+ O(Î±LG )2 kF(x) âˆ’ out1 (x)k22
â‰¤ O(Ï„v2 )2 Â· (kout1 (x)k2 + 1)2 + O(Î±LG )2 kF(x) âˆ’ out1 (x)k22
where the first inequality uses Lemma C.3a and Lemma C.3b, as well as the Lipscthiz continuity of G(x) (which satisfies kG(x) âˆ’ G(y)k â‰¤ LG kx âˆ’ yk); and the second inequality uses
1
2
e.
Ïƒv â‰¤ Ï„v and the definition of Î±
â€¢ For the â™¦ term, under expectation over (x, y) âˆ¼ D,
kâ™¦ âˆ’ out(x)k22 â‰¤ O (kout1 (x)k2 + 1)Ï„v2
where the inequality uses Lemma C.1b, Lemma C.1e and

1
Ïƒv

2

â‰¤ Ï„v2 .

In sum, we have
def

Err = f (W> âˆ’ W; x) + g(V> âˆ’ V; x) âˆ’ (F(x) + Î±G(F(x)) âˆ’ out(x)
satisfies
E

(x,y)âˆ¼D

kErrk22 â‰¤

h
E

(x,y)âˆ¼D

O(Ï„v + Î±LG )2 Â· kF(x) âˆ’ out1 (x)k22 + O Î±
e2 + (kout1 (x)k2 + 1)Ï„v2

23

2 i

.

Combining this with Claim C.7, and using kout1 (x)k2 â‰¤ kout1 (x) âˆ’ F(x)k2 + BF , we have
2
E kErrk22 â‰¤ O(Ï„v + Î±LG )2 Â· E kH(x) âˆ’ out(x)k22 + O Î±
e2 + (1 + BF )Ï„v2
(x,y)âˆ¼D

(x,y)âˆ¼D

+ O(Ï„v + Î±LG )2 Â· (Ï„v (BF + 1) + Î±BF â—¦G )2 .
âˆš
Using BF â—¦G â‰¤ kpF Cs (F)BF â‰¤ Ï„Î±v BF (see Fact A.3), we finish the bound on E(x,y)âˆ¼D kErrk22 .
As for the absolute value bound, one can naively derive that with high probability kf (W> âˆ’
W; x)k2 â‰¤ O(Ï„w ), kg(V> âˆ’ V; x)k2 â‰¤ O(Ï„w Ï„v ), kH(x)k2 â‰¤ Bâˆš
F + Î±BF â—¦G , and kout(Ï„w )k2 â‰¤
O(Ï„w ) (by Lemma
C.1c
and
C.1g).
Combining
them
with
B
â‰¤
kpF Cs (F) â‰¤ Ï„w and Î±BF â—¦G â‰¤
F


1

Î± BF LG + Cs (G) â‰¤ kpG Cs (G) BF LG + Cs (G) â‰¤ Ï„w finishes the proof.
Finally, we state a simple claim that bounds the norm of kout1 (x) âˆ’ F(x)k2 given the norm of
kout(x) âˆ’ H(x)k2 .
Claim C.7. In the same setting as Lemma C.1, if we additionally have Ï„v â‰¤
fixed x, with high probability over

1
polylog(m) ,

for every

A, W(0) , V(0) ,

e v (BF + 1) + Î±BF â—¦G ) .
kout1 (x) âˆ’ F(x)k2 â‰¤ 2kout(x) âˆ’ H(x)k2 + O(Ï„
Proof. We can rewrite


out1 (x) âˆ’ F(x) = (out(x) âˆ’ H(x)) âˆ’ ADV,W (V(0) + V)(out1 (x), 1) + Î±G(F(x)) .
e v (kout1 (x)k2 + 1)), and using
Using Lemma C.1g we have kADV,W (V(0) + V)(out1 (x), 1)k â‰¤ O(Ï„
the boundedness we have kG(F(x))k2 â‰¤ BF â—¦G . We also have kout1 (x)k2 â‰¤ kout1 (x)âˆ’F(x)k2 +BF .
Together, we have
e v (kout1 (x) âˆ’ F(x)k2 + BF + 1)) + Î±BF â—¦G .
kout1 (x) âˆ’ F(x)k2 â‰¤ kout(x) âˆ’ H(x)k2 + O(Ï„
Using Ï„v â‰¤

C.4

1
polylog(m)



we finish the proof.

Proof of Theorem 1


1
e
Theorem 1. Under Concept 1 or Concept 2, for every Î± âˆˆ 0, Î˜(
kpG Cs (G) ) and Î´ â‰¥ OPT +

e Î±4 (kpG Cs (G))4 (1 + BF )2 . There exist M = poly(CÎ± (F), CÎ± (G), pF , Î±âˆ’1 ) satisfying that for
Î˜
every m â‰¥ M , with high probability over A, W(0) , V(0) , for a wide range of random initialization
parameters Ïƒw , Ïƒv (see Table 1), choosing




(kpF Cs (F))2
Î±pG Cs (G) 2
e
e
e
Î·w = Î˜ (min{1, Î´}) Î·v = Î·w Â· Î˜
T =Î˜
min{1, Î´ 2 }
pF Cs (F)
With high probability, the SGD algorithm satisfies
T âˆ’1
1 X
E kH(x) âˆ’ out(Wt , Vt ; x)k22 â‰¤ O(Î´) .
T
(x,y)âˆ¼D
t=0

Proof of Theorem 1. We first assume that throughout the SGD algorithm, it satisfies
kWt kF â‰¤ Ï„w

and kVt kF â‰¤ Ï„v .

We shall prove in the end that (C.3) holds throughout the SGD algorithm.

24

(C.3)

On one hand, using Claim C.6, at any point Wt , Vt , we have
hâˆ‡W,V Obj(Wt , Vt ; (xt , yt )), (Wt âˆ’ W> , Vt âˆ’ V> ))i
= hyt âˆ’ out(Wt , Vt ; xt ), H(xt ) âˆ’ out(Wt , Vt ; xt ) + Errt i
1
â‰¥ kH(xt ) âˆ’ out(Wt , Vt ; xt )k22 âˆ’ 2kErrt k22 âˆ’ 2kH(xt ) âˆ’ yt k22
2
where Errt comes from Claim C.6. On the other hand, using Wt+1 = Wt âˆ’Î·w âˆ‡W Obj(Wt , Vt ; (xt , yt ))
and Vt+1 = Vt âˆ’ Î·v âˆ‡V Obj(Wt , Vt ; (xt , yt )), we have
hâˆ‡W,V Obj(Wt , Vt ; (xt , yt )), (W âˆ’ W> , V âˆ’ V> ))i
Î·w
Î·v
=
kâˆ‡W Obj(Wt , Vt ; (xt , yt ))k2F + kâˆ‡V Obj(Wt , Vt ; (xt , yt ))k2F
{z 2
}
|2
â™¥

1
1
1
1
kWt âˆ’ W> k2F âˆ’
kWt+1 âˆ’ W> k2F +
kVt âˆ’ V> k2F âˆ’
kVt+1 âˆ’ V> k2F
+
2Î·w
2Î·w
2Î·v
2Î·v
Recall from Claim C.5,
â™¥ â‰¤ O(Î·w + Î·v Ï„w2 ) Â· kyt âˆ’ out(Wt , Vt ; xt )k22 â‰¤ O(Î·w + Î·v Ï„w2 ) Â· kH(xt ) âˆ’ out(Wt , Vt ; xt )k22 + kH(xt ) âˆ’ yt k22
Therefore, as long as O(Î·w + Î·v Ï„w2 ) â‰¤ 0.1, it satisfies
1
1
1
kH(xt ) âˆ’ out(Wt , Vt ; xt )k22 â‰¤ 2kErrt k22 + 4kH(xt ) âˆ’ yt k22 +
kWt âˆ’ W> k2F âˆ’
kWt+1 âˆ’ W> k2F
4
2Î·w
2Î·w
1
1
+
kVt âˆ’ V> k2F âˆ’
kVt+1 âˆ’ V> k2F
2Î·v
2Î·v
After telescoping for t = 0, 1, . . . , T0 âˆ’ 1,
T0 âˆ’1
kWT0 âˆ’ V> k2F
kWT0 âˆ’ W> k2F
1 X
+
+
kH(xt ) âˆ’ out(Wt , Vt ; xt )k22
2Î·w T0
2Î·v T0
2T0
t=0

â‰¤

kW> k2F
kV> k2F
O(1)
+
+
2Î·w T0
2Î·v T0
T0

TX
0 âˆ’1

kErrt k22 + kH(xt ) âˆ’ yt k22 .

(C.4)

t=0

Choosing T0 = T , taking expectation with respect to {(xt , yt )}t=0,1,...,T âˆ’1 on both sides, and using
Claim C.6 (by noticing O(Ï„v + Î±LG ) â‰¤ 0.1) and the definition of OPT, we have
T âˆ’1
kW> k2F
kV> k2F
1 X
+
+ O(OPT + Î´0 )
E kH(x) âˆ’ out(Wt , Vt ; x)k22 â‰¤
4T
2Î·w T
2Î·v T
(x,y)âˆ¼D
t=0

where
2
Î´0 = Î˜ Î±
e2 + Ï„v2 (1 + BF ) + Î±Ï„v LG (BF + 1)

e Î±
=Î˜
e4 + Î±4 (kpG Cs (G))4 (1 + BF )2 + Î±4 (kpG Cs (G))2 L2G (BF + 1)2

e Î±4 (kpG Cs (G))4 (1 + BF )2
=Î˜
1
Above, the last inequality uses kpG C1s (G) â‰¤ O( 1+L
) (see Fact A.3) and the choice of Î±
e from
G
Lemma C.2.
Using kW> kF â‰¤ Ï„w /10, kV> kF â‰¤ Ï„v /10, we have as long as Î´ â‰¥ OPT + Î´0 ,
T âˆ’1
1 X
Ï„ 2 /Î·w + Ï„v2 /Î·v
E kH(x) âˆ’ out(Wt , Vt ; x)k22 â‰¤ O(Î´) as long as T â‰¥ â„¦( w
).
T
Î´
(x,y)âˆ¼D
t=0

Finally, we need to check that (C.3) holds. To do so, we use kErrt k2 â‰¤ O(Ï„ ) from Claim C.6 and
25



apply martingale concentration on (C.4) and derive that, with high probability


kWT0 âˆ’ V> k2F
kW> k2F
kV> k2F
kWT0 âˆ’ W> k2F
Ï„w
e
.
+
â‰¤
+
+ O(Î´) + O âˆš
2Î·w T0
2Î·v T0
2Î·w T0
2Î·v T0
T0
This implies
kWT0 k2F
kWT0 k2F
kW> k2F
kV> k2F
e
+
â‰¤
+
+ O(Î´) + O
4Î·w T0
4Î·v T0
Î·w T0
Î·v T0



Ï„
âˆšw
T0
2


.
2

Using kW> kF â‰¤ Ï„w /10 and kV> kF â‰¤ Ï„v /10, and using the relationship Î·Ï„ww = Ï„Î·vv , we have
 âˆš 
kWT0 k2F
kWT0 k2F
4kW> k2F
4kV> k2F
Î·w T0
e
.
+
â‰¤
+
+ 0.1 + O
Ï„w2
Ï„v2
Ï„w2
Ï„v2
Ï„w
Therefore, choosing

e
T =Î˜
kW

k2

Ï„w2
min{1, Î´ 2 }

kW


e (min{1, Î´}) â‰¤ 0.1
Î·w = Î˜

k2

we can ensure that Ï„T20 F + Ï„T20 F â‰¤ 1 with high probability for all T0 = 0, 1, . . . , T âˆ’ 1 (so (C.3)
w
v
holds).
Finally, we note that it satisfies poly(CÎ±e (F), CÎ±e (G), Î±
eâˆ’1 ) â‰¤ poly(CÎ± (F), CÎ± (G), pF , Î±âˆ’1 ) with
Î±
the choice Î±
e = k(pF Cs (F )+pG Cs (G)) .


D

Theorem 2 and Theorem 3 Proof Details

Our proof relies on the following two structural lemmas. The first one is a simple corollary of the
Parsevalâ€™s equality from Boolean analysis.
P
Q
Lemma D.1. For every k âˆˆ {2, 3, Â· Â· Â· , d}, for every function f (x) = S 0 âŠ†[d] Î»S 0 jâˆˆS 0 xj , suppose
there exists S âŠ† [d] of size k and i âˆˆ S such that
i
h
Q
1 2
Î±
(D.1)
Exâˆ¼U ({âˆ’1,1}d ) |f (x) âˆ’ (xi + Î± jâˆˆS xj )|2 â‰¤ 16
P
1 2
Then we must have Î»S â‰¥ 43 Î± and S 0 âŠ†[d],|S 0 |=k,S 0 6=S Î»2S 0 â‰¤ 16
Î± .
Proof of Lemma D.1. The lemma follows from the following equality that can be easily verified:
h
i
Y
X
E
|f (x) âˆ’ (xi + Î±
xj )|2 = (Î»{i} âˆ’ 1)2 + (Î»S âˆ’ Î±)2 +
Î»2S 0 .

xâˆ¼U ({âˆ’1,1}d )

S 0 âŠ†[d],S 0 6=S,S 0 6={i}

jâˆˆS

The next one can be proved by carefully bounding the matrix rank (see Section D.3).
Lemma D.2. For every Î± > 0, for every matrix M âˆˆ RN Ã—R where R â‰¥ 2N , then there can not
be vectors a1 , Â· Â· Â· , aR âˆˆ RN such that for every r âˆˆ [R]:
P
1 2
2
hMr , ar i â‰¥ 34 Î± and
r0 âˆˆ[R],r0 6=r hMr0 , ar i â‰¤ 16 Î± .

D.1

Proof of Theorem 2

âˆš
Throughout the proof of Theorem 2, for notational simplicity, we re-scale inputs x by d so that
x âˆˆ {Â±1}d , and also re-scale Wâˆ— in the target function (7.1) to Wâˆ— = (ei1 , ei2 , Â· Â· Â· eik ).
For notation simplicity, below we restate Theorem 2 with respect to one single output k = 1
and d1 = d. The full statement for multiple outputs and more general distributions is a simple
corollary (see Remark D.3).
26


d1
1
Theorem 2 (simplified). For every integers k, d, N satisfying 2 â‰¤ k â‰¤ d and N â‰¤ 1000
k ,
for every(Mercer) kernel K(x, y) : RdÃ—d â†’ R, for every x(1) , Â· Â· Â· , x(N ) âˆˆ Rd , there exist at least
0.99 Ã— kd many S âŠ† [d] of size k such that, for every i âˆˆ S, for every w âˆˆ RN and the associated
P
kernel function K(x) = nâˆˆ[N ] K(x, x(n) )wn ,


 2
Q
1 2
> 16
Exâˆ¼U ({âˆ’1,1}d ) K(x) âˆ’ xi + Î± jâˆˆS xj
Î± .
Proof of Theorem 2. By property of (mercer) kernel, there exists feature mapping Î¦(x) = (Ï†` (x))`âˆˆN
where each Ï†` : Rd â†’ R such that:
P
K(x, y) = `âˆˆN Ï†` (x)Ï†` (y) .
Since we only care x âˆˆ {âˆ’1, 1}d , we can write each Ï†` (x) in its (Boolean) Fourier basis:
P
Q
âˆ€x âˆˆ {âˆ’1, 1}d : Ï†` (x) = SâŠ†[d] Î»S,` jâˆˆS xj .
d

Given arbitrary x(1) , . . . , x(N ) âˆˆ Rd , we can define matrix M âˆˆ RN Ã—(k) as follows:
X
def
âˆ€n âˆˆ [N ], âˆ€S âŠ† [d] with |S| = k :
Mn,S =
Î»S,` Ï†` (x(n) ) .
`âˆˆN

For any w âˆˆ

RN ,

we can write
X
X X
K(x) =
K(x, x(n) )wn =
Ï†` (x)Ï†` (x(n) )wn
nâˆˆ[N ] `âˆˆN

nâˆˆ[N ]

=

X  X X
S 0 âŠ†[d]

Î»S 0 ,` Ï†` (x(n) )wn

Y

xj =

jâˆˆS 0

nâˆˆ[N ] `âˆˆN

X

Y

hMS 0 , wi Â·

xj

(D.2)

jâˆˆS 0

S 0 âŠ†[d]

Hence, byPLemma D.1, if for some S âŠ† [d] of size k, there exists i âˆˆ S and exists wS âˆˆ RN with
KS (x) = nâˆˆ[N ] K(x, x(n) )[wS ]n satisfying
h
i
Y
1
E
|KS (x) âˆ’ (xi + Î±
xj )|2 â‰¤ Î±2 ,
16
xâˆ¼U ({âˆ’1,1}d )
jâˆˆS

then it must satisfy
hMS , wS i â‰¥ 34 Î±

and

2
|S|âŠ†[d],|S 0 |=k,S 0 6=S hMS 0 , wS i

P

â‰¤

1 2
16 Î±

.


d

However, according to Lemma D.2, as long as k â‰¥ 1000N , we know that the above condition
cannot hold for at least 0.99 fraction of the S âŠ† [d] of size k. This completes the proof.

Remark D.3. In the full statement of Theorem 2, there are multiple outputs K1 (x), . . . , Kk (x). It
suffices to focus on an arbitrary (say the first) coordinate and then apply the above lower bound.

def
In the full statement of Theorem 2, we have x âˆ¼ D = U {âˆ’1, 1}d1 Ã— D2 for k â‰¤ d1 â‰¤ d. In
such a case, one can write each x = (x/ , x. ) for x/ âˆˆ Rd1 and x. âˆˆ Rdâˆ’d1 . Now, equation (D.2)
becomes
X
Y
E [K(x)] = E [K(x/ , x. )] =
hMS , wi Â·
xj
x. âˆ¼D2

x. âˆ¼D2

S 0 âŠ†[d1 ]

jâˆˆS 0

and the final statement can be derived using the following simple property, for every S âŠ† [d1 ]
i
i
h
h
Y
Y
E
|K(x/ , x. ) âˆ’ (xi + Î±
xj )|2 â‰¥
E
| E [K(x/ , x. )] âˆ’ (xi + Î±
xj )|2 .
(x/ ,x. )âˆ¼D

jâˆˆS

x/ âˆ¼U ({âˆ’1,1}d1 )

27

x. âˆ¼D2

jâˆˆS

D.2

Proof of Theorem 3

âˆš
For notation simplicity, we re-scale inputs x by d so that x âˆˆ {Â±1}d , and also re-scale Wâˆ— in the
target function (7.1) to Wâˆ— = (ei1 , ei2 , Â· Â· Â· eik ).
Again for notation simplicity, below we restate Theorem 3 with respect to one single output
k = 1 and d1 = d. The full statement for multiple outputs and more general distributions is
analogous (in the same spirit as Remark D.3).

d
1
Theorem 3 (simplified). For every integers k, d, D satisfying 2 â‰¤ k â‰¤ d and D â‰¤ 1000
k , for

every Î± âˆˆ (0, 1), for every feature mapping Ï† : Rd â†’ RD , there exist at least 0.99 Ã— kd many
S âŠ† [d] of size k such that, for every i âˆˆ S, for every w âˆˆ RD and the associated linear function
F(x) = w> Ï†(x),


 2
Q
1 2
> 16
Exâˆ¼U ({âˆ’1,1}d ) F(x) âˆ’ xi + Î± jâˆˆS xj
Î± .
Proof of Theorem 3. Let us write Ï†(x) = (Ï†1 (x), Â· Â· Â· , Ï†D (x)) where each Ï†i : Rd â†’ R. Since we
only focus on x âˆˆ {âˆ’1, 1}d we can write
P
Q
Ï†i (x) = SâŠ†[d] Î»S,i jâˆˆS xj
d

for some set of coefficients Î»S,i âˆˆ R. Now, define matrix M âˆˆ RDÃ—2 as follows:
âˆ€i âˆˆ [D],

âˆ€S âŠ† [d] :

Mi,S = Î»S,i .

We have for every w âˆˆ RD (that can possibly depend on S),
P
Q
w> Ï†(x) = SâŠ†[d] hMS , wi jâˆˆS xj
This is exactly (D.2) in the proof of Theorem 2, so the rest of the proof follows analogously by
applying Lemma D.2.


D.3

Proof of Lemma D.2

Proof of Lemma D.2. Suppose by way towards contradiction that there exist vectors a1 , Â· Â· Â· , aR âˆˆ
RN such that for every r âˆˆ [R]:
P
1 2
2
hMr , ar i â‰¥ 34 Î± and
r0 âˆˆ[R],r0 6=r hMr0 , ar i â‰¤ 16 Î±
Let us define br =

1
hMr ,ar i ar

so they become

hMr , br i = 1

and

P

r0 âˆˆ[R],r0 6=r hMr0 , br i

2

â‰¤

1
9

Now, defining matrix B = {br }râˆˆ[R] âˆˆ RN Ã—R , we can rewrite
B> M = I + E âˆˆ RRÃ—R

(D.3)

2
where E is matrix with zero diagonals. Since for every r âˆˆ [R], it satisfies
r0 âˆˆ[R] Er,r0 =
P
1
1
2
2
r0 âˆˆ[R],r0 6=r hMr0 , br i â‰¤ 9 , we conclude that kEkF â‰¤ 9 R.
1
Next, since E cannot have more than 9 R singular values that are â‰¥ 1. By the min-max
theorem for singular values (a.k.a. Courant-Fischer theorem), there exists a subspace U of RR with
dimension 98 R such that maxxâˆˆU,kxk2 =1 kExk2 < 1. As a result, for every non-zero x âˆˆ U , we have
k(I + E)xk2 â‰¥ kxk2 âˆ’ kExk2 > 0. This implies
8
rank(I + E) â‰¥ R .
9
To the contrary, we have rank(B> M) â‰¤ N â‰¤ 12 R. This gives a contradiction.


P

28

D.4

Proof of Corollary 7.1

Proof of Corollary 7.1. To apply Theorem 1, we need to carefully verify Concept 1 by appropriately
re-scaling. Without loss of generality suppose (i1 , . . . , ik ) = (1, . . . , k). For every i âˆˆ [k], let us
define
âˆš
d
def
def
zi = Fi (x) = âˆš xi
k
âˆš
which satisfies Cs (F) = O( d), pF = 1, and kF(x)k2 = 1. Next, let us define
X
X (âˆ’1)si zi k
kk
def
âˆš
(âˆ’1)s1 +Â·Â·Â·+sk
Gr (z) = âˆš
kk!2k sâˆˆ{0,1}k
k
iâˆˆ[k]
and one can verify that Gr (z) =
satisfies Cs (G) = 2O(k) and pG =

k/2 Q
kâˆš
iâˆˆ[k] zi and therefore Gr (F(x))
k
k
2 . In sum, we have constructed

F(x) + Î±G(F(x)) =

âˆš1
k

âˆš

dxi + Î±

Q

jâˆˆ[k] (

âˆš

=


dxj )

âˆš1
k

Q

âˆš
(
dxi ). It also
iâˆˆ[k]

iâˆˆ[k]

âˆš



and we can thus apply Theorem 1 (after rescaling the label by 1/ k).

E

Existential Tool

In this section we include a simple variant of the existential lemma from [4]. We include the proofs
only for completenessâ€™ sake.
Consider random function G((x, 1); W> ) = (G1 ((x, 1); W> ), . . . , Gk ((x, 1); W> )) in which
>

def

Gr ((x, 1); W ) =

m
X

ar,i Â· hwi> , (x, 1)i Â· 1hw(0) ,(x,1)iâ‰¥0
i

i=1

(0)

where W> âˆˆ RmÃ—(d+1) is a given matrix, W(0) âˆˆ RmÃ—(d+1) is a random matrix where each wi
I
i.i.d. from N (0, m
), and each ar,i is i.i.d. from N (0, 1).
We have the following main lemma of this section:

is

Lemma E.1. Given any F : Rd â†’ Rk with general complexity (p, Cs (F), CÎµ (F)), for every Îµ âˆˆ
(0, pkC1s (F ) ), there exists M = poly(CÎµ (F), 1/Îµ) such that if m â‰¥ M , then with high probability there
> ) âˆˆ RmÃ—d (that does not depend on x) with
is a construction W> = (w1> , . . . , wm

kW> k2,âˆ â‰¤

kpCÎµ (F )
m

and

(F )
e kpC
âˆšs
kW> kF â‰¤ O(
)
m
âˆš

satisfying, for every x âˆˆ Rd , with probability at least 1 âˆ’ eâˆ’â„¦( m)
Pk
>
r=1 |Fr (x) âˆ’ Gr ((x, 1); W )| â‰¤ Îµ Â· k(x, 1)k2 ,

E.1

Restate Lemma E.1

We first note that, by replacing (x, 1) with x, we can restate Lemma E.1 as follows. Consider a
target function Î¦ : Rd â†’ Rk where
 âˆ—

p
X
hw1,r,i , xi
def
âˆ—
âˆ—
ar,i Â· Ï†r,i
Â· hw2,r,i
, xi
Î¦r (x) =
kxk2
i=1

29

and Ï†r,i : R â†’ R has only zero-order and odd-order terms in its Taylor expansion at zero, and
âˆ— k = kw âˆ— k = 1, C (Î¦) = max {C (Ï† )} and C (Î¦) = max {C (Ï† )}. Let
|aâˆ—r,i | â‰¤ 1, kw1,i
2
Îµ
r,i
Îµ r,i
s
r,i
s r,i
2,i 2
def

Gr (x; W> ) =

m
X

ar,i Â· hwi> , xi Â· 1hw(0) ,xiâ‰¥0 .
i

i=1

be the similarly defined random function. We have the following:
Lemma E.1â€™. For every Îµ âˆˆ (0, pkCs1(Î¦,1) ), there exists M = poly(CÎµ (Î¦, 1), 1/Îµ) such that if m â‰¥ M ,
> ) âˆˆ RmÃ—d (that does not depend
then with high probability there is a construction W> = (w1> , . . . , wm
on x) with
kpCÎµ (Î¦, 1)
s (Î¦, 1)
e kpCâˆš
)
kW> k2,âˆ â‰¤
and kW> kF â‰¤ O(
m
m
âˆš

satisfying, for every x âˆˆ Rd , with probability at least 1 âˆ’ eâˆ’â„¦(
k
X

m)

Î¦r (x) âˆ’ Gr (x; W> ) â‰¤ Îµ Â· kxk2 ,

r=1

We stress that Lemma E.1â€™ is a modified version of Lemma G.1 from [4, ver.4]. The only
difference is that in their original Lemma G.1, the indicator function 1hw(0) ,xiâ‰¥0 has an additional
i

random bias term (that is, becomes 1hw(0) ,xi+b(0) â‰¥0 ). In our Lemma E.1â€™, we do not allow such bias
i
i
and thus we can only fit functions Î¦ whose Taylor expansions have only zero-order and odd-order
terms (as opposed to arbitrary smooth functions in the original Lemma G.1).
The proof of Lemma E.1â€™ is based on the following â€œindicator to functionâ€ lemma, which is a
simple modification from Lemma 5.2 of [4, ver.4]. It says that given unit vector wâˆ— âˆˆ Rd , we can
approximate function Ï†(hwâˆ— , xi) (over x) by designing a random function 1hw,xiâ‰¥0 h(hw, wâˆ— i) where
w is a random Gaussian and h(Â·) is a function at our choice. Again, the only difference between
our Lemma E.2 and Lemma 5.2 of [4, ver.4] is that we do not have the random bias term.
Lemma E.2 (indicator to function). For every smooth function Ï† that only
 has zero-order and
1
odd-order terms in its Taylor expansion at point zero, every Îµ âˆˆ 0, Cs (Ï†,1)
, there exists a function h : R â†’ [âˆ’CÎµ (Ï†, 1), CÎµ (Ï†, 1)] that is also CÎµ (Ï†, 1)-Lipschitz continuous with the following two
(equivalent) properties:
(a) For every x1 âˆˆ [âˆ’1, 1]:
h
E 1Î±

1 x1 +Î²1

âˆš

i
h(Î±
)
âˆ’ Ï†(x1 ) â‰¤ Îµ
1
1âˆ’x2 â‰¥0
1

where Î±1 , Î²1 âˆ¼ N (0, 1) are independent random variables.
(b) For every wâˆ— , x âˆˆ Rd with kwâˆ— k2 = kxk2 = 1:


E 1hw,xiâ‰¥0 h(hw, wâˆ— i) âˆ’ Ï†(hwâˆ— , xi) â‰¤ Îµ
where w âˆ¼ N (0, I) is an d-dimensional Gaussian.


Furthermore, h satisfies EÎ±1 âˆ¼N (0,1) h(Î±1 )2 â‰¤ Cs (Ï†, 1)2 .
In the remainder of this section, for sake of completeness, we first prove Lemma E.2 in Section E.2,
and then prove Lemma E.1â€™ and Section E.3.

30

E.2

Proof of Lemma E.2: Indicator to Function

Recall from [4] by renaming variablesp
it suffices to prove Lemma E.2a. For notation simplicity, let
us denote w0 = (Î±1 , Î²1 ) and x = (x1 , 1 âˆ’ x21 ) where Î±1 , Î²1 are two independent random standard
Gaussians.
Throughout the
p proof, we also take an alternative view of the randomness. We write hw0 , xi = Î±
and Î±1 = Î±x1 + 1 âˆ’ x21 Î² for two independent Î±, Î² âˆ¼ N (0, 1).20
We first make a technical claim involving in fitting monomials in x1 . It is a simplified version
of Claim B.1 of [4, ver.4].
Claim E.3. Let hi (x) be the degree-i Hermite polynomial (see Definition A.4 of [4, ver.4]). For
such that
every odd integer i â‰¥ 1 there exists constant p0i with |p0i | â‰¥ (iâˆ’1)!!
4
xi1 =

1
p0i

[hi (Î±1 ) Â· 1[hx, w0 i â‰¥ 0]]

E

w0 âˆ¼N (0,I)âˆ¼N (0,1)

(The proof of Claim E.3 is identical to that of the original Claim B.1 of [4, ver.4] by forcing the
bias term b0 = 0.)
We next use Claim E.3 to fit arbitrary functions Ï†(x1 ). By Taylor expansion, we have
Ï†(x1 ) = c0 +

âˆ
X

ci xi1 = c0 +

âˆ
X

c0i Â·

i=1

i=1, odd i

E

Î±,Î²,b0 âˆ¼N (0,1)



hi (Î±1 ) Â· 1[hx, w0 i + b0 â‰¥ 0]

where
c0i =

def

ci
,
p0i

|c0i | â‰¤

4 |ci |
(i âˆ’ 1)!!

(E.1)

Next, recall the following claim on absolute values of the Hermite polynomials (see Claim B.2 of
[4, ver.4]).
q
def
Claim E.4. Setting Bi = 100i1/2 + 10 log 1Îµ , we have


Pâˆ 0
|c
|
Â·
E
|h
(z)|
Â·
1[|z|
â‰¥
B
]
â‰¤ /8
(a)
i
i
zâˆ¼N
(0,1)
i
i=1


Pâˆ 0
(b)
i=1 |ci | Â· Ezâˆ¼N (0,1) |hi (Bi )| Â· 1[|z| â‰¥ Bi ] â‰¤ /8

 1
Pâˆ 0
(c)
i=1 |ci | Â· Ezâˆ¼N (0,1) |hi (z)| Â· 1[|z| â‰¤ Bi ] â‰¤ 2 CÎµ (Ï†, 1)
 d
 1
Pâˆ 0
(d)
i=1 |ci | Â· Ezâˆ¼N (0,1) dz hi (z) Â· 1[|z| â‰¤ Bi ] â‰¤ 2 CÎµ (Ï†, 1)
def
Now, let us define b
hi (Î±1 ) = hi (Î±1 ) Â· 1[|Î±1 | â‰¤ Bi ] + hi (sign(Î±1 )Bi ) Â· 1[|Î±1 | > Bi ] as the truncated
version of the Hermite polynomial hi (Â·). Using Claim E.4, we have
âˆ
h
i
X
0
b
Ï†(x1 ) = c0 + R (x1 ) +
c0i Â·
E
hi (Î±1 ) Â· 1[hx, w0 i â‰¥ 0]

i=1

Î±,Î²âˆ¼N (0,1)

where |R0 (x1 )| < /4 uses Claim E.4a and Claim E.4b. In other words, if we define
def

h(Î±1 ) = 2c0 +

âˆ
X

c0i Â· b
hi (Î±1 )

i=1

This is possible for the following reason. Let xâŠ¥ = ( 1 âˆ’ x21 , âˆ’x1 ) be unit vector orthogonal to x. We can write
w0 = Î±x + Î²xâŠ¥ where Î±, Î² âˆ¼ N (0, 1) are two independent Gaussians.
20

p

31

then we have


E

Î±,Î²âˆ¼N (0,1)


1[hx, w0 i â‰¥ 0] Â· h(Î±1 ) âˆ’ Ï†(x1 ) = R0 (x1 ) â‰¤ Îµ/4 .

As for the range of h, we use Claim E.4b and Claim E.4c to derive that
Îµ 1
|h(Î±1 )| â‰¤ 2c0 + + CÎµ (Ï†, 1) â‰¤ CÎµ (Ï†, 1) .
8 2
As for the Lipschitz continuity of h on its first coordinate Î±1 , we observe that for each i > 0,
d
b
hi (z) for |z| < Bi .
hi (z) has zero sub-gradient for all |z| â‰¥ Bi . Therefore, it suffices to bound dz
Replacing the use of Claim E.4c by Claim E.4d immediately gives us the same bound on the
Lipschitz continuity of h with respect to Î±1 .

As for the expected square EÎ±1 âˆ¼N (0,1) h(Î±1 )2 , we can write
h(Î±1 ) = 2c0 +

âˆ
X

c0i

Â¬
Â·b
hi (Î±1 ) = 2c0 +

i=1

âˆ
X

c0i Â· hi (Î±1 ) Â±

i=1

Îµ
4

Above, Â¬ uses Claim E.4a and Claim E.4b.
Using the othogonality condition of Hermite polynoâˆš
mials (that is, Exâˆ¼N (0,1) [hi (x)hj (x)] = 2Ï€j!Î´i,j ), we immediately have
2

E

Î±1 âˆ¼N (0,1)

2

[h(Î±1 ) ] â‰¤ O(Îµ +

c20 )

âˆ
X
+ O(1) Â·
(c0i )2 (i!)
i=1

âˆ
X
(i!) Â· |ci |2
2
2
â‰¤ O(Îµ + c0 ) + O(1) Â·
((i âˆ’ 1)!!)2
i=1

â‰¤ O(Îµ2 + c20 ) + O(1) Â·
Above, Â¬ uses inequality

i!
((iâˆ’1)!!)2

âˆš
â‰¤ 2 i for all i â‰¥ 1.

âˆ
X

i0.5 Â· |ci |2 â‰¤ Cs (Ï†, 1)2 .

i=1



This finishes the proof of Lemma E.2a.

E.3

Proof of Lemma E.1â€™

Without loss of generality we assume kxk2 = 1 in this proof. (Both Î¦ and G are positive homogeneous in x.)
âˆ—
âˆ—
Fit a single function aâˆ—r,i Ï†r,i (hw1,r,i
, xi)hw2,r,i
, xi. We first fix some r âˆˆ [k] and i âˆˆ [p]
>
d
(r,i)
and construct weights wj âˆˆ R . Let h (Â·) be the function h(Â·) constructed from Ï† = Ï†r,i using
Lemma E.2. We have |h(r,i) | â‰¤ CÎµ (Î¦, 1). Define
âˆš

def
(0)
âˆ—
âˆ—
wj> = ar,j aâˆ—r,i h(r,i)
mhwj , w1,i
i w2,i
(E.2)

where
that

âˆš

(0)

âˆ— i has the same distribution with Î± in Lemma E.2. By Lemma E.2, we have
mhwj , w1,i
1


E

(0)

wj ,ar,j




âˆš

(0)
âˆ—
âˆ—
ar,j 1hw(0) ,xiâ‰¥0 hwj> , xi = E aâˆ—r,i 1hw(0) ,xiâ‰¥0 h(r,i)
mhwj , w1,i
i hw2,i
, xi
(0)

j

j

wj

âˆ—
âˆ—
= aâˆ—r,i Ï†r,i (hw1,i
, xi)hw2,i
, xi Â± Îµ .

Fit a combination

P

iâˆˆ[p]

âˆ—
âˆ—
aâˆ—r,i Ï†r,i (hw1,r,i
, xi)hw2,r,i
, xi. We can re-define (the norm grows by

32

a maximum factor of p)
wj> = ar,j

X

aâˆ—r,i h(r,i)

âˆš


(0)
âˆ—
âˆ—
mhwj , w1,i
i w2,i

iâˆˆ[p]

and the same above argument gives

 X
âˆ—
âˆ—
>
aâˆ—r,i Ï†r,i (hw1,i
, xi)hw2,i
, xi Â± Îµp.
E
ar,j 1hw(0) ,xiâ‰¥0 hwj , xi =
(0)

j

wj ,ar,j

iâˆˆ[p]

Fit multiple outputs. If there are k outputs let us re-define (the norm grows by a maximum
factor of k)
âˆš

X
X
(0)
âˆ—
âˆ—
mhwj , w1,i
i w2,i
.
(E.3)
wj> =
ar,j
aâˆ—r,i h(r,i)
râˆˆ[k]

iâˆˆ[p]

and consider the quantity
def

Îr,j = ar,j hwj> , xi1hw(0) ,xiâ‰¥0 .
j

r0

By randomness of a we know that for
[Îr,j ]

E

(0)

6= r, E[ar,j ar0 ,j ] = 0. Thus, for every r âˆˆ [k], it satisfies

wj ,a1,j ,...,ak,j

ï£®
=

E

X

(0)
wj ,a1,j ,...,ak,j

ï£°

X

ar,j ar0 ,j

r0 âˆˆ[k]

= E ï£°
(0)
wj

=

iâˆˆ[p]

X

j

iâˆˆ[p]

ï£®
X

1hw(0) ,xiâ‰¥0 aâˆ—r0 ,i h(r,i)

âˆš

1hw(0) ,xiâ‰¥0 aâˆ—r,i h(r,i)
j

âˆš

ï£¹
(0)



âˆ—
âˆ—
, xiï£»
mhwj , w1,i
i hw2,i

ï£¹


(0)
âˆ—
âˆ—
mhwj , w1,i
i hw2,i
, xiï£»

âˆ—
âˆ—
aâˆ—r,i Ï†r,i (hw1,i
, xi))hw2,i
, xi Â± pÎµ = Î¦âˆ—r (x) Â± pÎµ .

iâˆˆ[p]

Now, re-scaling each wj> by a factor of
>

Gr (x; W ) =

m
X

1
m

Îr,j

and re-scaling Îµ by

1
2pk ,

we can write

h
i
Îµ
E Gr (x; W> ) = Î¦âˆ—r (x) Â±
.
2k

and

j=1

Now, we use |h(r,i) | â‰¤ CÎµ (Î¦, 1) and apply the concentration from Lemma B.3, which implies for our
2
4 2
parameter choice of m, with probability at least 1 âˆ’ eâˆ’â„¦(mÎµ /(k p CÎµ (Î¦,1)))
Îµ
|Gr (x; W> ) âˆ’ Î¦âˆ—r (x)| â‰¤
.
k
Norm on W> . According to its definition in (E.3), we have for each j âˆˆ [m], with high probability

e kpCÎµ (Î¦,1) (here the additional 1 is because we have re-scaled w> by 1 ). This means
kwj> k2 â‰¤ O
j
m
m
m
kpCÎµ (Î¦,1) 
>
e
kW k2,âˆ â‰¤ O
. As for the Frobenius norm,
m

kW> k2F =

X

kwj> k22 â‰¤

jâˆˆ[m]

Now, for each i âˆˆ [p], we know that

X
jâˆˆ[m]

(r,i)
jâˆˆ[m] h

P

2

e k p) Â·
O(
m2
âˆš

X

h(r,i)

âˆš

(0)

âˆ—
mhwj , w1,i
i

2

(E.4)

iâˆˆ[p]

2
(0)
âˆ— i
mhwj , w1,i
is a summation of i.i.d. random

variables, each with expectation at most Cs (Î¦, 1)2 by Lemma E.2. Applying Hoeffdingâ€™s concen33

âˆš

tration, we have with probability at least 1 âˆ’ eâˆ’â„¦( m)
âˆš
X
âˆš (0) 2
(0)
âˆ—
h(r,i)
â‰¤ m Â· Cs (Î¦, 1)2 + m3/4 Â· CÎµ (Î¦, 1)2 â‰¤ 2mCs (Î¦, 1)2
mhwj , w1,i
i, mbj
jâˆˆ[m]

e k2 p2 Cs (Î¦,1)2 ). This finishes the proof of Lemma E.1â€™.
Putting this back to (E.4) we have kW> k2F â‰¤ O(
m

References
[1] Zeyuan Allen-Zhu and Yuanzhi Li. Can SGD learn recurrent neural networks with provable generalization? In NeurIPS, 2019. Full version available at http://arxiv.org/abs/1902.01028.
[2] Zeyuan Allen-Zhu and Yuanzhi Li. Backward Feature Correction: How Deep Learning Performs Deep
Learning. arXiv preprint, January 2020. Full version available at http://arxiv.org/abs/2001.04413.
[3] Zeyuan Allen-Zhu and Yuanzhi Li. Feature purification: How adversarial training performs robust deep
learning. arXiv preprint arXiv:2005.10190, 2020.
[4] Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and Generalization in Overparameterized
Neural Networks, Going Beyond Two Layers. In NeurIPS, 2019. Full version available at http:
//arxiv.org/abs/1811.04918.
[5] Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. On the convergence rate of training recurrent neural
networks. In NeurIPS, 2019. Full version available at http://arxiv.org/abs/1810.12065.
[6] Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via overparameterization. In ICML, 2019. Full version available at http://arxiv.org/abs/1811.03962.
[7] Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang. On exact
computation with an infinitely wide neural net. arXiv preprint arXiv:1904.11955, 2019.
[8] Sanjeev Arora, Simon S. Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks. CoRR, abs/1901.08584,
2019. URL http://arxiv.org/abs/1901.08584.
[9] Ainesh Bakshi, Rajesh Jayaram, and David P Woodruff. Learning two layer rectified neural networks
in polynomial time. arXiv preprint arXiv:1811.01885, 2018.
[10] Digvijay Boob and Guanghui Lan. Theoretical properties of the global optimizer of two layer neural
network. arXiv preprint arXiv:1710.11241, 2017.
[11] Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with gaussian
inputs. arXiv preprint arXiv:1702.07966, 2017.
[12] Amit Daniely. Sgd learns the conjugate kernel class of the network. In Advances in Neural Information
Processing Systems, pages 2422â€“2430, 2017.
[13] Amit Daniely, Roy Frostig, and Yoram Singer. Toward deeper understanding of neural networks: The
power of initialization and a dual view on expressivity. In Advances in Neural Information Processing
Systems (NIPS), pages 2253â€“2261, 2016.
[14] Simon S Du, Jason D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. arXiv preprint arXiv:1811.03804, 2018.
[15] Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. arXiv preprint arXiv:1810.02054, 2018.
[16] Rong Ge, Jason D Lee, and Tengyu Ma. Learning one-hidden-layer neural networks with landscape
design. arXiv preprint arXiv:1711.00501, 2017.
[17] Rong Ge, Rohith Kuditipudi, Zhize Li, and Xiang Wang. Learning two-layer neural networks with
symmetric inputs. In International Conference on Learning Representations, 2019.
[18] Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of neural
networks. In Proceedings of the Conference on Learning Theory, 2018.
[19] Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition with deep recurrent

34

neural networks. In Acoustics, speech and signal processing (icassp), 2013 ieee international conference
on, pages 6645â€“6649. IEEE, 2013.
[20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.
In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770â€“778, 2016.
[21] Arthur Jacot, Franck Gabriel, and CleÌment Hongler. Neural tangent kernel: Convergence and generalization in neural networks. In Advances in neural information processing systems, pages 8571â€“8580,
2018.
[22] Kenji Kawaguchi. Deep learning without poor local minima. In Advances in Neural Information
Processing Systems, pages 586â€“594, 2016.
[23] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional
neural networks. In Advances in neural information processing systems, pages 1097â€“1105, 2012.
[24] Yuanzhi Li and Zehao Dou. When can wasserstein gans minimize wasserstein distance? arXiv preprint
arXiv:2003.04033, 2020.
[25] Yuanzhi Li and Yingyu Liang. Provable alternating gradient descent for non-negative matrix factorization with strong correlations. In Proceedings of the 34th International Conference on Machine
Learning-Volume 70, pages 2062â€“2070. JMLR. org, 2017.
[26] Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient
descent on structured data. In Advances in Neural Information Processing Systems, 2018.
[27] Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu activation. In
Advances in Neural Information Processing Systems, pages 597â€“607, 2017.
[28] Yuanzhi Li, Yingyu Liang, and Andrej Risteski. Recovery guarantee of non-negative matrix factorization
via alternating updates. In Advances in neural information processing systems, pages 4987â€“4995, 2016.
[29] Yuanzhi Li, Tengyu Ma, and Hongyang Zhang. Algorithmic regularization in over-parameterized matrix
sensing and neural networks with quadratic activations. In COLT, 2018.
[30] Tengyu Ma. CS229T/STAT231: Statistical Learning Theory (Fall 2017). https://web.stanford.edu/
class/cs229t/scribe_notes/10_17_final.pdf, October 2017. accessed May 2019.
[31] Martin J. Wainwright. Basic tail and concentration bounds. https://www.stat.berkeley.edu/
~mjwain/stat210b/Chap2_TailBounds_Jan22_2015.pdf, 2015. Online; accessed Oct 2018.
[32] Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural networks. In Conference on Learning Theory, pages 1376â€“1401, 2015.
[33] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do CIFAR-10 Classifiers
Generalize to CIFAR-10? arXiv preprint arXiv:1806.00451, 2018.
[34] Mark Rudelson and Roman Vershynin. Non-asymptotic theory of random matrices: extreme singular
values. In Proceedings of the International Congress of Mathematicians 2010 (ICM 2010) (In 4 Volumes) Vol. I: Plenary Lectures and Ceremonies Vols. IIâ€“IV: Invited Lectures, pages 1576â€“1602. World
Scientific, 2010.
[35] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the
game of go with deep neural networks and tree search. nature, 529(7587):484, 2016.
[36] Mahdi Soltanolkotabi, Adel Javanmard, and Jason D Lee. Theoretical insights into the optimization
landscape of over-parameterized shallow neural networks. arXiv preprint arXiv:1707.04926, 2017.
[37] Daniel Soudry and Yair Carmon. No bad local minima: Data independent training error guarantees for
multilayer neural networks. arXiv preprint arXiv:1605.08361, 2016.
[38] Yuandong Tian. An analytical formula of population gradient for two-layered relu network and its
applications in convergence and critical point analysis. arXiv preprint arXiv:1703.00560, 2017.
[39] Santosh Vempala and John Wilmes. Polynomial convergence of gradient descent for training one-hiddenlayer neural networks. arXiv preprint arXiv:1805.02677, 2018.
[40] Colin Wei, Jason D Lee, Qiang Liu, and Tengyu Ma. On the margin theory of feedforward neural
networks. arXiv preprint arXiv:1810.05369, 2018.

35

[41] Bo Xie, Yingyu Liang, and Le Song. Diversity leads to generalization in neural networks. arXiv preprint
Arxiv:1611.03131, 2016.
[42] Greg Yang. Scaling limits of wide neural networks with weight sharing: Gaussian process behavior,
gradient independence, and neural tangent kernel derivation. arXiv preprint arXiv:1902.04760, 2019.
[43] Yuchen Zhang, Jason D Lee, and Michael I Jordan. l1-regularized neural networks are improperly
learnable in polynomial time. In International Conference on Machine Learning, pages 993â€“1001, 2016.
[44] Kai Zhong, Zhao Song, Prateek Jain, Peter L Bartlett, and Inderjit S Dhillon. Recovery guarantees for
one-hidden-layer neural networks. arXiv preprint arXiv:1706.03175, 2017.
[45] Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Stochastic gradient descent optimizes overparameterized deep relu networks. arXiv preprint arXiv:1811.08888, 2018.

36

