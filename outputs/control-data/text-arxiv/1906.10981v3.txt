1

Learning Orthogonal Projections in Linear Bandits

arXiv:1906.10981v3 [cs.LG] 24 Oct 2019

Qiyu Kang, and Wee Peng Tay, Senior Member, IEEE

that the independent d-armed bandit problem can be viewed as
a special case of the general linear stochastic bandit problem,
where the set of d available decisions serves as a standard
orthonormal basis of Rd [14].
The linear stochastic bandit model has been successfully
applied to some real-world problems like personalized news
article recommendation [18], advertisement selection [19], and
information retrieval [20]. For example, in news recommendation applications, typical features, including the newsâ€™ topic
categories, the usersâ€™ race, gender, location, etc., can be treated
as components in each arm or decision vector, while the usersâ€™
clicks are the rewards. A plausible recommendation scheme
is to get as many clicks as possible. Another application is in
sequential clinical trials [21]â€“[23] whose aims are to balance
the correct identification of the best treatment (exploration) and
the effectiveness of the treatment during the trials (exploitation).
Index Termsâ€”Linear bandit, multi-armed bandit, orthogonal
The
classical sequential clinical trials containing d different
projection, discrimination aware
drugs can be modeled as an independent d-armed bandit
problem. However, this may be impractical as treatments may
I. I NTRODUCTION
utilize multiple drugs simultaneously. A more feasible and
ULTI-ARMED bandit (MAB) problems, introduced by general way is to model the treatment decision as a set of
Robbins [1] model the exploration and exploitation trade- mixed drugs instead. An arm corresponds to a mixed drugs
off of sequential decision making under uncertainty. In its treatment with specific dosages of each kind. The reward at
most basic paradigm, at each time step, the decision maker each time step is the curative efficacy after applying the mixed
is given d decisions or arms from which he is supposed to drug treatment to a patient at each trial.
However, in some scenarios, the decision maker is more
select one, and as a response, he observes a stochastic reward
interested
in some criterion other than maximizing the cumuafter each decision-making. The arm chosen at each time step
lative
reward
in the standard linear stochastic bandit model.
is based on the information gathered at all the previous time
One
example
is
a discrimination-aware movie recommendation
steps. Therefore, in order to maximize the expected cumulative
system.
To
avoid
racially discriminatory recommendations, a
reward, exploitation of the current empirically best arm and
userâ€™s
race
should
not play any role in the recommendation
exploration of less frequently chosen arms should be balanced
system.
However,
the
observed reward (number of clicks) may
carefully. Stochastic independence of rewards is assumed for
be
biased
by
racial
factors.
A black user may have a history
different arms in some works like [2]â€“[10]. In other works like
of
following
a
particular
black
actor, but it does not mean
[11]â€“[15], reward dependence between arms is assumed, which
that
he
or
she
should
always
be
recommended movies with
allows the decision maker to gather information for more than
black
actors.
For
example,
Netflix
last year angered black
one arm at each time step. One such specific assumption is
subscribers
with
targeted
posters
containing
black actors no
that arms are vectors containing numeric elements, and the
matter
how
minor
their
roles
in
the
film
are
[24].
In principle,
expected reward of choosing each arm is an inner product of
to
avoid
discrimination,
protected
or
sensitive
attributes
such
the arm vector with an unknown parameter vector [14]â€“[17].
as
race
or
gender
should
not
be
included
in
recommendation
In [14], [15], the authors proposed effective strategies that
balance exploration and exploitation based on the optimism- algorithms. Nonetheless, discarding such attributes directly
in-the-face-of-uncertainty principle. This principle maintains and modeling the reward as a linear function of the other
a high probability confidence set for the estimated parameter unprotected attributes may introduce system bias during the
vector, and at each time step, the decision maker chooses an arm learning process. In another example, some clinical trials seek
and a parameter vector from the decision set and the confidence to maximize the curative effect on one disease of a mixed drugs
set respectively, so that their inner product is maximized. Note treatment, while at the same time the patients who have this
disease may concurrently have another disease. These patients
This research is supported in part by the Singapore Ministry of Education
need to take other drugs that have a positive or negative impact
Academic Research Fund Tier 2 grant MOE2018-T2-2-019.
on the targeted disease. One typical example is the treatment
The authors are with the School of Electrical and Electronic Engineering,
Nanyang Technological University, Singapore. Email: kang0080@e.ntu.edu.sg, of hypertension in chronic kidney disease. Hypertension is
wptay@ntu.edu.sg.
present in more than 80% of the patients with chronic kidney
Abstractâ€”In a linear stochastic bandit model, each arm is a
vector in an Euclidean space and the observed return at each
time step is an unknown linear function of the chosen arm at that
time step. In this paper, we investigate the problem of learning
the best arm in a linear stochastic bandit model, where each
armâ€™s expected reward is an unknown linear function of the
projection of the arm onto a subspace. We call this the projection
reward. Unlike the classical linear bandit problem in which the
observed return corresponds to the reward, the projection reward
at each time step is unobservable. Such a model is useful in
recommendation applications where the observed return includes
corruption by each individualâ€™s biases, which we wish to exclude
in the learned model. In the case where there are finitely many
arms, we develop a strategy to achieve O(|D| log n) regret, where
n is the number of time steps and |D| is the number of arms.
In the case where each arm is chosen from an infinite compact
set, our strategy achieves O(n2/3 (log n)1/2 ) regret. Experiments
verify the efficiency of our strategy.

M

2

disease, and drugs like ACEi and ARB targeted for reducing by projecting both the decision arm and context vectors into a
proteinuria may also have an effect on blood pressure [25]. subspace.
To study the effect of a mixed drug treatment targeted for
In the stochastic multi-objective multi-armed bandit
controlling hypertension, the decision maker is supposed to (MOMAB) problem [30]â€“[33], the reward of making one
discount the impact of drugs like ACEi and ARB in the decision decision is a vector rather than the scalar reward in the
process.
standard multi-armed bandit problem. Because of the possible
In this paper, we propose a linear stochastic bandit for- conflicting objectives, a set of Pareto optimal arms [31] [34]
mulation that maximizes the (expected) cumulative reward is considered in the MOMAB problem instead of a single
over a subspace of decision attributes, based on the reward best arm. Scalarization techniques can be used to transform
observed for the full space. Specifically, each arm is projected a MOMAB problem to a single objective MAB, and an arm
orthogonally onto a target subspace U. The reward is then belonging to the Pareto optimal set is regarded as a best arm in
decomposed into two components, one of which is due to U, a particular scalarization function. In our work, we consider the
and the other is due to UâŠ¥ . We call the first component the decomposition of the reward where the decomposition is similar
projection reward and the second component the corruption. to a two-objective MAB applied with a linear scalarization
We develop a strategy that achieves1 O(|D| log n) cumulative function [31]. However, the difference between our model and
projection regret when there are finitely many |D| arms and MOMAB with linear scalarization is that we can only observe
where n is the number of time steps. In the case where the the sum of the (desired) projection reward and (undesired)
arms are drawn from an infinite compact set, we achieve a corruption at each time step rather than a reward vector
cumulative projection regret of O(n2/3 (log n)1/2 ). Here, the consisting of the two components as in MOMAB. Furthermore,
projection regret at each time step is defined as the difference our objective is to minimize the cumulative projection regret
between the current expected projection reward of making rather than minimizing the three types of regret defined in [31].
one decision and the oracle best expected projection reward.
This algorithm is based on the t -greedy policy [2], which
B. Our Contributions
is a simple and well-known algorithm for the standard finite
In this paper, we study the orthogonal projection problem
multi-armed bandit problem.
in linear bandits, where our aim is to maximize the expected
projection reward, which is an unknown linear function of a
A. Related Work
projection of the chosen arm onto a target subspace U âŠ† Rd .
Two categories of work are related to our problem of interest: In the case where there are finitely many arms, we develop a
1) the linear stochastic bandit model; and 2) the multi-objective strategy to achieve O(|D| log n) cumulative projection regret,
where n is the number of time steps and |D| is the number
Pareto bandit problem.
In the linear stochastic bandit model, the decision maker of arms. In the case where the arm is chosen from an
predicts the reward of an arm in Rd based on the given context infinite compact set, our strategy achieves O(n2/3 (log n)1/2 )
vector of this decision. In [26], the author proposed an algorithm cumulative projection regret.
In the linear stochastic bandit literature, the best cumulative
based on least squares estimation and highâˆšprobability confibound for the case of a compact decision set
dence bounds, and showed that it has O( dn log3/2 (n|D|)) regret upper
âˆš
regret upper bound for the case of finitely many decisions, is O(d n polylog(n)) [14], [15], where polylog(n) means
where |D| is the number of arms. The work [22] extended a polynomial in log n. If a further smoothness assumption
[26] to the problem with an arbitrary
âˆš compact set of arms same as that in [15]âˆšis made, we show that it is possible
and presented a policy with O(d n log3/2 n) regret upper to achieve the O(d n) projection regret upper bound in
bound. References [14], [15] further improved the regret our problem
âˆš formulation. However, the existence of a policy
upper bound using smaller confidence sets established using with O(d n polylog(n)) cumulative projection regret for the
martingale techniques. In [27], the authors proposed a linear general compact decision set remains an open question for our
bandit formulation with hidden features where the reward of problem. We verify our proposed policies on both synthetic
choosing one decision is the sum of two components, one of simulations and experiments on a wine quality dataset [35].
which is a linear function of the observable features, and the
The rest of this paper is organized as follows. In Section II,
other is a linear function of the unobservable features. They we present our system model and assumptions. In Section III,
applied a upper confidence bound (UCB)-type linear bandit we introduce our strategies and prove that they achieve
policy with a coordinate descent [28], [29] algorithm in which sublinear cumulative projection regret. In Section IV, we
estimating the hidden features and the unknown coefficients present simulations and experiments to verify the performance
jointly over time is achieved. Our work is different from the of our strategies. Section V concludes the paper.
above-mentioned papers in that we seek to maximize only the
Notations: We use E c to denote the complement of the
cumulative projection reward rather than the reward observed event E. The indicator function 1A (Ï‰) = 1 if and only if
for the full space. Furthermore, the projection reward is formed Ï‰ âˆˆ A. I is the identity matrix. For x, y âˆˆ Rd , let x| be the
transpose p
of x , and hx, yi = x| y. We use kxk2 to denote the
1 For non-negative functions f (n) and g(n), we write f (n) = O(g(n)) if
L
to denote the weighted L2 -norm
p2 -norm hx, xi and kxkAdÃ—d
lim supnâ†’âˆž f (n)/g(n) < âˆž, f (n) = o(g(n)) if limnâ†’âˆž f (n)/g(n) =
hx,
Axi
for
any
A
âˆˆ
R
is a positive definite matrix.
0, and f (n) = Î˜(g(n)) if 0 < lim inf nâ†’âˆž f (n)/g(n) â‰¤
lim supnâ†’âˆž f (n)/g(n) < âˆž.
We use U(a, b) to denote the continuous uniform distribution

3

whose density function has support [a, b], and N (0, Ï‘2 ) to
denote the Gaussian distribution with variance Ï‘2 and mean 0.
II. S YSTEM M ODEL
d

Let D âŠ† R be a compact set of decisions or arms from
which the decision maker has to choose an arm Xt at each
time step t. The observed return rt after choosing Xt is given
by
rt = hXt , Î¸i + Î·t ,

(1)

where Î¸ âˆˆ Rd is a fixed but unknown parameter and Î·t is
a zero mean independent and identically distributed (i.i.d.)
random noise.
In the standard linear bandit problem, the performance of
a policy is measured by the difference between the decision
makerâ€™s cumulative reward and the cumulative reward achieved
by the oracle policy with knowledge of Î¸. Formally, the goal
of the decision maker is to minimize the cumulative regret
over n time steps defined by
R(n) =

n
X


hX âˆ— , Î¸i âˆ’ E [hXt , Î¸i] ,

(2)

t=1

where X âˆ— = arg maxxâˆˆD hx, Î¸i is the optimal arm in the
standard linear bandit problem.
In our orthogonal projection linear bandit model, we consider
a decomposition of the observed return into two components.
Let U, whose dimension is not more than d, be a subspace of
the Euclidean vector space Rd . Let UâŠ¥ = {q âˆˆ Rd : hq, pi =
0, âˆ€p âˆˆ U} be the orthogonal complement of U. It is a standard
result [36] that Rd = UâŠ•UâŠ¥ , and each w âˆˆ Rd can be written
uniquely as a sum p + q, where p âˆˆ U and q âˆˆ UâŠ¥ . We define
the linear orthogonal projection operator as PU : Rd 7â†’ U such
that PU (w) = p. The operator PU can be represented by a
d Ã— d matrix whose rank is the dimension of U. The expected
observed return can therefore be decomposed as
hXt , Î¸i = hPU (Xt ) + PUâŠ¥ (Xt ), PU (Î¸) + PUâŠ¥ (Î¸)i
= hPU (Xt ), PU (Î¸)i + hPUâŠ¥ (Xt ), PUâŠ¥ (Î¸)i.

(3)

We call hPU (Xt ), PU (Î¸)i the projection reward on the subspace
U, which is denoted by rU,t . The other part of the observed
return, hPUâŠ¥ (Xt ), PUâŠ¥ (Î¸)i, is called the corruption. Different
from the standard linear bandit model, where the reward can
be observed (with random perturbation), in our orthogonal
projection linear bandit model, we do not directly observe the
projection reward rU,t . Furthermore, the cumulative projection
regret is defined as
RU (n) =

n
X
t=1

than the cumulative regret R(n), base on the observed returns
r1 , . . . , rnâˆ’1 .
For a concrete illustration, consider again the discrimination
prevention problem in a movie recommendation system described in Section I. An arm is a movie to be recommended to
a specific set of users and is represented by the features of the
movie. The features include the movie genre, movie length,
music types used, ages and races of the actors or actresses, etc.
Suppose that each arm is a d dimensional vector, where some of
its elements like the ages and races may lead to discriminatory
recommendations and are â€œprotectedâ€. To eliminate the effect
of those features when performing the recommendations, the
system should consider only the non-protected dimensions. In
[37], the authors proposed to control the discrimination effect
in a linear regression model, where PU in their formulation
can also be viewed as a projection matrix.
We use span(D) to denote the set of finite linear combinations of arms in D. In the ideal case, if at each time step t the
return rt is available without the random noise Î·t , and if D
is specified by linear inequality constraints, the problem then
degenerates to a linear programming problem that finds x âˆˆ D
to maximize
hPU (x), PU (Î¸)i
= hPU (x), Î¸i
= PU (x), Pspan(D) (Î¸) + Pspan(D)âŠ¥ (Î¸)
= PU (x), Pspan(D) (Î¸) + PU (x), Pspan(D)âŠ¥ (Î¸) .

(6)

However, since Pspan(D)âŠ¥ (Î¸) is unobservable even in the ideal
case, and arg maxxâˆˆD PU (x), Pspan(D) (Î¸) is not equal to
XUâˆ— (Î¸) in (5) when PU (x), Pspan(D)âŠ¥ (Î¸) 6= 0, a linear
cumulative projection regret is therefore inevitable in the worst
case. In order to get a sublinear cumulative regret, further
assumptions are required.
A possible assumption is PU (x), Pspan(D)âŠ¥ (Î¸) = 0 for
all x and Î¸. This is equivalent to saying U âŠ† span(D). To
see this, note that this assumption means that Pspan(D)âŠ¥ (Î¸) âˆˆ
kernel(PU ) = PUâŠ¥ = PUâŠ¥ . It follows that span(D)âŠ¥ âŠ† UâŠ¥ ,
which is equivalent to U âŠ† span(D). We make the following
assumption throughout this paper.
Assumption 1. We have U âŠ† span(D), the dimension of
span(D) is k, where dim(U) â‰¤ k â‰¤ d, and kPU (Î¸)k2 > 0.
Furthermore, the decision maker has access to k linearly
independent arms in D.

We use Dk âŠ† D to denote the set containing the k linearly

hPU (XUâˆ— (Î¸)), PU (Î¸)i âˆ’ E [hPU (Xt ), PU (Î¸)i] , independent arms in Assumption 1. The condition kPU (Î¸)k2 >
0 guarantees Î¸ âˆˆ
/ UâŠ¥ . We also make another assumption which
(4) is standard in the literature for the linear bandit problem.

where
XUâˆ— (Î¸) = arg max hPU (x), PU (Î¸)i

(5)

xâˆˆD

is the best projection arm. The objective in our model is
to minimize the cumulative projection regret RU (n) rather

Assumption 2. We have kÎ¸k2 â‰¤ S, and maxxâˆˆD kxk2 â‰¤ Z,
where S and Z are positive finite constants. The random
variables Î·t are drawn i.i.d. from a zero-mean
sub-Gaussian


2 2
distribution with parameter Ï‘, i.e., E eÎ¾Î·t â‰¤ eÎ¾ Ï‘ /2 for all
Î¾ âˆˆ R.

4

III. S TRATEGIES AND R EGRET A NALYSIS

B. Regret Analysis
1) Finitely Many Arms:

A. Greedy Projection Strategy
The decision set D may contain finitely or infinitely many
arms. In this section, we present a strategy with two settings,
the first is applicable for the finite-arm case, and the other is
applicable for the infinite-arm case. In the following, we use
Î¸Ì‚t to denote the L2 regularized least square estimate of Î¸ with
parameter Î» > 0, after the decision making at time step t:
Î¸Ì‚t = Vtâˆ’1

t
X

ri Xi ,

(7)

Theorem 1. Suppose Assumptions 1 and 2 hold, D contains
finitely many arms,


24dZ 2 Ï‘2
Î± > max
, 10
(10)
minx6=XUâˆ— âˆ†2x Î´Dk
where âˆ†x = hPU (XUâˆ— (Î¸)) âˆ’ PU (x), PU (Î¸)i, and Î´Dk is a
constant that depends on Dk . Then GENTRY has cumulative
projection regret of order O(|D| log n), where n is the number
of time steps and |D| is the number of arms.

i=1

Pt
where Vt = Î»I + i=1 Xi Xi| . We propose a Greedy projEctioN sTRategY (GENTRY) that accounts for the corruption in
the observed return to achieve sublinear cumulative projection
regret. GENTRY is based on the t -greedy policy [2], which
is a simple and well-known algorithm for the standard finite
multi-armed bandit problem. At each time step t, the t -greedy
policy chooses with probability 1 âˆ’ t the arm with the highest
empirical average reward, and with probability t a random arm.
Since in our problem, we focus our attention on the projection
reward, GENTRY chooses with probability 1 âˆ’ t the arm with
the highest empirical average projection reward defined as:
D
E
rÌ„tâˆ’1 (x) = PU (x), PU (Î¸Ì‚tâˆ’1 ) ,
(8)
for each arm x âˆˆ D. Another difference is, at each time step
t, GENTRY chooses with probability t a random arm from
Dk rather than D. We define two different settings of t as
follows:




Î±k
Î±k
and it = min 1, 1/3 ,
ft = min 1,
(9)
t
t
to handle the finite- and infinite-arm cases, respectively. The
hyperparameter Î± > 0 is a fixed positive constant. The
GENTRY strategy is summarized in Algorithm 1.
Algorithm 1 GENTRY
Inputs: Set t to ft or it according to whether the problem
is finite-arm or infinite-arm. Set hyperparameters Î± > 0
and Î» > 0.
1: Set t = 1.
2: loop
3:
Set t using (9).
4:
With probability 1âˆ’t , choose arg maxxâˆˆD rÌ„tâˆ’1 (x) and
with probability t choose a random arm from Dk .
5:
Update rÌ„(xt ) using (8).
6:
Set t = t + 1.
7: end loop

Theorem 1 shows that if we choose Î± to be sufficiently
large, GENTRY achieves order optimal regret.
Proof: For any arm x âˆˆ D, let the random variable Nt (x)
denote the total number of time steps within P
the first t time
n
steps that the arm x was chosen, i.e., Nn (x) = t=1 1{Xt =x} .
Since

n 
X
E [RU (n)] = E
hPU (XUâˆ— (Î¸)), PU (Î¸)i âˆ’ hPU (Xt ), PU (Î¸)i
t=1

=

X

E [Nn (x)] âˆ†x

xâˆˆD

=

X
xâˆˆD

âˆ†x

n
X

P (Xt = x) ,

(11)

t=1

where âˆ†x = hPU (XUâˆ— (Î¸)) âˆ’ PU (x), PU (Î¸)i is the projection
regret of arm x, it is sufficient to show that the probability
P (Xt = x) = O(tâˆ’1 ) for all suboptimal x âˆˆ D\{XUâˆ— }. We
have
t
P (Xt = x) â‰¤ + (1 âˆ’ t )P (rÌ„tâˆ’1 (x) â‰¥ rÌ„tâˆ’1 (XUâˆ— (Î¸))) ,
k
(12)
and
P (rÌ„tâˆ’1 (x) â‰¥ rÌ„tâˆ’1 (XUâˆ— (Î¸)))
âˆ†x
= P (rÌ„tâˆ’1 (x) â‰¥ rÌ„tâˆ’1 (XUâˆ— (Î¸)), rÌ„tâˆ’1 (x) â‰¥ hPU (x), PU (Î¸)i +
)
2


âˆ†x
âˆ—
+ P rÌ„tâˆ’1 (x) â‰¥ rÌ„tâˆ’1 (XU (Î¸)), rÌ„tâˆ’1 (x) < hPU (x), PU (Î¸)i +
2


âˆ†x
â‰¤ P rÌ„tâˆ’1 (x) â‰¥ hPU (x), PU (Î¸)i +
2


âˆ†x
+ P rÌ„tâˆ’1 (XUâˆ— (Î¸)) â‰¤ hPU (XUâˆ— (Î¸)), PU (Î¸)i âˆ’
.
2
(13)

We next bound the first term on the right-hand side of (13).
The analysis for the second term is similar.
From the proof of Theorem 2 in [14], we have that with
probability at least 1 âˆ’ Î´, for all t â‰¥ 0 and x âˆˆ Rd ,
s
!


D
E
1+t
1/2
x, Î¸Ì‚t âˆ’ Î¸ â‰¤ kxkVâˆ’1 Ï‘ d log
+Î» S .
In the following theorems, we give a sufficient condition
t
Î´
on the hyperparameter Î± for the strategy to achieve O(log n)
(14)
for the finite-arm case. We also show that our infinite-arm
p
strategy achieves O(n2/3 (log n)1/2 ) regret for the infinite-arm
Let Î²t
=
Ï‘ 3d log(1 + t) + Î»1/2 S. Since
case. The empirical impact of Î± is further studied in Section IV. hPU (x), PU (Î¸)i = hPU (x), Î¸i for the orthogonal projection
Parameter Î» can be set as a moderate value like Z 2 .
operator PU , by setting Î´ = t12 , we have

5

Because PU (x) âˆˆ span(D), using (17), we obtain
s
Ï‘


d log

1+t
Î´



+ Î»1/2 S = Ï‘

p

d log (t2 (t + 1)) + Î»1/2 S

< Î²t ,
so that for all x âˆˆ D\{XUâˆ— }, it follows from (14) that


P rÌ„t (x) âˆ’ hPU (x), PU (Î¸)i > Î²t kPU (x)kVâˆ’1
t
D
E
â‰¤ P PU (x), Î¸Ì‚t âˆ’ hPU (x), Î¸i >


 p
1/2
2
Ï‘ d log (t (t + 1)) + Î» S kPU (x)kVâˆ’1 .




âˆ†x
â‰¤ Î²t kPU (x)kVâˆ’1
t
2
!

2
kPU (x)k22
âˆ†x
â‰¤
â‰¤P
2Î²t
Î»D
t,min


2
4Î²t kPU (x)k22
= P Î»D
â‰¤
t,min
âˆ†2x


4Î²t2 Z 2
k
â‰¤ P Î»D
â‰¤
t,min
minx6=XUâˆ— âˆ†2x

P

(18)

t

Consider the function
1
(15)
â‰¤ 2.
X
t
f : {y : y âˆˆ span(D), kyk22 = 1} 7â†’ y| (
xx| )y. (19)
We then have


xâˆˆDk
âˆ†x
P rÌ„t (x) â‰¥ hPU (x), PU (Î¸)i +
2

We have f (y) > 0 since Dk contains k linearly independent
âˆ†x âˆ†x
> Î²t kPU (x)kVâˆ’1 arms. We also have Î´Dk := inf f (y) > 0 since f (y) is a
= P rÌ„t (x) â‰¥ hPU (x), PU (Î¸)i +
t
2
2
continuous function on a compact set. Define the event


âˆ†x


Â·P
> Î²t kPU (x)kVâˆ’1
t
4Î²t2 Z 2
2


, for all x âˆˆ Dk .
Î´Dk Nt (x) >
âˆ†x âˆ†x
minx6=XUâˆ— âˆ†2x
â‰¤ Î²t kPU (x)kVâˆ’1
+ P rÌ„t (x) â‰¥ hPU (x), PU (Î¸)i +
t
2
2


Under this event, using the last equality in (17), we obtain
âˆ†x
4Î²t2 Z 2
Â·P
â‰¤ Î²t kPU (x)kVâˆ’1
k
Î»D
t
2
t,min > minx6=X âˆ— âˆ†2x . Therefore, we have
U


âˆ†x
1
â‰¤ Î²t kPU (x)kVâˆ’1 .
(16)
â‰¤ 2 +P


t
t
2
4Î²t2 Z 2
k
P Î»D
â‰¤
t,min
minx6=XUâˆ— âˆ†2x
We next show that when the hyperparameter Î± is sufficiently


X
large,
4Î²t2 Z 2


â‰¤
P Nt (x) â‰¤
âˆ†x
minx6=XUâˆ— âˆ†2x Î´Dk
xâˆˆDk
P
â‰¤ Î²t kPU (x)kVâˆ’1 = O(tâˆ’1 ).

t
2
X 
4Î²t2 Z 2
â‰¤
NÌƒ
(x)
â‰¤
,
(20)
P
t
Since
minx6=XUâˆ— âˆ†2x Î´Dk
X
xâˆˆDk
Vt = Î»I +
Nt (x)xx|
xâˆˆD

= Î»Pspan(D)âŠ¥ (I) + Î»Pspan(D) (I) +

X

Nt (x)xx| ,

xâˆˆD

it can be shown by induction that the eigenvectors of Vt can
be divided into two groups, one in which all the eigenvectors
are in span(D), and another in which all the eigenvectors
are in span(D)âŠ¥ . Let Î»D
t,min be the smallest eigenvalue of Vt
Dk
in
span(D)
and
Î»
t,min be the smallest eigenvalue of (Î»I +
P
|
N
(x)xx
)
in span(D). If we define
t
xâˆˆDk
y1 =

arg min

hy, Vt yi,

yâˆˆspan(D),kyk22 =1

|
Î»D
t,min = y1 Vt y1

X

Nt (x)xx| )y1

xâˆˆDk

â‰¥
=

min

yâˆˆspan(D),kyk22 =1
k
Î»D
t,min

y| (Î»I +

t
1 X
P NÌƒt (x) â‰¤
i
2k i=1

!

For t â‰¥ t0 = dÎ±ke, t = ft =

t
1 X
â‰¤ exp âˆ’
i
10k i=1
Î±k
t ,

!
(21)

we then obtain

0

we then have
â‰¥ y1| (Î»I +

where NÌƒt (x) â‰¤ Nt (x) is the number of times that arm x âˆˆ Dk
was chosen randomly during the first t time steps. From the
proof of Theorem 3 in [2] (where Bernsteinâ€™s inequality [38]
was used), we have

X

Nt (x)xx| )y

xâˆˆDk

(17)

t
t
t
1 X
1 X
1 X
i =
i +
i
2k i=1
2k i=1
2k
i=t0 +1
Z t+1
Î±
1
Î±k
â‰¥ +
dx
2
2k t0 +1 t
Î± Î±
t+1
â‰¥ + log
2
2
Î±k + 2
Î±
(t + 1)e
= log
,
2
Î±k + 2

(22)

6

4Î² 2 Z 2

t
where e is Eulerâ€™s number. If Î±2 log (t+1)e
Î±k+2 â‰¥ minx6=X âˆ— âˆ†2x Î´Dk ,
U
from (20), we have


4Î²t2 Z 2
k
P Î»D
â‰¤
t,min
minx6=XUâˆ— âˆ†2x


X
4Î²t2 Z 2
â‰¤
P NÌƒt (x) â‰¤
minx6=XUâˆ— âˆ†2x Î´Dk
xâˆˆDk


X
Î±
(t + 1)e
â‰¤
P NÌƒt (x) â‰¤ log
2
Î±k + 2
xâˆˆDk
!
t
X
1 X
â‰¤
P NÌƒt (x) â‰¤
i
2k i=1
xâˆˆDk
!
t
X
1 X
i
â‰¤
exp âˆ’
10k i=1
xâˆˆDk
Î±

Î±k + 2 10
,
(23)
â‰¤k
(t + 1)e

or 2) when Xt is chosen as arg maxxâˆˆD rÌ„tâˆ’1 (x). We denote
these two event as Et and Etc respectively.
E [RU (n)]
" n
#
X
âˆ—
=E
hPU (XU ) âˆ’ PU (Xt ), PU (Î¸)i
t=1

=

n
X

t E [ hPU (XUâˆ— ) âˆ’ PU (Xt ), PU (Î¸)i | Et ]

t=1

+ (1 âˆ’ t )E [ hPU (XUâˆ— ) âˆ’ PU (Xt ), PU (Î¸)i | Etc ]
n
X
â‰¤
(2ZSt + E [ hPU (XUâˆ— ) âˆ’ PU (Xt ), PU (Î¸)i | Etc ]) ,
t=1

(26)

where the inequality follows from Assumption 1, kPU (x)k2 â‰¤
kxk2 for all x, and the Cauchy-Schwarz inequality.
We next derive an upper bound for the second term
E [ hPU (XUâˆ— ) âˆ’ PU (Xt ), PU (Î¸)i | Etc ] in the sum on the rightwhere the third inequality follows from (22), the penultimate hand side of (26). c
Under the event Et , we have
inequality follows fromp(21), and the last inequality from (22).
1/2
Recall that Î²t = Ï‘ 3d log(t + 1) + Î» S. Therefore, if
hPU (XUâˆ— ) âˆ’ PU (Xt ), PU (Î¸)i
D
E
24dZ 2 Ï‘2
, 10}, when t is sufficiently large, the
Î± > max{ min
2
x6=X âˆ— âˆ†x Î´Dk
= PU (XUâˆ— ) âˆ’ PU (Xt ), PU (Î¸Ì‚tâˆ’1 )
U
2 2
4Î²t Z
D
E
condition Î±2 log (t+1)e
Î±k+2 â‰¥ minx6=X âˆ— âˆ†2x Î´Dk is satisfied. We have
U
+ PU (XUâˆ— ) âˆ’ PU (Xt ), PU (Î¸) âˆ’ PU (Î¸Ì‚tâˆ’1 )


D
E
âˆ†x
â‰¤ PU (XUâˆ— ) âˆ’ PU (Xt ), PU (Î¸) âˆ’ PU (Î¸Ì‚tâˆ’1 ) ,
(27)
P rÌ„t (x) â‰¥ hPU (x), PU (Î¸)i +
2


1
âˆ†x
where the inequality follows from Xt = arg maxxâˆˆD rÌ„tâˆ’1 (x).
â‰¤ 2 +P
â‰¤ Î²t kPU (x)kVâˆ’1
t
We use the same Î²t defined in the proof of Theorem 1. For
t
2


2 2
any
t â‰¥ 2, let the event
1
4Î²t Z
k
â‰¤ 2 + P Î»D
n
o
t,min â‰¤
t
minx6=XUâˆ— âˆ†2x
âˆ—
âˆ—
âˆ’1 Î²tâˆ’1
F
=
hP
(X
âˆ’
X
),
P
(Î¸)i
â‰¥
kP
(X
âˆ’
X
)k
.
t
U
t
U
U
t
U
U
V
Î±


tâˆ’1
1
Î±k + 2 10
â‰¤ 2 +k
From (14) and (27), since hPU (x), PU (Î¸)i = hPU (x), Î¸i for
t
(t + 1)e
the orthogonal projection operator PU , we have
âˆ’1
= o(t ),
(24)
P (Ft+1 )
D

where the first inequality follows from (16), the second
E
âˆ—
âˆ—
inequality from (18) and the last inequality from (23).
â‰¤ P PU (XU âˆ’ Xt+1 ), Î¸ âˆ’ Î¸Ì‚t â‰¥ kPU (XU âˆ’ Xt+1 )kVâˆ’1 Î²t
t
A
similar
argument
shows
that
D
E
âˆ†
=
o(tâˆ’1 ). Then, â‰¤ P P (X âˆ— âˆ’ X ), Î¸ âˆ’ Î¸Ì‚
P rÌ„t (x) â‰¤ hPU (XUâˆ— ), PU (Î¸)i âˆ’ 2x
U
t+1
t
U
using (12) and (13), we have

 s


Î±
1+t
âˆ’1
âˆ’1
1/2
âˆ—
P (Xt = x) â‰¤ + o(t ) = O(t ),
(25)
âˆ’1
d
log
â‰¥
kP
(X
âˆ’
X
)
k
Ï‘
+
Î»
S
U
t+1
U
Vt
t
t2
From (11), we conclude that
1
â‰¤ 2,
(28)
t
E [RU (n)] = O(|D| log n).
where the final inequality follows from the concentration
The proof of Theorem 1 is now complete.
inequality (14) with Î´ = t12 . We then have
2) Infinitely Many Arms:


c
E hPU (XUâˆ— ) âˆ’ PU (Xt+1 ), PU (Î¸)i Et+1


Theorem 2. Suppose Assumptions 1 and 2 hold, and
c
= E hPU (XUâˆ— âˆ’ Xt+1 ), PU (Î¸)i Et+1
, Ft+1 P (Ft+1 )
D is a compact set containing infinitely many arms.
h
i
c
c
+ E hPU (XUâˆ— âˆ’ Xt+1 ), PU (Î¸)i1Ft+1
Et+1
Then,
 GENTRY
n âˆš ohas cumulative projection regret of order
h
i
O max k, d n2/3 (log n)1/2 , where n is the number of
2ZS
c
â‰¤ 2 + E kPU (XUâˆ— âˆ’ Xt+1 )kVâˆ’1 Î²t Et+1
(29)
time steps.
t
t
"
#
s
2ZS
1
Proof: The contribution to the projection regret at time
c
â‰¤ 2 + E 2ZÎ²t
Et+1
(30)
D
t
Î»t,min
step t comes from two cases: 1) when Xt is chosen randomly,

7

"
#
s
2ZS
1
â‰¤ 2 + E 2ZÎ²t
t
Î»D
t,min
v "
#
u
u
2ZS
1
â‰¤ 2 + 2ZÎ²t tE D
t
Î»t,min
v "
#
u
u
2ZS
1
t
â‰¤ 2 + 2ZÎ²t E Dk ,
t
Î»t,min

(31)

(32)

where
â€¢
â€¢

(29) follows from (28);
(30) follows because kPU (XUâˆ— âˆ’ Xt )k2Vâˆ’1 â‰¤ kPU (XUâˆ— âˆ’
t

Xt )k22 /Î»D
t,min ;
â€¢ (31) follows from Jensenâ€™s inequality; and
â€¢ (32) follows from (17).


1
We next show that E Dk
= O(tâˆ’2/3 ). We first show
Î»t,min


2/3
2/3
k
P Î»D
â‰¤ O(eâˆ’t ). Similar to what we have done
t,min < t
in (20), define the following event:
n
o
Î±
Nt (x) > t2/3 , for all x âˆˆ Dk .
2
Under this event, from the last equality in (17), we get
Î± 2/3
Î´Dk . Therefore, we have
2t

Î± 2/3 
k
P Î»D
â‰¤
t Î´Dk
t,min
2

X 
Î±
â‰¤
P Nt (x) â‰¤ t2/3
2
xâˆˆDk

X 
Î±
â‰¤
P NÌƒt (x) â‰¤ t2/3 ,
2

k
Î»D
t,min

>

(33)

where NÌƒt (x) is the number of times that arm x âˆˆ Dk was
chosen randomly during the first t time steps. Recall from (21)
that
!

t
1 X
â‰¤ exp âˆ’
i
10k i=1



For t â‰¥ t0 = (Î±k)3 , t = it =
0

Î±k
t1/3

where the penultimate inequality follows from (17).
From (32) and (37), when t is sufficiently large, we have


c
E hPU (XUâˆ— ) âˆ’ PU (Xt+1 ), PU (Î¸)i Et+1
v "
#
u
u
2ZS
1
t
â‰¤ 2 + 2ZÎ²t E Dk
t
Î»t,min
s
3 âˆ’1
2ZS
(38)
â‰¤ 2 + 2ZÎ²t
t 3
t
Î±Î´Dk
Finally, from (26), we have

xâˆˆDk

t
1 X
P NÌƒt (x) â‰¤
i
2k i=1

when t is sufficiently large. It follows from (33) to (35) that

Î± 2/3 
k
P Î»D
â‰¤
t Î´Dk
t,min

X 2
Î±
â‰¤
P NÌƒt (x) â‰¤ t2/3 ,
2
xâˆˆDk
!
t
X
1 X
â‰¤
P NÌƒt (x) â‰¤
i ,
2k i=1
xâˆˆDk

 Î±
(36)
â‰¤ k exp âˆ’ t2/3
10
Now, we can conclude that when t is sufficiently large,
"
#
1
E Dk
Î»t,min
"
#

Î± 2/3
1
Î± 2/3 
Dk
k
Î»t,min > t Î´Dk P Î»D
=E
t Î´Dk
t,min >
Dk
2
2
Î»t,min
#
"


Î± 2/3
Î± 2
1
Dk
k
Î»t,min â‰¤ t Î´Dk P Î»D
t 3 Î´Dk
+E
t,min â‰¤
Dk
2
2
Î»t,min
 Î±

 k
2tâˆ’2/3  Dk
Î±
â‰¤
P Î»t,min > t2/3 Î´Dk + exp âˆ’ t2/3
Î±Î´Dk
2
Î»
10
âˆ’2/3
3t
â‰¤
,
(37)
Î±Î´Dk

E [RU (n)]
n
X
â‰¤
(2ZSt + E [ hPU (XUâˆ— ) âˆ’ PU (Xt ), PU (Î¸)i | Etc ])
t=1

s
!
2ZSÎ±k
3 âˆ’1
3
t
+ C1
+ 2ZÎ²t
â‰¤
Î±Î´Dk
t1/3
t=1
s
!
Z n
C2 k
d log x âˆ’ 1
â‰¤
+ 6ZÏ‘
x 3 dx + C3
Î±Î´Dk
x1/3
1

n âˆš o

= O max k, d n2/3 (log n)1/2 ,
n
X

!
(34)

and we obtain

t
t
t
1 X
1 X
1 X
i =
i +
i
2k i=1
2k i=1
2k
i=t0 +1
Z t+1
1 0
1
Î±k
â‰¥
t +
dx
2k
2k t0 +1 t1/3
Î±3 k 2
3Î± 2/3 3Î± 3 3
â‰¥
+
t âˆ’
(Î± k + 2)2/3 .
2
4
4
Î±
â‰¥ t2/3 ,
(35)
2

(39)

where C1 , C2 and C3 are constants. The proof of Theorem 2
is now complete.
C. Additional Smoothness Assumption
In this subsection, we show that if an additional smoothness
assumption similar to that in [15] is made, we can achieve better
cumulative projection regret order than O(n2/3 (log n)1/2 ) for
the infinite-arm case.

8

Assumption 3. There exists J âˆˆ R+ such that for all Î¸1 ,
Î¸2 âˆˆ Rd ,
kPU (XUâˆ— (Î¸1 )

âˆ’

XUâˆ— (Î¸2 ))k2

PU (Î¸1 )
PU (Î¸1 )
.
â‰¤J
âˆ’
kPU (Î¸1 )k kPU (Î¸1 )k 2
(40)

This is to say the projection of the decision set D and all
Î¸ âˆˆ Rd onto the subspace U satisfies the SBAR(J) condition
[15], [39]. For example, if the decision set D is a ball or
an elliposid, it satisfies the condition
 (40).
h In thei proof of
1/2
k
Theorem 2, we bound (27) with O Î²t (E 1/Î»D
] =
t,min )
âˆ’1/3

O(t
log t). With the additional Assumption 3, we can
improve this bound, and further get a tighter upper bound
for the cumulative projection regret in the following result.

"
2

â‰¤Î» S E
2

Proof. The inequalities (26) and (27) from the proof of
Theorem 2 still hold. We have
E [hPU (XUâˆ— ) âˆ’ PU (Xt ), PU (Î¸)i | Etc ]
hD
E
i
â‰¤ E PU (XUâˆ— ) âˆ’ PU (Xt ), PU (Î¸) âˆ’ PU (Î¸Ì‚tâˆ’1 ) Etc
h
i
â‰¤ E kPU (XUâˆ— ) âˆ’ PU (Xt )k2 PU (Î¸) âˆ’ PU (Î¸Ì‚tâˆ’1 )
Etc
2
"
PU (Î¸)
PU (Î¸Ì‚tâˆ’1 )
PU (Î¸) âˆ’ PU (Î¸Ì‚tâˆ’1 )
â‰¤E J
âˆ’
kPU (Î¸)k kPU (Î¸Ì‚tâˆ’1 )k
2
ï£®
ï£¹
2
2J PU (Î¸) âˆ’ PU (Î¸Ì‚tâˆ’1 )
ï£¯
ï£º
2
â‰¤ Eï£°
Etc ï£»
kPU (Î¸)k
ï£®

2J PU (Î¸ âˆ’ Î¸Ì‚tâˆ’1 )

ï£¯
= Eï£°

kPU (Î¸)k

2

2ï£¹

(41)

where the first inequality follows from (27), the second inequality follows from Cauchy-Schwarz inequality, the third inequality
follows from Assumption 3, the penultimate inequality follows
from Lemma 3.5 in [15], and the last equality follows from
independence.


2
We next bound E PU (Î¸ âˆ’ Î¸Ì‚t )
. From (7), we have,
2

Î¸Ì‚t =

Î»I +

t
X

!âˆ’1
Xi Xi|

i=1

=

Î»I +

t
X

2

ri Xi

â‰¤Î» S E
"
â‰¤ Î»2 S 2 E

Xi Xi|

t
X
(hXi , Î¸i + Î·i )Xi ,
i=1



2

â‰¤ E ï£° PU (Î»Vtâˆ’1 Î¸)

2
2

+

t
X
i=1

1

2

PU (Vtâˆ’1 Xi )Î·i

ï£¹
ï£»

2

+ Î¶E

1

2

â‰¤Î» S E

#

1

2

â‰¤Î» S E

" i=1
t
X

#
PU (Vtâˆ’1 Xi )| PU (Vtâˆ’1 Xi )
#
tr


PU (Vtâˆ’1 )Vtâˆ’1 (Xi Xi| )

"
+ Î¶E tr

PU (Vtâˆ’1 )Vtâˆ’1 (

t
X

!#
Xi Xi|

i=1

#


+ Î¶E dÎ»max (PU (Vtâˆ’1 ))
#

"
+ dÎ¶E

2
(Î»D
t,min )

"

t
X

i=1

2
(Î»D
t,min )

"
2

#

2
(Î»D
t,min )

#

k
2
(Î»D
t,min )

#

Î»D
t,min
"

+ dÎ¶E

1
1
k
Î»D
t,min

#
,

(42)

where the second inequality has used the
 condition that Î·t
are i.i.d. and the standard result that E Î·t2 â‰¤ Î¶ where Î¶ is
a constant depending on Ï‘ since Î·t has the zero-mean subGaussian distribution with parameter Ï‘.
Using a similar argument as that in the proof of Theorem 2,
we can show
"
#
"
#
1
1
âˆ’1/2
E Dk
= O(t
), and E
= O(tâˆ’1 ).
k
2
Î»t,min
(Î»D
)
t,min
(43)
# proof steps are skipped here for brevity. Finally, using
The
Etc(26), (26) and (42), we have
E [RU (n)]
n
X
â‰¤
2ZSst + E [ hPU (XUâˆ— ) âˆ’ PU (Xt ), PU (Î¸)i | Etc ]
t=1
n
X

dO(tâˆ’1/2 )

t=1
âˆš
= O(d n).

(44)

The proof of Theorem 3 is now complete.
IV. S IMULATIONS AND P ERFORMANCE E VALUATION
In this section, we verify the efficiency of our strategy by
performing simulations on synthetic data and a wine quality
dataset [35]. We compare against the following strategies:
1) the Uncertainty Ellipsoid (UE) policy [15], which chooses
p
arg max rÌ„tâˆ’1 (x) + Î± log t min{d log t, |D|}kxkVâˆ’1
xâˆˆD

i=1

!âˆ’1

i=1

which yields

E PU (Î¸ âˆ’ Î¸Ì‚t )
ï£®

t
X

1

2

"
+ Î¶E

2
(Î»D
t,min )

"

â‰¤

2ï£º
ï£»

1

2

â‰¤Î» S E
2

#

2
(Î»D
t,min )

"

2

Theorem 3. Suppose Assumptions 1 to 3 hold and D is a
compact set withn infinitely
o many arms. Then, GENTRY with
âˆš
t = st := min 1, Î±k
has cumulative projection regret of
t
âˆš
order O(d n), where n is the number of time steps.

1

2

tâˆ’1

at each time step t;
2) the CorrUpted GReedy StratEgy (CURSE)
D by replacE
ing arg maxxâˆˆD rÌ„tâˆ’1 (x) with arg maxxâˆˆD x, Î¸Ì‚tâˆ’1 in
GENTRY, i.e., the corruption in the observed return is
not accounted for; and
3) the caRelEss GReEdy sTrategy (REGRET) where the Xi
in GENTRY are all replaced by PU (Xi ). To avoid any
doubts, the full REGRET is shown in Algorithm 2.
In each simulation, we perform 2000 trials, each with 104
time steps. To compare the projection regret performance

9

Inputs: Set t to
or it according to the number of arms;
set hyper parameter Î± > 0 and Î» > 0.
1: loop
2:
Update t .
3:
With probability 1 âˆ’ t , choose
D
E
arg max PU (x), PU (Î¸Ìƒtâˆ’1 ) ,
xâˆˆD

where
Î¸Ìƒt =

t
X

!âˆ’1
|

PU (Xi )PU (Xi ) + Î»I

i=1

t
X

ri PU (Xi ),

i=1

and withDprobability t choose
E a random arm from Dk .
4:
Update PU (x), PU (Î¸Ìƒtâˆ’1 ) . Set t = t + 1.
5: end loop

of different strategies, we compute the average empirical
cumulative projection regret over all the trials. For convenience,
this average is referred to as the average cumulative projection
regret.
A. Experiments on Synthetic Data
In this section, we compare the performance of different
strategies using synthetic data. We use three settings in the
simulations:
(a) For the finite-arm case, in each trial, we generate the
decision set D âŠ† Rd containing K arms, in which each
arm is associated with a d-dimension feature vector. Each
dimension of the feature vector is drawn i.i.d. from the
uniform distribution U(âˆ’1, 1). Each dimension of the
ground-truth parameter Î¸ is also drawn from U(âˆ’1, 1).
We next generate the orthogonal projection matrix PU =
A(A| A)âˆ’1 A| , where A is a d Ã— u (u < d) matrix
whose elements are generated randomly from U(âˆ’1, 1).
The noise Î·t at each time step is drawn i.i.d. from the
Gaussian distribution N (0, Ï‘2 ) with variance Ï‘2 . The
decision set D, parameter Î¸ and projection matrix PU are
fixed in each trial.
(b) We use the same setting as in setting (a) except that the
projection matrix PU in this setting is a diagonal matrix
whose (i, i) entry is 1 for i = 1, Â· Â· Â· , u and 0 otherwise.
This means that the last d âˆ’ u dimensions of Î¸ are the
protected features.
(c) For the infinite-arm case, in each trial, the decision set
D is limited to a convex set for ease of computation.
Specifically, we use the convex set D:
(
)
d
X
D= x:
x(i) log x(i) â‰¤ 5 and x(i) â‰¥ 0 âˆ€ i ,
i=1

(45)
where x(i) is the i-th entry of x, 0 log 0 := 0, and Î¸, PU
and Î·t are generated the same way as in setting (a).
In the following, GENTRY, CURSE, and REGRET use the
setting  = ft described in Section III when in setting (a) and

(b) for the finite-arm case. Accordingly,  = it is used when
in setting (c) for the infinite-arm case. In all simulations, the
parameter Î» is set to 1.

average cumulative projection regret

ft

GENTRY
CURSE
REGRET
UE

20000
15000
10000
5000
0
0.001 0.01

0.1

1

10

30

100

(a) Using setting (a) with d = 10, K = 45, and u = 5

average cumulative projection regret

Algorithm 2 REGRET

20000
15000

GENTRY
CURSE
REGRET
UE

10000
5000
0
0.001 0.01

0.1

1

10

30

100

(b) Using setting (c) with d = 4, and u = 2
Fig. 1. Regret comparison between different strategies with Ï‘ = 0.5 and
varying Î±.

1) Varying Î±: All the strategies depend on the hyperparameter Î±, with a larger Î± corresponding to more exploration
on average. We do simulations using setting (a) with d = 10,
K = 45, Ï‘ = 0.5, u = 5, and setting (c) with d = 4, Ï‘ = 0.5,
u = 2. Figs. 1a and 1b show how the average cumulative
projective regret at time step 104 changes with different Î± in
each strategy. We observe the following:
â€¢ We note that a moderate Î± is optimal for each strategy.
When Î± is too large, the strategy explores too frequently,
leading to a large projection regret as expected. On the
other hand, a small Î± results in little exploration, and
good arms cannot be found efficiently.
â€¢ In Fig. 1a, GENTRY with Î± = 1 outperforms all the other
benchmark strategies. This is because the other strategies
do not achieve an asymptotically unbiased estimation of
the projection reward. Fig. 1b shows similar results.
In the following simulations, for a fair comparison, we
tune the parameter Î± for each strategy to achieve the best
average asymptotic cumulative projection regret for that strategy.
Specifically, we set Î± = 1 for all the strategies except UE,

3000
2000
1000
0

100
percentage of trials

GENTRY
CURSE
REGRET
UE

4000

0

2000

4000 6000
time steps t

8000

10000

(a) Average cumulative projection regret

80
60
40

GENTRY
CURSE
REGRET
UE

5000
4000
3000
2000
1000
0

100
percentage of trials

average cumulative projection regret

which is given a Î± = 0.1 when using setting (a) and (b). When
using setting (c), we set Î± = 0.01 for all the strategies except
REGRET, which is given a Î± = 0.1.

average cumulative projection regret

10

0

2000

4000 6000
time steps t

8000

10000

(a) Average cumulative projection regret

80
60
40
20
0

20

GENTRY

CURSE

REGRET
policy

UE

(b) Percentage of trials where the best decision is found

0

GENTRY

CURSE

REGRET
policy

UE

Fig. 3. Performance comparison between different strategies using setting (b)
with d = 10, K = 45, Ï‘ = 0.5 and u = 5.

(b) Percentage of trials where the best decision is found

2) Results and Analysis: In the simulations for the finitearm case using settings (a) and (b), we set d = 10, K = 45,
Ï‘ = 0.5 and u = 5. The simulation results are shown in Figs. 2
and 3. We observe the following:
â€¢ The average cumulative projection regrets of different
strategies are shown in Figs. 2a and 3a. We see that
GENTRY has obvious sublinear cumulative projection
regret performance. The other benchmark strategies all
suffer from a linearly increasing cumulative projection
regret. This verifies that if our objective is to maximize
the cumulative projection reward instead of the cumulative
return (including the corruption), GENTRY is more
appropriate.
â€¢ Figs. 2b and 3b show the percentage of trials in which
the optimal arm is chosen for more than 90% of the time
in the last 200 time steps. We see that GENTRY finds
the optimal arm in most trials, and outperforms the other
strategies by a significant margin.
In the simulations for the infinite-arm case using setting (c),
we set d = 4, Ï‘ = 0.5 and u = 2. From Fig. 4, we

also observe that GENTRY has obvious sublinear cumulative
projection regret performance. The other benchmark strategies
have linearly increasing cumulative projection regret. This
verifies the efficiency of GENTRY for the infinite-arm case.

average cumulative projection regret

Fig. 2. Performance comparison between different strategies using setting (a)
with d = 10, K = 45, Ï‘ = 0.5 and u = 5.

GENTRY
CURSE
REGRET
UE

5000
4000
3000
2000
1000
0

0

2000

4000 6000
time steps t

8000

10000

Fig. 4. Average cumulative projection regret using setting (c) with d = 4,
u = 2 and Ï‘ = 0.5.

11

B. Experiments on Wine Quality Dataset

average cumulative projection regret

We next compare the performance of different strategies
using the wine quality dataset, which contains 11-dimension
description vectors of 4898 white wines and their ratings (scores
between 0 and 10). In each trial, we randomly select 200
wines with ratings 4, 5, 6, 7 and 8, as the decision set D,
since among all the wines there are only 5 wines with ratings
larger than 8 and 20 wines with ratings less than 4. Each
dimension of the wine description vector is a physicochemical
characteristic like volatile acidity, chlorides, or density. Due
to privacy and logistic issues [35], only the physicochemical
characteristics and ratings are available (e.g., there is no data
about grape types, wine brand, wine selling price, etc.). We
add one additional feature drawn i.i.d. from U(0, 1) as the
protected feature. The corresponding rating of each wine is
then corrupted by subtracting 4 times this protected feature
value from the original rating. In this experiment, we take
the original rating as the projection reward. Finally, we add a
constant 1 as the constant feature to each description vector.
If we put the protected feature as the final dimension of each
wine, then d = 13, and the projection matrix PU is defined as
a diagonal matrix whose (i, i) entry is 1 for i = 1, Â· Â· Â· , 12, and
0 for i = 13. As there are finitel many arms, we use  = ft in
this experiment. The results are shown in Fig. 5, from which
we observe that GENTRY outperforms all the other strategies.

GENTRY
CURSE
REGRET
UE

4000
3000
2000
1000
0

0

2000

4000 6000
time steps t

8000

10000

Fig. 5. Average cumulative projection regret using the wine quality dataset.

V. C ONCLUSION
We have formulated the orthogonal projection problem in
the linear stochastic bandit model, where the objective is to
maximize the cumulative projection reward over a subspace of
decision attributes based on observed returns that consist of
the projection reward with corruption. Our proposed GENTRY
achieves sublinear projection regret for the finite- and infinitearm cases. Experiments verify the efficiency of our strategy. Our
formulation and strategy are useful in avoiding discrimination
in recommendation systems and in mixed drug treatment trials.
In this paper, we have assumed that the target subspace is
known beforehand when decomposing the reward. However,

in practice, we may not know what is a suitable projection
subspace a priori. It is of interest in future research to develop
methods to learn this subspace in conjunction with the best
arm using possibly additional side-information.
R EFERENCES
[1] H. Robbins, â€œSome aspects of the sequential design of experiments,â€
Bull. Amer. Math. Soc., vol. 58, no. 5, pp. 527â€“535, Jul. 1952.
[2] P. Auer, N. Cesa-Bianchi, and P. Fischer, â€œFiniteâ€“time analysis of the
multiarmed bandit problem,â€ Mach. Learn., vol. 47, no. 2, pp. 235â€“256,
May 2002.
[3] T. L. Lai and H. Robbins, â€œAsymptotically efficient adaptive allocation
rules,â€ Advances Appl. Math., vol. 6, no. 1, pp. 4â€“22, Mar. 1985.
[4] J.-Y. Audibert, R. Munos, and C. SzepesvÃ¡ri, â€œExplorationâ€“exploitation
tradeoff using variance estimates in multi-armed bandits,â€ Theoretical
Comput. Sci., vol. 410, no. 19, pp. 1876â€“1902, Oct. 2009.
[5] R. Agrawal, â€œSample mean based index policies by O(log n) regret for
the multi-armed bandit problem,â€ Advances Appl. Probability, vol. 27,
no. 4, pp. 1054â€“1078, Dec. 1995.
[6] A. N. Burnetas and M. N. Katehakis, â€œOptimal adaptive policies for
sequential allocation problems,â€ Advances Appl. Math., vol. 17, no. 2,
pp. 122â€“142, Jun. 1996.
[7] Q. Kang and W. P. Tay, â€œTask recommendation in crowdsourcing based on
learning preferences and reliabilities,â€ arXiv preprint arXiv:1807.10444,
2018.
[8] Y. Liu and M. Liu, â€œAn online learning approach to improving the
quality of crowd-sourcing,â€ IEEE/ACM Trans. Netw., vol. 25, no. 4, pp.
2166â€“2179, Aug 2017.
[9] S. Klos nÃƒlâ€™e MÃƒijller, C. Tekin, M. van der Schaar, and A. Klein,
â€œContext-aware hierarchical online learning for performance maximization
in mobile crowdsourcing,â€ IEEE/ACM Trans. Netw., vol. 26, no. 3, pp.
1334â€“1347, June 2018.
[10] Q. Kang and W. P. Tay, â€œSequential multi-class labeling in crowdsourcing,â€
IEEE Trans. Knowl. Data Eng., vol. 31, no. 11, pp. 2190 â€“ 2199, Nov.
2019.
[11] S. Pandey, D. Chakrabarti, and D. Agarwal, â€œMulti-armed bandit problems
with dependent arms,â€ in Proc. Int. Conf. Mach. Learn., Oregon, USA,
Jun. 2007, pp. 721â€“728.
[12] E. L. Presman, Sonin, I. N, and Sonin, Sequential Control with Incomplete
Information. London, UK: Academic Press, 1990.
[13] A. Goldenshluger, A. Zeevi et al., â€œWoodroofeÃ¢AÌ†ZÌs one-armed bandit
problem revisited,â€ Ann. Appl. Probability, vol. 19, no. 4, pp. 1603â€“1633,
Nov. 2009.
[14] Y. Abbasi-Yadkori, D. PÃ¡l, and C. SzepesvÃ¡ri, â€œImproved algorithms for
linear stochastic bandits,â€ in Proc. Advances Neural Inf. Process. Syst.,
Granada, Spain, Dec. 2011, pp. 2312â€“2320.
[15] P. Rusmevichientong and J. N. Tsitsiklis, â€œLinearly parameterized bandits,â€
Math. Oper. Res., vol. 35, no. 2, pp. 395â€“411, May 2010.
[16] Y. Gai, B. Krishnamachari, and R. Jain, â€œCombinatorial network
optimization with unknown variables: Multi-armed bandits with linear
rewards and individual observations,â€ IEEE/ACM Trans. Netw., vol. 20,
no. 5, pp. 1466â€“1478, Oct 2012.
[17] S. Agrawal and N. Goyal, â€œThompson sampling for contextual bandits
with linear payoffs,â€ in Proc. Int. Conf. Mach. Learn., Georgia, USA,
Jun. 2013, pp. 127â€“135.
[18] L. Li, W. Chu, J. Langford, and R. E. Schapire, â€œA contextual-bandit
approach to personalized news article recommendation,â€ in Proc. Int.
Conf. World Wide Web, New York, USA, Apr. 2010, pp. 661â€“670.
[19] N. Abe, A. W. Biermann, and P. M. Long, â€œReinforcement learning with
immediate rewards and linear hypotheses,â€ Algorithmica, vol. 37, no. 4,
pp. 263â€“293, Dec. 2003.
[20] Y. Yue and C. Guestrin, â€œLinear submodular bandits and their application
to diversified retrieval,â€ in Proc. Advances Neural Inf. Process. Syst.,
Granada, Spain, Dec. 2011, pp. 2483â€“2491.
[21] S. S. Villar, J. Bowden, and J. Wason, â€œMulti-armed bandit models for
the optimal design of clinical trials: Benefits and challenges,â€ Statist.
Sci., vol. 30, no. 2, p. 199, May 2015.
[22] V. Dani, T. P. Hayes, and S. M. Kakade, â€œStochastic linear optimization
under bandit feedback,â€ in Proc. Annu. Conf. Learn. Theory, Helsinki,
Finland, Jul. 2008, pp. 355â€“366.
[23] V. Kuleshov and D. Precup, â€œAlgorithms for multi-armed bandit problems,â€
arXiv preprint arXiv:1402.6028, 2014.
[24] Why netflix features black actors in promos to black users. Accessed:
Jun. 25, 2019. [Online]. Available: https://www.wired.com/story/
why-netflix-features-black-actors-promos-to-black-users

12

[25] R. D. Toto, â€œTreatment of hypertension in chronic kidney disease,â€
Seminars Nephrology, vol. 25, no. 6, pp. 435â€“439, Nov. 2005.
[26] P. Auer, â€œUsing confidence bounds for exploitation-exploration trade-offs,â€
J. Mach. Learn. Res., vol. 3, no. 11, pp. 397â€“422, Nov. 2002.
[27] H. Wang, Q. Wu, and H. Wang, â€œLearning hidden features for contextual
bandits,â€ in Proc. Int. Conf. Inform. Knowl. Manag., Indianapolis, USA,
Oct. 2016, pp. 1633â€“1642.
[28] A. Uschmajew, â€œLocal convergence of the alternating least squares
algorithm for canonical tensor approximation,â€ J. Matrix Anal. Appl.,
vol. 33, no. 2, pp. 639â€“652, Jun. 2012.
[29] J. Friedman, T. Hastie, and R. Tibshirani, â€œRegularization paths for
generalized linear models via coordinate descent,â€ J. Statist. Softw.,
vol. 33, no. 1, pp. 1â€“22, Aug. 2010.
[30] M. M. Drugan, A. NowÃ©, and B. Manderick, â€œPareto upper confidence
bounds algorithms: an empirical study,â€ in Proc. IEEE Symp. Adaptive
Dynamic Programming Reinforcement Learn., Orlando, USA, Dec. 2014,
pp. 1â€“8.
[31] M. M. Drugan and A. Nowe, â€œDesigning multi-objective multi-armed
bandits algorithms: A study,â€ in Proc. Int. Joint Conf. Neural Netw.,
Dallas, USA, Aug 2013, pp. 1â€“8.
[32] S. Q. Yahyaa, M. M. Drugan, and B. Manderick, â€œAnnealing-pareto multiobjective multi-armed bandit algorithm,â€ in Proc. IEEE Symp. Adaptive
Dynamic Programming Reinforcement Learn., Orlando, USA, Dec. 2014,
pp. 1â€“8.
[33] â€”â€”, â€œThe scalarized multi-objective multi-armed bandit problem: An
empirical study of its exploration vs. exploitation tradeoff,â€ in Proc. Int.
Joint Conf. Neural Netw., Beijing, China, Jul. 2014, pp. 2290â€“2297.
[34] E. Zitzler, L. Thiele, M. Laumanns, C. M. Fonseca, and V. Da Fonseca Grunert, â€œPerformance assessment of multiobjective optimizers: An
analysis and review,â€ IEEE Trans. Evol. Comput., vol. 139, no. 2, pp.
117â€“132, May 2002.
[35] P. Cortez, A. Cerdeira, F. Almeida, T. Matos, and J. Reis, â€œModeling wine
preferences by data mining from physicochemical properties,â€ Decision
Support Syst., vol. 47, no. 4, pp. 547â€“553, May 2009.
[36] S. J. Leon, Linear Algebra with Applications. Upper Saddle River, NJ:
Pearson, 2010.
[37] T. Calders, A. Karim, F. Kamiran, W. Ali, and X. Zhang, â€œControlling
attribute effect in linear regression,â€ in Proc. Int. Conf. Data Mining,
Dallas, USA, Dec. 2013, pp. 71â€“80.
[38] D. Pollard, Convergence of Stochastic Processes. Berlin, DE: Springer,
1984.
[39] E. S. Polovinkin, â€œStrongly convex analysis,â€ Sbornik: Math., vol. 187,
no. 2, p. 259, Feb. 1996.

