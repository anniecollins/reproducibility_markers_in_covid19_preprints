arXiv:1907.01938v2 [stat.CO] 20 Apr 2021

Model-based clustering and classification using mixtures
of multivariate skewed power exponential distributions
Utkarsh J. Dang‚àó
Ryan P. Browne‚Ä†

Michael P. B. Gallaugher‚àó‚àó
Paul D. McNicholas‚àó‚àó

‚àó Department

of Health Outcomes and Administrative Sciences, Binghamton University, SUNY,
New York, United States.
‚àó‚àó Department of Mathematics & Statistics, McMaster University, Ontario, Canada.
‚Ä† Department of Statistics & Actuarial Sciences, University of Waterloo, Ontario, Canada.

Abstract
Families of mixtures of multivariate power exponential (MPE) distributions have
already been introduced and shown to be competitive for cluster analysis in comparison
to other mixtures of elliptical distributions, including mixtures of Gaussian distributions. A family of mixtures of multivariate skewed power exponential distributions
is proposed that combines the flexibility of the MPE distribution with the ability to
model skewness. These mixtures are more robust to variations from normality and
can account for skewness, varying tail weight, and peakedness of data. A generalized
expectation-maximization approach, which combines minorization-maximization and
optimization based on accelerated line search algorithms on the Stiefel manifold, is
used for parameter estimation. These mixtures are implemented both in the unsupervised and semi-supervised classification frameworks. Both simulated and benchmark
data are used for illustration and comparison to other mixture families.

1

Introduction

Mixture modeling has been firmly established in the literature as a useful method for finding
homogeneous groups within heterogeneous data. Using mixture models for cluster analysis
has a long history (Hasselblad, 1966; Day, 1969) dating at least to Wolfe (1965), who used
a Gaussian mixture model for clustering. When using mixture models for clustering, which
is known as model-based clustering, mixture models are used to partition data points to
learn group memberships, or labels, of observations with no known labels. If some observations are a priori labeled, a semi-supervised analogue of model-based clustering is used

1

and this is known as model-based classification. Extensive details on model-based clustering and classification are given by McNicholas (2016a) and recent reviews are provided by
Bouveyron and Brunet-Saumard (2014) and McNicholas (2016b).
A G-component finite mixture model assumes that a random vector X has density of
the form
G
X
f (x|œë) =
œÄg fg (x|Œ∏ g ),
g=1

P
where g = 1, . . . , G, œÄg > 0 are the mixing proportions with G
g=1 œÄg = 1, and fg (¬∑) are
the component densities. The Gaussian mixture model (see, e.g., Banfield and Raftery,
1993; Celeux and Govaert, 1995; Tipping and Bishop, 1999; McNicholas and Murphy, 2008)
remains popular due to its mathematical tractability. However, it is inflexible in the presence
of cluster skewness and different levels of cluster kurtosis, and has been known to result in an
overestimate of the number of clusters and poor density estimation for known clusters (see
Franczak et al., 2014; Dang et al., 2015, for examples). Therefore, it has become popular to
consider mixtures of more flexible distributions for clustering to deal with such scenarios.
Mixture models that can deal with varying cluster tail-weight, skewness and/or concentration, and kurtosis are increasingly becoming common. A small selection of such mixtures include mixtures of multivariate t-distributions (Peel and McLachlan, 2000; Andrews and McNicholas,
2012), mixtures of normal inverse Gaussian distributions (Karlis and Santourian, 2009; Subedi and McNicho
2014; O‚ÄôHagan et al., 2016), mixtures of skew-t distributions (Lin, 2010; Murray et al., 2014;
Vrbik and McNicholas, 2014; Lee and McLachlan, 2014,0), mixtures of shifted asymmetric
Laplace distributions (Morris and McNicholas, 2013; Franczak et al., 2014), mixtures of multivariate power exponential distributions (Dang et al., 2015), mixtures of variance-gamma
distributions (McNicholas et al., 2017), and mixtures of generalized hyperbolic distributions
and variations thereof (Browne and McNicholas, 2015; Murray et al., 2017).
Two common approaches to introducing skewness are by means of a normal variancemean mixture model, and via hidden truncation using an elliptical distribution and a skewing
function.
‚àö The former assumes that a random vector X can be written in the form X = ¬µ +
W Œ± + W V , where ¬µ and Œ± are location and skewness vectors, respectively, V ‚àº N (0, Œ£),
W ‚ä• V , and W > 0 is a positive random variable with density h(w|Œò). Depending on
the distribution of W , different skewed distributions can be derived, e.g., the generalized
hyperbolic, skew-t, variance-gamma and normal inverse Gaussian distributions. The hidden
truncation approach makes use of a combination of an elliptical distribution and a skewing
function. For example, a random vector X follows a multivariate skew normal distribution
with skewness Œ± if its density can be written as f (x) = 2œÜp (x|¬µ, Œ£)Œ¶(Œ±‚Ä≤ x), where œÜp (¬∑)
is the density of the p-dimensional normal distribution and Œ¶(¬∑) denotes the cumulative
distribution function of the standard normal distribution (Azzalini and Valle, 1996).
The multivariate power exponential (MPE) distribution (GoÃÅmez et al., 1998) has been
used in many different applications (e.g., Lindsey, 1999; Cho and Bui, 2005; Verdoolaege et al.,
2008) and was recently used in the mixture model context by Dang et al. (2015). Depending
2

on the shape parameter Œ≤, either a leptokurtic or platykurtic distribution can be obtained.
Specifically, if Œ≤ ‚àà (0, 1) then the distribution is leptokurtic, which is characterized by a
thinner peak and heavy tails compared to the Gaussian distribution. If Œ≤ > 1, a platykurtic
distribution is obtained, which is characterized by a flatter peak and thin tails compared to
the Gaussian distribution. Other distributions can also be obtained for specific values of the
shape parameter, for example, for Œ≤ = 0.5, the distribution is a Laplace (double-exponential)
distribution and, for Œ≤ = 1, it is a Gaussian distribution. Furthermore, when Œ≤ ‚Üí ‚àû, the
MPE becomes a multivariate uniform distribution.
Dang et al. (2015) derived a family of mixtures of MPE distributions but those mixtures could only account for elliptical clusters. Previously, skew power exponential distributions have been discussed in the univariate case with constrained Œ≤ (Azzalini, 1986;
DiCiccio and Monti, 2004; da Silva Ferreira et al., 2011) or in the multivariate case as scale
mixture of skew normal with constrained Œ≤ (Branco and Dey, 2001). Herein, we present
mixtures based on a novel multivariate skewed power exponential (MSPE) distribution. As
compared to earlier proposals, this distribution is more suitable for clustering and classification purposes and can be used for a wide range of Œ≤ (heavy, Gaussian, and light tails).
Using an eigen-decomposition of the component scale distributions (aÃÄ la Celeux and Govaert,
1995), we construct a family of 16 MSPE mixture models for use in both clustering and semisupervised classification. These models can account for varying tail weight (heavy, Gaussian,
or light), peakedness (thinner or thicker than Gaussian), and skewness of mixture components.

2

Background

Using the parametrization given by GoÃÅmez et al. (1998), a random vector X follows a pdimensional power exponential distribution if the density is



pŒì 2p
1
‚àí 21
Œ≤


f (x|¬µ, Œ£, Œ≤) =
|Œ£| exp ‚àí Œ¥(x) ,
(1)
p
p
2
p/2
œÄ Œì 1 + 2Œ≤ 21+ 2Œ≤

where Œ¥(x) := Œ¥ (x|¬µ, Œ£) = (x ‚àí ¬µ)‚Ä≤ Œ£‚àí1 (x ‚àí ¬µ), ¬µ is the location parameter, Œ£ is a
positive-definite scale matrix, and Œ≤ determines the kurtosis. In a similar manner to Azzalini and Valle
(1996), Lin et al. (2014) derived the multivariate skew t-normal distribution by using an elliptical multivariate t-distribution and the cumulative distribution function of the standard
normal distribution as the skewing function.
Herein, the skewness function is still the N(0, 1) cumulative distribution function while
the elliptical distribution is now the MPE distribution. Specifically, a random vector X

3

follows a p-dimensional skew power exponential distribution if the density is of the form
f (x|¬µ, Œ£, Œ≤, œà) = 2 g(x|¬µ, Œ£, Œ≤) Œ¶(œà ‚Ä≤ Œ£‚àí1/2 (x ‚àí ¬µ)),


 

2pŒì 2p
1
‚Ä≤ ‚àí1/2
‚àí 21
Œ≤


exp
‚àí
Œ¶
œà
Œ£
(x
‚àí
¬µ)
|Œ£|
Œ¥(x)
=
p
p
2
œÄ p/2 Œì 1 + 2Œ≤
21+ 2Œ≤

(2)

with location vector ¬µ, scale matrix Œ£, shape parameter Œ≤, and skewness vector œà. Some
special cases of this distribution include the skew normal distribution (Œ≤ = 1), a variant
of a skew Laplace distribution (Œ≤ = 0.5), the power exponential distribution (œà = 0) and
a generalization of the multivariate uniform distribution (Œ≤ ‚Üí ‚àû, œà = 0). Examples of
contours of the MSPE distribution are given in Figure 1.

Figure 1: Contours of the multivariate skew power exponential distribution for different
values of the shape and skewness parameters with ¬µ = (0, 0)‚Ä≤ and an identity scale matrix.
The middle panel, with Œ≤ = 1, is a multivariate skew normal distribution.

3
3.1

Mixtures of MSPE Distributions
Inference

An iterative procedure is used for parameter estimation; specifically, a generalized expectationmaximization (GEM) algorithm (Dempster et al., 1977) with conditional maximization steps.
The expectation-maximization (EM) algorithm (Dempster et al., 1977) is an iterative procedure in which the conditional expected value of the complete-data log-likelihood is maximized
on each iteration to yield parameter updates. As opposed to the EM algorithm, the conditional maximization steps increase, rather than maximize, the conditional expected value of
the complete data log-likelihood in each iteration of a GEM algorithm. Consider a random
sample x1 , . . . , xn from a p-dimensional MSPE mixture distribution from a population with
4

G subgroups. If we define
(
1 if xi is from group g,
zig =
0 otherwise,
then the complete-data log-likelihood can be written as
"

N X
G
X
pŒì p2
1

Lc (Œò) =
zig log 2œÄg 
|Œ£g |‚àí 2
p
1+
Œì 1 + 2Œ≤pg 2 2Œ≤g œÄ p/2
i=1 g=1
#

 

Œ¥ig (xi )Œ≤g
√ó exp ‚àí
Œ¶ œà ‚Ä≤g Œ£‚àí1/2
(xi ‚àí ¬µg ) .
g
2
For parsimony, an eigen-decomposition is commonly imposed on component scale matrices using the re-parameterization Œ£g = Œªg Œìg ‚àÜg Œìg ‚Ä≤ , where ‚àÜg is a diagonal matrix with
entries proportional to the eigenvalues of Œ£g (with |‚àÜg | = 1), Œªg is the associated constant
of proportionality, and Œìg is a p √ó p orthogonal matrix of the eigenvectors of Œ£g with entries ordered according to the eigenvalues (Banfield and Raftery, 1993; Celeux and Govaert,
1995). A subset of eight models was considered in Dang et al. (2015) including the most
parsimonious (EII) and the fully unconstrained (VVV) models for their family of mixture
models using elliptical power exponential distributions (Table 1). Herein, we consider the
same eight models to form a family of mixtures of skewed power exponential distributions.
Table 1: Nomenclature, scale matrix structure, and the number of free scale parameters for
the eigen-decomposed family of models.
Model
Œªg
‚àÜg
Œìg
Œ£g
Free Parameters
EII
VII

Equal
Variable

Spherical
Spherical

‚Äì
‚Äì

ŒªI
Œªg I

EEI
VVI

Equal
Variable

Equal
Variable

Axis-Aligned
Axis-Aligned

Œª‚àÜ
Œªg ‚àÜg

EEE
EEV
VVE
VVV

Equal
Equal
Variable
Variable

Equal
Equal
Variable
Variable

Equal
Variable
Equal
Variable

ŒªŒì‚àÜŒì‚Ä≤
ŒªŒìg ‚àÜŒì‚Ä≤g
Œªg Œì‚àÜg Œì‚Ä≤
Œªg Œìg ‚àÜg Œì‚Ä≤g

1
G
p
Gp
p (p + 1) /2
Gp(p + 1)/2 ‚àí (G ‚àí 1)p
p(p + 1)/2 + (G ‚àí 1)p
Gp (p + 1) /2

After initialization (Section 3.2), the algorithm proceeds as follows.

5

E-Step: In the E-step, the group membership estimates zÃÇig are updated using


œÄÃÇg f xi |¬µÃÇg , Œ£ÃÇg , Œ≤ÃÇg , œàÃÇ g
,

zÃÇig := E c [Zig |xi ] = P
G
Œò
œÄÃÇ
f
x
|
¬µÃÇ
,
Œ£ÃÇ
,
Œ≤ÃÇ
,
œàÃÇ
i
j
j
j
j
j=1 j

for i = 1, . . . , N and g = 1, . . . , G.
P
M-Step: The update for œÄg is œÄÃÇg = ng /N, where ng = N
i=1 zÃÇig . However, the updates for
¬µg , Œ£g , Œ≤g and œà g are not available in closed form. For estimating Œ≤g , either a NewtonRaphson method or a root finding algorithm may be used and is identical to the estimate
in Dang et al. (2015). In our implemented code, we constrain Œ≤g to be less than 20 for
numerical stability. Let
Q := EŒò [Lc (Œò|x)],
Then, a Newton-Raphson update is used for the location parameter ¬µÃÇg with the following:
N
N
X
X
(xi ‚àí ¬µÃÇg )) ‚àí1/2
œÜ(œà ‚Ä≤g Œ£‚àí1/2
‚àÇQ
g
Œ≤ÃÇg ‚àí1 ‚àí1
Œ£ÃÇg (xi ‚àí ¬µÃÇg ) ‚àí
=Œ≤ÃÇg
zÃÇig Œ¥ig (xi )
zÃÇig
Œ£g œà g ,
‚Ä≤
‚àí1/2
‚àÇ¬µg
Œ¶(œà
Œ£
(x
‚àí
¬µÃÇ
))
i
g
g
g
i=1
i=1

(3)

N

i
X h
‚àÇ2Q
Œ≤ÃÇg ‚àí1 ‚àí1
Œ≤ÃÇg ‚àí2 ‚àí1
‚Ä≤ ‚àí1
Œ£ÃÇ
=
Œ≤ÃÇ
zÃÇ
‚àíŒ¥
(x
)
Œ£ÃÇ
‚àí
2(
Œ≤ÃÇ
‚àí
1)Œ¥
(x
)
(x
‚àí
¬µÃÇ
)(x
‚àí
¬µÃÇ
)
Œ£ÃÇ
g
ig
ig
i
g
ig
i
i
i
g
g
g
g
g
‚àÇ¬µg ¬µ‚Ä≤g
i=1
‚àí

N
X

zÃÇig œà ‚Ä≤g Œ£‚àí1/2
(xi ‚àí ¬µÃÇg )
g

(xi ‚àí ¬µÃÇg ))
œÜ(œà ‚Ä≤g Œ£‚àí1/2
g

‚àí1

‚àí1

Œ£ÃÇg œà g œà ‚Ä≤g Œ£ÃÇg

(xi ‚àí ¬µÃÇg ))
Œ¶(œà ‚Ä≤g Œ£‚àí1/2
g
#
"
2
N
X
(xi ‚àí ¬µÃÇg ))
œÜ(œà‚Ä≤g Œ£‚àí1/2
‚àí1
‚àí1
g
Œ£ÃÇg œà g œà ‚Ä≤g Œ£ÃÇg ,
‚àí
zÃÇig
‚Ä≤
‚àí1/2
Œ¶(œà g Œ£g (xi ‚àí ¬µÃÇg ))
i=1
i=1

‚Ä≤ ‚àí1

where Œ¥ig (xi ) := xi ‚àí ¬µÃÇg Œ£ÃÇg xi ‚àí ¬µÃÇg .
For estimating the skewness parameter œà, the density is first re-parameterized as
f (x|¬µ, Œ£, Œ≤, œà) =2 g(x|¬µ, Œ£, Œ≤) Œ¶(œà ‚Ä≤ Œ£‚àí1/2 (x ‚àí ¬µ))
=2 g(x|¬µ, Œ£, Œ≤) Œ¶(Œ∑ ‚Ä≤ (x ‚àí ¬µ)),
where Œ∑ = Œ£‚àí1/2 œà. A quadratic lower-bound principle (BoÃàhning and Lindsay, 1988; Hunter and Lange,
2004) on the relevant part of the complete-data log-likelihood using the re-parameterized
density uses the following property to construct a quadratic minorizer:
log(Œ¶(s)) ‚â• log(Œ¶(s0 )) +

1
œÜ(s0 )
(s ‚àí s0 ) + (‚àí1)(s ‚àí s0 )2 ,
Œ¶(s0 )
2

6

where ‚àí1 is the lower bound of the second derivative in the Taylor series around s0 . Then,
an estimate for Œ∑ g yields
#‚àí1 " N
" N
X œÜ(Œ∑ ‚Ä≤g0 (xi ‚àí ¬µÃÇg ))
X
‚Ä≤
zÃÇig
Œ∑g =
zÃÇig (xi ‚àí ¬µÃÇg )(xi ‚àí ¬µÃÇg )
(xi ‚àí ¬µÃÇg )
Œ¶(Œ∑ ‚Ä≤g0 (xi ‚àí ¬µÃÇg ))
i=1
i=1
#
N
X
+
zÃÇig (xi ‚àí ¬µÃÇg )(xi ‚àí ¬µÃÇg )‚Ä≤ Œ∑ g0
i=1

and we can back transform to obtain œà g = Œ£1/2
g Œ∑g .
For the scale matrices Œ£g , the estimation makes use of minorization-maximization algorithms (MM; Hunter and Lange, 2000,0) by exploiting the concavity of the functions containing Œ£g (or parts of its decomposition) and accelerated line search algorithms on the
Stiefel manifold (Absil et al., 2009; Browne and McNicholas, 2014), with different updates
depending on if the latest estimate for Œ≤g was less than 1, or greater than or equal to 1. For
more details, see Dang et al. (2015). Combining the constraints of the eigen-decomposition
in Table 1, with constraining Œ≤g to be equal or different between groups results in a family
of 16 models. For example, a VVIE model represents a VVI scale structure (as in Table 1)
and the shape parameter constrained to be equal between groups (Œ≤g = Œ≤).

3.2

Initialization

It is well known that the performance of the EM algorithm depends heavily on the starting
values. The following strategy is adopted. The group memberships are initialized using
the k-means algorithm (Hartigan and Wong, 1979). Once these initial memberships are set,
the ¬µg are initialized using the group sample means and the Œ£g are initialized using the
group sample covariance matrices. The kurtosis parameters Œ≤g are initialized to 0.5, and the
skewness is initialized as a zero vector.

3.3

Convergence, Model Selection, and Performance Assessment

Following Lindsay (1995) and McNicholas et al. (2010), the iterative GEM algorithm is
stopped based on the Aitken‚Äôs acceleration (Aitken, 1926). Specifically, an asymptotic estimate of the log-likelihood at iteration k + 1 is compared with the current log-likelihood
value and considered converged when the difference is less than some positive «´. Here, we
use «´ = 0.005.
In a general clustering scenario, the number of groups is generally not known a priori
and the covariance model is not known. Therefore, a model selection criterion is required.
The most common criterion for model selection is the Bayesian information criterion (BIC;
Schwarz, 1978). The BIC can be written as
BIC = 2l(œëÃÇ) ‚àí m log n,
7

(4)

where m is the number of free parameters, n is the sample size and l(œëÃÇ) is the maximized
log-likelihood. When written as in (4), a greater BIC represents a superior model fit. The
integrated complete likelihood (ICL; Biernacki et al., 2000) was also considered for model
selection; however, in initial testing, the ICL did not consistently outperform the BIC in
simulations and thus, for the remainder of this manuscript, we use BIC consistently.
To evaluate classification performance, we use the adjusted Rand index (ARI; Hubert and Arabie,
1985). The ARI compares two different partitions; specifically, in our case, the estimated
classification and the (known) true classifications. The ARI takes a value of 1 when there
is perfect classification and has expected value 0 under random classification. See Steinley
(2004) for extensive details on the ARI.

4
4.1

Analyses
Overview

The performance of the MSPE mixture models is compared with mixture model implementations based on the MPE distribution (Dang et al., 2015), the generalized hyperbolic distribution (MixGHD; Tortora et al., 2018), and the Gaussian distribution (mixture; Browne et al.,
2018). We chose these mixtures for comparison as Gaussian mixtures remain widely used
and the generalized hyperbolic distribution has special cases that include some parameterizations of inverse Gaussian, variance-gamma, skew-t (note there are formulations of the
skew-t distribution that cannot be obtained from the generalized hyperbolic), multivariate
normal-inverse Gaussian, and asymmetric Laplace distribution. Using these comparators,
we get comparisons to mixtures based on purely elliptical (Gaussian), elliptical with flexible
kurtosis modeling (MPE), and skewed (generalized hyperbolic) distributions. For a fair comparison, we restrict the models, if available (MixGHD only has the unconstrained model), in
the other implementations to those in Table 1. In addition, we use BIC as the model selection criterion, G = 1, . . . , 4 for all simulations and real data analyses, and k-means (with one
random start, the default for mixture and MixGHD) as the initialization for all algorithms.
Data from the MSPE distribution is simulated using a Metropolis-Hastings rule.

4.2
4.2.1

Simulations
Simulation 1: Heavy and light-tailed skewed clusters

A three-component mixture is simulated with 500 observations in total. Group sample sizes
are sampled from a multinomial distribution with mixing proportions (0.35, 0.25, 0.4)‚Ä≤. The
first component is simulated from a heavy-tailed three-dimensional MSPE distribution with
¬µ1 = (3, 3, 0)‚Ä≤, Œ≤1 = 0.85, and œà 1 = (‚àí5, ‚àí10, 0)‚Ä≤ . The second component is simulated with
¬µ2 = (3, 6, 0)‚Ä≤, Œ≤2 = 0.9, and œà 2 = (15, 10, 0)‚Ä≤. The third component is simulated with light
tails with ¬µ3 = (4, 2, 0)‚Ä≤, Œ≤3 = 2, and œà 3 = œà 2 . Lastly, the scale matrices were common to
8

all three components with diag(‚àÜg ) = (4, 3, 1)‚Ä≤ and
Ô£´
Ô£∂
0.36 0.48 ‚àí0.80
Œìg = Ô£≠‚àí0.80 0.60 0.0 Ô£∏ ,
0.48 0.64 0.6

for g = 1, 2, 3. The simulated components are not well separated (an example scatterplot
matrix is given in Figure 2). All four mixture implementations are run on 100 such data sets
for G = 1, . . . , 4.
X1

X3

X2

X1

10

X2

5

0

‚àí5
5.0

2.5

X3

0.0

‚àí2.5

‚àí5.0
‚àí5

0

5

10

‚àí5

0

Cluster

5
1

2

10
3

Figure 2: An example scatterplot matrix of the three-component mixture of Simulation 1.
For the MSPE family, a three-component model is selected by the BIC all 100 times. For
the MPE family, the BIC selects a three-component (four-component) model 88 (12) times.
Here, when the four-component models are selected, this is because the model chosen has
split up the heavy-tailed cluster into two separate components. For one of the models, only
two observations were fitted in a cluster. For the mixture family, the BIC selects a threecomponent model each time. On the other hand, for the MixGHD algorithm, two-component
(three-component) component models are selected 47 (53) times. This under-fitting ‚Äî the
two heavy-tailed components are merged ‚Äî may be due to the use of the BIC or the fact
that only the unconstrained scale model is possible with the MixGHD algorithm.
9

The ARI values for the selected MSPE models range from 0.72 to 0.91, with a median
(mean) ARI value of 0.82 (0.82). The selected MPE models yield ARI values ranging between
0.68 and 0.91, with a median (mean) value of 0.80 (0.79). The selected mixture models
yield ARI values ranging between 0.71 and 0.91, with a median (mean) value of 0.80 (0.80).
Similarly, the MixGHD algorithm yields ARI values ranging between 0.40 and 0.85, with a
median (mean) value of 0.66 (0.64). For the MSPE models, an EEEV model was selected
82 out of the 100 times.
4.2.2

Simulation 2: Heavy-tailed and Gaussian skewed clusters

A three-component mixture is simulated with 500 observations in total. Group sample
sizes are sampled from a multinomial distribution with mixing proportions (0.3, 0.45, 0.25)‚Ä≤.
The first component is simulated from a three-dimensional skewed normal distribution (i.e.,
Œ≤1 = 1) with ¬µ1 = (0, 1, 2)‚Ä≤, and œà 1 = (3, 5, 10)‚Ä≤. The second component is simulated
from a heavy-tailed three-dimensional MSPE distribution with ¬µ2 = (0, 4, 2)‚Ä≤, Œ≤2 = 0.8, and
œà 2 = (‚àí3, 5, ‚àí5)‚Ä≤ . The third component is simulated with ¬µ3 = (‚àí2, ‚àí3, 0)‚Ä≤ , Œ≤3 = 0.9, and
œà 3 = (5, 10, ‚àí5)‚Ä≤. Lastly, the scale matrices are
Ô£´
Ô£∂
Ô£´
Ô£∂
1.00 0.50 0.40
1.00 0.30 0.20
Œ£1 = Ô£≠0.50 1.50 0.35Ô£∏
and
Œ£2 = Œ£3 = Ô£≠0.30 1.50 0.30Ô£∏ .
0.40 0.35 1.20
0.20 0.30 1.20

Again, the simulated components are not well separated and all four mixture implementations are run on 100 such data sets for G = 1, . . . , 4.
For the MSPE family, a three-component (four-component) component model is selected
by the BIC 99 (1) times. For the MPE and MixGHD mixtures, the BIC selects a threecomponent model all 100 times. On the other hand, for the mixture family, the BIC selects a
three-component (four-component) model 95 (5) times. In all cases when the four component
models are selected, this is because the model chosen has split up one of the heavy-tailed
clusters into two separate components.
The ARI values for the selected MSPE models range from 0.76 to 0.99, with a median
(mean) ARI value of 0.94 (0.93). The selected MPE models yield ARI values ranging between
0.78 and 0.96, with a median (mean) value of 0.90 (0.90). Similarly, the MixGHD algorithm
yields ARI values ranging between 0.86 and 0.99, with a median (mean) value of 0.93 (0.92).
The selected mixture models yield ARI values ranging between 0.64 and 0.95, with a median
(mean) value of 0.90 (0.88). For the MSPE models, an EEEV model is selected 76 out of
100 times.
4.2.3

Simulation 3: Two light-tailed elliptical clusters

A simulation from Dang et al. (2015) is replicated, where a two-component EIIV model is
simulated with 450 observations with the sample sizes for each group sampled from a binomial
10

distribution with success probability 0.45. Both components had identity scale matrices and
zero skewness. The first component is simulated from a two-dimensional MPE distribution
with ¬µ1 = (0, 0)‚Ä≤ and Œ≤1 = 2 while the second component is simulated using ¬µ2 = (2, 0)‚Ä≤ and
Œ≤2 = 5. Again, the simulated components are not well separated. All four algorithms are run
on 100 such data sets. For the MSPE and MPE families, a two-component model is selected
by the BIC for 100 and 99 data sets, respectively. The dataset where a three-component
model is selected for the MPE models involves a cluster of five observations that are tightly
clustered with tiny eigenvalues. On the other hand, for the mixture family, the BIC selects
a two, three, and four-component model 77, 14, and 9 times, respectively. Similarly, for the
MixGHD algorithm, one, two, and three-component models are selected 1, 73, and 26 times,
respectively. Here, more components are being fitted to deal with the light-tailed nature of
the data.
For the MPE family (which was used to simulate the data), the ARI values for the
selected models range from 0.81 to 0.95, with a median (mean) ARI value of 0.88 (0.88).
The MSPE family performed similarly as expected (the MPE is a special case of the MSPE
family) with the ARI values for the selected models ranging from 0.81 to 0.95, with a median
(mean) ARI value of 0.88 (0.88). The selected mixture models yield ARI values ranging
between 0.35 and 0.96, with a median (mean) value of 0.85 (0.79). Similarly, the MixGHD
algorithm yields ARI values ranging between 0 and 0.90, with a median (mean) value of 0.69
(0.67). For the MSPE and MPE models, an EIIV model was selected 89 and 95 times out of
100, respectively. Because the ranges of ARI can be more reflective of one poor or great fit,
the median, first and third quartile of the ARIs for the best fitting models are summarized
in Table 2 for all simulations.
Table 2: Performance comparison of four mixture model families on simulations. For each
simulation and implementation, a frequency table of number of groups of the best selected
model according to the BIC is provided. Median ARI is provided as well as the first and
third quartiles for ARI across 100 datasets in each simulation in parenthesis as table entries.
MSPE
MPE
MixGHD
mixture

Frequencies
ARI
Frequencies
ARI
Frequencies
ARI
Frequencies
ARI

Simulation 1

Simulation 2

Simulation 3

3 (100)
0.82 (0.80, 0.85)
3 (88); 4 (12)
0.80 (0.76, 0.83)
2 (47); 3 (53)
0.66 (0.51, 0.75)
3 (100)
0.80 (0.77, 0.82)

3 (99); 4 (1)
0.94 (0.92, 0.95)
3 (100)
0.90 (0.88, 0.91)
3 (100)
0.93 (0.91, 0.94)
3 (95); 4 (5)
0.90 (0.87, 0.91)

2 (100)
0.88 (0.86, 0.90)
2 (99); 3 (1)
0.88 (0.86, 0.90)
1 (1); 2 (73); 3 (26)
0.69 (0.57, 0.77)
2 (77); 3 (14); 4 (9)
0.85 (0.74, 0.89)

11

4.3

Dataset Descriptions

For assessment of performance on benchmark data, we considered the following ‚Äúbenchmark‚Äù data (often used for comparison of clustering algorithms) available through various R
packages:
‚Ä¢ Iris Dataset: The iris dataset (included with R) consists of 150 observations, 50
each of 3 different species of iris. There are four different variables that were measured,
namely the petal length and width and the sepal length and width.
‚Ä¢ Female Voles Dataset: The fvoles dataset (Flury, 2012) had 6 measurements
and the age in days of 86 female voles from two different species (californicus and
ochrogaster).
‚Ä¢ Wine Dataset:The thirteen variable wine dataset (Hurley, 2012) had 13 different
measurements of chemical aspects of 178 Italian wines from three different regions.
‚Ä¢ Swiss Banknote Dataset: The Swiss banknote dataset (Tortora et al., 2018) looked
at 6 different measurements from 100 genuine and 100 counterfeit banknotes. The
measurements were length, width of the right and left edges, the top and bottom
margin widths and the length of the diagonal.
Several other datasets were also considered, on which performance of the different algorithms
is summarized in the Supporting Information.

4.4

Unsupervised Classification

Unsupervised classification is performed on (scaled) datasets mentioned above using the
same comparison distributions as on the simulated data. The ARI and the number of
groups chosen by the BIC is shown in Table 3. In the case of the fvoles dataset, skewed yet
light-tailed clusters are fit. Notice that in this case, the MixGHD model only finds one group,
which could be due to the fact that MixGHD cannot account well for lighter tails. However,
both MPE and mixture perform equally well here.
In the case of the iris data, the MPE mixture performs best while the MSPE mixture and
the other two comparators achieve the same solution (although a level of supervision helps for
MSPE; see Table 5). For the iris dataset, it appears that all except the MPE model classify
types 2 and 3 together as one group as is commonly seen for this dataset in the unsupervised
case. On the other hand, in the case of the wine data, the MSPE mixture achieves perfect
classification and performs much better than the skewed generalized hyperbolic distribution
mixture model, which under fits the number of groups. Previously, in Dang et al. (2015), a
solution was obtained using MPE mixtures which misclassified only one observation; however,
here the best fitting MPE mixture based on the BIC was a four-component model, likely due
to different starting values (the one misclassification fit was the third best model according
12

Table 3: Performance comparison of four mixture model families on benchmark data for
the unsupervised scenarios. Dimensionality and the number of known groups (i.e., classes)
are in parenthesis following each dataset name. For each implementation, the ARI and the
number of components are provided in parenthesis as table entries.
Dataset
MSPE
MPE
MixGHD
mixture
fvoles (p = 7, G = 2)
0.91 (2) 0.91 (2) 0 (1)
0.91 (2)
banknote (p = 5, G = 2) 0.86 (3) 0.68 (4) 0.98 (2) 0.68 (4)
iris (p = 4, G = 3)
0.57 (2) 0.92 (3) 0.57 (2) 0.57 (2)
wine (p = 13, G = 3)
1 (3)
0.68 (4) 0.48 (2) 0.93 (3)
to the BIC). Note that in the semi-supervised case as well (Table 5), the MSPE mixtures
perform the best followed by the MPE mixtures. The estimates of the beta parameters from
the MSPE fit yielding perfect clustering generally indicate heavier tails while MixGHD under
fits the number of components.
The banknote data is interesting in that while the MPE and Gaussian mixture models
split the counterfeit and genuine banknotes into four different groups with the same classification overall (Table 4), the best fitting MSPE model fits three components and the MixGHD
model splits the observations into two groups only.
Table 4: Classification tables for the cluster analysis of each model and dataset.
Dataset
fvoles

MSPE
1 2
1 41 2
2 0 43

MPE
1 2
1 41 2
2 0 43

iris

1 2
1 50 0
2 0 50

3
0
50

wine

1 2
1 59 0
2 0 71
3 0 0

3
0
0
48

banknote

1
2
3

1 2
85 0
15 1
0 99

MixGHD
1

mixture
1 2
1 41 2
2 0 43

1 2
41 45

1
2
3

1 2 3
50 0 0
0 50 4
0 0 46

1 2
1 50 0
2 0 50

3
0
50

1
2
3
4

1 2 3
50 0 0
0 46 0
9 20 0
0 5 48

1 2
1 59 68
2 0 3

3
0
48

1 2
1 75 0
2 24 0
3 1 15
4 0 85

13

1
2 99
1 1

2
0
100

1
2

1 2 3
50 0 0
0 50 50

1
3
2

1 2 3
58 1 0
1 68 0
0 2 48
1
1 75
2 0
3 24
4 1

2
0
85
0
15

4.5

Semi-Supervised Classification

Using the same datasets as in the unsupervised classification case, semi-supervised classification is now considered. Following McNicholas (2010), the (observed) likelihood in the
model-based classification framework can be written
L(œë | x1 , . . . , xn ) =

k Y
G
Y

i=1 g=1

[œÄg f (xi |Œ∏ g )]

zig

n
G
Y
X

j=k+1 h=1

[œÄh f (xj |Œ∏ h )],

where the first k observations have known component memberships (i.e., labels) and fg (¬∑)
are the component densities. For each dataset, we take 25 labelled/unlabelled splits with
25% supervision. In Table 5, we display the median ARI values along with the first and
third quartiles over the 25 splits. In the case of the wine data, as mentioned in Section 4.4,
the MSPE mixtures exhibit the best performance, followed by the MPE mixtures. On the
fvoles data, the MixGHD algorithm performs the best in terms of median ARI. For the iris
dataset, the MSPE models show the best performance and for the banknote dataset, the
MSPE, MPE, and MixGHD algorithms exhibit similar performance. A reviewer noted that
using mixtures of canonical fundamental skew t (CFUST) distributions (Lee and McLachlan,
2016), one can obtain an ARI close to 1 with there being only one misclassification. However,
we were unable to obtain this solution; perhaps due to different initialization.
Table 5: Median ARI values along with first and third quartiles in parenthesis for the
four different models for each dataset for the semi-supervised runs. Dimensionality and the
number of known groups (i.e., classes) are in parenthesis following each dataset name.
Dataset
fvoles (p = 7, G = 2)
banknote (p = 5, G = 2)
iris (p = 4, G = 3)
wine (p = 13, G = 3)

MSPE
0.94 (0.88, 1)
0.97 (0.97, 1)
0.93 (0.90, 0.95)
1 (1, 1)

MPE
0.94 (0.88, 0.94)
0.97 (0.97, 1)
0.92 (0.90, 0.93)
0.98 (0.98, 0.98)

MixGHD
1 (0.88, 1)
0.97 (0.97, 1)
0.90 (0.87, 0.92)
0.95 (0.93, 0.98)

mixture
0.94 (0.88,
0.97 (0.97,
0.92 (0.88,
0.82 (0.68,

0.94)
0.97)
0.92)
0.90)

It is interesting to note that for the wine dataset, the semi-supervised results for mixture
seem to be worse than the clustering results. Note that this phenomenon, whereby cluster
analysis can obtain better results compared to using semi-supervised classification, has been
noted before by Vrbik and McNicholas (2015) and Gallaugher and McNicholas (2019).

5

Discussion

A multivariate skewed power exponential distribution is introduced that is well suited for
density estimation purposes for a wide range of non-Gaussian data. The family of 16 MSPE
mixtures presented herein allow for robust mixture models for model-based clustering on
skewed as well as symmetric components. These models can model components with varying
14

levels of peakedness and tail-weight (light, heavy, Gaussian) simultaneously with skewness.
As a result, these models are well suited to model heterogeneous data with non-Gaussian
components.
In addition to these properties, special cases of the MSPE distribution include the skewnormal distribution among others. The performance of such mixtures for clustering is investigated on a wide range of simulated scenarios ‚Äî on heavy-tailed, light-tailed, Gaussian, and
skewed components, and their combinations ‚Äî and on benchmark data commonly used for
investigating clustering and classification. At present, model selection is performed using the
BIC and, although this performs well in most cases, it is by no means perfect and alternative
criteria could be considered in more detail.
Through simulations, we showed scenarios where such skewed mixtures are comparative
to or better than widely used elliptical mixture models (mixtures of Gaussians) or skewed
mixture models gaining increasing attention (mixtures of generalized hyperbolic distributions). When looking at real (benchmark) datasets, we compared in the context of both
unsupervised classification (i.e., clustering) and semi-supervised classification. On these, the
MSPE model performed just as well or better on some of the investigated data sets compared to three other mixture model families/algorithms. The analysis on the real datasets
in the unsupervised case displayed some possible weaknesses, which may be related to initializations or the choice of BIC as the model selection criterion. Performance improved
substantially for some of the data when a small level of supervision was introduced in a
model-based classification framework and the MPSE distributions.
There are numerous areas of possible future work. One such area would be to consider a
mixture of factor analyzers with the MSPE distribution for high dimensional data. A matrix
variate extension, in a similar manner to Gallaugher and McNicholas (2018) might also be
interesting for modeling three-way data.

Code and Data Accessibility Statement
All data used here are publicly available; references have been provided within the bibliography. An implementation of mixtures of skewed power exponential distributions and mixtures
of power exponential distributions is available as an R package mixSPE (Dang et al., 2021).

References
Absil, P.-A., Mahony, R., and Sepulchre, R. (2009). Optimization Algorithms on Matrix
Manifolds. Princeton University Press.
Aitken, A. C. (1926). On Bernoulli‚Äôs numerical solution of algebraic equations. In Proceedings
of the Royal Society of Edinburgh, pages 289‚Äì305.

15

Andrews, J. L. and McNicholas, P. D. (2012). Model-based clustering, classification, and discriminant analysis via mixtures of multivariate t-distributions. Statistics and Computing,
22(5), 1021‚Äì1029.
Azzalini, A. (1986). Further results on a class of distributions which includes the normal
ones. Statistica, 46(2), 199‚Äì208.
Azzalini, A. and Valle, A. D. (1996). The multivariate skew-normal distribution. Biometrika,
83, 715‚Äì726.
Banfield, J. D. and Raftery, A. E. (1993). Model-based Gaussian and non-Gaussian clustering. Biometrics, 49(3), 803‚Äì821.
Biernacki, C., Celeux, G., and Govaert, G. (2000). Assessing a mixture model for clustering
with the integrated completed likelihood. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 22(7), 719‚Äì725.
BoÃàhning, D. and Lindsay, B. G. (1988). Monotonicity of quadratic-approximation algorithms.
Annals of the Institute of Statistical Mathematics, 40(4), 641‚Äì663.
Bouveyron, C. and Brunet-Saumard, C. (2014). Model-based clustering of high-dimensional
data: A review. Computational Statistics and Data Analysis, 71, 52‚Äì78.
Branco, M. D. and Dey, D. K. (2001). A general class of multivariate skew-elliptical distributions. Journal of Multivariate Analysis, 79(1), 99‚Äì113.
Browne, R. P. and McNicholas, P. D. (2014). Orthogonal Stiefel manifold optimization
for eigen-decomposed covariance parameter estimation in mixture models. Statistics and
Computing, 24(2), 203‚Äì210.
Browne, R. P. and McNicholas, P. D. (2015). A mixture of generalized hyperbolic distributions. Canadian Journal of Statistics, 43(2), 176‚Äì198.
Browne, R. P., ElSherbiny, A., and McNicholas, P. D. (2018). mixture: Mixture Models for
Clustering and Classification. R package version 1.5.
Celeux, G. and Govaert, G. (1995). Gaussian parsimonious clustering models. Pattern
Recognition, 28(5), 781‚Äì793.
Cho, D. and Bui, T. D. (2005). Multivariate statistical modeling for image denoising using
wavelet transforms. Signal Processing: Image Communication, 20(1), 77‚Äì89.
da Silva Ferreira, C., Bolfarine, H., and Lachos, V. H. (2011). Skew scale mixtures of normal
distributions: Properties and estimation. Statistical Methodology, 8(2), 154 ‚Äì 171.

16

Dang, U. J., Browne, R. P., and McNicholas, P. D. (2015). Mixtures of multivariate power
exponential distributions. Biometrics, 71(4), 1081‚Äì1089.
Dang, U. J., Browne, R. P., and Gallaugher, M P Band McNicholas, P. D. (2021). mixSPE:
Mixtures of power exponential and skew power exponential distributions for use in modelbased clustering and classification. R package version 0.9.1.
Day, N. E. (1969). Estimating the components of a mixture of normal distributions.
Biometrika, 56(3), 463‚Äì474.
Dempster, A. P., Laird, N. M., and Rubin, D. B. (1977). Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society. Series B , 39(1),
1‚Äì38.
DiCiccio, T. J. and Monti, A. C. (2004). Inferential aspects of the skew exponential power
distribution. Journal of the American Statistical Association, 99(466), 439‚Äì450.
Flury, B. (2012). Flury: data sets from Flury, 1997 . R package version 0.1-3.
Fraley, C., Raftery, A. E., Murphy, T. B., and Scrucca, L. (2012). mclust version 4 for R:
Normal mixture modeling for model-based clustering, classification, and density estimation. Technical Report 597, Department of Statistics, University of Washington, Seattle,
Washington, USA.
Franczak, B. C., Browne, R. P., and McNicholas, P. D. (2014). Mixtures of shifted asymmetric Laplace distributions. IEEE Transactions on Pattern Analysis and Machine Intelligence, 36(6), 1149‚Äì1157.
Franczak, B. C., Browne, R. P., McNicholas, P. D., and Burak, K. L. (2018). MixSAL:
Mixtures of Multivariate Shifted Asymmetric Laplace (SAL) Distributions. R package
version 1.0.
Gallaugher, M. P. B. and McNicholas, P. D. (2018). Finite mixtures of skewed matrix variate
distributions. Pattern Recognition, 80, 83‚Äì93.
Gallaugher, M. P. B. and McNicholas, P. D. (2019). On fractionally-supervised classification:
weight selection and extension to the multivariate t-distribution. Journal of Classification,
36(2), 232‚Äì265.
GoÃÅmez, E., Gomez-Viilegas, M. A., and Marin, J. M. (1998). A multivariate generalization
of the power exponential family of distributions. Communications in Statistics-Theory
and Methods, 27(3), 589‚Äì600.
Hartigan, J. A. and Wong, M. A. (1979). A k-means clustering algorithm. Journal of the
Royal Statistical Society: Series C , 28(1), 100‚Äì108.
17

Hasselblad, V. (1966). Estimation of parameters for a mixture of normal distributions.
Technometrics, 8(3), 431‚Äì444.
Hubert, L. and Arabie, P. (1985). Comparing partitions. Journal of Classification, 2(1),
193‚Äì218.
Hunter, D. R. and Lange, K. (2000). Rejoinder to discussion of ‚ÄúOptimization transfer using
surrogate objective functions‚Äù. Journal of Computational and Graphical Statistics, 9(1),
52‚Äì59.
Hunter, D. R. and Lange, K. (2004). A tutorial on MM algorithms. The American Statistician, 58(1), 30‚Äì37.
Hurley, C. (2012). gclus: Clustering Graphics. R package version 1.3.1.
Karlis, D. and Santourian, A. (2009). Model-based clustering with non-elliptically contoured
distributions. Statistics and Computing, 19(1), 73‚Äì83.
Lee, S. and McLachlan, G. J. (2014). Finite mixtures of multivariate skew t-distributions:
some recent and new results. Statistics and Computing, 24(2), 181‚Äì202.
Lee, S. X. and McLachlan, G. J. (2016). Finite mixtures of canonical fundamental skew
t-distributions: the unification of the restricted and unrestricted skew t-mixture models.
Statistics and Computing, 26(3), 573‚Äì589.
Lin, T.-I. (2010). Robust mixture modeling using multivariate skew t distributions. Statistics
and Computing, 20(3), 343‚Äì356.
Lin, T.-I., Ho, H. J., and Lee, C.-R. (2014). Flexible mixture modelling using the multivariate
skew-t-normal distribution. Statistics and Computing, 24(4), 531‚Äì546.
Lindsay, B. G. (1995). Mixture models: theory, geometry and applications. In NSF-CBMS
Regional Conference Series in Probability and Statistics, pages 1‚Äì163.
Lindsey, J. K. (1999). Multivariate elliptically contoured distributions for repeated measurements. Biometrics, 55(4), 1277‚Äì1280.
McNicholas, P. D. (2010). Model-based classification using latent Gaussian mixture models.
Journal of Statistical Planning and Inference, 140(5), 1175‚Äì1181.
McNicholas, P. D. (2016a). Mixture Model-Based Classification. Chapman & Hall/CRC
Press, Boca Raton.
McNicholas, P. D. (2016b). Model-based clustering. Journal of Classification, 33(3), 331‚Äì
373.
18

McNicholas, P. D. and Murphy, T. B. (2008). Parsimonious Gaussian mixture models.
Statistics and Computing, 18(3), 285‚Äì296.
McNicholas, P. D., Murphy, T. B., McDaid, A. F., and Frost, D. (2010). Serial and parallel
implementations of model-based clustering via parsimonious Gaussian mixture models.
Computational Statistics & Data Analysis, 54(3), 711‚Äì723.
McNicholas, S. M., McNicholas, P. D., and Browne, R. P. (2017). A mixture of variancegamma factor analyzers. In Big and Complex Data Analysis, pages 369‚Äì385. Springer
International Publishing, Cham.
Morris, K. and McNicholas, P. D. (2013). Dimension reduction for model-based clustering via
mixtures of shifted asymmetric Laplace distributions. Statistics and Probability Letters,
83(9), 2088‚Äì2093.
Murray, P. M., Browne, R. B., and McNicholas, P. D. (2014). Mixtures of skew-t factor
analyzers. Computational Statistics and Data Analysis, 77, 326‚Äì335.
Murray, P. M., Browne, R. B., and McNicholas, P. D. (2017). Hidden truncation hyperbolic
distributions, finite mixtures thereof, and their application for clustering. Journal of
Multivariate Analysis, 161, 141‚Äì156.
Nakai, K. and Kanehisa, M. (1991). Expert system for predicting protein localization sites
in gram-negative bacteria. Proteins: Structure, Function, and Bioinformatics, 11(2),
95‚Äì110.
Nakai, K. and Kanehisa, M. (1992). A knowledge base for predicting protein localization
sites in eukaryotic cells. Genomics, 14(4), 897‚Äì911.
O‚ÄôHagan, A., Murphy, T. B., Gormley, I. C., McNicholas, P. D., and Karlis, D. (2016).
Clustering with the multivariate normal inverse Gaussian distribution. Computational
Statistics and Data Analysis, 93, 18‚Äì30.
Peel, D. and McLachlan, G. J. (2000). Robust mixture modelling using the t distribution.
Statistics and Computing, 10(4), 339‚Äì348.
Schwarz, G. (1978). Estimating the dimension of a model. The Annals of Statistics, 6(2),
461‚Äì464.
Steinley, D. (2004). Properties of the Hubert-Arabie adjusted Rand index. Psychological
Methods, 9, 386‚Äì396.
Subedi, S. and McNicholas, P. D. (2014). Variational Bayes approximations for clustering
via mixtures of normal inverse Gaussian distributions. Advances in Data Analysis and
Classification, 8(2), 167‚Äì193.
19

Tipping, M. E. and Bishop, C. M. (1999). Mixtures of probabilistic principal component
analysers. Neural Computation, 11(2), 443‚Äì482.
Tortora, C., ElSherbiny, A., Browne, R. P., Franczak, B. C., , McNicholas, P. D., and Amos,
D. D. (2018). MixGHD: Model Based Clustering, Classification and Discriminant Analysis
Using the Mixture of Generalized Hyperbolic Distributions. R package version 2.2.
Venables, W. N. and Ripley, B. D. (2002). Modern Applied Statistics with S . Springer, New
York, fourth edition. ISBN 0-387-95457-0.
Verdoolaege, G., De Backer, S., and Scheunders, P. (2008). Multiscale colour texture retrieval
using the geodesic distance between multivariate generalized Gaussian models. In 2008
15th IEEE International Conference on Image Processing, pages 169‚Äì172.
Vrbik, I. and McNicholas, P. D. (2014). Parsimonious skew mixture models for model-based
clustering and classification. Computational Statistics and Data Analysis, 71, 196‚Äì210.
Vrbik, I. and McNicholas, P. D. (2015). Fractionally-supervised classification. Journal of
Classification, 32(3), 359‚Äì381.
Wolfe, J. H. (1965). A computer program for the maximum likelihood analysis of types.
Technical Bulletin 65-15, U.S. Naval Personnel Research Activity.

20

A

Supporting Information

Supporting Information: Additional information for this article is available.

A.1

Performance on additional datasets

In addition to those considered in the main body of the manuscript, we also considered the
following data available through various R packages:
Crabs Dataset The crabs dataset (Venables and Ripley, 2002) contains 200 observations
with 5 different variables that measure characteristics of crabs. there were 100 males and
100 females, and two different species of crabs, orange and blue. This effectively creates four
different groups of crabs based on gender/species combinations with 50 observations in each
group.
Bankruptcy Dataset The bankruptcy dataset (Tortora et al., 2018) looked at the ratio
of retained earnings to total assets, and the ratio of earnings before interests and taxes to
total assets of 33 financially sound and 33 bankrupt American firms.
Yeast Dataset A subset of the yeast dataset from Nakai and Kanehisa (1991,9) sourced
through the MixSAL package (Franczak et al., 2018) is also used. There are measurements on
three variables: McGeoch‚Äôs method for signal sequence recognition (mcg), the score of the
ALOM membrane spanning region prediction program (alm), and the score of discriminant
analysis of the amina acid content of vacuolar and extracellular proteins (vac) along with the
possible two cellular localization sites, CYT (cytosolic or cytoskeletal) and ME3 (membrane
protein, no N-terminal signal) for the proteins.
Diabetes Dataset The diabetes dataset (Fraley et al., 2012) considered 145 non-obese
adult patients with different types of diabetes classified as normal, overt and chemical. There
were three measurements, the area under the plasma glucose curve, the area under the plasma
insulin curve and the steady state plasma glucose.
Unsupervised classification Unsupervised classification, i.e., clustering, is performed on
the (scaled) datasets mentioned above using the same comparison distributions as on the
simulated data. The ARI and the number of groups chosen by the BIC is shown in Table 6.
We see that for the crabs that the MPE distribution exhibits the best performance while the
other three methods choose three groups; however, the clusters found are a little different
(Table 7). For example, the MSPE model perfectly separates one species of crab from the
other; however, for the first species it does not differentiate between the sexes. For the
second species, there are only 4 misclassifications for differentiating the sexes. The mixture
and MixGHD models have similar fits but worse performance.
21

Table 6: Performance comparison of four mixture model
Dataset
MSPE
MPE
crabs (p = 5, G = 4)
0.67 (3) 0.82 (4)
yeast (p = 3, G = 2)
0.49 (3) 0.40 (4)
bankruptcy (p = 2, G = 2) 0 (2)
0.77 (2)
diabetes (p = 3, G = 3)
0.44 (2) 0.66 (3)

families on
MixGHD
0.56 (3)
0.83 (2)
0 (1)
0.45 (2)

benchmark data
mixture
0.61 (3)
0.40 (4)
0.58 (3)
0.66 (3)

Dimensionality and the number of known groups (i.e., classes) are in parenthesis following
each dataset name. For each implementation, the ARI and the number of components are
provided in parenthesis as table entries.
The bankruptcy data shows some interesting results. The MixGHD model fits only one
cluster to the data. The MPE mixtures perform the best, fitting one light-tailed and one
heavy-tailed component, which seems reasonable (Figure 3), resulting in four misclassifications. Although the BIC chose two components for the MSPE mixtures, the best fitted
model had two heavy-tailed, skewed components and the clustering was no better than random assignment (Table 7). This was surprising and given the much better performance in
the semi-supervised case (Table 8) suggests that this is due to poor initialization.

Ratio of earnings before interests and taxes

0

‚àí100

‚àí200

‚àí300

‚àí200

‚àí100

0

Ratio of retained earnings
Cluster

Bankruptcy

Financially Sound

Figure 3: Bankruptcy data with colours indicating known grouping.
For the yeast dataset, apart from the MixGHD mixtures, the other three mixtures overfit
the number of components. Finally, when looking at the diabetes dataset, the additional
flexibility of being able to model skewness does not seem an asset as both the MSPE and
22

MixGHD implementations perform worse than the MPE and Gaussian cases, both under
fitting the number of groups (seem to combine the normal and chemical classes but able
to differentiate from the overt class). Moreover, the estimates of skewness were not trivial,
and both groups had heavier tails in the MSPE fit. On the other hand, for the MPE fit,
the tails were approximately Gaussian (common Œ≤g ‚âà 1). The relative performance of the
MPE versus MSPE mixtures in Table 8 suggests that there are cases in which using skewed
mixtures might not be ideal, and could be a possible subject of future work.
Table 7: Classification tables for the cluster analysis of each model and dataset.
Dataset

crabs

MSPE
1
1 0
2 50
3 0

1
2

bankruptcy

yeast

diabetes

1
2
3

2 3 4
0 0 46
50 0 0
0 50 4

MPE
1 2 3 4
0 1 0 47
11 49 0 0
39 0 0 0
0 0 50 3

1
2
3
4

1 2
17 14
16 19

1
1 33
2 0

1
2
74
1
355 10
34 152

1 2 3
1 2 0 30
2 34 76 3

1
2
3
4

1
2
3

MixGHD
1 2 3 4
1 0 1 0 44
2 39 0 50 6
3 11 49 0 0

2
4
29

1 2
1 33 33

1
2
81
0
3 133
51
6
328 24
1 2 3
9 72 0
26 4 6
1 0 27

1
2
1 449 13
2 14 150

1
2

1 2 3
31 75 2
5 1 31

mixture
1
2
3

1 2 3 4
0 0 4 50
37 0 46 0
13 50 0 0

1
2
3

1 2
22 4
0 29
11 0

1
2
1 307 20
2
5 135
3 128
0
4 23
8
1 2
1 26 4
2 9 72
3 1 0

3
6
0
27

Semi-supervised classification For each dataset, we take 25 labelled/unlabelled splits
with 25% supervision. In Table 8, we display the median ARI values along with the first and
third quartiles over the 25 splits. Note that for the yeast data, while in the unsupervised
context, MixGHD performed superior to the MSPE mixture, in the semi-supervised context,
the MSPE models perform better than the MixGHD models. Similarly, for the diabetes data,
the MSPE mixtures perform the best. For the crabs and bankruptcy datasets, although
the MSPE models do not show the best performance, the performance is close to the other
mixture distributions. Note that as seen for the wine dataset, for the bankruptcy dataset, for
the mixture algorithm, semi-supervised runs result in poorer results than in the clustering
case, as we addressed in the main text.

23

Table 8: Median ARI values along with the first and third quartiles in parenthesis for the
four different models for each dataset for the semi-supervised runs. Dimensionality and the
number of known groups (i.e., classes) are in parenthesis following each dataset name.
Dataset
crabs (p = 5, G = 4)
yeast (p = 3, G = 2)
bankruptcy (p = 2, G = 2)
diabetes (p = 3, G = 3)

MSPE
0.80 (0.78, 0.82)
0.84 (0.83, 0.85)
0.77 (0.63, 0.84)
0.76 (0.69, 0.82)

MPE
0.86 (0.83, 0.88)
0.78 (0.75, 0.81)
0.77 (0.70, 0.84)
0.73 (0.68, 0.80)

24

MixGHD
0.85 (0.82, 0.86)
0.82 (0.81, 0.85)
0.77 (0.77, 0.84)
0.73 (0.70, 0.80)

mixture
0.86 (0.83, 0.88)
0.74 (0.67, 0.77)
0.40 (0.30, 0.57)
0.74 (0.68, 0.75)

