Gaussian approximation for
empirical barycenters
Nazar Buzun
n.buzun@skoltech.ruâˆ—

arXiv:1904.00891v2 [math.ST] 17 Jun 2019

Abstract
In this work we consider Wasserstein barycenters (average in Wasserstein distance) in Fourier
basis. We prove that random Fourier parameters of the barycenter converge to Gaussian
random vector by distribution. The convergence approximation has been done in finiteâˆš
sample condition with rate O(p/ n) depending on measures count (n) and the dimension of
parameters (p).
Keywords: GAR, Wasserstein distance, multivariate central limit theorem, statistical
learning, convex analysis.

1

Introduction

Monge-Kantorovich distance or Wasserstein distance is a distance between measures. It represents a transportation cost of measure Âµ1 into the other measure Âµ2 .
Ë†
Wp (Âµ1 , Âµ2 ) =

min

1/p
kx âˆ’ yk dÏ€(x, y)
p

Ï€âˆˆÎ [Âµ1 ,Âµ2 ]

Â´
where the condition Ï€ âˆˆ Î [Âµ1 , Âµ2 ] means that Ï€(x, y) has two marginal distributions: y dÏ€(x, y) =
Â´
dÂµ1 (x) and x dÏ€(x, y) = dÂµ2 (y). We focus on regularized W1 distance with probabilistic space
{IRd , B(k Â· k2 ), L1 }
Ë†
f
W1 (Âµ1 , Âµ2 ) = min
kx âˆ’ ykdÏ€(x, y) + RÎµ (Ï€)
Ï€âˆˆÎ [Âµ1 ,Âµ2 ]

where RÎµ (Ï€) is a relatively small addition which improves differential properties of the distance.
Namely without RÎµ (Ï€) we can only bound the first derivative, with it we can bound also the
second derivative. There is the notion of mean in Wasserstein distance, called barycenter Âµ
b. And
it is the main object in this paper.
Âµ
b = argmin
Âµ

n
X

f1 (Âµ, Âµi )
W

i=1

Barycenters are center-of-mass generalization. If we look at the barycenter of a set of uniform
measures it fits the common structure form of these measures. If the measures are sampled
from some distribution then their barycenter can be treated as an empirical approximation of the
distribution mean. A simple example is a circles set with means {mi } and radiusâ€™s {ri }.
Ë† 2Ï€

1
2
W2 (m1 , r1 ), (m2 , r2 ) =
[(m2 âˆ’ m1 ) âˆ’ (r2 âˆ’ r1 ) cos(a)]2 + [(r2 âˆ’ r1 ) sin(a)]2 da
2Ï€ 0
= (m2 âˆ’ m1 )2 + (r2 âˆ’ r1 )2
âˆ—

Skolkovo Institute of Science and Technology, Bolshoy Boulevard 30, bld. 1 Moscow, Russia 121205

1

Figure 1: Illustration for W2 distance computation between two circles (m1 , r1 ) and (m2 , r2 ).
P
P
Their W22 barycenter is also c circle with mean m = n1 ni=1 mi and radius r = n1 ni=1 ri . We refer
to papers [1], [7] for the overview of the barycenters and related study.
It is well known that the center-of-mass in l2 norm converges to a Gaussian random vector.
As for the barycenter it is also expected to have some Gaussian properties. For example the
measures are Gaussian themselves or one-dimensional or circles set then the Gaussian structure
of the barycenter is evident. In circles set case the mean and radius converges to some Gaussian
variables as a sum of independent observations according to Central Limit Theorem. In onedimensional case denoting distribution functions by Fi (x)
Ë† 1
|F1âˆ’1 (s) âˆ’ F2âˆ’1 (s)|ds
W1 (Âµ1 , Âµ2 ) =
0

one get
n

1 X âˆ’1
Fbâˆ’1 (s) =
F (s)
n i=1 i
In the case with Gaussian measures with zero mean and variances {Si }
1/2

1/2

W22 (Âµ1 , Âµ2 ) = tr(S1 ) + tr(S2 ) âˆ’ 2 tr((S2 S1 S2 )1/2 )
and for some non-random matrix Sâˆ— [11]
n

1 X 1/2
(S Si Sâˆ—1/2 )1/2 + O(1/n)
Sb =
n i=1 âˆ—
In both last cases one have a mean of independent random variables that converges to Gaussian
random variable (or to Gaussian process in case of Fbâˆ’1 (s) by Donskerâ€™s Theorem). In general case
it appears to be very difficult to reveal such convergence because the barycenter doesnâ€™t have an
explicit equation and it is an infinite-dimensional object. In order to handle with this difficulty
we propose to construct a sum of independent variables using projection into Fourier basis and
some novel results from statistical learning theory. The perspective of Fourier Analysis provides a
suitable representation of the Wasserstein distance and it is already studied in the literature [10].
Denote a range of size p of the barycenter Fourier coefficients by


db
Âµ
(x)
Î¸b = Fp
dx
The first our result is that for some non-random matrix D, non-random vector Î¸âˆ— and independent
random vectors {Î¾i }


n

 X
p
âˆ—
Î¾i = O âˆš
D Î¸b âˆ’ Î¸ âˆ’
n
i=1
2

Further we show that for some Gaussian vector Z




âˆ—



W1 D(Î¸b âˆ’ Î¸ ), Z = O
and

p3/2
âˆš
n






p
âˆ—
b
P kD(Î¸ âˆ’ Î¸ )k > x âˆ’ P (kZk > x) = O âˆš
n


Statistical Application: The last statement allows as to obtain the confidence region of parameter
Î¸b and describe the distribution inside the region. The bootstrap procedure validity follows from
b using bootstrap it would be close by quantiles to
our proof as well. If one sample kD(Î¸boot âˆ’ Î¸)k
the random variable kD(Î¸b âˆ’ Î¸âˆ— )k. This is also relates to the construction of the confidence region.
The Structure of this paper is following. The main Theorems are in Section 2. In Sections 5,6
we compute derivatives of the Wasserstein distance using infimal convolution of support functions.
Section 3 deals with independent parametric models and describes how one can approximate
parameter deviations by a sum of independent random vectors {Î¾i }. In Section 4 we explore the
barycenters model and check the required assumptions from the 3-rd Section. The final part,
Gaussian approximation of the parameter Î¸bp , is completed in Section 7, where we prove that {Î¾i }
is close to Z by distribution and by W1 .

2

The main result

Consider a set of random measures (random measure is a measure-valued random element)
with densities Ï†1 , . . . , Ï†n . Let the barycenter measure Âµ
b has density Ï†b and Fourier coefficients
âˆ
b âˆˆ IR .
Î¸b = Î¸(Ï†)
n
X
f1 (Ï†, Ï†i )
Ï†b = argmin
W
Ï†

Let Fourier basis
function f

{Ïˆk }âˆ
k=1

i=1

has a Gram function of the scalar product G(x), such that for some
Ë†
hf, Ïˆk iG = f (x)Ïˆk (x)G(x)dx
Ë†

and
Î¸(Ï•)[k] =

Ï•(x)Ïˆk (x)dx

Denote Fourier coefficients of the other measures âˆ€i : Î¸i = Î¸(Ï•i ) âˆˆ IRâˆ . Basing on Lemma 9 define
an independent parametric model with dataset (Î¸1 , . . . , Î¸n ) and parameter Î¸.
L(Î¸) =

n
X

l(Î¸ âˆ’ Î¸i )

i=1

where
T
f
l(Î¸ âˆ’ Î¸i ) = max
T hÎ·, Î¸ âˆ’ Î¸i i âˆ’ ÎµÎ· (K â—¦ G)Î· = W1 (Ï†, Ï†i )
Î·âˆˆ

and
that

T

Ex

Ex is a Sobolev ellipsoids intersection. Each ellipsoid Ex has matrix Kx = âˆ‡T Ïˆâˆ‡Ïˆ T (x) such
ï£«
ï£¶2
X
ï£­
Î·k âˆ‡Ïˆk (x)ï£¸ = Î· T Kx Î·
kâˆˆNd+

and
\


Ex = Î· : âˆ€x : Î· T Kx Î· â‰¤ 1

3

Define a positive matrix K â—¦ G =

Â´

Kx G(x)dx
ï£«
1/T 2
ï£¬
ï£¬
K â—¦G=ï£¬ 0
ï£­ 0
...

such that in case Ïˆk (x) = eik
ï£¶
0
0
...0
ï£·
..
.
0
. . . 0ï£·
ï£·
. . . k 2 /T 2 . . . 0ï£¸
...
...
...

T x/T

Define for this model MLE parameter value and reference parameter value:
Î¸b = argmin L(Î¸)
Î¸

Î¸âˆ— = argmin IEL(Î¸)
Î¸

Define a local region around Î¸âˆ—
â„¦(r) = {Î¸ : kD(Î¸ âˆ’ Î¸âˆ— )k â‰¤ r}
where D is the Fisher matrix of the model
D2 = âˆ’âˆ‡2 IEL(Î¸âˆ— )
Theorem 1. Let the random Fourier parameters of the dataset have a common density Î¸1 . . . Î¸n âˆ¼
q(Î¸) and it fulfills condition âˆ€Î¸ âˆˆ â„¦(r)
Ë†
CQ
kDâˆ’1 âˆ‡q(Î¸)kdÎ¸ = âˆš
n
b Î¸âˆ— âˆˆ IRâˆ are Fourier coefficients of the MLE and reference barycenter defined above, then
Let Î¸,
with probability 1 âˆ’ eâˆ’t
D(Î¸b âˆ’ Î¸âˆ— ) âˆ’ Dâˆ’1 âˆ‡L(Î¸âˆ— ) â‰¤ â™¦(r, t)
where â™¦(r, t) is defined in Section 4 and has asymptotic
âˆš
âˆš


âˆš
nO(rCQ + r pD + 2t)
1

+o âˆš
â™¦(r, t) =
n
ÎµÎ»min DK â—¦ GD
and pD is an ellipsoid entropy (Section 3.3.) with matrix D
s
X log2 (Î»2 (D))
i
pD =
2
Î»i (D)
i
and with probability 1 âˆ’ eâˆ’t
âˆ’1

âˆ—

r â‰¤ 4kD âˆ‡L(Î¸ )k â‰¤

âˆš
âˆš
8 n(1 + 2t)
1/2

Î»min DK â—¦ GD



Proof. Basing on Theorem 3 one have to prove Assumptions 1,2,3 from which follows
D(Î¸b âˆ’ Î¸âˆ— ) âˆ’ Dâˆ’1 âˆ‡L(Î¸âˆ— ) â‰¤ {Î´(r) + z(t)}r = â™¦(r, t)
with probability 1 âˆ’ eâˆ’t . The Assumptions 1,2,3 are proven in Section 4 where also is shown that
Ë†
rn
 kDâˆ’1 âˆ‡q(Î¸)kdÎ¸
Î´(r) =
ÎµÎ»min DK â—¦ GD
4

tR
2t(v2 + 2RE) +
3
p
E = 12v 2pD + 24RpD

z(t) = E +

p

where
v2 =
and
R=

n

DK â—¦ GD

Îµ2 Î»2min

ÎµÎ»min

1

DK â—¦ GD

Setting v and R in previous equations gives an asymptotic
âˆš
 
âˆš
âˆš
n(12 2pD + 2t)
1
 +O
z(t) =
n
ÎµÎ»min DK â—¦ GD
Lemma 6 gives bound
kDâˆ’1 âˆ‡lk â‰¤

1
1/2
Î»min

DK â—¦ GD



From this bound Hoefdingâ€™s inequality [5] follows bound for kDâˆ’1 âˆ‡L(Î¸âˆ— )k.
Define additional Fisher matrix corresponded to the projection into first p elements of the parameter Î¸ (ref. for details in Section 3).
2
2
âˆ’2
2
DÌ†2 = DpÃ—p
âˆ’ DpÃ—âˆ
DâˆÃ—âˆ
DâˆÃ—p

such that
2

D =



2
2
DpÃ—p
DpÃ—âˆ
2
2
DâˆÃ—p DâˆÃ—âˆ



and define the gradient of the projection into first p elements of the parameter Î¸.
Ë˜ = âˆ‡1...p âˆ’ D2 Dâˆ’2 âˆ‡p...âˆ
âˆ‡
pÃ—âˆ âˆÃ—âˆ
Theorem 2. Let Î¸bp , Î¸pâˆ— âˆˆ IRp are the first p Fourier coefficients of the MLE and reference barycenâˆ—
Ë˜
)]). Then with probability (1 âˆ’ eâˆ’t ) W1 and
ters, and Z is a Gaussian vector N (0, Var[DÌ†âˆ’1 âˆ‡L(Î¸
probability distances to Z are bounded as follows
W1 (DÌ†(Î¸bp âˆ’ Î¸pâˆ— ), Z) â‰¤ Âµ3 O(log(n)) + â™¦(r, t)
and âˆ€z âˆˆ IR+

|IP (kD(Î¸b âˆ’ Î¸âˆ— )k > z) âˆ’ IP (kZk > z)| â‰¤ CA Âµ3 O(log2 n) + â™¦(r, t)
where â™¦(r, t) is defined in Theorem 1, CA = O(1/z) is anti-concentration constant defined in
Section 7 and
âˆš
4 2p
Âµ3 â‰¤ 1/2

Î»min DK â—¦ GD
Proof. Bind Theorems 1 and 11. Form Theorem 3 follows that the bound in Theorem 1 also holds
for projection of the parameter Î¸:
âˆ—
Ë˜
kDÌ†(Î¸bp âˆ’ Î¸pâˆ— ) âˆ’ DÌ†âˆ’1 âˆ‡L(Î¸
)k â‰¤ â™¦(r, t)

So with probability 1 âˆ’ eâˆ’t
W1 (DÌ†(Î¸bp âˆ’ Î¸pâˆ— ), Z) = min IEkDÌ†(Î¸bp âˆ’ Î¸pâˆ— ) âˆ’ Zk
b
Ï€(Î¸,Z)

5

âˆ—
Ë˜
â‰¤ W1 (DÌ†âˆ’1 âˆ‡L(Î¸
), Z) + â™¦(r, t)

Furthermore from Theorem 11 follows
âˆ—
Ë˜
W1 (DÌ†âˆ’1 âˆ‡L(Î¸
), Z) â‰¤

âˆš



p
2Âµ3 1 + log(2 tr{Î£}Âµ2 ) âˆ’ log(Âµ3 )

âˆ—
Ë˜
Ë˜ âˆ— âˆ’ Î¸i )
where Î£ = Var[DÌ†âˆ’1 âˆ‡L(Î¸
)] and setting Xi = DÌ†âˆ’1 âˆ‡l(Î¸

Âµ3 =

n
X

IEkÎ£

âˆ’1/2

(Xi âˆ’

Xi0 )kkÎ£ âˆ’1/2 Xi kkXi

âˆ’

Xi0 k

â‰¤ 4 max kXi k

i=1

n
X

IEXiT Î£ âˆ’1 Xi

i=1
n
X

(
IEXiT Î£Xi = tr Î£ âˆ’1

n
X

)
IEXi XiT

=p

i=1

i=1

Ë˜ âˆ— âˆ’ Î¸i )k â‰¤ kDâˆ’1 âˆ‡l(Î¸âˆ— âˆ’ Î¸i )k â‰¤
max kXi k = kDÌ†âˆ’1 âˆ‡l(Î¸

1
1/2
Î»min

DK â—¦ GD



Analogically one can make a consequence from Theorems 1 and 12. Let CA is the anti-concentration
constant of the distribution IP (kZk > x), then
|IP (kDÌ†(Î¸bp âˆ’ Î¸pâˆ— )k > z) âˆ’ IP (kZk > z)|
âˆ—
Ë˜
â‰¤ |IP (kDÌ†âˆ’1 âˆ‡L(Î¸
)k > z) âˆ’ IP (kZk > z)| + CA â™¦(r, t)

and
âˆ—
Ë˜
|IP (kDÌ†âˆ’1 âˆ‡L(Î¸
)k > z) âˆ’ IP (kZk > z)| â‰¤ CA Âµ3 O(log2 n)

As for the anti-concentration constant it can be estimated from Pinskerâ€™s inequality applied in the
next equation


Ë†
1
âˆ†
âˆ’xT Î£ âˆ’1 x/2
IP (kZk âˆˆ [z, z + âˆ†]) =
e
dx = O
log(tr{Î£})
(2Ï€ det Î£)1/2 z<kxk<z+âˆ†
z

3

Statistical learning theory

In this section we consider an infinite dimensional statistical model L(Î¸). Let parameter Î¸
consists of two parts (u, v), such that u = Î¸1...p âˆˆ IRp . Given a finite dataset we are going to
find MLE deviations basing on three assumptions listed below. Further we will specify these
assumptions for independent models and apply to barycenter model from the previous section.

3.1.

General approach

Let the Likelihood function L(Î¸) = L(Î¸, Y ) depends on parameters vector Î¸ = (u, v) and a
fixed dataset Y of size n. Denote the parameter MLE and refernce values:
Î¸b = argmax L(Î¸)
Î¸

Î¸âˆ— = argmax IEL(Î¸)
Î¸

We are going to study deviations of Î¸b and u in the following sense. For some matrix D and random
vector Î¾
âˆš
1. kÎ¸b âˆ’ Î¸âˆ— k is expected to be of order O(1/ n).
6

2. D(Î¸b âˆ’ Î¸âˆ— ) â‰ˆ Î¾
b âˆ’ L(Î¸âˆ— ) â‰ˆ kÎ¾k2 /2
3. L(Î¸)
Denote the stochastic part of the Likelihood
Î¶(Î¸) = L(Î¸) âˆ’ IEL(Î¸)
Involve the Fisher matrix
2

2

âˆ—

D = âˆ’âˆ‡ IEL(Î¸ ) =



2
Du2 Duv
2
Dvu
Dv2



2
= 0). One
It would be easier to deal with the model if matrix D2 has block-diagonal view (Duv
can make parameter replacement in order to satisfy to this condition. Define a new variable
Ï‘ = Ï‘(u, v) such that
âˆ‡Ï‘ âˆ‡Tu IEL(Î¸âˆ— ) = âˆ‡Ï‘ âˆ‡Tu IEL(Î¸âˆ— ) = 0

and
2
u,
Ï‘ = v + Dvâˆ’2 Dvu

or in other words the parameters transformation matrix is




I
0
I
0
âˆ’1
S=
, S =
2
2
Dvâˆ’2 Dvu
I
âˆ’Dvâˆ’2 Dvu
I
The gradient in the new coordinates (u, Ï‘) may be obtained by rule âˆ‡(u, Ï‘) = (S âˆ’1 )T âˆ‡(u, v). Use
Ë˜ for the first part of it
notation âˆ‡
Ë˜ = âˆ‡u (u, Ï‘) = âˆ‡u âˆ’ D2 Dâˆ’2 âˆ‡v
âˆ‡
uv v
The Fisher matrix after parameters replacement changes by rule D2 (u, Ï‘) = (S âˆ’1 )T D2 S âˆ’1 , so in
the new coordinates it has view
 2

DÌ†
0
2
2
âˆ—
âˆ—
D (u, Ï‘) = âˆ’âˆ‡ IEL(u , Ï‘ ) =
0 DÏ‘2
2
2
DÌ†2 = Du2 âˆ’ Duv
Dvâˆ’2 Dvu

Define a local region around point Î¸âˆ—
â„¦(r) = {Î¸ : kD(Î¸ âˆ’ Î¸âˆ— )k â‰¤ r}
Now we write down three conditions on the Likelihood derivatives essential for the deviations of
b The first and second conditions holds in the local region â„¦(r). The third condition is required
Î¸.
to make expansion of local statements to the whole parameter space IRâˆ . Further we will show
that from these conditions also follows deviation bounds of the parameter u
b or in other words
from deviations bound of Î¸b follows bound of u
b.
Assumption 1: In the region â„¦(r)
âˆ’Dâˆ’1 {âˆ‡IEL(Î¸) âˆ’ âˆ‡IEL(Î¸âˆ— )} âˆ’ D(Î¸ âˆ’ Î¸âˆ— ) â‰¤ Î´(r)r
Assumption 2: In the region â„¦(r) with probability 1 âˆ’ eâˆ’t
sup

Dâˆ’1 {âˆ‡Î¶(Î¸) âˆ’ âˆ‡Î¶(Î¸âˆ— )} â‰¤ z(t)r

Î¸âˆˆâ„¦(r)

Assumption 3: The Likelihood function is convex (âˆ’âˆ‡2 L(v) â‰¥ 0) or the expectation of Likelihood function is upper-bounded by a strongly convex function (IEL(v âˆ— ) âˆ’ IEL(v) â‰¥ br2v ).
Use notation
{Î´(r) + z(t)}r = â™¦(r, t)
7

Theorem 3. [9] Let the Likelihood function is convex (âˆ’âˆ‡2 L(v) â‰¥ 0) and for rv (assigned further)
Î´(r) + z(t) â‰¤ 1/2. Then under Assumptions 1, 2 with probability 1 âˆ’ eâˆ’t
r â‰¤ 4kDâˆ’1 âˆ‡L(Î¸âˆ— )k
kD(Î¸b âˆ’ Î¸âˆ— ) âˆ’ Dâˆ’1 âˆ‡L(Î¸âˆ— )k â‰¤ â™¦(r, t)
âˆ—
Ë˜
kDÌ†(b
u âˆ’ uâˆ— ) âˆ’ DÌ†âˆ’1 âˆ‡L(Î¸
)k â‰¤ â™¦(r, t)
b > L(Î¸âˆ— )) follows that the local region â„¦(r) that includes
Proof. From (âˆ’âˆ‡2 L(Î¸) â‰¥ 0) and (L(Î¸)
Î¸b should covers the next region
â„¦(r) âŠƒ {Î¸ : L(Î¸) â‰¥ L(Î¸âˆ— )}
Estimate the minimum possible radius of â„¦(r) that satisfy the previous condition.
1
1
0 â‰¥ L(Î¸âˆ— ) âˆ’ L(Î¸) = âˆ’(Î¸ âˆ’ Î¸âˆ— )T âˆ‡L(Î¸âˆ— ) âˆ’ (Î¸ âˆ’ Î¸âˆ— )T âˆ‡2 Î¶(Î¸0 )(Î¸ âˆ’ Î¸âˆ— ) + kD(Î¸0 )(Î¸ âˆ’ Î¸âˆ— )k2
2
2
z(t) 2 1 âˆ’ Î´(r) 2
r +
r
2
2
r(1 âˆ’ Î´(r) âˆ’ z(t)) â‰¤ 2kDâˆ’1 âˆ‡L(Î¸âˆ— )k

{with probability (1 âˆ’ eâˆ’t )} â‰¥ âˆ’kDâˆ’1 âˆ‡L(Î¸âˆ— )kr âˆ’

r â‰¤ 4kDâˆ’1 âˆ‡L(Î¸âˆ— )k
From Assumptions 1, 2 follows that
b âˆ’ âˆ‡L(Î¸âˆ— )}k â‰¤ â™¦(r, t)
kD(Î¸b âˆ’ Î¸âˆ— ) + Dâˆ’1 {âˆ‡L(Î¸)
kD(Î¸b âˆ’ Î¸âˆ— ) âˆ’ Dâˆ’1 âˆ‡L(Î¸âˆ— )k â‰¤ â™¦(r, t)
Not that for the coordinates transform S there is an invariant:
  âˆ’1




âˆ—
Ë˜
Ë˜
u âˆ’ uâˆ—
DÌ†
0
DÌ† 0
âˆ‡L(u,
Ï‘) âˆ’ âˆ‡L(u
, Ï‘âˆ— )
+
Ï‘ âˆ’ Ï‘s
0 DÏ‘âˆ’1
0 DÏ‘
âˆ‡Ï‘ L(u, Ï‘) + âˆ‡Ï‘ L(uâˆ— , Ï‘âˆ— )
= kD(Î¸ âˆ’ Î¸âˆ— ) + Dâˆ’1 {âˆ‡L(Î¸) âˆ’ âˆ‡L(Î¸âˆ— )}k
Since



DÌ†âˆ’1
0


  2
u
DÌ† 0
= Î¸T S T [(S âˆ’1 )T D2 (S âˆ’1 )]SÎ¸ = kDÎ¸k2
Ï‘
0 DÏ‘
  2
Ë˜
0
âˆ‡
= âˆ‡T S âˆ’1 [(S âˆ’1 )T D2 (S âˆ’1 )]âˆ’1 (S âˆ’1 )T âˆ‡ = âˆ‡T Dâˆ’2 âˆ‡
DÏ‘âˆ’1
âˆ‡Ï‘
 T  
Ë˜
u
âˆ‡
= âˆ‡T S âˆ’1 SÎ¸ = âˆ‡T Î¸
Ï‘
âˆ‡Ï‘

Subsequently basing on this invariant
âˆ—
Ë˜
kDÌ†(b
u âˆ’ uâˆ— ) âˆ’ DÌ†âˆ’1 âˆ‡L(Î¸
)k

â‰¤ kD(Î¸b âˆ’ Î¸âˆ— ) âˆ’ Dâˆ’1 âˆ‡L(Î¸âˆ— )k â‰¤ â™¦(r, t)

8

3.2.

Independent models

Consider independent models (models with independent observations) and obtain a simpler
variant of the Assumption 2 for this case. Involve three basic Lemmas for that.
Lemma 1 (Bernsteinâ€™s inequality [5]). Let X1 . . . Xn be independent real-valued random variables.
Assume that exist positive numbers v and R such that
2

v =

n
X

IEXi2

i=1

and for all integers q â‰¥ 3
n
X

IE [Xi ]q+ â‰¤

i=1

q! 2 qâˆ’2
v R
2

Then for all Î» âˆˆ (1, 1/R)
log IEeÎ»

P

i (Xi âˆ’IEXi )

â‰¤

v 2 Î»2
2(1 âˆ’ RÎ»)

Lemma 2 (Dudleyâ€™s entropy integral [5]). Let â„¦ be a finite pseudometric space and let f (Î¸)
(Î¸ âˆˆ â„¦) be a collection of random variables such that for some constants a, v, R > 0, for all
Î¸1 , Î¸2 âˆˆ â„¦ and all 0 < Î» < (Rd(Î¸1 , Î¸2 ))âˆ’1
v2 Î»2 d2 (Î¸1 , Î¸2 )
log IE exp{Î»(f (Î¸1 ) âˆ’ f (Î¸2 ))} â‰¤ aÎ»d(Î¸1 , Î¸2 ) +
2(1 âˆ’ RÎ»d(Î¸1 , Î¸2 ))
Then for any Î¸0 âˆˆ â„¦,
Ë†
IE[sup f (Î¸) âˆ’ f (Î¸0 )] â‰¤ 3ar + 12v
Î¸

r/2

Ë†
p
log N (Îµ, â„¦)dÎµ + 12R

r/2

log N (Îµ, â„¦)dÎµ

0

0

where r = supÎ¸âˆˆâ„¦ d(Î¸, Î¸0 ) and N (Îµ, â„¦) is covering number.
Lemma 3 (Bousquet inequality [5]). Consider independent random variables X1 . . . Xn and let
F : X â†’ R be countable set of functions that satisfy conditions IEf (Xi ) = 0 and kf kâˆ â‰¤ R.
Define
n
X
Z = sup
f (Xi )
f âˆˆF i=1

Let v2 â‰¥ supf âˆˆF

Pn

i=1

IEf 2 (Xi ) then with probability 1 âˆ’ eâˆ’t
Z < IEZ +

p
tR
2t(v2 + 2RIEZ) +
3

Apply three previous Lemmas in order to simplify Assumption 2 for independent models. Likelihood of an independent model is a sum of independent functions:
(L âˆ’ IEL)(Î¸) = Î¶(Î¸) =

n
X

Î¶i (Î¸)

i=1

Note that Î¶i depends from the implicit i-th element from the dataset, such that Î¶i (Î¸) = Î¶i (Î¸, Yi ).
Theorem 4. Let D2 be the Fisher matrix defined above and âˆ€Î¸ âˆˆ â„¦(r)
sup

n
X

IE(uT Dâˆ’1 âˆ‡2 Î¶i (Î¸)Dâˆ’1 u)2 â‰¤ v2

kuk=1 i=1

9

and
kDâˆ’1 âˆ‡2 Î¶i (Î¸)Dâˆ’1 k â‰¤ R
Then Assumption 2 fulfills inside â„¦(r) with probability 1 âˆ’ eâˆ’t and


p
tR
2
z(t) â‰¤ E + 2t(v + 2RE) +
3
where

12v
E=
r

Ë†

âˆ

0

Ë†
p
12R âˆ
2 log N (Îµ, â„¦(r))dÎµ +
2 log N (Îµ, â„¦(r))dÎµ
r 0

Proof. Set a random process for each i:
1
Xi (Î³, Î¸) = Î³ T {âˆ‡Î¶i (Î¸) âˆ’ âˆ‡Î¶i (Î¸âˆ— )}
r
Such that
X

sup
kDÎ³kâ‰¤r

Xi (Î³, Î¸) = kDâˆ’1 {âˆ‡Î¶(Î¸) âˆ’ âˆ‡Î¶(Î¸âˆ— )}k

i

âˆ—

âˆ€ fixed (Î³, Î¸) âˆˆ â„¦(r, 0) Ã— â„¦(r, Î¸ ) and kuk = 1:
sup IE
u

X
X1
(Î³âˆ‡2 Î¶(Î¸)T Dâˆ’1 u)2
(âˆ‡Î¸ Xi (Î³, Î¸)T Dâˆ’1 u)2 = sup IE
r
u
i
i
â‰¤ sup IE
u

X

(uT Dâˆ’1 âˆ‡2 Î¶(Î¸)T Dâˆ’1 u)2 â‰¤ v2

i

Analogically
sup IE
u

X

(âˆ‡Î³ Xi (Î³, Î¸)T Dâˆ’1 u)2 â‰¤ v2

i

âˆ€i âˆˆ 1, . . . , n :
kDâˆ’1 âˆ‡Xi (Î³, Î¸)k â‰¤ R
Apply Lemma 1 for the sum of random variables X(Î³, Î¸) =

P

i

Xi (Î³, Î¸) when (Î³, Î¸) are fixed.

log IE exp Î» (X(Î³1 , Î¸1 ) âˆ’ X(Î³2 , Î¸2 ))


= log IE exp Î» (Î³1 âˆ’ Î³2 )T âˆ‡Î³ X(Î³, Î¸) + log IE exp Î» (Î¸1 âˆ’ Î¸2 )T âˆ‡Î¸ X(Î³, Î¸)

â‰¤ sup log IE exp Î» kD(Î³1 âˆ’ Î³2 )kuT Dâˆ’1 âˆ‡Î³ X(Î³, Î¸)
u


+ sup log IE exp Î» kD(Î¸1 âˆ’ Î¸2 )kuT Dâˆ’1 âˆ‡Î¸ X(Î³, Î¸)
u

â‰¤

v2 Î»2 kD(Î³2 âˆ’ Î³1 )k2
v2 Î»2 kD(Î¸2 âˆ’ Î¸1 )k2
+
2(1 âˆ’ RÎ»kD(Î³2 âˆ’ Î³1 )k) 2(1 âˆ’ RÎ»kD(Î¸2 âˆ’ Î¸1 )k)

â‰¤

v2 Î»2 d212
2(1 âˆ’ RÎ»d12 )
d212 = kD(Î¸2 âˆ’ Î¸1 )k2 + kD(Î³2 âˆ’ Î³1 )k2

Denote
Î¥ = â„¦(r) Ã— â„¦(r)
such that log N (Îµ, Î¥ ) = 2 log N (Îµ, â„¦(r)). Then with Lemma 2 we obtain
Ë† âˆp
Ë† âˆ
IE sup X(Î³, Î¸) â‰¤ 12v
log N (Îµ, Î¥ )dÎµ + 12R
log N (Îµ, Î¥ )dÎµ
Î³,Î¸

0

0

Applying Lemma 3 to the random variable Z = supÎ³,Î¸ X(Î³, Î¸) completes the proof.
10

3.3.

Covering numbers and entropy

Below one can read a short excerpt about an entropy of ball and ellipsoid. The general formula
for the covering number N of a convex set â„¦ in Rp with an arbitrary distance d(Î¸1 , Î¸2 ) is
 p
volume(â„¦ + (Îµ/2)B1 ) 2
N (Îµ, â„¦) â‰¤
volume(B1 )
Îµ
where B1 is a unit ball.
Ball entropy: Let â„¦ = Br and d(Î¸1 , Î¸2 ) = kÎ¸1 âˆ’ Î¸2 k then

p
2
N (Îµ, B1 ) â‰¤ 1 +
Îµ
and since N (Îµr, Br ) = N (Îµ, B1 )
Ë†
Ë† âˆp
log N (Îµ, Br )dÎµ = r

1

p
log N (Îµ, B1 )dÎµ

0

0

âˆš
â‰¤r p

Ë†

1

p
âˆš
log(3/Îµ)dÎµ â‰¤ 1.42r p

0

Ë†

and

âˆ

log N (Îµ, Br )dÎµ â‰¤ 2.1rp
0

Ellipsoid entropy: Let â„¦ = Er (D) and d(Î¸1 , Î¸2 ) = kD(Î¸1 âˆ’ Î¸2 )k. The entropy in this case is rather
complicate in calculation. So we provide here only the the final statement from V. Spokoinyâ€™s
lecture notes [9].
s
Ë† âˆp
X logÎ± (Î»2 (D))
âˆš
i
log N (Îµ, Er (D))dÎµ . r Î± âˆ’ 1
2
Î»i (D)
0
i
Ë†

and

âˆ

log N (Îµ, Er (D))dÎµ . r

X

0

4

i

1
Î»i (D)

Barycenters model

We are going to show that Assumptions 1,2,3 are fulfilled for the barycenters model defined
in Section 2. Also we need to estimate â™¦(r, t). Remind that we deal with Likelihood function
L(Î¸) = L(Î¸, {Î¸i }ni=1 ) where implicit random vectors {Î¸i }ni=1 is a dataset of Fourier coefficients
corresponded to the random measures {Âµi }ni=1 .
Assumption 1:
kDâˆ’1 {âˆ‡2 IEL(Î¸) âˆ’ âˆ‡2 IEL(Î¸âˆ— )}Dâˆ’1 k â‰¤ kDâˆ’1 {âˆ‡3 IEL(Î¸)Dâˆ’1 }Dâˆ’1 kr
Let q(Î¸i ) be the distribution of each Î¸i then
3

âˆ‡ IEi L(Î¸ âˆ’ Î¸i ) =

n Ë†
X

3

âˆ‡ l(Î¸ âˆ’ Î¸i )q(Î¸i )dÎ¸i = âˆ’

i=1

kDâˆ’1 {âˆ‡3 IEL(Î¸)Dâˆ’1 }Dâˆ’1 k â‰¤

n Ë†
X

âˆ‡2 l(Î¸ âˆ’ Î¸i ) Ã— âˆ‡q(Î¸i )dÎ¸i

i=1

Ë†

kDâˆ’1 âˆ‡2 L(Î¸ âˆ’ Î¸x )Dâˆ’1 kkDâˆ’1 âˆ‡p(Î¸x )kdÎ¸x

and from the consequence of Theorem 7 one gets
kDâˆ’1 âˆ‡2 l(Î¸ âˆ’ Î¸i )Dâˆ’1 k â‰¤
11

ÎµÎ»min

1

DK â—¦ GD

âˆ’1

2

âˆ—

2

âˆ’1

kD {âˆ‡ IEL(Î¸) âˆ’ âˆ‡ IEL(Î¸ )}D k â‰¤

ÎµÎ»min

Subsequently
Î´(r) =

ÎµÎ»min

rn

DK â—¦ GD

rn

DK â—¦ GD

Ë†
kDâˆ’1 âˆ‡q(Î¸)kdÎ¸

Ë†
kDâˆ’1 âˆ‡q(Î¸)kdÎ¸

Assumption 2: From Theorem 4 follows that if:
Îµ2 Î»2min

n
 â‰¤ v2
DK â—¦ GD

ÎµÎ»min

1
 â‰¤R
DK â—¦ GD

and

then
z(t) â‰¤ E +

p

2t(v2 + 2RE) +

tR
3

where
E = 12v

p
2pD + 24R pD

and pD is ellipsoid entropy with matrix D
pD =

s
X log2 (Î»2 (D))
i
2
Î»i (D)

i

Assumption 3: Each model component l(Î¸ âˆ’ Î¸i ) is convex since
l(Î»Î¸1 + (1 âˆ’ Î»)Î¸2 âˆ’ Î¸i ) = l(Î»(Î¸1 âˆ’ Î¸i ) + (1 âˆ’ Î»)(Î¸2 âˆ’ Î¸i ))
= max hÎ·, Î»(Î¸1 âˆ’ Î¸i ) + (1 âˆ’ Î»)(Î¸2 âˆ’ Î¸i ))i
Î·âˆˆW2,1

â‰¤ max hÎ·, Î»(Î¸1 âˆ’ Î¸i )i + max hÎ·, (1 âˆ’ Î»)(Î¸1 âˆ’ Î¸i )i
Î·âˆˆW2,1

Î·âˆˆW2,1

= Î»l(Î¸1 âˆ’ Î¸i ) + (1 âˆ’ Î»)l(Î¸2 âˆ’ Î¸i )
Note that l2 is also convex as a composition of convex functions and the complete model L is convex
(âˆ‡2 L > 0) as a positive aggregation of convex functions. Combination of these assumptions is
b
used in the proof of Theorem 1 which gives the deviation bound of parameter Î¸.

5

Support functions

Bounds for the first and second derivatives of the Likelihood of barycenters model involves
additional theory from Convex analysis.
Def (*). Legendreâ€“Fenchel transform of a function f : X â†’ IR or the convex conjugate function
calls
f âˆ— (y) = sup(hx, yi âˆ’ f (x))
xâˆˆX

Def (s). Support function for a convex body E is
s(Î¸) = sup Î¸T Î·
Î·âˆˆE

Note that for indicator function Î´E (Î·) of a convex set E the conjugate function is support function
of E
Î´Eâˆ— (Î¸) = s(Î¸)
12

Def (âŠ•). Let f1 , f2 : E â†’ IR are convex functions. The infimal convolution of them is
(f1 âŠ• f2 )(x) =

inf (f1 (x1 ) + f2 (x2 ))

x1 +x2 =x

Lemma 4. [2] Let f1 , f2 : E â†’ IR are convex lower-semi-continuous functions. Then
(f1 âŠ• f2 )âˆ— = f1âˆ— + f2âˆ—
(f1 + f2 )âˆ— = f1âˆ— âŠ• f2âˆ—
Lemma 5. The support function of intersection E = E1 âˆ© E2 is infimal convolution of support
functions for E1 and E2
s(Î¸) = inf (s1 (Î¸1 ) + s2 (Î¸2 ))
Î¸1 +Î¸2 =Î¸

where
s1 (Î¸) = sup Î¸T Î·,

s2 (Î¸) = sup Î¸T Î·

Î·âˆˆE1

Î·âˆˆE2

Proof. Firstly
Î´E1 âˆ©E2 (Î·) = Î´E1 (Î·) + Î´E2 (Î·),
(Î´E1 + Î´E2 )âˆ— = Î´Eâˆ— 1 âŠ• Î´Eâˆ— 2
With additional property
intdom Î´E1 âˆ© dom Î´E2 = intE1 âˆ© E2 6= âˆ…
one have
(Î´E1 + Î´E2 )âˆ— = Î´Eâˆ— 1 âŠ• Î´Eâˆ— 2

Lemma 6. Let a support function s(Î¸) is differentiable, then its gradient lies on the border of
corresponded convex set E
âˆ‡s(Î¸) = Î·b(Î¸) âˆˆ âˆ‚E
where
Î·b(Î¸) = argmax Î· T Î¸
Î·âˆˆE

Proof. It follows from the convexity of E and linearity of optimization functional.
âˆ‚b
Î· (Î¸)
âˆ‚b
Î· (Î¸) T
=0â‡’
Î¸=0
âˆ‚kÎ¸k
âˆ‚Î¸
âˆ‚b
Î· (Î¸) T
âˆ‡s(Î¸) =
Î¸ + Î·b(Î¸) = Î·b(Î¸)
âˆ‚Î¸

13

Figure 2: Optimization related to support function.
Lemma 7. [2] Let f1 , f2 : E â†’ IR are convex continuous functions. Then the subdifferential of
their infimal convolution can be computed by following formula
[
âˆ‚(f1 âŠ• f2 )(x) =
âˆ‚f (x1 ) âˆ© âˆ‚f (x2 )
x=x1 +x2

Consequence. If in addition f1 , f2 are differentiable, then their infimal convolution is differentiable and âˆƒx1 , x2 : x = x1 + x2
âˆ‡(f1 âŠ• f2 )(x) = âˆ‡f1 (x1 ) = âˆ‡f2 (x2 )

There are
Lemma 8. Let f1 , . . . , fm : E â†’ IR are convex and two times differentiable functions.
P
t
=
1
following upper bound for the second derivative of the infimal convolution âˆ€t : m
i=1 i
âˆ‚âˆ‡T (f1 âŠ• . . . âŠ• fm )(x) 

m
X

t2i âˆ‡2 f (xi )

i=1

where

Pm

i=1

xi = x.

Proof. Use notation f = f1 âŠ• . . . âŠ• fm . Let
f (y) =

X

fi (yi )

i

According to Lemma 7 if all the functions are differentiable then
X
âˆ‡f (y) =
ti âˆ‡fi (yi )
i

From the definition âŠ• also follows that
f (y + z) â‰¤

X

fi (yi + ti z)

i

Make Tailor expansion for the left and right parts and account equality of the first derivatives.
X
z T âˆ‚âˆ‡T f (y + Î¸z)z â‰¤
t2i z T âˆ‡2 fi (yi + Î¸i z)z
i

Since direction z was chosen arbitrarily then dividing both parts of the previous equation by
kzk2 â†’ 0 we come to inequality
X
âˆ‚âˆ‡T f (y) 
t2i âˆ‡2 fi (yi )
i

14

Remark. One can find another provement of the similar Theorem in book [2] (Theorem 18.15).
Theorem 5. Let f1 , . . . , fm : E â†’ IR are convex and two times differentiable functions. There
are following upper bounds for infimal convolution f = f1 âŠ• . . . âŠ• fm derivatives âˆ€Î³ and some
matrix A
f (xi )
Î³ T âˆ‚âˆ‡T f (x)Î³ â‰¤ max Î³ T âˆ‡2 fi (xi )Î³
i
f (x)
and
Î³ T âˆ‚âˆ‡T f 2 (x)Î³ â‰¤ 2(Î³ T âˆ‡f (x))2 + 2 max Î³ T âˆ‡2 fi (xi )Î³f (xi )
i

Proof. Choosing appropriate {ti } in Lemma 8 one get the required upper bounds. Set
f (xi )
ti = Pm
j=1 f (xj )
and since

m
X

f (xj ) = f (x)

j=1

then
X
i

t2i Î³ T âˆ‡2 fi (yi )Î³ â‰¤ max ti Î³ T âˆ‡2 fi (yi )Î³ = max Î³ T âˆ‡2 fi (xi )Î³f (xi )
i

i

To prove the second formula apply this inequality in
âˆ‚âˆ‡T f 2 = 2âˆ‡f âˆ‡T f + 2f âˆ‚âˆ‡f

Consequence. Let s1 , . . . , sm : E âˆ— â†’ IR are support functions of the bounded convex smooth
sets E1 , . . . , Em . There is upper bound for the derivatives of support function s of intersection
E1 âˆ© . . . âˆ© Em , such that âˆ€i
Î³ T âˆ‚âˆ‡T s(x)Î³ â‰¤

maxi Î³ T âˆ‚Î·i /âˆ‚xi Î³si (xi )
s(x)

Î³ T âˆ‚âˆ‡T s2 (x)Î³ â‰¤ 2(Î³ T Î·i )2 + 2 max Î³ T âˆ‚Î·i /âˆ‚xi Î³si (xi )
i

Proof. It follows from Theorem 5 and Lemma 6.

15

6

Wasserstein distance as a support function

Def (W-dual). Consider two random variables X and Y âˆˆ Rp with densities Ï•X and Ï•Y . Define
Wasserstein distance in dual form between them as
W1 (Ï•X , Ï•Y ) =

{IEf (X) âˆ’ IEf (Y )}

max

âˆ€x:kâˆ‡f (x)kâ‰¤1

where âˆ€x : kâˆ‡f (x)k â‰¤ 1 means that function f is 1-Lipshits. Note that if Ï€(x, y) is a joint
distribution with marginals Ï•X and Ï•Y then this definition is equivalent to the original definition
W1 (Ï•X , Ï•Y ) = min IEkX âˆ’ Y k
Ï€

which follows from Kantorovich-Rubinstein duality [6]. Involve a normalized Fourier basis {Ïˆk (x)}kâˆˆNp
with scalar product Gram function G(x).
Def (W âˆ’ dual âˆ’ regularised). Consider two random variables X and Y âˆˆ Rp with densities Ï•X
and Ï•Y . Define a penalized Wasserstein distance between them in dual form as


Ë†
2
f (Ï•X , Ï•Y ) =
W
max
IEf (X) âˆ’ IEf (Y ) âˆ’ Îµ kâˆ‡f (x)k G(x)dx
âˆ€x:kâˆ‡f (x)kâ‰¤1

The regulariser term in this definition allows to bound the second derivative of the distance which
will be shown below. Wasserstein distance is a support function (ref. Def(s)) in Fourier basis. In
which connection
X
Î·k (f )Ïˆk (x)
f (x) =
k

Ë†

where
Î·k (f ) = hf, Ïˆk iG =

f (x)Ïˆk (x)G(x)dx

Now we can rewrite the expectation difference as
Ef (X) âˆ’ Ef (Y ) = hf,

Ï•X
Ï•Y
iG âˆ’ hf,
iG = hÎ·(f ), Î¸(Ï•X )i âˆ’ hÎ·(f ), Î¸(Ï•Y )i
G
G
Ë†

where
Î¸k (Ï•) =

Ï•(x)Ïˆk (x)dx

Define positive symmetric matrices
ï£« T
ï£¶
âˆ‡ Ïˆ1 (x)
ï£¬ ... ï£·

ï£· âˆ‡Ïˆ1 (x) . . . âˆ‡Ïˆk (x) . . . = (âˆ‡T Ïˆ(x))(âˆ‡Ïˆ T (x))
Kx = ï£¬
T
ï£­âˆ‡ Ïˆk (x)ï£¸
...
Ë†

and
K â—¦G=

Kx G(x)dx

Each Kx is positive, since Î· T Kx Î· = kâˆ‡f (x)k2 . Condition âˆ€x : kâˆ‡f (x)k â‰¤ 1 is equivalent in
Fourier basis to
ï£±
ï£¼
!2
ï£²
ï£½
\
X
T
Î·âˆˆ
Ex = Î· : âˆ€x :
Î·k âˆ‡Ïˆk (x) = Î· Kx Î· â‰¤ 1
ï£³
ï£¾
k

An important remark is that
\


Ex âŠ‚ Î· : Î· T (K â—¦ G)Î· â‰¤ 1

Finally we have come to the Wasserstein distance in Fourier basis.
16

Lemma 9. Let random vectors X and Y have densities Ï•X and Ï•Y with Fourier
T coefficients Î¸X
and Î¸Y , then the Wasserstein distance is the support function of the convex set Ex defined above,
i.e.
W1 (Ï•X , Ï•Y ) = max
T hÎ·, Î¸X âˆ’ Î¸Y i
Î·âˆˆ

Ex

As for regularised case
T
f1 (Ï•X , Ï•Y ) = max
W
T hÎ·, Î¸X âˆ’ Î¸Y i âˆ’ ÎµÎ· K â—¦ GÎ·
Î·âˆˆ

Ex

Remind that barycenters Likelihood consists of independent components li (Î¸ âˆ’ Î¸i ) with a random
vectors Î¸i âˆˆ IRâˆ and parameter Î¸ âˆˆ IRâˆ .
T
l(Î¸ âˆ’ Î¸i ) = max
T hÎ·, Î¸ âˆ’ Î¸i i âˆ’ ÎµÎ· K â—¦ GÎ·
Î·âˆˆ

Ex

Note that by definition the dual function of l is
lâˆ— (Î·) = Î´T Ex (Î·) + ÎµÎ· T K â—¦ GÎ·
Consequently from Lemma 4 follows that
T
âˆ—
âˆ—
l(Î¸ âˆ’ Î¸i ) = Î´T
Ex (Î¸ âˆ’ Î¸i ) âŠ• (ÎµÎ· K â—¦ GÎ·) (Î¸ âˆ’ Î¸i )

1
T
âˆ’1
= max
(1)
T hÎ·, Î¸ âˆ’ Î¸i i âŠ• (Î¸ âˆ’ Î¸i ) (K â—¦ G) (Î¸ âˆ’ Î¸i )
Î·âˆˆ Ex
Îµ

T
Apply Theorem 5 taking into account Ex âŠ‚ Î· : Î· T (K â—¦ G)Î· â‰¤ 1 , one gets the following bounds
on the derivatives of the function l.
Theorem 6. The gradient upper bounds for functions l and l2 are
kDâˆ’1 âˆ‡lk â‰¤

1
1/2
Î»min

kDâˆ’1 âˆ‡l2 (Î¸ âˆ’ Î¸i )k â‰¤

DK â—¦ GD



2k(K â—¦ G)âˆ’1/2 (Î¸ âˆ’ Î¸i )k

1/2
Î»min DK â—¦ GD

Proof. Denote
Î· âˆ— (Î¸) = argmax
Î·T Î¸
T
Î·âˆˆ

Ex

Use the equation (1). By the consequence of Lemma 7 and Lemma 6 âˆƒÎ¸0 :
âˆ‡l(Î¸ âˆ’ Î¸i ) = Î· âˆ— (Î¸0 )
Since k(K â—¦ G)1/2 Î· âˆ— k â‰¤ 1
kDâˆ’1 âˆ‡lk = kDâˆ’1 Î· âˆ— k = kDâˆ’1 (K â—¦ G)âˆ’1/2 (K â—¦ G)1/2 Î· âˆ— k â‰¤ kDâˆ’1 (K â—¦ G)âˆ’1/2 k
and from âˆ‡l2 = 2lâˆ‡l one gets
kDâˆ’1 âˆ‡l2 (Î¸ âˆ’ Î¸i )k â‰¤ 2l(Î¸ âˆ’ Î¸i )kDâˆ’1 âˆ‡lk â‰¤ 2k(K â—¦ G)âˆ’1/2 (Î¸ âˆ’ Î¸i )kkDâˆ’1 âˆ‡lk

Theorem 7. The second derivative upper bounds for functions l and l2 are
kDâˆ’1 âˆ‚âˆ‡T l(Î¸ âˆ’ Î¸i )Dâˆ’1 k â‰¤

1
minx Î»min (DKx D)k(K â—¦ G)âˆ’1/2 (Î¸ âˆ’ Î¸i )k

1
ÎµÎ»min (DK â—¦ GD)
2
kDâˆ’1 âˆ‚âˆ‡T l2 Dâˆ’1 k â‰¤
minx Î»min (DKx D)

kDâˆ’1 âˆ‚âˆ‡T l(Î¸ âˆ’ Î¸i )Dâˆ’1 k â‰¤

17

Remark. Matrix Kx may be singular which makes the first bound non-informative. The second
bound comes from the regulariser ÎµÎ· T K â—¦ GÎ· and has big coefficient (1/Îµ). It is a weak part of the
current theory and requires an improvement or an example which shows that this bound it tight.
Proof. Consider support function with one ellipsoid.
sx (Î¸) = max hÎ·, Î¸i = kKxâˆ’1/2 Î¸k
Î· T Kx Î·â‰¤1

Denote Î· âˆ— (Î¸) = argmaxhÎ·, Î¸i, Î· T Kx Î· â‰¤ 1.
Î· âˆ— (Î¸) =

Kxâˆ’1 Î¸
âˆ’1/2

kKx

Î¸k

âˆ‚Î· âˆ— (Î¸)
K âˆ’1 Î¸T Kxâˆ’1 Î¸ âˆ’ Kxâˆ’1 Î¸Î¸T Kxâˆ’1
= x
âˆ‚Î¸
(Î¸T Kxâˆ’1 Î¸)3/2
For some vector kÎ³k = 1 and property kak2 kbk2 â‰¥ (aT b)2
Î³ T Kxâˆ’1 Î³Î¸T Kxâˆ’1 Î¸ âˆ’ Î³ T Kxâˆ’1 Î¸Î¸T Kxâˆ’1 Î³ â‰¤ kKxâˆ’1 kÎ¸T Kxâˆ’1 Î¸
âˆ‚Î· âˆ— (Î¸)
kKxâˆ’1 k
â‰¤
âˆ‚Î¸
(Î¸T Kxâˆ’1 Î¸)1/2
Apply Theorem 5
kDâˆ’1 âˆ‚âˆ‡T l(Î¸ âˆ’ Î¸i )Dâˆ’1 k â‰¤ max Dâˆ’1
x

maxx kDâˆ’1 Kxâˆ’1 Dâˆ’1 k
âˆ‚Î·xâˆ— (Î¸xâˆ— ) âˆ’1 sx (Î¸xâˆ— )
D
â‰¤
âˆ‚Î¸
s(Î¸ âˆ’ Î¸i )
k(K â—¦ G)âˆ’1/2 (Î¸ âˆ’ Î¸i )k

The second bound for this norm follows directly from Lemma 8 and equation (1). Now consider
the squared Wasserstein distance (l2 ) which has a better derivative bound. From Theorem 5 one
gets
âˆ‚Î· âˆ— (Î¸xâˆ— )
kKxâˆ’1/2 Î¸xâˆ— kDâˆ’1
kDâˆ’1 âˆ‚âˆ‡T l2 Dâˆ’1 k â‰¤ 2 max Dâˆ’1 Î· âˆ— (Î¸xâˆ— )Î· âˆ— (Î¸xâˆ— )T Dâˆ’1 + Dâˆ’1
x
âˆ‚Î¸
Note that

âˆ‚Î· âˆ— (Î¸)
(Kxâˆ’1 Î¸)(Kxâˆ’1 Î¸)T
kKxâˆ’1/2 Î¸k = Kxâˆ’1 âˆ’
âˆ’1/2
âˆ‚Î¸
kKx Î¸k2
Î· âˆ— (Î¸)Î· âˆ— (Î¸)T +

âˆ‚Î· âˆ— (Î¸)
kKxâˆ’1/2 Î¸k = Kxâˆ’1
âˆ‚Î¸

Finally
kDâˆ’1 âˆ‚âˆ‡T l2 Dâˆ’1 k â‰¤ 2 max kDâˆ’1 Kxâˆ’1 Dâˆ’1 k
x

Remark. Note that the Wasserstein distance also may be differentiated directly. In paper [8] one
may find the lemma about directional derivative. For directions h1 , h2 it holds
d0W (Âµ1 , Âµ2 )(h1 , h2 ) =

max
(u,v)âˆˆÎ¦(Âµ1 ,Âµ2 )

âˆ’(hu, h1 i + hv, h2 i)

where
Î¦ = {(u, v) : hu, Âµ1 i + hv, Âµ2 i = dW (Âµ1 , Âµ2 ), âˆ€(x, y) : u(x) + v(y) â‰¤ kx âˆ’ yk}

18

7

Gaussian approximation

Def (Hk ). The multivariate Hermite polynomial Hk is defined by
Hk (x) = (âˆ’1)|k| ex

T Î£ âˆ’1 x/2

âˆ‚ |k|
T âˆ’1
eâˆ’x Î£ x/2
k
k
p
1
âˆ‚ ...âˆ‚

Lemma 10. Consider a Gaussian vector Z âˆ¼ N (0, Î£) and two functions h and fh such that
Ë† 1
fh (x) = âˆ’
IEh(Z(x, t))dt
0

âˆš
âˆš
h(Z(x, t)) = h( tx + 1 âˆ’ tZ) âˆ’ IEh(Z)
Then fh is a solution of the Stein equation
h(x) = (tr{âˆ‡2 Î£} âˆ’ xT âˆ‡)fh (x)
and
âˆ‚ |k|
fh (x) = âˆ’
âˆ‚ k1 . . . âˆ‚ kp

Ë†

1

0

|k|

1 t 2 âˆ’1
IEHk (Z)h(Z(x, t))dt
âˆ’1
2 (1 âˆ’ t) |k|
2

Consequence.
Ë†
2

2

âˆ‡ fh (x) âˆ’ âˆ‡ fh (y) = âˆ’
0

1

1
IEH2 (Z){h(Z(x, t)) âˆ’ h(Z(y, t))}dt
2(1 âˆ’ t)

where
H2 (Z) = (Î£ âˆ’1 Z)(Î£ âˆ’1 Z)T âˆ’ Î£ âˆ’1
= Î£ âˆ’1/2 {(Î£ âˆ’1/2 Z)(Î£ âˆ’1/2 Z)T âˆ’ I}Î£ âˆ’1/2

X=

n
X

Xi

i=1
2

IEh(X) = IE tr{âˆ‡ Î£}fh (X) âˆ’ IE

n
X

XiT âˆ‡fh (X)

i=1

=

n
X


IEXiT âˆ‡2 fh (X 0 + Î¸(Xi âˆ’ Xi0 )) âˆ’ âˆ‡2 fh (X 0 ) (Xi âˆ’ Xi0 )

i=1

=

n
X


IE(Î£ âˆ’1/2 Xi )T Î£ 1/2 âˆ‡2 fh (X 0 + Î¸(Xi âˆ’ Xi0 )) âˆ’ âˆ‡2 fh (X 0 ) Î£ 1/2 (Î£ âˆ’1/2 (Xi âˆ’ Xi0 ))

i=1

For a unit vector kÎ³k = 1 and conditional expectation IEâˆ’i = IE(Â·|Xi , Xi0 )

Î³ T IEâˆ’i Î£ 1/2 âˆ‡2 fh (X 0 + Î¸(Xi âˆ’ Xi0 )) âˆ’ âˆ‡2 fh (X 0 ) Î£ 1/2 Î³
Ë†
=
0

1

1
IEâˆ’i {((Î£ âˆ’1/2 Z)T Î³)2 âˆ’ 1}{h(Z(X 0 + Î¸(Xi âˆ’ Xi0 ), t)) âˆ’ h(Z(X 0 , t))}dt
2(1 âˆ’ t)
Ë† 1âˆ’Î±
Ë† 1
t1/2
1
â‰¤
Adt +
Bdt
1/2
2(1 âˆ’ t)
0
1âˆ’Î± (1 âˆ’ t)
âˆš
A
â‰¤ âˆ’ log(Î±) + 2B Î±
2
19






2B
â‰¤ A 1 + log
A
âˆš
âˆš
A = kXi âˆ’ Xi0 k IEâˆ’i |((Î£ âˆ’1/2 Z)T Î³)2 âˆ’ 1| kâˆ‡h( t(X 0 + Î¸1 (Xi âˆ’ Xi0 ) + 1 âˆ’ tZ)k
âˆš
âˆš
B = IEâˆ’i |((Î£ âˆ’1/2 Z)T Î³)2 âˆ’ 1| kZk kâˆ‡h( t(X 0 + Î¸(Xi âˆ’ Xi0 ) + Î¸2 1 âˆ’ tZ)k
Lemma 11 (Multivariate Berryâ€“Esseen TheoremPwith Wasserstein distance). Consider a sequence
of independent zero-mean random vectors X = ni=1 Xi in IRp with a covariance matrix
IEXX T = Î£
Then the Wasserstein distance between X and Gaussian vector Z âˆˆ N (0, Î£) has following upper
bound


p
âˆš
dW (X, Z) â‰¤ 2Âµ3 1 + log(2 tr{Î£}Âµ2 ) âˆ’ log(Âµ3 )
where
Âµ3 =

n
X

IEkÎ£ âˆ’1/2 (Xi âˆ’ Xi0 )kkÎ£ âˆ’1/2 Xi kkXi âˆ’ Xi0 k

i=1

Âµ2 =

n
X

IEkÎ£ âˆ’1/2 (Xi âˆ’ Xi0 )kkÎ£ âˆ’1/2 Xi k

i=1

Remark. In i.i.d case with Î£ = Ip

dW (X, Z) = O

p3/2 log(n)
âˆš
n



These is the same theorem with a different provement in paper [3].
Lemma 12 (Multivariate
Theorem). Consider a sequence of independent zero-mean
Pn Berryâ€“Esseen
p
random vectors X = i=1 Xi in IR with a covariance matrix
IEXX T = Î£
Let a function Ï• : IRp â†’ IR+ be sub-additive:
Ï•(x + y) â‰¤ Ï•(x) + Ï•(y)
and with Gaussian vector Z âˆˆ N (0, Î£) fulfills the anti-concentration property, such that
IP (Ï•(Z) > x) âˆ’ IP (Ï•(Z) > x + âˆ†) â‰¤ CA âˆ†
Then the measure difference between X and Gaussian vector Z has following upper bound âˆ€x
!
p


2
2IEÏ• (Z)Âµ2
4Âµ2
|IP (Ï•(X) > x) âˆ’ IP (Ï•(Z) > x)| â‰¤ 22CA Âµ3 log
log
C A Âµ3
20CA Âµ23
where
Âµ3 =

n
X

IEkÎ£ âˆ’1/2 (Xi âˆ’ Xi0 )kkÎ£ âˆ’1/2 Xi kÏ•(Xi âˆ’ Xi0 )

i=1

Âµ2 =

n
X

IEkÎ£ âˆ’1/2 (Xi âˆ’ Xi0 )kkÎ£ âˆ’1/2 Xi k

i=1

20

Proof. Define a smooth indicator function
ï£±
ï£´
t<x
ï£²0,
gx,âˆ† (t) = (t âˆ’ x)/âˆ†, t âˆˆ [x, x + âˆ†]
ï£´
ï£³
1,
t>x+âˆ†
Set h = gx,âˆ† â—¦ Ï•. Denote the required bound by Î´:
|IP (Ï•(X) > x) âˆ’ IP (Ï•(Z) > x)|
â‰¤ max |IEgx,âˆ† â—¦ Ï•(X) âˆ’ IEgx,âˆ† â—¦ Ï•(Z)| â‰¤ Î´
âˆ†

Note that from sub-additive property of the function Ï• follows


gx,âˆ† Ï•(X + dX) â‰¤ gx,âˆ† Ï•(X) + Ï•(dX)
and
0
gx,âˆ†
(t) =

and

1
1I[x < t < x + âˆ†]
âˆ†


1
IP (Ï•(Z) > x) âˆ’ IP (Ï•(Z) > x + âˆ†) â‰¤ CA
âˆ†
 2Î´
1
2Î´
0
IEgx,âˆ†
(Ï•(Z(X, t))) â‰¤
â‰¤ CA +
IP (Ï•(Z) > x) âˆ’ IP (Ï•(Z) > x + âˆ†) +
âˆ†
âˆ†
âˆ†
0
IEgx,âˆ†
(Ï•(Z)) =

IEâˆ’i h(Z(X 0 + Î¸(Xi âˆ’ Xi0 ), t)) âˆ’ IEâˆ’i h(Z(X 0 , t))


â‰¤ IEâˆ’i gx,âˆ† Ï•(Z(X 0 , t)) + Ï•(Xi âˆ’ Xi0 ) âˆ’ IEâˆ’i gx,âˆ† Ï•(Z(X 0 , t)

0
â‰¤ IEâˆ’i gx,âˆ†
Ï•(Z(X 0 , t)) + Î¸Ï•(Xi âˆ’ Xi0 ) Ï•(Xi âˆ’ Xi0 )


2Î´
Ï•(Xi âˆ’ Xi0 )
â‰¤ CA +
âˆ†
Analogically
0

0

IEh(Z(X , t)) âˆ’ IEh(Z(X + Î¸(Xi âˆ’

Xi0 ), t))


â‰¤

2Î´
CA +
âˆ†



Ï•(Xi âˆ’ Xi0 )

Apply this inequality denoting Îµ2 = (Î£ âˆ’1/2 Z)T Î³)2 âˆ¼ N 2 (0, 1)
IEâˆ’i {((Î£ âˆ’1/2 Z)T Î³)2 âˆ’ 1}{h(Z(X 0 + Î¸(Xi âˆ’ Xi0 ), t)) âˆ’ h(Z(X 0 , t))}
â‰¤ IEâˆ’i Îµ2 {gx,âˆ† [Ï•(Z(X 0 , t)) + Ï•(Xi âˆ’ Xi0 )] âˆ’ gx,âˆ† [Ï•(Z(X 0 , t))]}
+IEâˆ’i h(Z(X 0 , t)) âˆ’ IEâˆ’i h(Z(X 0 + Î¸(Xi âˆ’ Xi0 ), t))


2Î´
â‰¤ |Ï„ âˆ’ 1| CA +
Ï•(Xi âˆ’ Xi0 ) + IE 1I[Îµ2 > Ï„ ]Îµ2
âˆ†
Lemma 13. Let a random variable Îµ has a tail bound âˆ€x â‰¥ x0
IP (Îµ > h(x)) â‰¤ eâˆ’x
Then for a function g : IR+ â†’ IR+ with derivative g 0 : IR+ â†’ IR+
Ë† âˆ
âˆ’x0
IE 1I[Îµ > h(x0 )]g(Îµ) â‰¤ g(h(x0 ))e
+
eâˆ’x g 0 (h(x))h0 (x)dx
x0

21

In particular

Ë†
âˆ’x0

IE 1I[Îµ > h(x0 )]Îµ â‰¤ h(x0 )e

âˆ

eâˆ’x h0 (x)dx

+
Ë†

x0
âˆ

eâˆ’x h(x)râˆ’1 h0 (x)dx

IE 1I[Îµ > h(x0 )]Îµr â‰¤ h(x0 )r eâˆ’x0 + r
x0

For Îµ âˆ¼ N (0, 1) we have
IP (Îµ >

âˆš

2x) â‰¤ eâˆ’x

and by means of the previous lemma we get
IE 1I[Îµ2 > Ï„ ]Îµ2 = 2IE 1I[Îµ >

âˆš 2
Ï„ ]Îµ â‰¤ 2(Ï„ + 2)eâˆ’Ï„ /2

IEâˆ’i {((Î£ âˆ’1/2 Z)T Î³)2 âˆ’ 1}{h(Z(X 0 + Î¸(Xi âˆ’ Xi0 ), t)) âˆ’ h(Z(X 0 , t))}


2Î´
â‰¤ |Ï„ âˆ’ 1| CA +
Ï•(Xi âˆ’ Xi0 ) + 2(Ï„ + 2)eâˆ’Ï„ /2
âˆ†
We need also another upper bound for this expectation when t close to 1.
IEâˆ’i {((Î£ âˆ’1/2 Z)T Î³)2 âˆ’ 1}h(Z(X 0 , t))
âˆš
âˆš
âˆš
= IEâˆ’i {((Î£ âˆ’1/2 Z)T Î³)2 âˆ’ 1}{h( tX 0 + 1 âˆ’ tZ) âˆ’ h( tX 0 )}
âˆš
0
â‰¤ IEâˆ’i |((Î£ âˆ’1/2 Z)T Î³)2 âˆ’ 1| |gx,âˆ†
| Ï•( 1 âˆ’ tZ)
1p
â‰¤
2IEÏ•2 (Z)
âˆ†
Set âˆ† = Î´/(2CA )
B=

2CA p
2IEÏ•2 (Z) Âµ2
Î´

Set Ï„ = 2 log(4Âµ2 /(CA Âµ3 ))
A = 5|Ï„ âˆ’ 1|CA Âµ3 + 2(Ï„ + 2)eâˆ’Ï„ /2 Âµ2


4Âµ2
â‰¤ 11CA Âµ3 log
C A Âµ3
âˆš
A
log(Î±) + 2B Î± + CA âˆ†
2
â‰¤ 2A (1 + log(2BÎ´) âˆ’ log(Î´) âˆ’ log(A))

Î´â‰¤âˆ’

â‰¤ 2A (1 + log(2BÎ´) âˆ’ 2 log(A) + log log(2BÎ´) âˆ’ log log(A))
!
p


2IEÏ•2 (Z)Âµ2
4Âµ2
â‰¤ 22CA Âµ3 log
log
C A Âµ3
20CA Âµ23

Remark. In i.i.d case with Î£ = Ip and Ï†(x) = O(kxk)

|IP (Ï•(X) > x) âˆ’ IP (Ï•(Z) > x)| = O CA Âµ3 log2 (n)
Note that lemma 12 improves the classical Multivariate Berryâ€“Esseen Theorem [4] for the case of
sub-additive functions Ï†(x) = O(kxk). Namely it answers the open question â€œWhether one can
remove or replace the factor p1/4 by a better one (eventually by 1)â€.
22

Remark. In i.i.d case with Î£ = Ip and Ï†(x) = kxk, which is rather common is statistical learning
âˆš
theory, one have CA = O(1/ p) and
log2 (n)
|IP (kXk > x) âˆ’ IP (kZk > x)| = O p âˆš
n




***
The author thanks Prof. Roman Karasev, Prof. Vladimir Spokoiny and Prof. Dmitriy Dylov
for discussion and contribution to this paper.

References
[1] Martial Agueh and Guillaume Carlier. Barycenters in the Wasserstein space. SIAM Journal
on Mathematical Analysis, 43(2):904â€“924, 2011.
[2] Heinz H. Bauschke and Patrick L. Combettes. Convex Analysis and Monotone Operator
Theory in Hilbert Spaces. Springer Publishing Company, Incorporated, 1st edition, 2011.
[3] V. Bentkus. A new method for approximations in probability and operator theories. Lithuanian Mathematical Journal, 43(4):367â€“388, Oct 2003.
[4] V. Bentkus. On the dependence of the berryâ€“esseen bound on dimension. Journal of Statistical
Planning and Inference, 2003.
[5] Massart P. Boucheron S., Lugosi G. Concentration inequalities: A nonasymptotic theory of
independence. Oxford University Press, 2013.
[6] D.A. Edwards. On the kantorovichâ€“rubinstein theorem. Expositiones Mathematicae, 29(4):387
â€“ 398, 2011.
[7] Nicolas Papadakis JeÌreÌmie Bigot, Elsa Cazelles. Penalized barycenters in the wasserstein
space. arXiv:1606.01025, 2016.
[8] Axel Munk Max Sommerfeld. Inference for empirical wasserstein distances on finite spaces.
arXiv:1610.03287v2, 2016.
[9] V. Spokoiny. Nonparametric estimation: parametric view. 2016.
[10] Stefan Steinerberger. Wasserstein distance, fourier series and applications. arXiv:1803.08011,
2018.
[11] Anja Sturm Thomas Rippl, Axel Munk. Limit laws of the empirical wasserstein distance:
Gaussian distributions. arXiv:1507.04090v2, 2015.

23

