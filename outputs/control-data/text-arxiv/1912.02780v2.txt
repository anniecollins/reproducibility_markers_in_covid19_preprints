Automatic Differentiation for Second Renormalization of Tensor Networks

arXiv:1912.02780v2 [cond-mat.str-el] 4 Jul 2020

Bin-Bin Chen,1, 2 Yuan Gao,1 Yi-Bin Guo,3, 4 Yuzhi Liu,5 Hui-Hai Zhao,6
Hai-Jun Liao,3, 7 Lei Wang,3, 7 Tao Xiang,3, 4, 8 Wei Li,1, ‚àó and Z. Y. Xie9, ‚Ä†
1
School of Physics, Key Laboratory of Micro-Nano Measurement-Manipulation
and Physics (Ministry of Education), Beihang University, Beijing 100191, China
2
Physics Department, Arnold Sommerfeld Center for Theoretical Physics, and Center for NanoScience,
Ludwig-Maximilians-Universit√§t, Theresienstrasse 37, 80333 Munich, Germany
3
Institute of Physics, Chinese Academy of Sciences, P.O. Box 603, Beijing 100190, China
4
School of Physics, University of Chinese Academy of Sciences, Beijing 100049, China
5
Department of Physics, Indiana University, Bloomington, Indiana 47405, USA
6
Alibaba Quantum Laboratory, Alibaba Group, Beijing, China
7
Songshan Lake Materials Laboratory, Dongguan, Guangdong 523808, China
8
Kavli Institute for Theoretical Sciences, University of Chinese Academy of Sciences, Beijing 100190, China
9
Department of Physics, Renmin University of China, Beijing 100872, China
(Dated: July 7, 2020)

Tensor renormalization group (TRG) constitutes an important methodology for accurate simulations of
strongly correlated lattice models. Facilitated by the automatic differentiation technique widely used in deep
learning, we propose a uniform framework of differentiable TRG (‚àÇTRG) that can be applied to improve various TRG methods, in an automatic fashion. ‚àÇTRG systematically extends the essential concept of second
renormalization [PRL 103, 160601 (2009)] where the tensor environment is computed recursively in the backward iteration. Given the forward TRG process, ‚àÇTRG automatically finds the gradient of local tensors through
backpropagation, with which one can deeply ‚Äútrain‚Äù the tensor networks. We benchmark ‚àÇTRG in solving the
square-lattice Ising model, and demonstrate its power by simulating one- and two-dimensional quantum systems
at finite temperature. The global optimization as well as GPU acceleration renders ‚àÇTRG a highly efficient and
accurate manybody computation approach.

Introduction.‚Äî In the investigation of strongly correlated
quantum states and materials, tensor renormalization group
(TRG) constitutes a thriving field that is playing an increasingly important role recently. In the diverse family of TRG
approaches, there include the coarse-graining TRG [1, 2],
higher-order TRG (HOTRG) [3], and tensor network renormalization [4‚Äì7]. They have been put forward to evaluate
classical statistical systems as well as expectation values out
of two-dimensional (2D) tensor network states [8]. There are
also TRG methods developed to simulate d-dimensional quantum lattice models at finite temperature [9‚Äì17], whose Euclidean path integral constitutes a (d + 1)-dimensional worldsheet.
In the course of TRG process, environment of local tensors
should be taken into account for conducting a precise truncation, through, e.g., isometric renormalization transformations
in the tensor bases. This can be traced back to the renowned
density matrix renormalization group [18], where the effects
of environment are reflected in the reduced density matrix
of ‚Äúsystem" subblock. For generic tensor networks, second
renormalization group (SRG) has been proposed to improve
the process of tensor renormalization [3, 19‚Äì21]. In SRG, the
environment of local tensors is computed recursively, between
different scales of a hierarchical network, with which a global
optimization is feasible.
Recently, profound interplay between deep learning and
tensor network algorithms has raised great interest [22‚Äì30].

‚àó
‚Ä†

w.li@buaa.edu.cn
qingtaoxie@ruc.edu.cn

(b)

(a)
T
T

‚Ä†

T

T
‚Ä†

T

‚Ä†

T
‚Ä†

T

‚Ä†

T

(c)

FIG. 1. (Color online) (a) shows a TRG step where the scale transformations w are introduced along both directions, while only vertical
renormalization is involved in (b), which eventually compresses the
tensor network into a 1D structure that can then be contracted exactly. (c) plots the computational graph of the forward TRG process
as well as the backpropagation.

Among others, the differentiable programming is of particular
interest for tensor networks. Given the computational graph
generated in forward process, the gradient of corresponding
variables can be calculated through the chain rule of derivatives in the backpropagation, with which the neural network
can be deeply trained. The automatic calculation of gradients can be obtained within machine precision, and with same
computational complexity of forward process. Very recently,
this idea of differentiable programming that exploits a gradient based optimization, has been introduced to optimize tensor

2
networks [31].
In this work, we regard the renormalization transformation as input parameters of the TRG program, and point out
that the SRG backward iteration bears a correspondence with
the backpropagation algorithm in differentiable programming.
Inspired by this substantial connection, we turn the idea of
SRG into a generalized versatile framework, i.e., differentiable TRG (‚àÇTRG).
In ‚àÇTRG, the forward TRG process is made fully differentiable, and the renormalization transformations are optimized globally and automatically through the backpropagation. We apply ‚àÇTRG to simulate thermal equilibrium states
at finite temperature, and achieve significantly improved accuracy over previous methods [9, 16]. The efficiency is demonstrated by implementing ‚àÇTRG with PyTorch [32, 54], which
facilitates the GPU computing and shows a high performance
of about 40 times acceleration over a single CPU core.
Correspondence between SRG and backpropagation.‚Äî
Backpropagation is a widely used method for training deep
neural networks [33‚Äì36], where the gradients of parameters
can be computed through a reverse-mode automatic differentiation [56]. On the other hand, SRG plays a very similar role
in tensor-network algorithms as the backpropagation. To be
specific, as shown in Figs. 1(a,c), a hierarchical tensor network can be constructed
n o by piling up a series of isometric
RG transformations w(i) , with i = 1, 2, ..., n for each layer.
n o
In a well-designed TRG program, the input tensors w(i) are
successively applied to the tensors T and the output can be a
tensor trace in general, e.g., partition function of a statistical
system.
SRG takes then jobo to further optimize the renormalization
transformations w(i) in the backward iteration, by making
use of the environment. An adjoint tensor of w(i) at scale i is
‚àÇZ
defined as the gradient w(i) (‚â° ‚àÇw
(i) ), which can be related to
the environment through
1 (i)
w ,
Ni

(1)

where Ni denotes the times w(i) appearing in the network, and
Z the tensor trace to be maximized. It directly follows from
Eq. (1) that the recursive
n orelations used in the backward iteration to determine E (i) in SRG, can be recasted into the
derivative chain rule form, as depicted in Fig. 1(c). Remind
that the multiplications there with Jacobians ‚àÇT (i+1) /‚àÇT (i) and
‚àÇT (i+1) /‚àÇw(i) , etc, are conducted implicitly. They constitute
sequences of tensor contractions exactly equivalent to the recursive tensor contractions in SRG [56].
Differentiable tensor renormalization group.‚Äî Being
aware of the intimate relation between the backpropagation
and SRG, we now extend the latter to a more general and
flexible framework, ‚àÇTRG, with the help of well-developed
automatic differentiation packages [37], e.g., autograd [55]
and PyTorch [32, 54]. With these facilities, ‚àÇTRG can record
all operations performed on the input variables (tensors), and
compute the derivatives [e.g. Eq. (1)] automatically
n o in the
backward iterations, with which the parameters w(i) can be
optimized. Not limited within the original proposals [3, 19],

| f/f|

Ew(i) =

the idea of SRG can be applied to various TRG schemes
through the framework of differentiable programming. Below we consider two different ‚àÇTRG schemes following the
HOTRG [3] and exponential TRG (XTRG) [16], as shown in
Figs. 1(a) and (b), respectively.
Once the environment Ew(i) is obtained, one can optimize
(i)
w with resorting to, e.g., standard quasi-Newton optimization method [56], or quasi-optimal schemes through tensor
decompositions of Ew(i) [3, 19]. Remind that a tensor decomposition scheme that keeps w(i) isometric has been developed
in the context of multi-scale entanglement renormalization
ansatz (MERA) algorithm [38, 39], which are mainly adopted
in the simulations below. MERA update involves a singular
value decomposition (SVD) Ew = US V ‚Ä† and a replacement
w = UV ‚Ä† , which maximizes the cost function Z = Tr (Ew ¬∑ w),
with O(D4 ) time complexity. Here D is the geometric bond
dimension of a tensor.
Due to the intrinsic nonlinearility (in w) in the optimization
problem, ni inner iterations are introduced in a single step of
MERA update (ni = 5‚Äì10 in practice). Moreover, thanks to
the convenient access to Ew(i) , in ‚àÇTRG we can deeply optimize
the tensor network via sweep optimizations. In practice, we
scan from inner to outer layers n s times until the resultsncon-o
verge, thus assuring a highly accurate global update of w(i)
tensors.
‚àÇTRG of 2D Ising model.‚Äî As a first demonstration, we apply ‚àÇTRG, with two specific implementations in Figs. 1(a,b),
to solve the classical Ising model on the square lattice. Following the standard procedure, we can write down a square
tensor-network representation consisted of rank-4 tensors T ,
whose TRG contraction results in the partition function Z
[56].
In Fig. 1(a), after n steps of renormalizations, we obtain a

10

7

10

8

10

9

10

10

10

11

2.24

TRG, D = 24

64

128

HOTRG D = 24

HOSRG
D = 24

2.25

2.26

2.27
T

2.28

2.29

2.30

FIG. 2. (Color online) The relative errors of free energy |Œ¥ f / f | in the
vicinity of critical temperature T c , obtained from HOTRG, HOSRG,
and ‚àÇTRG calculations. We perform ni = 5‚Äì10 iterations within each
MERA update, and the total number of sweeps n s ranges from a few
iterations to a few hundreds, depending on the specific temperature
and scheme, until the relative errors reach a convergence criterion of
 = 10‚àí4 . The D = 24 ‚àÇTRG calculations follows the scheme in
Fig. 1(a), while D = 64 and 128 cases the scheme in Fig. 1(b).

3

FIG. 3. (Color online) (a) Comparisons of relative errors of free energy between linearized TRG (LTRG) and ‚àÇTRG. In the initial œÅ(œÑ)
of ‚àÇTRG, œÑ ' 5 √ó 10‚àí5 , which is also used as the Trotter slice in the
LTRG calculations. The results are shown with optimization depth
nd ‚â§ 4, ni = 10 in a single MERA update, and overall sweep iterations ns = 3. (b) Comparisons of the computational walltime of
‚àÇTRG on GPU and CPU with up to 16 cores, benchmarked on the
infinite XY chain. The calculations are carried out on Nvidia Tesla
V100 GPU and Intel Xeon 6230 CPU, with retained bond dimensions
up to D = 360. The dashed line depicts the scaling th ‚àº D4 .

single tensor representing the whole system of 2n √ó 2n sites (in
practice n = 25 guarantees the thermodynamic limit), whose
self-contraction leads to the partition function Z. On the other
hand, after n steps of renormalization, one arrives at an effective 1D system, whose complete contraction also leads to an
accurate measure of the partition function.
In Fig. 2, we show the accuracies of ‚àÇTRG implementations, together with the HOTRG and HOSRG data for comparisons. Owing to the sweep update, ‚àÇTRG leads to errors
clearly smaller than those of HOTRG, while achieving, as expected, the same accuracies as HOSRG [53].
The two schemes of ‚àÇTRG in Figs. 1(a) and (b) have different computational costs. The latter is considerably less
resource-demanding, i.e., O(D4 ) in computational time, while
it is O(D7 ) in Fig. 1(a). The memory costs are also dramatically different, i.e., O(D5 ) for Fig. 1(a) and O(D3 ) for (b).
Therefore, we can push the ‚àÇTRG simulations in Fig. 1(b)
with bond states up to D = 128, reaching much higher precision as shown in Fig. 2. From the comparison shown in Fig. 2,
as well as other considerations, we chose the ‚àÇTRG scheme in
Fig. 1(b) to simulate quantum models as presented below.
Infinite quantum XY chain.‚Äî Now we employ ‚àÇTRG to
simulate the exactly solvable quantum XY chain
X
HXY =
S ix S xj + S iy S yj .
(2)
hi, ji

We start with preparing the density matrix œÅ(œÑ) through (second order) Trotter-Suzuki decomposition [56]. A very small
imaginary-time step œÑ is chosen to ensure that Trotter errors
are negligible. Given the matrix product operator (MPO) representation of œÅ(œÑ), we proceed to cool down the system exponentially fast, with the ‚àÇTRG algorithm shown in Fig. 1(b).
The results are shown in Fig. 3(a), where the relative error
|Œ¥ f / f | curves rise up from very small values at high temperature and increase monotonically as T decreases.
In Fig. 3(a), benchmarking with the analytical solution

FIG. 4. (Color online) (a) Relative errors of transverse-field Ising
model on a 4 √ó 4 open square lattice, at critical transverse field
h = hc . (b) ‚àÇTRG results with various D are plotted versus optimization depths nd = 1, 2, 3 and 4. The comparison is at a fixed low
temperature Œ≤ ' 105, and the standard XTRG results are shown with
solid lines.

[40, 41, 56], we compare the relative errors |Œ¥ f / f | between
‚àÇTRG and LTRG, where the latter follows a cooling procedure linear in Œ≤(‚â° 1/T ) [9]. It is observed that ‚àÇTRG with
depth nd = 1 (i.e., optimizing exclusively the current layer in
the course of cooling) already outperforms LTRG in both efficiency and accuracy. By sweeping into nd (up to 4) layers, the
accuracy is found to improve continuously in the relatively
high to intermediate temperature regime due to better optimization. At low temperature, on the other hand, the enhancement of accuracy is marginal due to the limited expressibility
of the tensor network with a given bond dimension D = 32.
Therefore, we show also in Fig. 3(a) the results of larger bond
dimensions (up to 512), with a fixed depth nd = 4. There we
observe that |Œ¥ f / f | decreases monotonically and attains very
high accuracy, with relative error ‚àº 10‚àí7 at low temperature
(down to Œ≤ ' 400).
GPU acceleration.‚Äî We implement ‚àÇTRG with the PyTorch library, and take advantage of GPU computing to significantly accelerate the simulations. In Fig. 3(b), we show
the elapsed hours th versus D in the simulations of infinite
XY chain on GPU and CPU, respectively. To quantify the
speedup, th is monitored at Œ≤ = 12.8, where the computation
time falls well into a logarithmic scaling regime vs. Œ≤, i.e.,
th ‚àù ln Œ≤ [56].
From Fig. 3(b), we observe approximately 40 times GPU
acceleration (for D = 360 calculations), as compared to single core CPU calculations, and over 7 times speedup to the 16core parallel job. Moreover, in Fig. 3(b), the th curves show
algebraic scaling vs. D, i.e., th ‚àº DŒ≥ , for sufficiently large D
where Œ≥ values are found slightly less than 4. These appealing
benchmarks, together with previous tests in Ref. [42], suggest that GPU acceleration indeed constitutes a very promising technique to be fully explored in quantum manybody computations, particularly in tensor network simulations.
Thermodynamics of finite-size quantum lattice models.‚Äî
Now we apply ‚àÇTRG to finite-size chains and cylindrical geometries of finite width W (and length L), and try to approach the thermodynamics limit by increasing the system
size, which has been proved to be very successful in groundstate simulations of quantum frustrated magnets [44, 45].

4

0.5

0.2
0.4

(*) T = 0.36

0.6
0.0

0.2

1/W

(*)

()

0.6

0.0 (c)
0.4

0.70
0.75

0.6 (*)
0.8
0.2

QMC

( ) T = 0.61
(*) T = 0.36

0.0

0.2

1/W

()
1.0

L

L
W=4
W=6
W=8
W = 10
W = 12
(30 √ó 30)

hx = 1.0

Tc 0.42

(d)

L
W=4
W=6
W=8
W = 10

L

QMC

(30 √ó 30)

5.0 0.2

T

0.6

W=4
W = 6 0.4
W=8
W = 10
W = 12 0.2

0.0
hx = 1.5 0.4
0.3

cm (T)

u(T)

0.2

(b)

cm (T)

u(T)

0.0 (a) ( ) T = 0.61

0.2

W=4
W=6
W=8
W = 10

1.0
T

0.1
5.0

0.0

FIG. 5. (Color online) The plot show internal energy u(T ) and spin
specific heat cm (T ) of transverse field Ising model, with h x = 1 [(a,
b)] and h x = 1.5 [(c, d)]. ‚àÇTRG calculations are run on cylinders of
various widths W (with length L extrapolated to infinity). The energy data show excellent agreements with the quantum Monte Carlo
(QMC) result [43] on a 30 √ó 30 square lattice in both fields. For the
h x = 1 case, the peak position in cm (T ) provides an accurate estimate,
‚àº 1% relative error, of critical temperature.

Note that the sweep optimization needs to be adapted when
applied
n o to the finite-size systems, i.e., we not only scan
w(i) between different scales but also among different lattice
sites/bonds.
For a finite-size system on the 1D or 2D lattice, the high-T
density matrix œÅ(œÑ) can be initialized through a discretizationerror-free series expansion technique [46, 56]. It has been
shown to be preferable, over Trotter-Suzuki type initializations, in dealing 2D systems defined on, e.g., long cylinders
[16, 17]. The benchmark results on the finite-size XY chain
can be found in Supplementary Materials [56], where deep
optimization into nd layers gains remarkable improvement in
accuracy.
As a demonstration for 2D simulations, below we focus on
the transverse-field Ising model on a square lattice, i.e.,
X
X
H=
JS iz S zj ‚àí h x
S ix ,
(3)
hi, ji

i

where J = ‚àí1 (ferromagnetic) is set as the energy scale. The
model undergoes a magnetic order-disorder quantum phase
transition at a critical field hc ' 1.52219(1) [47]. Through a
snake-path mapping into quasi-1D lattice [17], the interaction
information of the Hamiltonian Eq. (3) on a width W cylinder can be encoded in a compact MPO of bond dimension
DH = W + 2 [16, 48]. Given the MPO representation of H,
‚àÇTRG works automatically and produces accurate results as
benchmarked below.
Firstly, we run ‚àÇTRG simulations on a 4√ó4 square lattice
at the critical transverse field h = hc , and compare the results
to exact diagonalization data. In Fig. 4(a), the relative errors

|Œ¥ f / f | are plotted vs. Œ≤. One can observe a high accuracy
with an optimization depth nd = 3, which continuously improves upon increasing the bond dimension D. Moreover, to
reveal the effects of nd , in Fig. 4(b) we show |Œ¥ f / f | vs. nd at
low temperature, as compared to XTRG data. Indeed the accuracy improves considerably, by orders of magnitude, as nd
increases. For example, the D = 64, nd = 4 ‚àÇTRG accuracy
even goes parallel with the D = 128 XTRG one.
Large-scale simulations and finite-temperature phase
transition.‚Äî Next, we conduct ‚àÇTRG calculations of quantum Ising model on cylinders with various widths W (up
to 12) and lengths L. The transverse field is first fixed at
h x = 1.0 ‚âà 2/3hc , giving rise to a spin order in low temperature. The long-range order melts at a critical temperature T c ' 0.42, through a second-order phase transition [13].
In Fig. 5, we retain only moderate bond dimension up to
D = 128 in the calculations, and the optimization depth is
up to nd = 4 layers. The internal energy u(T ) and magnetic
specific heat cm (T ) are computed from the first and second numerical derivatives of f (T ), respectively. Following the line
developed in XTRG [16, 17], we exploit a z-shift technique as
well as numerical interpolation to collect dense enough data
points and ensure a negligible differential error [56]. Moreover, to eliminate the finite-length effects, we perform an extrapolation to L = ‚àû, via linear fitting or energy subtraction
[56].
Collecting the extrapolated data at each width W, we compare the internal energy u(T ) with QMC data in Fig. 5(a). A
very good agreement of our cylindrical results with the largescale QMC data is obtained. The latter is computed on a
30 √ó 30 square lattice with periodic boundary condition (i.e.,
torus) which mimics the thermodynamic limit. Furthermore,
as shown in the inset, we zoom in at two selected temperatures and find there relative errors |Œ¥u/u| ‚àº 10‚àí3 (W = 12
result), with respect to QMC. In Fig. 5(b), by further taking
the derivatives of internal energy u(T ), we obtain the specific
heat curves cm (T ). It is observed that the peak in cm (T ) gets
sharper as W increases, signaling the existence of a phase transition, and the peak locations for wide cylinders are in very
good agreement with T c ' 0.42 in the thermodynamic limit.
In Fig. 5(c,d), we provide u(T ) and cm (T ) at transverse field
h x = 1.5 ' 0.99hc , in close vicinity of the quantum phase transition point. Again, u(T ) results are in excellent agreement
(|Œ¥u/u| ‚àº 10‚àí4 ) with the QMC data as shown in Fig. 5(c). It is
observed in Fig. 5(d) that the specific heat shows a round peak
at around T/J = 0.7, which has well converged vs. system
sizes and does not correspond to any phase transition, which
should occur at below T/J = 0.2.
Conclusion and outlook.‚Äî Inspired by the essential correspondence between the backpropagation and SRG of tensor
networks, we propose the framework of ‚àÇTRG. With ‚àÇTRG,
we make much better use of tensor parameters by increasing
the optimization depth, instead of merely enlarging parameter space dimension D. As a result, a moderate D can lead
to an unprecedented high accuracy in simulating thermodynamics of 2D quantum models. Bearing the virtue of SRG,
it can optimize both the wave function representation and the
renormalization transformations, globally and automatically.

5
Therefore, ‚àÇTRG constitutes a promising tool to investigate
very challenging manybody problems, e.g., frustrated antiferromagnets, fermionic Hubbard models, which are currently of
great research interest.
Acknowledgments.‚Äî We are indebted to Jian Cui, Jan von
Delft, Yannick Meurice, and Andreas Weichselbaum for helpful discussions. This work was supported by the National

Natural Science Foundation of China (Grant Nos. 11774420,
11834014, 11974036, and 11774398), the National R&D
Program of China (Grants Nos.
2016YFA0300503,
2017YFA0302900), German Research Foundation (DFG
WE4819/3-1) under Germany‚Äôs Excellence Strategy - EXC2111 - 390814868 and by the Research Funds of Renmin University of China (Grants No. 20XNLG19). Our code implementation in PyTorch is publicly available at this https URL.

[1] M. Levin and C. P. Nave, ‚ÄúTensor renormalization group approach to two-dimensional classical lattice models,‚Äù Phys. Rev.
Lett. 99, 120601 (2007).
[2] Z.-C. Gu and X.-G. Wen, ‚ÄúTensor-entanglement-filtering renormalization approach and symmetry-protected topological order,‚Äù Phys. Rev. B 80, 155131 (2009).
[3] Z. Y. Xie, J. Chen, M. P. Qin, J. W. Zhu, L. P. Yang, and T. Xiang, ‚ÄúCoarse-graining renormalization by higher-order singular
value decomposition,‚Äù Phys. Rev. B 86, 045139 (2012).
[4] G. Evenbly and G. Vidal, ‚ÄúTensor network renormalization,‚Äù
Phys. Rev. Lett. 115, 180405 (2015).
[5] G. Evenbly and G. Vidal, ‚ÄúTensor network renormalization
yields the multiscale entanglement renormalization ansatz,‚Äù
Phys. Rev. Lett. 115, 200401 (2015).
[6] S. Yang, Z.-C. Gu, and X.-G. Wen, ‚ÄúLoop optimization for
tensor network renormalization,‚Äù Phys. Rev. Lett. 118, 110504
(2017).
[7] M. Bal, M. Mari√´n, J. Haegeman, and F. Verstraete, ‚ÄúRenormalization group flows of Hamiltonians using tensor networks,‚Äù
Phys. Rev. Lett. 118, 250602 (2017).
[8] H. C. Jiang, Z. Y. Weng, and T. Xiang, ‚ÄúAccurate determination of tensor network state of quantum lattice models in two
dimensions,‚Äù Phys. Rev. Lett. 101, 090603 (2008).
[9] W. Li, S.-J. Ran, S.-S. Gong, Y. Zhao, B. Xi, F. Ye, and G. Su,
‚ÄúLinearized tensor renormalization group algorithm for the calculation of thermodynamic properties of quantum lattice models,‚Äù Phys. Rev. Lett. 106, 127202 (2011).
[10] P. Czarnik, L. Cincio, and J. Dziarmaga, ‚ÄúProjected entangled
pair states at finite temperature: Imaginary time evolution with
ancillas,‚Äù Phys. Rev. B 86, 245101 (2012).
[11] Y.-L. Dong, L. Chen, Y.-J. Liu, and W. Li, ‚ÄúBilayer linearized
tensor renormalization group approach for thermal tensor networks,‚Äù Phys. Rev. B 95, 144428 (2017).
[12] A. Kshetrimayum, M. Rizzi, J. Eisert, and R. Or√∫s, ‚ÄúTensor network annealing algorithm for two-dimensional thermal
states,‚Äù Phys. Rev. Lett. 122, 070502 (2019).
[13] P. Czarnik and J. Dziarmaga, ‚ÄúVariational approach to projected
entangled pair states at finite temperature,‚Äù Phys. Rev. B 92,
035152 (2015).
[14] P. Corboz, P. Czarnik, G. Kapteijns, and L. Tagliacozzo, ‚ÄúFinite
correlation length scaling with infinite projected entangled-pair
states,‚Äù Phys. Rev. X 8, 031031 (2018).
[15] P. Czarnik and P. Corboz, ‚ÄúFinite correlation length scaling with
infinite projected entangled pair states at finite temperature,‚Äù
Phys. Rev. B 99, 245107 (2019).
[16] B.-B. Chen, L. Chen, Z. Chen, W. Li, and A. Weichselbaum,
‚ÄúExponential thermal tensor network approach for quantum lattice models,‚Äù Phys. Rev. X 8, 031082 (2018).
[17] H. Li, B.-B. Chen, Z. Chen, J. von Delft, A. Weichselbaum, and
W. Li, ‚ÄúThermal tensor renormalization group simulations of
square-lattice quantum spin models,‚Äù Phys. Rev. B 100, 045110

(2019).
[18] S. R. White, ‚ÄúDensity matrix formulation for quantum renormalization groups,‚Äù Phys. Rev. Lett. 69, 2863‚Äì2866 (1992).
[19] Z. Y. Xie, H. C. Jiang, Q. N. Chen, Z. Y. Weng, and T. Xiang,
‚ÄúSecond renormalization of tensor-network states,‚Äù Phys. Rev.
Lett. 103, 160601 (2009).
[20] H. H. Zhao, Z. Y. Xie, Q. N. Chen, Z. C. Wei, J. W. Cai, and
T. Xiang, ‚ÄúRenormalization of tensor-network states,‚Äù Phys.
Rev. B 81, 174411 (2010).
[21] H.-H. Zhao, Z.-Y. Xie, T. Xiang, and M. Imada, ‚ÄúTensor network algorithm by coarse-graining tensor renormalization on
finite periodic lattices,‚Äù Phys. Rev. B 93, 125115 (2016).
[22] G. Carleo and M. Troyer, ‚ÄúSolving the quantum many-body
problem with artificial neural networks,‚Äù Science 355, 602‚Äì606
(2017).
[23] J. Carrasquilla and R. G. Melko, ‚ÄúMachine learning phases of
matter,‚Äù Nature Physics 13, 431‚Äì434 (2017).
[24] E. P. L. van Nieuwenburg, Y.-H. Liu, and S. D. Huber, ‚ÄúLearning phase transitions by confusion,‚Äù Nature Physics 13, 435‚Äì
439 (2017).
[25] E. Stoudenmire and D. J. Schwab, ‚ÄúSupervised learning with
tensor networks,‚Äù in Advances in Neural Information Processing Systems 29, edited by D. D. Lee, M. Sugiyama, U. V.
Luxburg, I. Guyon, and R. Garnett (Curran Associates, Inc.,
2016) pp. 4799‚Äì4807.
[26] S. Foreman, J. Giedt, Y. Meurice, and J. Unmuth-Yockey,
‚ÄúExamples of renormalization group transformations for image
sets,‚Äù Phys. Rev. E 98, 052129 (2018).
[27] Z.-Y. Han, J. Wang, H. Fan, L. Wang, and P. Zhang, ‚ÄúUnsupervised generative modeling using matrix product states,‚Äù Phys.
Rev. X 8, 031012 (2018).
[28] C. Guo, Z. M. Jie, W. Lu, and D. Poletti, ‚ÄúMatrix product
operators for sequence-to-sequence learning,‚Äù Phys. Rev. E 98,
042114 (2018).
[29] S.-H. Li and L. Wang, ‚ÄúNeural network renormalization group,‚Äù
Phys. Rev. Lett. 121, 260601 (2018).
[30] M. Koch-Janusz and Z. Ringel, ‚ÄúMutual information, neural
networks and the renormalization group,‚Äù Nature Physics 14,
578‚Äì582 (2018).
[31] H.-J. Liao, J.-G. Liu, L. Wang, and T. Xiang, ‚ÄúDifferentiable
programming tensor networks,‚Äù Phys. Rev. X 9, 031041 (2019).
[32] A. Paszke, G. Chanan, Z. Lin, S. Gross, E. Yang, L. Antiga,
and Z. Devito, ‚ÄúAutomatic differentiation in PyTorch,‚Äù in Conference on Neural Information Processing Systems (NIPS 2017)
(Long beach, CA, USA).
[33] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, ‚ÄúLearning
representations by back-propagating errors,‚Äù Nature (London)
323, 533‚Äì536 (1986).
[34] D. B. Parker, ‚ÄúLearning logic report,‚Äù (1985), MIT, TR-47.
[35] Y. LeCun, L. D. Jackel, B. Boser, J. S. Denker, H. P. Graf,
I. Guyon, D. Henderson, R. E. Howard, and W. Hubbard,

6

[36]
[37]
[38]
[39]
[40]
[41]

[42]

[43]

[44]

[45]

[46]

[47]
[48]

[49]

[50]
[51]

[52]

[53]

[54]
[55]
[56]

‚ÄúHandwritten digit recognition: applications of neural network
chips and automatic learning,‚Äù IEEE Communications Magazine 27, 41‚Äì46 (1989).
Y. Lecun, Y. Bengio, and G. Hinton, ‚ÄúDeep learning,‚Äù Nature
(London) 521, 436‚Äì444 (2015).
Numerical Optimization, 2nd ed., Springer Series in Operations
Research and Financial Engineering (New York).
G. Vidal, ‚ÄúEntanglement renormalization,‚Äù Phys. Rev. Lett. 99,
220405 (2007).
G. Evenbly and G. Vidal, ‚ÄúAlgorithms for entanglement renormalization,‚Äù Phys. Rev. B 79, 144108 (2009).
H.-H. Tu, ‚ÄúUniversal entropy of conformal critical theories on
a klein bottle,‚Äù Phys. Rev. Lett. 119, 261603 (2017).
L. Chen, H.-X. Wang, L. Wang, and W. Li, ‚ÄúConformal thermal tensor network and universal entropy on topological manifolds,‚Äù Phys. Rev. B 96, 174429 (2017).
A. Milsted, M. Ganahl, S. Leichenauer, J. Hidary, and G. Vidal, ‚ÄúTensorNetwork on TensorFlow: A Spin Chain Application
Usfing Tree Tensor Networks,‚Äù (2019), arXiv:1905.01331.
B. Bauer et al., ‚ÄúThe ALPS project release 2.0: open source
software for strongly correlated systems,‚Äù Journal of Statistical
Mechanics: Theory and Experiment 2011, P05001 (2011).
S. R. White and A. L. Chernyshev, ‚ÄúNe√©l order in square and triangular lattice heisenberg models,‚Äù Phys. Rev. Lett. 99, 127004
(2007).
Y.-C. He, M. P. Zaletel, M. Oshikawa, and F. Pollmann, ‚ÄúSignatures of dirac cones in a dmrg study of the kagome heisenberg
model,‚Äù Phys. Rev. X 7, 031020 (2017).
B.-B. Chen, Y.-J. Liu, Z. Chen, and W. Li, ‚ÄúSeries-expansion
thermal tensor network approach for quantum lattice models,‚Äù
Phys. Rev. B 95, 161104 (2017).
H. W. J. Bl√∂te and Y. Deng, ‚ÄúCluster Monte Carlo simulation of
the transverse ising model,‚Äù Phys. Rev. E 66, 066110 (2002).
H. Li, Y. D. Liao, B.-B. Chen, X.-T. Zeng, X.-L. Sheng,
Y. Qi, Z. Y. Meng, and W. Li, ‚ÄúKosterlitz-Thouless melting of magnetic order in the triangular quantum Ising material
TmMgGaO4 ,‚Äù Nat. Commun. 11, 1111 (2020).
E. Efrati, Z. Wang, A. Kolan, and L. P. Kadanoff, ‚ÄúReal-space
renormalization in statistical mechanics,‚Äù Rev. Mod. Phys. 86,
647‚Äì667 (2014).
K. G. Wilson, ‚ÄúThe renormalization group: Critical phenomena
and the Kondo problem,‚Äù Rev. Mod. Phys. 47, 773‚Äì840 (1975).
L. P. Kadanoff, ‚ÄúVariational principles and approximate renormalization group calculations,‚Äù Phys. Rev. Lett. 34, 1005‚Äì1008
(1975).
L. Chen, D.-W. Qu, H. Li, B.-B. Chen, S.-S. Gong, J. von Delft,
A. Weichselbaum, and W. Li, ‚ÄúTwo-temperature scales in the
triangular lattice Heisenberg antiferromagnet,‚Äù Phys. Rev. B 99,
140404(R) (2019) .
Here for the sake of computational cost, we exploit the MERA
update for w(i) at the first scale after the environment is obtained
through the backward process, while the rest w(i) on higher layers are update via HOTRG technique.
The official website of PyTorch is at https://pytorch.org.
See, e.g., https://github.com/HIPS/autograd.
In Supplemental Material, we briefly recapitulate the basic idea
of TRG in Sec. A, SRG in Sec. B, backpropagation in Sec. C,
and the Quasi-Newton optimization of isometries in Sec. D.
Detailed discussions on the correspondence between SRG and
backpropagation are provided in Sec. E, and the initialization of
œÅ(œÑ) in Sec. F. Besides, the analytical solutions (Sec. G), computational hours in infinite XY chain (Sec. H), finite XY chain
results (Sec. I), the z-shift technique (Sec. J), and the energy extrapolation in 2D transverse-field Ising model (Sec. K) are also

presented.

7
y

Supplemental Materials: Automatic
Differentiation for Second Renormalization of
Tensor Networks

0
T (i) x1

x1
w(i)

Œ±

Tensor network representation of the Ising model and its
renormalization

In this section, we briefly discuss some basic notions on
the real-space tensor renormalization group (TRG) methods,
which was referred to as rewiring method in literatures, e.g.,
Ref. [49]. For the sake of simplicity, we take the Ising model
on the square lattice as an example. Due to the locality of
the interaction, the partition function has a compact tensornetwork representation [20], i.e.,
Z=

X

e‚àíŒ≤H({œÉ}) =

{œÉ}

XY

T œÉ(a)i œÉ j œÉk œÉl

(A1)

a

{œÉ}

where {œÉ} denotes the classical spin configurations in the
Hamiltonian H. The tensor T (a) is defined on a local plaquette, labeled as a, containing the four original spins. The central
idea of renormalization group lies in the concept of renormalization transformation [50, 51], in which a set of new coupling
constants {K} is sought to represent the Hamiltonian as
H=

X

Ki s(a)
i ,

(A2)

y0
FIG. S1. Illustration of forward process in Eq. (B3). Note that in the
backward recursive equation Eq. (B4), the upper T (i) is denoted as
T (i;u) , and the lower one T (i;d) .

B.

Z=

XY
{Œ±}

Second renormalization group

In the following, we recapitulate SRG in the higherorder tensor renormalization group (HOTRG) algorithm, i.e.,
HOSRG [3, 21]. For a lattice system with translational invariance, T (i) is used to denote the local tensors renormalized
at the i-th scale. i = 1 denotes the initial scale at which the
original Hamiltonian is defined, and w(i) is the i-th isometric
renormalization transformation. As shown in Fig. S1, given
the renormalization transformations, we can perform a forward HOTRG iteration,
X

(i+1)
T Œ±Œ≤yy
0 =

jx1 x2 x10 x20

a,i

where s(a)
i defined at a larger scale is a ‚Äúblock spin", i.e., combination of original spins {œÉ} in the plaquette a.
Accordingly, in the rewiring method, TRG finds a set of
0
tensors {T (a ) } defined at plaquette a0 (thus at a larger length
scale) to represent the partition function, i.e.,
0

T Œ±(ai Œ±)j Œ±k Œ±l ,

a0

where the tensors T are obtained following a similar line of
Kadanoff‚Äôs RG transformation: We introduce new statistical
variables {Œ±} (as geometric indices of tensors) and then trace
out the original variables {œÉ}. TRG methods provide accurate
tool and versatile platform for studying the conformal criticality and universality near phase transition temperatures [49].
In TRG, the transformations between T œÉi œÉ j œÉk œÉl at small
length scale and S Œ±i Œ± j Œ±k Œ±l at a larger one constitute the most
important parameters of the program. Once the transformations are given, they renormalize the system and result in the
partition function Z. In most cases, approximations have to
be introduced in the course of TRG, while causing minimal
loss in the partition function. We point out that, while most
TRG programs employs renormalization transformations that
are found only locally [1, 2, 4, 6, 7], the second renormalization group (SRG) [3, 19‚Äì21] manages to find proper transformations that optimizes the partition function globally.

Œ≤

0
T (i) x2

x2
A.

w(i)

j

(i)
T x(i)1 x0 y j T x(i)2 x0 jy0 w(i)
x1 x2 Œ± w x0 x0 Œ≤
1

2

(B3)

1 2

where the subscripts x1 , x2 , Œ± of w(i)
x1 x2 Œ± can be understood as
the new statistical variables introduced at different scales, with
which the Hamiltonian can be written down, c.f., Eq. (A2).
The transformations w(i) between two scales i and i + 1 are
determined by the local higher-order singular value decompositions according to tensors T (i) .
In the forward HOTRG iteration described above, the transformations w(i) is determined locally [3]. To achieve a global
optimization, the environment tensor E should be considered.
We punch a ‚Äúhole‚Äù by removing the target tensor in the tensor
network and contract all the indices except those of the target.
A backward iteration should be involved to accomplish this
task and perform the global optimization.
For example, the environment of T (i) at the target scale i can
be obtained exploiting the following recursive relation
E (i;u)
=
x1 x0 y j
1

X
Œ±Œ≤y0 x2 x20

(i+1) (i;d)
(i)
(i)
EŒ±Œ≤yy
0 T x x0 jy0 w x1 x2 Œ± w x0 x0 Œ≤
2
2

(B4)

1 2

where the superscripts Œª = (u, d) denote the upper and lower
T (i) in Fig. S1, and environment EP(i) is averaged over two
equivalent environment, i.e., E (i) = Œª E (i;Œª) /2, with Œª = u, d.
Given the environment tensor E (i) , we can update the renormalization transformation w(i) that globally optimizes the partition function.

8
C.

Backpropagation in neural networks

Generally speaking, a deep feedforward neural network sets
up a mapping between a set of input signals x(1) , such as images, and a set of output signals y, say, categories, through
a multi-layer transformation F , i.e., y = F (x(1) ), where F
is represented as a composition of many arranged linear (L)
transformations separated by nonlinear (N) mappings. To be
specific, an n-layer neural network F can be expressed as

 
compute exp B(i) numerically and then truncate this unitary
matrix (of dimension D2 , see main text) into a D2 √óD isometry
w(i) .
Note the computational cost of this ‚Äútailored" quasi-Newton
approach is of time complexity O(D6 ), higher than the MERA
update O(D4 ) that we used in the main text.

E.

F = N (n) L(n) ...N (2) L(2) N (1) L(1)

(C5)

where the linear transformations L‚Äôs contain most of the variational parameters œâ‚Äôs that need to be optimized. Roughly
speaking, the major goal of training a neural network is to find
the optimal œâ‚Äôs which minimize the target objective function
L characterizing the discrepancy between the actual and predicted labels.
In essence, each layer of the neural network, i.e., L(i)
followed by N (i) , perform the transformations on the input
data/feature x(i) , and its output x(i+1) can be regarded as a new
representation and a higher-level abstraction of original input
[29, 30]. This is very similar to the renormalization transformation of TRG described in the main text, where new bases
are introduced in different scales to represent the original partition function.
To train the neural network, the gradients of the objective
function L with respect to the parameters œâ‚Äôs are required for
a global optimization. The backpropagation algorithm, arguably the most successful approach for training deep neural
networks, exploits the chain rule of derivatives to efficiently
compute these gradients [33‚Äì36]. To be concrete, the backpropagation method relates the gradients in two neighboring
layers as follows
‚àÇL
‚àÇL
‚àÇx(i+1)
= (i+1) ¬∑
.
(i)
‚àÇx
‚àÇx
‚àÇx(i)

(C6)

In deep learning, automatic differentiation (AD) technique
is employed to compute the derivative ‚àÇx‚àÇL(i) following the recursive equation Eq. (C6), in an analytically rigorous way. Furthermore, the derivatives with respect to the parameters œâ(i‚àí1)
(i)
can be similarly obtained via one more AD step through ‚àÇœâ‚àÇx(i‚àí1) ,
with which one can update œâ(i‚àí1) and regenerate x(i) in the i-th
layer.
D.

Quasi-Newton optimization of isometries

Given the gradient
=
[see Eq. (1) in the main
text], the quasi-Newton approach constitutes a class of efficient algorithm for parameter optimization. Here in ‚àÇTRG, to
impose the isometric
 constraint, we choose the parametrization w(i) = exp B(i) , where B(i) is an anti-symmetric real
square matrix of dimension D2 .
With this parameterization, the standard gradient based LBFGS method [37] can be employed to update B(i) so as to
maximize Z. After an exponential operation, we can then
Ew(i)

1 (i)
Ni w

Details on the correspondence between SRG and
backpropagation

Suppose the tensor elements in w(i) s are independent variational parameters, we can bridge SRG and backpropagation
approach, in the context of higher-order tensor renormalizations, by comparing Eq. (B4) and Eq. (C6).
From SRG to backpropagation: Firstly, we assert the following formula which equals ‚Äúpunching a hole" in the tensor
network to the derivative
(i;Œª)
Eabcd

‚àÇZ
1
¬∑
=
=
Ni ‚àÇT (i;Œª)
abcd

P

j

(i, j;Œª)
Eabcd
,
Ni

(E7)

where 2Ni = 2(n‚àíi+1) is the number of T (i) copies (labeled by
index j) in the partition function tensor network, i.e., Z =
 ‚äó2Ni
Tr T (i)
. Matter of fact, the equation
Z=

X

(i, j;Œª) (i, j;Œª)
Eabcd
T abcd

abcd

holds at all scales i and location j. Next, we point out that
the tensor contractions in Eq. (B4) is nothing but multiplying
Jacobian to the tensor E (i+1) , i.e.,
(i+1)
‚àÇT Œ±Œ≤yy
0

‚àÇT x(i;u)
0
1x zj

= Œ¥yz ¬∑

X

(i)
(i)
T x(i;d)
0 0 w x1 x2 Œ± w x0 x0 Œ≤ ,
2 x jy
2

x2 x20

1

(E8)

1 2

which is the derivative of T (i+1) with respect to T (i;u/d) , selfevident in Fig. S1. Combining Eq. (E7) and Eq. (E8) together,
we can rewritten Eq. (B4) as
‚àÇZ
(i)
‚àÇT abcd

=

X

‚àÇZ

xyzw

(i+1)
‚àÇT xyzw

¬∑

(i+1)
‚àÇT xyzw
(i)
‚àÇT abcd

(E9)

P
(i+1)
(i+1)
where ‚àÇT‚àÇT (i) = Œª ‚àÇT
is assumed.
‚àÇT (i;Œª)
Therefore, we reach the conclusion that the recursive relation of environment in the SRG backward iteration, as
expressed in Eq. (B4), is exactly the derivative chain rule
Eq. (C6) in backpropagation method of deep learning. In
Eq. (E9), we have customized the backpropogation with the
tensor network context to emphasize the in-depth link between the two.
Moreover, to show the correspondence in a more intuitive
way, we introduce the following notations

9

c

b  E (i) = 1 ¬∑ ‚àÇZ ,
abcd
2Ni ‚àÇT (i)
abcd
d

a

z

x

y

 E (i+1)
xyzw =

w
and represent the Jacobian

x

z=c
b
a
d

‚àÇT (i+1)
‚àÇT (i)

1
‚àÇZ
,
¬∑
(i+1)
Ni ‚àÇT xyzw

(E10)

in Eq. (E8) as

z
y

+

x

c

y


a
b
w=d

w

Then we can picturize Eq. (E9) as
Ô£Æ
Ô£ØÔ£Ø
1 Ô£ØÔ£ØÔ£ØÔ£Ø
= √ó Ô£ØÔ£ØÔ£Ø
+
2 Ô£ØÔ£∞

(i)
‚àÇT abcd

.

(E11)

HXY =

Ô£π
Ô£∫Ô£∫Ô£∫
Ô£∫Ô£∫Ô£∫
Ô£∫Ô£∫Ô£∫
Ô£∫Ô£ª

+

(E12)
where the open indices represent identities, same as those in
Eq. (E10) and Eq. (E11).
From backpropagation to SRG: Now we start from Eq. (E9)
and try to recover the SRG operation. Given that the tensor network derivative equals environment tensor E (i) (up to
a factor), the key step in backpropagation approach is also the
computing of Jacobian ‚àÇT (i+1) /‚àÇT i . In AD technique, this is
implicitly expressed as a sequence of tensor contractions exactly reverse the forward process, as also shown in Eq. (E11)
(while from right hand side to left). That is to say, the Jacobian
(i)
(i)
is computed in AD by contracting T x(i;d)
0 0 , w x1 x2 Œ± , w x0 x0 Œ≤ with
2 x jy
2

FIG. S2. (Color online) Elapsed hours th scaling versus Œ≤, where a
logarithmic scaling, i.e., th ‚àº ln Œ≤ can be seen in both GPU and CPU
(1, 4, and 16 cores) runs.

To perform the Trotter-Suzuki decomposition, we rewrite
the Hamiltonian

Ô£π
Ô£∫Ô£∫Ô£∫
Ô£∫Ô£∫Ô£∫
Ô£∫Ô£∫Ô£∫
Ô£∫Ô£ª

Ô£Æ
Ô£ØÔ£ØÔ£Ø
Ô£ØÔ£Ø
√ó Ô£ØÔ£ØÔ£ØÔ£Ø
Ô£ØÔ£∞

1
= √ó
2

(i+1)
‚àÇT xyzw

1 2

the derivative ‚àÇZ/‚àÇT (i+1) = Ni+1 E (i+1) from the (i + 1)-th layer.
Therefore, we again arrive at Eq. (E12) and confirm that the
recursive relation used in SRG is equivalent to the chain-rule
AD procedure in backpropagation.

X

hi,i+1 =

X

y
x
S ix S i+1
+ S iy S i+1
,

as
HXY = Ho + He ,
P
where Ho(e)
=
i‚ààodd(even) hi,i+1 contains odd(even)
terms. Therefore, up to O(œÑ3 ) Trotter error, e‚àíœÑHXY =
œÑ
œÑ
e‚àí 2 Ho e‚àíœÑHe e‚àí 2 Ho . With sufficiently small œÑ, e.g., œÑ ' 5 √ó 10‚àí5
in Fig. 3 of the main text, the Trotter error has been very
well-controlled in practice.
On the other hand, for finite-size systems, including 2D systems mapped into quasi-1D chains with ‚Äúlong-range" interactions, we employ the series expansion of the density matrix

e‚àíœÑH =

Nc
X
(‚àíœÑ)n
n=1

F.

Initialization of œÅ(œÑ)

In the simulation of quantum lattice models, we start from
high temperature density operator œÅ(œÑ), with a given manybody Hamiltonian H. There are two ways preparing œÅ(œÑ)
and obtaining its matrix product operator (MPO) representation. For infinite 1D quantum chains, we can employ a
Trotter-Suzuki decomposition of œÅ(œÑ) = e‚àíœÑH , while for an
finite-size system the series expansion technique offers us a
discretization-error-free approach to prepare the MPO œÅ(œÑ).
Given the initial œÅ(œÑ), we can perform successively the exponential cooling procedure down to the require low temperature.

(F13)

i

i

n!

Hn,

(F14)

to realize an initialization of œÅ(œÑ). By retaining sufficient
large Nc , Eq. (F14) is free of any essential expansion error.
Therefore, given an MPO representation of H, the density matrix œÅ(œÑ) can be computed via the series-expansion machinery
[46], for both 1D and 2D finite-size systems.
Given the MPO representation of initial œÅ0 (œÑ) at high temperature, the system can be cooled down linearly (LTRG) [9]
or exponentially (XTRG) [13, 16] along the Œ≤ axis. Due to the
much fewer truncation steps, it has been shown that XTRG
constitutes a more accurate way of thermodynamic simulations [16, 17, 52] and is thus adopted in the current work of
‚àÇTRG.

10
G.

Exact solution of quantum XY chain at finite temperature

We hereby provide the exact expression of partition function for 1-D quantum XY chain,
N 
 JX

y
‚àí
+
x
=
S i+ S i+1
+ S i‚àí S i+1
S ix S i+1
+ S iy S i+1
2 i=1
i=1
(G15)
with the periodic boundary condition S ¬±N+1 = S 1¬± . Exploiting
the Jordan-Wigner transformation
Ô£±
P
+
‚àíiœÄ k< j c‚Ä†k ck ‚Ä†
Ô£¥
Ô£¥
S
=
e
cj,
Ô£¥
j
Ô£¥
Ô£¥
P
Ô£≤ ‚àí
iœÄ k< j c‚Ä†k ck ‚Ä†
(G16)
S
=
e
c
Ô£¥
Ô£¥
j
j,
Ô£¥
Ô£¥
Ô£¥
Ô£≥S z = c‚Ä† c j ‚àí 1 ,
j
j
2

H=J

N 
X

the Hamiltonian can be expressed as a spinless fermionic
tight-binding chain,
Ô£´N‚àí1
Ô£∂
Ô£∑Ô£∑Ô£∑
J Ô£¨Ô£¨Ô£¨Ô£¨X ‚Ä†
‚Ä†
H = Ô£¨Ô£¨Ô£≠ ci ci+1 ‚àí QcN c1 Ô£∑Ô£∑Ô£∑Ô£∏ + h.c.
(G17)
2 i=1
PN

‚Ä†

with the parity Q ‚â° e‚àíiœÄ j=1 c j c j being a conserved quantity.
The Hilbert space then splits into two independent sectors:
Q = 1 (even particle number) sector; Q = ‚àí1, (odd particle
number )sector, and the Hamiltonian can be expressed as,
H=

N
JX ‚Ä†
c ci+1 + h.c.,
2 i=1 i

(G18)

with periodic(anti-periodic) boundary condition cN+1 = ¬±c1
in even(odd) sector. Through Fourier transformation c j =
‚àö P
1/ N q e‚àíiq j cq , the Hamiltonian gets diagonalized into,
X
X
H=J
cos (q) c‚Ä†q cq =
q c‚Ä†q cq ,
(G19)
q

q

where k‚Äôs are summed over modes q = q+ ‚â° 2œÄn/N in even
sector while over q = q‚àí ‚â° 2œÄ(n + 1/2)/N in odd sector, with
n = 0, 1, ¬∑ ¬∑ ¬∑ , (N ‚àí 1), to cope with corresponding boundary
conditions.
With even particle number constraint in Q = 1 sector, the
many-body states with odd numbers of modes should be excluded, when calculating the partition function [40, 41]. Thus,
the partition function Z+ in even sector is,
1Y
1Y
(1 + e‚àíŒ≤q+ ) +
(1 ‚àí e‚àíŒ≤q+ ).
(G20)
Z+ (Œ≤) =
2 q
2 q
+

+

Similarly, the many-body states with even numbers of modes
occupied should be excluded in Q = ‚àí1 sector, and the partition function reads as,
1Y
1Y
Z‚àí (Œ≤) =
(1 + e‚àíŒ≤q‚àí ) ‚àí
(1 ‚àí e‚àíŒ≤q‚àí ).
(G21)
2 q
2 q
‚àí

H.

XY chain: computational hours th versus Œ≤

Here we provide the elapsed real time th vs. Œ≤ in simulating
the infinite XY chain. In Fig. S2, we show the D = 256 runs
with various numbers of CPU cores (Xeon Gold 6230) as well
as on the GPU (Tesla V100). We can see clearly a logarithmic
scaling between th and Œ≤, for Œ≤ & 0.1. This logarithmic instead
of linear scaling of wall time th vs. Œ≤ clearly indicates the
exponential speed up in ‚àÇTRG. In addition, from Fig. S2 we
can see that in all cases (either GPU or CPU computations
with various cores) Œ≤ = 12.8, the temperature point we have
selected in Fig. 3(b) of the main text, is located well in the
logarithmic regime. That is to say, it constitutes a well suitable
sampling temperature point for checking the th vs. D scaling
in Fig. 3(b).

I. ‚àÇTRG calculations of finite-size XY chain

Due to the absence of translational invariance in finite-size
systems, when applying ‚àÇTRG to such systems, the isometries
w are bond dependent. Therefore, extra care is required in the
optimization of w tensors, as will be elaborated below.
Following that introduced in Sec. F, we prepare the matrix product operator (MPO) representation of œÅ(œÑ) via the series expansion. After the initialization, similar to the infinite
cases, one perform iteratively renormalization of tensors to
cool down the system from high to low temperatures, and can
also sweep into inner nd layers. Nevertheless, there exist in
finite-size ‚àÇTRG algorithms bond-dependent isometries to be
optimized, thus sweeps amongst different bonds are required,
along with those between different temperature scales.
The finite-size XY spin chain can be solved exactly by a
Jordan-Wigner transformation that maps the system into a
non-interacting spinless fermion chain, from which the partition function can be readily obtained [16]. In Fig. S3, we
perform the calculation of an L = 50 XY chain and show the
relative errors of free energy for various dimensions D (up to
D = 128) and depths nd (up to 3). Similar to the observations
for infinite-size chain shown in Fig. 3 of the main text, we
can see in Fig. S3 the accuracy improves significantly as the
sweep depth nd increases. Moreover, as shown in Fig. S3, the
improvement gets more and more pronounced as D increases
from D = 32 to 128. In particular, the improvement of accuracies gains over a wide range of temperatures, i.e., from high
down to low temperatures. This again reveals unambiguously
the advantage of deep optimization in ‚àÇTRG.

‚àí

Finally, one arrives at the partition function in the entire
Hilbert space as,
Z = Z+ + Z‚àí ,

(G22)

from which one can calculate the free energy and other thermodynamic quantities.

J.

The z-shift technique

In this appendix, we will briefly recapitulate the z-shift
technique for the computation of thermodynamic quantities
in ‚àÇTRG. Below we take the internal energy u as an example, which is obtained by taking numerical derivative of free

| f/f|

11

10

5

10

7

10

9

10

11

10

13

Note that following the new grid {z ¬∑ Œ≤i } the simulations can
be performed in parallel to the original {Œ≤i } run, thus constituting a highly efficient approach in XTRG [16] as well as
‚àÇTRG.
To be specific, as shown in Fig. 5 of main text, we conduct the ‚àÇTRG simulations by following 4 sets of temperature
1
1
3
grids {z ¬∑ Œ≤i } with z-factor chosen to be z = 20 , 2 /4 , 2 /2 , 2 /4 . Before taking the numerical derivative Eq. (J23), in practice we
further employ an interpolation of free energy data to reach
an even denser temperature grid, i.e., totally 16 sets with
1
15
z = 20 , 2 /16 , ..., 2 /16 , which turns out to essentially eliminate
the differential errors.

L = 50 XY chain

D = 32, nd = 1
D = 32, nd = 2
D = 64, nd = 1
D = 64, nd = 2

10

100

1

D = 128, nd = 1
D = 128, nd = 2
D = 128, nd = 3

101

102

FIG. S3. (Color online) Relative errors of free energy |Œ¥ f / f | in
L = 50 quantum XY chain computed by ‚àÇTRG. There is continuous
improvement in the accuracies with the increase of bond dimensions
D = 32, 64, and 128, as well as the sweep depths nd = 1, 2, and 3.
0.0 (a)

(b)

u(T)

0.2
0.4 (*)
0.6

()

0.2

W=4
L=4
L=6
L=8
L = 10
L

0.46
0.48

( ) T = 0.61
(*) T = 0.51

0.50
0.52

QMC

5.0 0.0

1.0

T

0.1

1/L

0.2

0.3

FIG. S4. (Color online) (a) Internal energy u(T ) of TFI in field
h x = 1.0 with fixed cylinder width W = 4 and various L (up to 10),
which is used to extrapolate to L = ‚àû. It is benchmarked by QMC
data with W = 4, L = ‚àû with similar extrapolations performed. (b)
demonstrates the extrapolations through both the linear fitting and the
subtraction technique (depicted as the cross marks), where excellent
agreement is seen between the two schemes. The dotted horizontal
line goes strictly through the extrapolated values (the star symbols),
which is in perfect agreement with the subtraction results u sub (cross
marks).

energy f , i.e.,
u‚â°

‚àÇ(Œ≤ f ) ‚àÇ(Œ≤ f ) 1
=
¬∑
‚àÇŒ≤
‚àÇln Œ≤ Œ≤

(J23)

as adopted in previous XTRG simulations [16, 17, 52]. In the
case of the temperature grid denoted as
{Œ≤i } ‚â° {2i ¬∑ œÑ0 } = {œÑ0 , 2œÑ0 , 4œÑ0 , ..., 2n œÑ0 }
being sparse, one can resort to the z-shift technique by shifting
the initial temperature œÑ by a z-factor
œÑ = z ¬∑ œÑ0 ,

with z ‚àà [1, 2),

and thus obtain a new grid
{z ¬∑ Œ≤i } ‚â° {2i ¬∑ œÑ} = {œÑ, 2œÑ, 4œÑ, ..., 2n œÑ}.

(J24)

K.

Energy extrapolations of 2D transverse-field Ising model

In this section, we demonstrate the extrapolations of internal energy u(T ) in transverse-field Ising (TFI) model on the
cylindrical square lattice, via both linear fitting and subtraction methods, as mentioned in the main text. In Fig. S4(a),
we show the internal energy in a W = 4 TFI model with various length L = 4, 6, 8, 10. To some extent, they already show
nice convergence with each other as well as to the large-scale
QMC data. Nonetheless, one can still extrapolate further to
L = ‚àû limit to eliminate the small finite-length effects, by either linear fitting uL = u‚àû + b/L or energy subtractions, i.e.,
e sub = (eL+2 ‚àí eL )/2W representing the ‚Äòbulk‚Äô energy. As
shown in Fig. S4(b), both schemes generate mutually consistent energies in the L = ‚àû limit, as indicated by the horizontal
grey dotted lines which goes through exactly the extrapolated
value [the asterisk symbol in Fig. S4(b)]. Remind that the subtracted energy values get converged much faster to the infinite
length limit than linear extrapolation and thus constitutes a
more efficient technique in practice for extracting bulk energy
expectation values.

