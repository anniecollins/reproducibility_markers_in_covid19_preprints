Space Efficient Approximation to Maximum Matching Size from
Uniform Edge Samples
Michael Kapralov
EPFL

Slobodan MitrovicÌ
MIT

Ashkan Norouzi-Fard
Google Zurich

Jakab Tardos
EPFL

arXiv:1907.05725v1 [cs.DS] 12 Jul 2019

Abstract
Given a source of iid samples of edges of an input graph G with n vertices and m edges, how
many samples does one need to compute a constant factor approximation to the maximum matching
size in G? Moreover, is it possible to obtain such an estimate in a small amount of space? We show
that, on the one hand, this problem cannot be solved using a nontrivially sublinear (in m) number
of samples: m1âˆ’o(1) samples are needed. On the other hand, a surprisingly space efficient algorithm
for processing the samples exists: O(log2 n) bits of space suffice to compute an estimate.
Our main technical tool is a new peeling type algorithm for matching that we simulate using a
recursive sampling process that crucially ensures that local neighborhood information from â€˜denseâ€™
regions of the graph is provided at appropriately higher sampling rates. We show that a delicate
balance between exploration depth and sampling rate allows our simulation to not lose precision
over a logarithmic number of levels of recursion and achieve a constant factor approximation. The
previous best result on matching size estimation from random samples was a logO(1) n approximation
[Kapralov et alâ€™14], which completely avoided such delicate trade-offs due to the approximation factor
being much larger than exploration depth.
Our algorithm also yields a constant factor approximate local computation algorithm (LCA) for
matching with O(d log n) exploration starting from any vertex. Previous approaches were based
on local simulations of randomized greedy, which take O(d) time in expectation over the starting
vertex or edge (Yoshida et alâ€™09, Onak et alâ€™12), and could not achieve a better than d2 runtime.
Interestingly, we also show that unlike our algorithm, the local simulation of randomized greedy that
e 2
is the basis of the most
âˆš efficient prior results does take â„¦(d )  O(d log n) time for a worst case edge
even for d = exp(Î˜( log n)).

1

Contents
1 Introduction
1.1 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3
5

2 Preliminaries

6

3 Our Algorithm and Technical Overview
3.1 Offline Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.2 IID Edge Stream Version of Algorithm 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . .

6
6
8

4 Sample Complexity of Algorithm 2
11
4.1 Sample Complexity of Level Tests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
4.2 Sample Complexity of IID-Peeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
5 Correctness of Algorithm 2 (proof of Theorem 5)
5.1 Definitions and Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
c . . . . . . . . . . . . . . .
5.2 Lower Bound: Constructing a Near-Optimal Matching from M
5.3 Upper bound: Constructing a Near-Optimal Vertex Cover . . . . . . . . . . . . . . . . . .

14
14
17
19

6 LCA
6.1 Overview of Our Approach . . . . . . . . .
6.2 Related Work . . . . . . . . . . . . . . . . .
6.3 Algorithms . . . . . . . . . . . . . . . . . .
6.4 Query Complexity . . . . . . . . . . . . . .
6.5 Approximation Guarantee . . . . . . . . . .
6.6 Memory Complexity and Consistent Oracles
6.7 Proof of Theorem 2 . . . . . . . . . . . . . .

22
23
23
24
25
27
27
29

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

e 2 ) for Simulation of Randomized
7 Lower Bound of â„¦(d
7.1 Lower Bound for Infinite Graphs . . . . . . . . . . . .
7.2 Depth and Variance Analysis for Infinite Graphs . . .
7.3 The Lower Bound Instance (Truncated H d, ) . . . . .

Greedy
29
. . . . . . . . . . . . . . . . . . . . 30
. . . . . . . . . . . . . . . . . . . . 34
. . . . . . . . . . . . . . . . . . . . 35

8 Lower-bound on the Number of Sampled Edges
8.1 Overview . . . . . . . . . . . . . . . . . . . . . . . .
8.2 Preliminaries . . . . . . . . . . . . . . . . . . . . . .
8.3 Constructing Graphs with Identical k-Level Degrees
8.4 Increasing Girth via Graph Lifting . . . . . . . . . .
8.5 k-Edge Subgraph Statistics in G and H . . . . . . .

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

36
36
38
39
43
45

9 Analysis of the algorithm on a random permutation stream
9.1 Introduction and Technical Overview . . . . . . . . . . . . . . .
9.2 Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9.3 Padding and total variation distance . . . . . . . . . . . . . . .
9.4 Bounding KL-divergence . . . . . . . . . . . . . . . . . . . . . .
9.5 The full algorithm . . . . . . . . . . . . . . . . . . . . . . . . .

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

46
46
47
48
50
55

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

A Proof of Lemma 5

62

B Oversampling Lemma

65

C Proofs omitted from Section 7

67

D Details omitted from Section 8
68
D.1 Proof of Theorem 9 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68
D.2 Proof of Lemma 23 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
E Proofs omitted from Section 9

70

2

1

Introduction

Large datasets are prevalent in modern data analysis, and processing them requires algorithms with a
memory footprint much smaller than the size of the input, i.e. sublinear space. The streaming model of
computation, which captures this setting, has received a lot of attention in the literature recently. In this
model the edges of the graph are presented to the algorithm as a stream in some order. It has recently
been shown that randomly ordered streams allow for surprisingly space efficient estimation of graph
parameters by nontrivial memory vs sample complexity tradeoffs (see, e.g. [KKS14, MMPS17, PS18]
for approximating matching size and other graph properties such as number of connected components,
weight of MST and independent set size). Memory vs sample complexity tradeoffs for learning problems
have also recently received a lot of attention in literature [Raz16, Raz17, KRT17, GRT18, BGY18]. In
this paper we study the following question:
How many iid samples of edges of a graph G are necessary and sufficient for
estimating the size of the maximum matching in G to within a constant factor?
Can such an estimate be computed using small space?
We give nearly optimal bounds for both questions, developing a collection of new techniques for
efficient simulation of matching algorithms by random sampling. Our main result is
Theorem 1. There exists an algorithm (Algorithm 2) that, given access to iid edge-samples of a graph
G = (V, E) with n vertices and m edges produces a constant factor approximation to maximum matching
size in G using O(log2 n) bits of memory and at most m samples with probability at least 4/5.
The sample complexity of Algorithm 2 is essentially optimal: for every constant C, every m between
n1+o(1) and â„¦(n2 ) it is information theoretically impossible to compute a C-approximation to maximum
matching size in a graph with high constant probability using fewer than m1âˆ’o(1) iid samples from the
edge set of G, even if the algorithm is not space bounded.
The proof of the upper bound part in Theorem 1 is given in Section 4 (more precisely, Section 4.2).
The proof of the lower bound part of Theorem 1 follows from Theorem 8 in Section 8.
The core algorithmic tool underlying Theorem 1 is a general method for implementing peeling type
algorithms efficiently using sampling. In particular, our approach almost directly yields a constant factor
approximate local computation algorithm (LCA) for maximum matching in a graph G with degrees
bounded by d using O(d log n) queries and O(log d) exploration depth, whose analysis is deferred to
Section 6.
Theorem 2. Let G be a graph with n vertices and maximum degree d. Then there exists a random
matching M , such that E [|M |] = Î˜(MM(G)), and an algorithm that with high probability:
â€¢ Given an edge e of G, the algorithm reports whether e is in M or not by using O(d log n) queries.
â€¢ Given a vertex v of G, the algorithm reports whether v is in M or not by using O(d log n) queries.
Moreover, this algorithm can be executed by using O(d log3 n) bits of memory.
We note that the most efficient LCAâ€™s for matching [LRY17] require d4 logO(1) n exploration to achieve
a constant factor approximation, and are based on the idea of simulating the randomized greedy algorithm
locally. Indeed, this runtime complexity follows from the beautiful result of Yoshida et al. [YYI09] or Onak
et al. [ORRR12] that shows the size of the query tree of the randomized greedy is O(d) in expectation over
the starting edge. Applying a Markov bound to discard vertices/edges on which the exploration takes
too long leads to the desired complexity. Moreover, in a degree d bounded graph matching size could
be as small as n/d, with only a 1/d fraction of vertices and edges involved in the matching. Therefore,
a multiplicative (as opposed to multiplicative-additive) constant factor approximation based on average
case results of [YYI09, ORRR12] must use a Markov bound with a loss of at least a factor of d in the
size of the query tree with respect to the average, and hence cannot lead to a better than O(d2 ) runtime
overall. At the same time, Theorem 2 yields the near-optimal runtime of O(d log n), going well beyond
what is achievable with the above mentioned average case results.
At this point it is natural to wonder if the average case analysis of [YYI09, ORRR12] can be improved
to show that the size of the query tree is O(d) in expectation for any given edge as opposed to for a
random one. Surprisingly, we show in Section 7 that this is not possible:

3

âˆš
Theorem 3. There exists an absolute constant b > 0 such that for every n, d âˆˆ [5, exp(b log n)] and
 âˆˆ [1/d, 1/2] there exists a graph G with n vertices and maximum degree d + 1, and an edge e such that
running YYI-Maximal-Matching(e, Ï€) from Algorithm 10 results in an exploration tree of size at least
1
Â·Â·
8

 2âˆ’
d
,
2

in expectation.
In Section 9 we prove that our algorithm is robust to correlations that result from replacing the iid
stream of edge-samples with a random permutation stream. Formally, we prove
Theorem 4. There exists an algorithm that given access to a random permutation edge stream of a graph
G = (V, E), with n vertices and m â‰¥ 3n edges, produces an O(log2 n) factor approximation to maximum
matching size in G using O(log2 n) bits of memory in a single pass over the stream with probability at
least 3/4.
Theorem 4 improves upon the previous best known logO(1) n-approximation due to [KKS14], where
the power of the logarithm was quite large (we estimate that it is at least 8). The proof is given in
Section 9.
Let us now provide a brief overview of some of the techniques we use in this paper.
Our techniques: approximating matching size from iid samples. As mentioned above, our
approach consists of efficiently simulating a simple peeling-type algorithm, using a small amount of space
and samples. This approach was previously used in [KKS14] to achieve a polylogarithmic approximation
to maximum matching size in polylogarithmic space. As we show below, major new ideas are needed to
go from a polylogarithmic approximation to a constant factor approximation. The reason for this is the
fact that any matching size approximation algorithm needs to perform very deep (logarithmic depth)
exploration in the graph, leading to Ï‰(1) long chains of dependencies (i.e., recursive calls) in any such
simulation. Indeed, both the work of [KKS14] and our result use a peeling type algorithm that performs
a logarithmic number of rounds of peeling as a starting point. The work of [KKS14] simulated this
process by random sampling, oversampling various tests by a logarithmic factor to ensure a high degree
of concentration, and losing extra polylogarithmic factors in the approximation to mitigate the effect of
this on sample complexity of the recursive tests. Thus, it was crucial for the analysis of [KKS14] that
the approximation factor is much larger than the depth of exploration of the algorithm that is being
simulated. In our case such an approach is provably impossible: a constant factor approximation requires
access to â„¦(log n) depth neighborhoods by known lower bounds (see [KMW16] and Section 8). A method
for circumventing this problem is exactly the main contribution of our work â€“ we show how to increase
sampling rates in deeper explorations of the graph, improving confidence of statistical estimation and
thereby avoiding a union bound over the depth, while at the same time keeping the number of samples
low. A detailed description of the algorithm and the analysis are presented in Section 3.
Our techniques: tight sample complexity lower bound and tight instances for peeling
algorithms. As the second result in Theorem 1 shows, the sample complexity of our algorithm is
essentially best possible even among algorithms that are not space constrained. The lower bound (see
Section 8) is based on a construction of two graphs G and H on n vertices such that for a parameter k
(a) matching size in G is smaller than matching size in H by a factor of nâ„¦(1)/k but (b) there exists a
bijection from vertices of G to vertices of H that preserves k-depth neighborhoods up to isomorphism.
To the best of our knowledge, this construction is novel. Related constructions have been shown in the
literature (e.g. cluster trees of [KMW16]), but these constructions would not suffice for our lower
bound, since they do not provide a property as strong as (b) above.
Our construction proceeds in two steps. We first construct two graphs G0 and H 0 that have identical
k-level degrees (see Section 8.3). These two graphs are indistinguishable based on k-level degrees and
their maximum matching size differs by an nâ„¦(1/k) factor, but their neighborhoods are not isomorphic
due to cycles. G0 and H 0 have n2âˆ’O(1/k) edges and provide nearly tight instances for peeling algorithms
that we hope may be useful in other contexts. The second step of our construction is a lifting map
(see Theorem 11 in Section 8) that relies on high girth Cayley graphs and allows us to convert graphs
with identical k-level vertex degrees to graphs with isomorphic depth-k neighborhoods without changing
matching size by much. The details are provided in Section 8.4.

4

Finally, the proof of the sampling lower bound proceeds as follows. To rule out factor C approximation
in m1âˆ’o(1) space, take a pair of constant (rather, mo(1) ) size graphs G and H such that (a) matching
size in G is smaller than matching size in H by a factor of C and (b) for some large k one has that
k-depth neighborhoods in G are isomorphic to k-depth neighborhoods in H. Then the actual hard input
distribution consists of a large number of disjoint copies of G in the NO case and a large number of
copies of H in the YES case, possibly with a small disjoint clique added in both cases to increase the
number of edges appropriately. Since the vertices are assigned uniformly random labels in both cases,
the only way to distinguish between the YES and the NO case is to ensure that at least k edge-samples
land in one of the small copies of H or G. Since k is small, the result follows. The details can be found
in Section 8.
Our techniques: random permutation streams. As mentioned above, in Section 9 (see Theorem 4)
we extend our result to a stream of edges in random order which improves upon the previous best known
logO(1) n-approximation due to [KKS14], where the power of the logarithm was quite large (we estimate
that it is at least 8). In order to establish Theorem 4 we exhibit a coupling between the distribution
of the state of the algorithm when run on an iid stream and when run on a permutation stream. The
approach is similar in spirit to that of [KKS14], but carrying it out for our algorithm requires several
new techniques. Our approach consists of bounding the KL divergence between the distribution of the
state of the algorithm in the iid and random permutation settings by induction on the level of the tests
performed in the algorithm. In order to make this approach work, we develop a restricted version of
triangle inequality for KL divergence that may be of independent interest (see Lemma 27 in Section 9).

1.1

Related Work

Over the past decade, matchings have been extensively studied in the context of streaming and related
settings. The prior work closest to ours is by Kapralov et al. [KKS14]. They design an algorithm that
estimates the maximum matching size up to poly log n factors by using at most m iid edge-samples.
Their algorithm requires O(log2 n) bits of memory. As they prove, this algorithm is also applicable
to the scenario in which the edges are provided as a random permutation stream. The problem of
approximating matching size using o(n) space has received a lot of attention in the literature, including
random order streams [CJMM17, MMPS17] and worst case streams [MV18, BGM+ 19, MV16, CCE+ 16,
EHL+ 15, EHM16, BS15, AKL17]. The former, i.e., [MMPS17, CJMM17], are the closest to our setting
since both of these works consider random streams of edges. However, the results mostly apply to
bounded degree graphs due to an (at least) exponential dependence of the space on the degree. The
latter consider worst case edge arrivals, but operate under a bounded arboricity assumption on the
input graph. Very recently constant space algorithms for approximating some graph parameters (such as
number of connected components and weight of the minimum spanning tree) from random order streams
were obtained in [PS18] (see also [MMPS17]).
An extensive line of work focused on computing approximate maximum matchings by using OÌƒ(n)
memory. The standard greedy algorithm provide a 2 approximation. It is known that no single-pass
streaming algorithm (possibly randomized) can achieve better than 1 âˆ’ 1/e approximation while using
OÌƒ(n) memory [Kap13, GKK12]. For both unweighted and weighted graphs, better results are known
when a stream is randomly ordered. In this scenario, it was shown how to break the barrier of 2
approximation and obtain a 2 âˆ’  approximation, for some small constant  > 0 [KMM12, Kon18,
AKLY16, AK17, PS17, ABB+ 19, AB19, GW19, GKMS18]. If multiple passes over a stream are allowed,
a line of work [FKM+ 05, EKS09, AG11b] culminated in an algorithm that in O(poly(1/)) passes and
OÌƒ(n) memory outputs a (1 + )-approximate maximum matching in bipartite graphs. In the case of
general graphs, (1 + )-approximate weighted matching can be computed in (1/)O(1/) passes and also
OÌƒ(n) memory [McG05, ELMS11, Zel12, AG11a, AG11b] and [GKMS18].
In addition to the LCA results we pointed to above, close to our work are [RTVX11, ARVX12, LRY17,
Gha16, GU19] who also study the worst-case oracle behavior. In particular, [GU19] showed that there
exists an oracle that given an arbitrary chosen vertex v outputs whether v is in some fixed maximal
independent set or not by performing dO(log log d) Â· poly log n queries. When this oracle is applied to the
line graph, then it reports whether a given edge is in a fixed maximal matching or not. We provide
further comments on LCA related work in Section 6.2.

5

2

Preliminaries

Graphs In this paper we consider unweighted undirected graphs G = (V, E) where V is the vertex set
and E is the edge set of the graph. We denote |V | by n and |E| by m. Furthermore, we use d to signify
the maximum degree of G or an upper bound on the maximum degree.
Matching In this paper, we are concerned with estimating the size of a maximum matching. Given
a graph G = (V, E), a matching M âŠ† E of G is a set of pairwise disjoint edges, i.e., no two edges
share a common vertex. If M is a matching of the maximum cardinality, then M is also said to be
a maximum matching. We use MM (G) to refer to the cardinality of a maximum matching of G. A
fractional matching Mf : E â†’ R+ is a function assigning weights to the edges such
P that the summation
of weights assigned to the edges connected to each vertex is at most one, i.e., eâˆˆE:vâˆˆe Mf (e) â‰¤ 1, for
each vertex v. The size of a fractional matching (denoted by |Mf |) is defined to be the summation of
the weights assigned to the edges. Notice that any matching can also be seen as a fractional matching
with weights in {0, 1}. It is well-known that
MM (G) â‰¤

max

fractional matching Mf of G

|Mf | â‰¤

3
MM (G) ,
2

hence,
MM (G) =

max

fractional matching Mf of G

Î˜(|Mf |).

Therefore, to estimate the cardinality of a maximum matching, it suffice to estimates the size of a
fractional maximum matching. In this work, we show that the estimate returned by our algorithm is by
a constant factor smaller than the size of a fractional maximum matching, which implies that it is also
by a constant factor smaller than MM (G).
Vertex cover We also lower-bound the estimate returned by our algorithm. To achieve that, we prove
that there is a vertex cover in G of the size within a constant factor of the output of our algorithm.
Given a graph G = (V, E), a set C âŠ† V is a vertex cover if each edge of the graph is incident to at least
one vertex of the set C. It is folklore that the size of any vertex cover is at least the size of any matching.
Vertex neighborhood Given a graph G = (V, E), we use N (v) to denote the vertex neighborhood of
v. That is, N (v) = {w : {v, w} âˆˆ E}.

3

Our Algorithm and Technical Overview

In this section we present our algorithm for estimating the matching size from at most m iid edgesamples. We begin by providing an algorithm that estimates the matching size while having full access
to the graph (see Algorithm 1) and then, in Section 3.2, we show how to simulate Algorithm 1 on iid
edge-samples. We point out that Algorithm 1 uses O(m) memory, while our simulation uses O(log2 n)
bits of memory.

3.1

Offline Algorithm

First we introduce a simple peeling algorithm which we will be simulating locally. This algorithm
constructs a fractional matching M and a vertex cover C which are within a constant factor of each
other. As noted in Section 2, by duality theory this implies that both M and C are within a constant
factor of the optimum. Algorithm 1 begins with both M and C being empty sets and augments them
simultaneously in rounds. When the weight of a vertex1 v is higher than a threshold Î´, the algorithm
adds v to the vertex cover C and removes v along with all its incident edges. When this happens, we
say that v and its edges have been peeled off. In the ith round the weight of the matching on each edge
that has not yet been peeled off is increased by ciâˆ’1 /d. For any v âˆˆ V , we denote the total weight of M
adjacent to v by
X
M (v) =
M ((v, w)).
wâˆˆN (v)
1 Remark that we use the term weight of an edge to refer to the fractional matching assigned to that edge, i.e., the
value computed by Edge-Level-Test Algorithm 3. Similarly, we use the term weight of a vertex as the summation of the
weights of the edges connected to it.

6

Algorithm 1 Offline peeling algorithm for constructing a constant factor approximate maximum
fractional matching and a constant factor approximate minimum vertex cover
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:

procedure Global-Peeling(G = (V, E), Î´, c)
. Î´ and c are two constants that we fix later
Aâ†V
. Set of active vertices
Câ†âˆ…
. C is the vertex cover that the algorithm constructs
M :Eâ†’R
M (e) â† 0 âˆ€e âˆˆ E
. Initially, the fractional matching for all the edges is zero
for i = 1 to J + 1 do
. Represents the rounds of peeling
for e âˆˆ E âˆ© A Ã— A do
. Remaining edges
M (e) â† M (e) + ciâˆ’1 /d
. Increases weight on edges
for v âˆˆ A do
P
if M (v) â‰¥ Î´ then
. Recall that M (v) = wâˆˆN (v) M ((v, w))
A â† A\{v}
. Remove the vertex
C â† C âˆª {v}
. Add the vertex to the vertex cover
return (M, C)

This process continues for J + 1 rounds before all the edges (but not necessarily all the vertices) are
peeled off from the graph. Note that at the last (J + 1st ) iteration the amount of weight assigned to each
of the non-peeled-off edges is at least cJ /d and hence it suffices to set J = Î˜(log d) for all the vertices v
with non-zero degree to be peeled off. Therefore, at this point C is indeed a vertex cover.
At the termination of Algorithm 1, any vertex v that has been added to C at some point must have
at least Î´ matching adjacent to it. More precisely,
X
M (e) â‰¥ Î´|C| â‰¥ Î´MM (G) .
(1)
eâˆˆE

However, since the rate at which M is increased on each edge only increases by a multiplicative factor
of c per round, the weight adjacent to any vertex cannot be much higher. Indeed, if v is peeled off in
round i + 1, then M (v) is at most Î´ in round i. In the intervening one round M (v) could increase by
at most cÎ´, therefore M (v) is at most (c + 1)Î´ at the point when v is peeled off. Naturally, if a vertex
is peeled off in the first round, M (v) is at most 1. In conclusion M (v) is at most max((c + 1)Î´, 1); thus
scaling down M by a factor of max((c + 1)Î´, 1) produces a (valid) fractional matching which is within a
max(c + 1, 1/Î´) factor of C. Therefore,
X
eâˆˆE

3
M (e) â‰¤ max((c + 1)Î´, 1) MM (G) ,
2

(2)

where the factor 3/2 is the result of the gap between a fractional matching and the maximum matching
explained in preliminaries. Combining Eq. (1) and Eq. (2) we get a constant approximation guarantee.
Main challenges in simulating Algorithm 1 and comparison to [KKS14]. The approach of
starting with a peeling type algorithm and performing a small space simulation of that algorithm by
using edge-samples of the input graph has been used in [KKS14]. However, the peeling algorithm that
was used is significantly weaker than Algorithm 1 and hence much easier to simulate, as we now explain.
Specifically, the algorithm of [KKS14] repeatedly peels off vertices of sufficiently high degree, thereby
partitioning edges of the input graph into classes, and assigns uniform weights on edges of every class
(vertices of degree â‰ˆ ci in the residual graph are assigned weight â‰ˆ ciâˆ’1 /n). This fact that the weights
are uniform simplifies simulation dramatically, at the expense of only an O(log n) approximation. Indeed,
in the algorithm of [KKS14] the weight on an edge is equivalent to the level at which the edge disappears
from the graph, and only depends on the number of neighbors that the endpoints have one level lower.
In our case the weight of an edge at level i is composed of contributions from all levels smaller than
i. As we show below, estimating such weights is much more challenging due to the possibility to errors
accumulating across the chain of â„¦(log d) (possibly â„¦(log n)) levels. In fact, techniques for coping with
this issue, i.e., avoiding a union bound over log n levels, are a major contribution of our work.

7

3.2

IID Edge Stream Version of Algorithm 1

In this section we describe our simulation of Algorithm 1 in the regime in which an algorithm can learn
about the underlying graph only by accessing iid edges from the graph.
Remark 1. In the following we will assume that m â‰¥ n for simplicity. In the case where this is not
true we can simply modify the graph by adding a new vertex v0 to V and adding an n-star centered at
v0 to the input graph G to get G0 . This increases the matching size by at most 1. Also, knowing m and
V we can simply simlulate sampling an iid edge G0 . Indeed, whenever we need to sample an edge of G0
n
m
we sample an iid edge of G and with probability n+m
we sample uniformly from
with probability n+m
v0 Ã— V . For simplicity we will omit this subroutine from our algorithms and assume that m â‰¥ n in the
input graph G.
Remark 2. Recall that d is an upper bound on the maximum degree of G. For the purposes of the iid
sampling algorithm, we can simply set d to be n, as it can be any upper bound on the maximum degree.
In this model of computation the runtime will actually not depend on d. We still carry d throughout the
analysis, as it will be crucial in the LCA implementation (Section 6). However, the reader is encouraged
to consider d = n for simplicity.
Algorithm 2 IID edge sampling algorithm approximating the maximum matching size of G, MM (G).
1:
2:
3:
4:
5:
6:
7:

procedure IID-Peeling(G = (V, E))
M0 â† 0
. The value of estimated matching
tâ†1
. The number of iid edges that we run Edge-Level-Test on
while samples lasts do
M 0 â† Sample(G, t)
t â† 2t
return M 0
. The estimated size of the matching that our algorithm returns

8:
9:
10:
11:
12:

procedure Sample(G = (V, E), t)
M0 â† 0
for k = 1 to t do
e â† iid edge from the stream
13:
M 0 â† M 0 + Edge-Level-Test(e)
0
14:
return m Â· Mt

. Starting fractional matching

. The fractional matching assigned to this edge

Algorithm 3 Given an edge e, this algorithm returns a fractional matching-weight of e.
1:
2:
3:
4:
5:
6:
7:
8:

procedure Edge-Level-Test(e = (u, v))
w â† 1/d
. By definition, every edge gets the weight 1/d
for i = 1 to J + 1 do
. Recall that J = blogc dc âˆ’ 1
if Level-i-Test(u) and Level-i-Test(v) then. Check if both endpoints pass the i-th test
w â† w + ci /d
. Increase the weight of the edge
else
return w
return w

Algorithm 2 is a local version of Algorithm 1 which can be implemented by using iid edge-samples.
Notice that for the sake of simplicity in both presentation of the algorithms and analysis, we referred to
the iid edge oracle as stream of random edges. Instead of running the peeling algorithm on the entire
graph, we select a uniformly random edge e and estimate what the value of M in Algorithm 1 would be
on this edge. This is done by the procedure Edge-Level-Test(e) (Algorithm 3). The procedures IIDPeeling and Sample are then used to achieve the desired variance and will be discussed in Section 4;
they are not necessary for now, to understand the intuition behind Algorithm 2.
Notice that in Algorithm 1 if the two endpoints of e = {u, v} get peeled off at rounds i1 and
Pmin(i1 ,i2 ) iâˆ’1
i2 respectively, then we can determine the value of M (e) to be
c /d. Therefore, when
i=1
evaluating Edge-Level-Test(e) we need only to estimate what round u and v make it to. To this
end we use the procedure Level-j-Test(v) which estimates whether a vertex survives the j th round of
8

Algorithm 4 Given a vertex v that has passed the first j rounds of peeling, this algorithm returns
whether or not it passes the j + 1st round.
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:

procedure Level-(j + 1)-Test(v)
Sâ†0
. The estimate of the weight of this vertex
do
for k = 1 to cj Â· m
d
e â† next edge in the stream
. Equivalent of sampling and iid edge
if e is adjacent to v then
w â† the other endpoint of e
iâ†0
. Represents the last level that w passes
while i â‰¤ j and Level-i-Test(w) do
. Level-0-Test returns true by definition
S â† S + ciâˆ’j
. The contribution that edge e gives with respect to the current
sampling rate
if S â‰¥ Î´ then
. If yes, then the weight of the vertex is high
return false
iâ†i+1
return true

Algorithm 1. However, instead of calculating M (v) exactly, by looking recursively at all neighbors of v,
we only sample the neighborhood of v at some appropriate rate to get an unbiased estimator of M (v).
Additionally, both Edge-Level-Test and Level-j-Test determinate early if the output becomes
clear. In Edge-Level-Test(e), if either endpoint returns false in one of the tests we may stop, since
the value of M (e) depends only on the endpoint that is peeled off earlier. In Level-j-Test, if the
variable â€™Sâ€™, which is used as an accumulator, reaches the threshold Î´ we may stop and return false
even if the designated chunk of the stream has not yet been exhausted. This speed-up will be crucial in
the sample complexity analysis that we provide in Section 4.
Main challenges that Algorithm 3 resolves and comparison to [KKS14]. We now outline the
major ideas behind our constant factor approximation algorithm and compare them to the techniques
used by the polylogarithmic approximation of [KKS14].
Precision of level estimates despite lack of concentration. As discussed above, the crucial
feature of our algorithm is the fact that the total weight of a vertex that is assigned to level j (i.e.,
fails Level-j-Test) is contributed by vertices at all lower levels, in contrast to the work of [KKS14],
where only contributions from the previous level were important. Intuitively, a test is always terminated
as soon as it would need to consume more samples than expected. In fact, one can show that without
this even a single call of Level-j-Test may run for Ï‰(m) samples in expectation, in graphs similar to
the hard instances for greedy, constructed in Section 7. As we will see in Section 4 the early stopping
rule lends itself to extremely short and elegant analysis of sample complexity. However, the correctness
of Algorithm 2 is much less straightforward. For example, note that we are approximately computing
the weight of a vertex at level j by sampling, and need such estimates to be precise. The approach
of [KKS14] to this problem was simple: C log n neighbors on the previous level were observed for a large
constant factor C to ensure that all estimates concentrate well around their expectation. This lead to
a blowup in sample complexity, which was reduced by imposing an aggressive pruning threshold on the
contribution of vertices from the previous level, at the expense of losing another logarithmic factor in the
approximation quality. The work of [KKS14] could afford such an approach because the approximation
factor was much larger than the depth of the recursive tests (approximation factor was polylogarithmic,
whereas the depth merely logarithmic). Since we are aiming for a constant factor approximation, we
cannot afford this approach.
The core of our algorithm is Level-j-Test of Algorithm 4. Indeed, once the level of two vertices,
b
b
u and v, have been determined by repeated use of Level-j-Test to be L(u)
and L(v)
respectively, the
connecting edge has fractional weight of exactly
b
b
min{L(u),
L(v)}

X

c(u, v) def
M
=

ci /d.

(3)

i=0

P
c(u, v), and
This means that the size of the matching constructed by the algorithm is e=(u,v)âˆˆE M
our matching size estimation algorithm (Algorithm 2) simply samples enough edges to approximate the
9

sum to a constant factor
Thus, in order to establish correctness of our algorithm it
P multiplicatively.
c(u, v) is a constant factor approximation to the size of the maximum
suffices to show that e=(u,v)âˆˆE M
matching in G. We provide the formal analysis in Section 5, and give an outline here for convenience of
the reader.
c on all the edges is approximately equal to the
Our goal will be to prove that the total weight of M
maximum matching size of G. Consider by analogy the edge weighting M of Algorithm 1. Here, M is
designed to be such that the weight adjacent to any vertex (that is the sum of the weight of adjacent
edges) is about a constant, with the exception of the vertices that make it to the last (T + 1st ) peeling
c is designed similarly, however, classifications of some
level. This is sufficient to prove correctness. M
vertices may be inaccurate.
First of all, if a vertex is misclassified to a peeling level much higher than where it should be, it
may have a superconstant amount of weight adjacent to it. Since among n vertices some few are bound
to be hugely misclassified we cannot put a meaningful upper bound on the weight adjacent to a worst
c, when normalized by a constant, is a fractional matching.
case vertex; we cannot simply say that M
Instead, we choose some large constant threshold Î» and discard all vertices whose adjacent weight is
higher than Î» (we call these bad vertieces), then normalize the remaining matching by Î». By analyzing
c in Corollary 1, we get that M
c concentrates quadratically around its
the concentration properties of M
expectation, that is

h
i
 h
i
c(v) > x = O E M
c(v) /x2 .
P M
Using this we can show that discarding bad vertices will not significantly change the total weight of the
graph. For details see Section 5.2 and particularly Corollary 2, which states that only a small fraction
of the total weight is adjacent to bad vertices, in expectation:
i
h
ii
h
h
c(v) .
c(v) â‰¥ Î» â‰¤ 1 E M
c(v) Â· 1 M
E M
4
A more difficult problem to deal with is that vertices may be misclassified to lower peeling levels than
where they should be. Indeed, the early stopping rule, (see Line 10 of Algorithm 4) means that vertices
have a lot of chances to fail early. For example, a vertex v which should reside at some high (Î˜(log n))
level must survive Î˜(log n) inaccurate tests to get there. One might reasonably think that some vertices
are misclassified to lower levels even in the typical case; this however turns out to not be true. The
key realization is that the Level-(j + 1)-Test(v) behaves very similarly to the Level-j-Test(v) with
one of the crucial differences being that Level-(j + 1)-Test(v) samples the neighborhood of v c-times
more aggressively. A multiplicative increase in sampling rate translates to a multiplicative decrease in
the error probability of the test, as formalized by Lemma 1 from Section 5.3:
Lemma 1 (Oversampling lemma). For sufficiently small Î´ > 0 and large enough c the following holds.
PK
def
Let
X =
k=1 Yk be a sum of independent random variables Yk taking values in [0, 1],
 and X =
P
c
1
i=1 Xi where Xi are iid copies of X. If E [X] â‰¤ Î´/3 and P [X â‰¥ Î´] = p, then P X â‰¥ Î´ â‰¤ p/2.
c
More formally, consider some vertex v whose peeling level should be about j âˆ— = Î˜(log n). When
running Level-j-Test(v) for some j  j âˆ— the variable S in Algorithm 4 is an unbiased estimator of
the weight currently adjacent on v, and E [S]  Î´. However, we have no bound on the variance of S
and so it is entirely possible that with as much as constant probability S exceeds Î´ and the test exits in
Line 10 to return false. Over a logarithmic number of levels we cannot use union bound to bound the error
probability. Instead, let S 0 be the same variable in the subsequent run of Level-(j + 1)-Test(v). S 0 can
be broken into contributions from neighbors at levels below j (denoted by A below), and contributions
from the neighbors at level j (denoted by B below). It can be shown that
S0 = A + B
and
P [S 0 â‰¥ Î´] â‰¤ P [A â‰¥ Î´] + P [B â‰¥ 1] ,
due to the integrality of B. However, A is simply an average taken over c iid copies of S, and so we
can apply the oversampling lemma, with X = S. (Note that S satisfies the condition of being the
independent sum of bounded variables, as different iterations of the for loop in Line 3 are independent
and the contribution of each to S is small.) As a result we get
1
Â· P [S â‰¥ Î´]
2
which ultimately allows us to get a good bound on the total error probability, summing over all j âˆ— levels.
P [A â‰¥ Î´] â‰¥

10

Sample complexity analysis of tests. Note that as Algorithm 4, in order to test whether a vertex
v is peeled off at iteration at most j it suffices to sample a cjâˆ’1 /d fraction of the stream and run
recursive tests on neighbors of v found in that random sample. It is crucial for our analysis that the tests
are sample efficient, as otherwise our algorithm would not gather sufficient information to approximate
matching size from only m samples (or, from a single pass over a randomly ordered permutation stream
â€“ see Section 9). One might hope that the sample complexity of Level-j-Test is dominated by the
samples that it accesses explicitly (as opposed to the ones contributed by recursive calls), and this turns
out to be the case. The proof follows rather directly by induction, essentially exactly because lower level
tests, say Level-i-Test for i < j, by the inductive hypothesis use a ciâˆ’1 /d fraction of the stream, and
contribute ciâˆ’jâˆ’1 to the counter S maintained by the algorithm (see Algorithm 4). Since the algorithm
terminates as soon as the counter reaches a small constant Î´, the claim essentially follows, and holds
deterministically for any stream of edges â€“ the proof is given in Section 4 below (see Lemma 2).
Sample complexity of approximating matching size. A question that remains to be addressed
is how to actually use the level tests to approximate the maximum matching size. We employ the
following natural approach: keep sampling edges of the graph using the stream of iid samples and testing
the received edges (the way we process these samples is novel). In [KKS14], where a polylogarithmic
approximation was achieved, the algorithm only needed to ascertain that at least one of the logarithmic
classes (similar to the ones defined by our peeling algorithm) contains nontrivial edge mass, and could
discard the others.
Since we aim to obtain a constant factor approximation, this would not be sufficient. Instead, our
Line 9 maintains a counter that it updates with estimated weights of the edges sampled from the
stream. Specifically, an edge e = (u, v) is declared to be a level j edge if both u and v pass all
Level-i-Test tests with i < j and at least one of them fails Level-j-Test â€“ we refer to this
procedure as Edge-Level-Test(e) (see Algorithm 3). Such an edge contributes approximately cjâˆ’1 /d
to a counter M 0 that estimates matching size. It turns out to be very helpful to think of approximating
matching size as a fraction of the stream length, i.e., have M 0 âˆˆ [0, 1] (see Line 9 for the actual
test and Line 13 for the application inside Line 9). Our estimate is then the average of the weights of
all sampled edges. We then need to argue that the variance of our estimate is small enough to ensure
that we can get a constant multiplicative approximation without consuming more than m samples.
This turns out to be a very natural variance calculation: the crucial observation is that whenever an
edge e sampled from the stream is assigned weight ciâˆ’1 /d due to the outcomes of Level-j-Test on
the endpoints (see Line 13 of Line 9), the cost of testing it (in terms of samples consumed by recursive
calls) is comparable to its contribution to the estimate. This implies that at most m samples are
sufficient â€“ the details are given in the proof of Theorem 6

4

Sample Complexity of Algorithm 2

We begin by analyzing the sample complexity of a single Level-j-Test test. After that, in Theorem 6,
we prove that this sample complexity suffices to estimate the matching size by using no more than m iid
edge-samples.

4.1

Sample Complexity of Level Tests

Lemma 2. For every c â‰¥ 1, Î´ â‰¤ 1/2, and graph G = (V, E), let Ï„j be the maximum possible number of
samples required by Level-j-Test defined in Algorithm 3, for j âˆˆ [1, J + 1]. Then, with probability one
we have:
m
(4)
Ï„j â‰¤ 2cjâˆ’1 Â· .
d
Proof. We prove this lemma by induction that Eq. (4) holds for each j.
Base of induction For j = 1 the bound Eq. (4) holds directly as
Ï„1 â‰¤
by the definition of the algorithm.

11

m
,
d

Inductive step. Assume that Eq. (4) holds for j. We now show that Eq. (4) holds for j + 1 as well.
Consider any vertex v âˆˆ V and Level-(j + 1)-Test(v). Let Î±i be the number of recursive
Level-i-Test calls invoked during a worst case run of Level-(j + 1)-Test(v) (that is when
Level-(j + 1)-Test(v) consumes Ï„j+1 samples). Then, Ï„j+1 can be upper-bounded as
j

Ï„j+1 â‰¤ cj Â·

m X
+
Î±i Ï„i .
d
i=1

Moreover, from Eq. (4) and our inductive hypothesis, it holds
j

Ï„j+1

m X
m
+
â‰¤c Â·
Î±i Â· 2ciâˆ’1 Â·
d
d
i=1
j

m
=c Â·
Â·
d
j

1+2

j
X

!
iâˆ’1âˆ’j

c

Î±i

.

(5)

i=1

P
iâˆ’1âˆ’j
Our goal now is to upper-bound
Î±i . During the execution, Level-(j + 1)-Test
iâ‰¤j c
maintains the variable S. Consider any time during Level-(j + 1)-Test when a recursive call is made
to Level-i-Test, for i â‰¥ 1, in Line 8 of Algorithm 4. Immediately preceding this, in Line 9 of the
previous iteration of the loop, the variable S would have been incremented by ciâˆ’1âˆ’j . This happens Î±i
times for every i â‰¥ 1. Consider now the state of the algorithm just before the last recursive
P call is made
to a lower level test. By the time the last recursive call is made, S has been increased by iâ‰¤j ciâˆ’1âˆ’j Î±i
in total. However, just before the last recursive test is made, the algorithm does not exit to return
false in Line 10, meaning that S < Î´ at this point. Hence
X
ciâˆ’1âˆ’j Î±i â‰¤ Î´.
iâ‰¤j

This together with Eq. (5) leads to
Ï„j+1 â‰¤ cj Â·

m
m
Â· (1 + 2Î´) â‰¤ 2cj Â· ,
d
d

as desired, where the last inequality follows by the assumption that Î´ â‰¤ 1/2.

4.2

Sample Complexity of IID-Peeling

Lemma 3. For every c â‰¥ 2, 0 < Î´ â‰¤ 1/2, and graph G = (V, E), for any edge e = (u, v) âˆˆ E, if Me is
the output of an invocation of Edge-Level-Test(e), then with probability one this invocation used at
most 4Me Â· m edge-samples.
Proof. As before, let Ï„j be the maximum possible number samples required by Level-j-Test. From
Lemma 2 we have Ï„j â‰¤ 2cjâˆ’1 Â· m
d.
Let I be the last value of i, at which the algorithm Edge-Level-Test(e) exits the while loop in
Line 7 of Algorithm 3. Alternately if the algorithm exits in Line 8, let I = J. This means
PIâˆ’1 that the
variable w has been incremented for all values of i from 1 to I âˆ’ 1, making w equal to i=0 ci /d. On
the other hand, in the worst case scenario, Level-i-Test has been called on both u and v for values of
i from 1 to I. Therefore, the number of samples used by this invocation of Edge-Level-Test (e) is at
most
Iâˆ’1 i
I
I
X
X
X
2ciâˆ’1 Â· m
c
2
Ï„i â‰¤ 2
= 4m Â·
= 4Me Â· m.
d
d
i=0
i=1
i=1

In Section 5, we show the following theorem.
Theorem 5. For sufficiently small Î´ > 0 and large enough c the following holds. For a graph G = (V, E)
and an edge e âˆˆ E, let Me denote the value returned by Edge-Level-Test(e) (Algorithm 3). Then,
X
E [Me ] = Î˜(MM (G)).
eâˆˆE

12

Given this, we now prove that our main algorithm outputs a constant factor approximation of the
maximum matching size.
Theorem 6. For sufficiently small Î´ > 0 and large enough c the following holds. For a graph G = (V, E),
IID-Peeling (Algorithm 2) wit probability 4/5 outputs a constant factor approximation of MM (G) by
using at most m iid edge-samples.
We now give the proof of the main algorithmic result of the paper:
Proof of Theorem 1 (first part): The proof of the first part of Theorem 1 now follows from Theorem 6
and the fact that IID-Peeling has recursion depth of O(log n), where each procedure in the recursion
maintains O(1) variables, hence requiring O(log n) bits of space. Therefore, the total memory is O(log2 n).
Proof of Theorem 6: We first derive an upper bound on the number of edges that Sample
(see Algorithm 2) needs to test in order to obtain a constant factor approximation to maximum
matching size with probability at least 9/10, and then show that the number of iid samples that the
corresponding edge tests use is bounded by m, as required. We define
def

Âµ = Eeâˆ¼U (E) [Me ]
for convenience, where U (E) is the uniform distribution on E. Now we have Âµ Â· m = Î˜(M M (G)) by
Theorem 5. We now show that our algorithm obtains a multiplicative approximation to Âµ using at most
m samples.
Upper bounding number of edge tests that suffice for multiplicative approximation of Âµ.
We now analyze the number of edge-samples used by Algorithm 2. We first analyze the sample-complexity
of method Sample, and later of method IID-Peeling.
Let Et = {e1 , . . . , et } be a list of t iid edge-samples taken by Sample (G, t) (Line 11 of Algorithm 2),
where t is a parameter passed on Line 5 of IID-Peeling. We now show that if t â‰¥ 160/Âµ, then
#
"
t
1X
Mei âˆ’ Âµ > Âµ/2 < 1/10,
(6)
P
t i=1
where the probability is over the choice of Et as well as over the randomness involved in sampling Met
for i = 1, . . . , t.
PJ
We prove Eq. (6) by Chebyshevâ€™s inequality. For e âˆ¼ U (E) we have that Me â‰¤ i=0 ci /d â‰¤ 2cJ /d â‰¤
2/c, since J = blogc dc âˆ’ 1.


Var [Me ] â‰¤ E Me2 â‰¤ 2/c Â· E [Me ] = 2Âµ/c.
h P
i
t
We thus get by Chebyshevâ€™s inequality, using the fact that Var 1t i=1 Mei = 1t Var [Me ], that
"
P

#
t
8
1X
Mei âˆ’ Âµ â‰¥ Âµ/2 â‰¤ Var [Me ] /(t Â· (Âµ/2)2 ) â‰¤
,
t i=1
tcÂµ

and hence Eq. (6) holds for any t â‰¥ 160/cÂµ, as required.
Upper bounding total sample complexity of edge tests. It remains to upper bound the total
number of iid edge-samples consumed by the edge tests. For an edge e âˆˆ E let Ze denote the fraction of
our overall budget of m samples needed to finish the invocation Edge-Level-Test(e) (equivalently, let
m Â· Ze be the number of samples taken). By Lemma 3 we have
Ze â‰¤ 4Me .

(7)

Since e1 , . . . , et are uniform samples from the edges of the graph, we have
" t
#
t
t
X
X
X
E
Z ei =
E [Zei ] â‰¤
4E [Mei ] = 4Âµt.
i=1

i=1

i=1

Hence, t executions of Edge-Level-Test(ei ) in expectation require at most 4Âµt Â· m edges. Each
of these invocations of Edge-Level-Test is performed by Sample(G, t). In addition to invoking
13

Edge-Level-Test, Sample(G, t) samples t edges on Line 12 to obtain e1 , . . . , et . Therefore, the total
sample complexity of Sample(G, t) in expectation is
4tÂµ Â· m + t â‰¤ 5tÂµ Â· m.
In the last inequality we upper-bounded t by tÂµ Â· m. This upper-bound holds as Me â‰¥ 1/d (Line 2 of
Algorithm 3), so Âµ â‰¥ 1/d â‰¥ 1/n and hence Âµ Â· m â‰¥ 1. Also, here we used that m â‰¥ n by Remark 1.
Upper bounding sample complexity of IID-Peeling. Finally, we bound the expected sample
complexity of IID-Peeling from Algorithm 2. IID-Peeling terminates only when it runs out of
samples, but we consider it to have had enough samples if it completes the call of Sample with a
parameter t â‰¥ 160/cÂµ. (In particular let tâˆ— be the smallest power of 2 greater than 160Âµ/c; this is the
value of t we aim for, as all values of t are powers of 2. tâˆ— â‰¤ 320/cÂµ.) The expected number of iid
samples required is at most
5Âµ Â· m + 10Âµ Â· m + Â· Â· Â· + 5tâˆ— Âµ Â· m â‰¤ 10tâˆ— Âµ Â· m â‰¤ 3200m/c
By Markovâ€™s inequality we thus have that IID-Peeling the call of Sample for t = tâˆ— within
32000m/c â‰¤ m samples with probability at least 9/10. (Let c â‰¥ 32000.) Conditioned on this, the
algorithm succeeds with probability at least 9/10, therefore it succeeds with overall probability at least
4/5, as claimed.

5

Correctness of Algorithm 2 (proof of Theorem 5)

The main result of this section is the following theorem.
Theorem 5. For sufficiently small Î´ > 0 and large enough c the following holds. For a graph G = (V, E)
and an edge e âˆˆ E, let Me denote the value returned by Edge-Level-Test(e) (Algorithm 3). Then,
X
E [Me ] = Î˜(MM (G)).
eâˆˆE

Overview of techniques. The main challenge in proving this claim is that the function Level-j-Test
(Algorithm 3) potentially returns different outputs for the same vertex on different runs. Moreover, the
outputs of two independent invocations could differ with constant probability. The propagation of such
unstable estimates over Î˜(log d) (possibly Î˜(log n)) peeling steps could potentially result in a significant
error in the estimation of MM (G). The previous works avoid this issue by loosening the approximation
factor to O(poly log n), which allows a union bound over all vertices of the graph. We cannot afford
this. Instead, we control error propagation by showing that contributions of lower levels to higher level
tests have progressively smaller variance, and hence the total error stays bounded. This is nontrivial to
show, however, since none of the involved random variables concentrate particularly well around their
expectation â€“ our main tool for dealing with this is the Oversampling Lemma (Lemma 1) below.
An orthogonal source of difficulty stems from the fact that without polylogarithimic oversampling
Level-j-Testâ€™s are inherently noisy, and might misclassify nontrivial fractions of vertices across Ï‰(1)
levels. Informally, to cope with this issue we charge the cost of the vertices that the Level-j-Testâ€™s
misclassify to the rest of the vertices. More precisely, we show that although the algorithm might
misclassify the layer that a vertex belongs to for a constant fraction of the vertices, the amount of
resulting error of the edges adjacent to such vertices in the estimation of the matching is low compared
to the contribution of the rest of the edges. This enables us to bound the total error cost by a small
constant with respect to the size of the maximum matching.

5.1

Definitions and Preliminaries

In order to establish Theorem 5 we analyze the propagation of the error introduced by misclassification
in a new setting, not strictly corresponding to any of our algorithms. We first consider a hypothetical
set of tests. Namely, for each vertex v âˆˆ V , we consider executions of Level-i-Test(v) for i = 1, 2, . . .
until a test fails. Then we use these outcomes to categorize all the vertices of G into levels; these levels
loosely corresponding to those of Algorithm 1. We then define a set of edge weights based on these levels,
14

c : E â†’ R. Most of the section is then devoted to showing that this is close to a maximum fractional
M
matching in the sense that
i
X h
c(e) = Î˜(MM (G)).
E M
eâˆˆE

Defining a (nearly optimal) vertex cover. Our main tool in upper bounding the size of the
maximum matching in G is a carefully defined (random) nested sequence of V = Vb0 âŠ‡ Vb1 âŠ‡ . . . âŠ‡ VbJ+1
As we show later in Section 5.3 (see the proof of Lemma 6), the set


h
i
1
def
b
(8)
C = v âˆˆ V P v âˆˆ VT +1 â‰¤ 1 âˆ’ 2
3c
turns out to be a nearly optimal deterministic vertex cover in G.
Since our algorithm is essentially an approximate and randomized version of a peeling algorithm for
approximating matching size (Algorithm 1), our analysis is naturally guided by a sequence of random
subsets Vbj of the vertex set V . These subsets loosely correspond to the set of vertices in V that survived
j rounds of the peeling process. The sets are defined by the following process performed independently
by all vertices of G. Every v âˆˆ V keeps running Level-j-Test (v) (Algorithm 3) for j = 0, 1, 2, . . . ,
b
while the tests return true. Let L(v)
denote the largest j such that the corresponding test returned
true. We then let
n
o
def
b
Vbj = v âˆˆ V : L(v)
â‰¥ j , for j = 0, . . . , J + 1.
(9)
b
Note that the variables L(v)
are independent, and V = Vb0 âŠ‡ Vb1 âŠ‡ Vb2 âŠ‡ . . . âŠ‡ VbJ+1 with probability 1.
b
Also, L(v)
values can be defined via the sets Vbj as
def
b
L(v)
= maximum i such that v âˆˆ Vbi .

(10)

Recall that Vb0 equals V , so for any v there is at least one i such that v âˆˆ Vbi , and hence the definition
Eq. (10) is valid.
Our proof crucially relies on a delicate analysis of the probability of a given vertex v belonging to Vbj+1
conditioned on v belonging to Vbj for various j = 0, . . . , J. To analyze such events we define, for every
v âˆˆ V , the random variable Sj (v) as follows. Let r = cj m/d and let e1 , . . . , er be i.i.d. uniform samples
(with repetition) from the edge set E of G. Let i1 â‰¤ . . . â‰¤ iQ , where Q â‰¤ r, be the subset of indices
corresponding to edges in the sample that are incident on v, i.e., eia = (v, wa ) for every a = 1, . . . , Q
b a ) be independent samples from
(note that Q is a random variable). For every a = 1, . . . , Q let La âˆ¼ L(w
b
the distribution L(wa ) defined above. We now let
def

Sj (v) =

Q min{L
X
Xa ,j}
a=1

ciâˆ’j .

(11)

i=0

In other words, Sj (v) is simply the value of the variable S during the call Level-(j + 1)-Test
(Algorithm 3). In particular, we have
n
o
Vbj+1 = v âˆˆ Vbj Sj (v) < Î´ .
(12)
def
Remark 3. Note that Eq. (11) and Eq. (12), together with Vb0 = V define Vbj recursively (and in a
non-cyclic manner). Indeed, Vbj+1 depends on Sj (v) which depends on min{La , j} whose distribution is
defined by Vbi , i â‰¤ j only.

We also define Sj (v) in a compact way and use that definition in the proofs extensively
cj m/d
def

Sj (v) =

X

1 [v âˆˆ ek ]

min(L(ek \v),j)

X

ciâˆ’j .

(13)

i=0

k=1
ek âˆ¼ U (E)

Remark 4. Note that here L(w) is a random variable independently sampled from the distribution of
b
L(w),
similarly to La in Eq. (11).
Remark 5. Here U (E) denotes the uniform distribution over E. Also we denote by v âˆˆ e the fact that
e is adjacent to v, where v is a vertex and e is an edge. In this case we denote the other endpoint of e
by e\v. We use this notation heavily throughout the analysis.
15

c. We now define a (random) fractional assignment
Defining the fractional pseudo-matching M
c of mass to the edges of G that our analysis will be based on: we will later show (see Lemma 4 in
M
c is close to some matching of G. For every edge e = (u, v) âˆˆ E we let
Section 5.2) that M
b
b
min{L(u),
L(v)}

X

c(u, v) def
M
=

ci /d,

(14)

i=0

c| of the pseudo-matching M
c to be the summation of its fractional matching mass
and define the size |M
along all the edges:
X
c| def
c(e).
|M
=
M
(15)
eâˆˆE

c
We
h note
i that M is a random variable, and we show later in Section 5.2 (see Lemmas 4 and 6) that
c| is a constant factor approximation to the size of a maximum matching in G. The following
E |M
natural auxiliary definitions will be useful.
For every vertex v âˆˆ V , we let
b
b
min{L(v),
L(w)}

X

X

wâˆˆN (v)

i=0

c(v) def
M
=

X

ci /d =

c(v, w),
M

(16)

wâˆˆN (v)

denote amount of fractional mass incident to v. Similarly, we let
b
min{L(w),j}

cj (v) def
M
=

X

X

wâˆˆN (v)

i=0

ci /d,

(17)

b
denote the amount of fractional mass contributed to v by its neighbors conditioned on L(v)
= j.
c
In the upcoming section we will prove upper and lower bounds on M to prove that it is within
a constant factor of the matching number of G. Supposing we have this result, it is easy to deduce
Theorem 5.
c(e) for some edge e = (u, v). M
c(e) is
Proof of Theorem 5. Consider the marginal distribution of M
essentially the result of running independent Level-i-Testâ€™s on u and v to determine at which level
either vertex is peeled off, and then updating the edge weight in accordance with Edge-Level-Test.
c(e) is identical to that of Edge-Level-Test (e). Therefore
Thus the marginal distribution of M
i
h
i
X
X h
c(e) = E |M
c| = Î˜(|M |),
E [Me ] =
E M
eâˆˆE

eâˆˆE

thus proving the theorem.
Next we state two observations that follow directly from the definitions above.
Observation 1. For any vertex v and any j the following holds:
cj (v) depends only on vertices other than v, therefore it is independent of L(v).
b
(a) M
c(v) = M
cb (v).
(b) M
L(v)
h
i
h
i
Pj
cj (v) = E [Sj (v)] = P
bi Â· ci /d.
P
w
âˆˆ
V
(c) E M
wâˆˆN (v)
i=0
Observation 2. For any vertex v and any j, it holds that
(c + 1) Â· E [Sj (v)] â‰¥ E [Sj+1 (v)] ,
where Sj (v) is defined in Eq. (13).

16

Proof. This follows directly from the formula of E [Sj (v)] in Observation 1 c:
h
i
X
E [Sj+1 (v)] âˆ’ E [Sj (v)] =
P v âˆˆ Vbj+1 Â· cj+1 /d
wâˆˆN (v)

â‰¤

X

h
i
P v âˆˆ Vbj Â· cj+1 /d

wâˆˆN (v)

â‰¤cÂ·

j
X
X

h
i
P w âˆˆ Vbi Â· ci /d

i=0 wâˆˆN (v)

= c Â· E [Sj (v)] ,
which implies the observation.
We will use the following well-known concentration inequalities.
Theorem 7 (Chernoff bound). Let X1 , . . . , Xk be independent random variables taking values in [0, a].
def Pk
Let X = i=1 Xi . Then, the following inequalities hold:
(a) For any Î´ âˆˆ [0, 1] if E [X] â‰¤ U we have

P [X â‰¥ (1 + Î´)U ] â‰¤ exp âˆ’Î´ 2 U/(3a) .
(b) For any Î´ > 1 if E [X] â‰¤ U we have
P [X â‰¥ (1 + Î´)U ] â‰¤ exp (âˆ’(Î´ + 1) log(Î´ + 1)U/3a) â‰¤ exp (âˆ’Î´U/(3a)) .
(c) For any Î´ > 0 if E [X] â‰¥ U we have

P [X â‰¤ (1 âˆ’ Î´)U ] â‰¤ exp âˆ’Î´ 2 U/(2a) .

5.2

c
Lower Bound: Constructing a Near-Optimal Matching from M

c|. Namely, we show that the mass
As the main result of this section, we prove a lower bound on |M
c is at most a constant factor larger than the size of a maximum matching.
defined by M
c|). Let c â‰¥ 20 and 0 < Î´ â‰¤ 1. For any graph G = (V, E), there exists a
Lemma 4 (Lower-bound on |M
h
i
c| = O(|M |), where M
c is defined in Eq. (15).
feasible fractional matching M such that E |M
The following lemma, which is the main technical result that we use in the proof of Lemma 4, shows
c(v) is large at some level, then it is very likely that v does not
that if the pseudo-matching weight M
pass the next vertex tests. This lemma is crucial in proving an upper-bound on the estimated size of the
matching.
c(v)). For any vertex v, any j > 0, and constants c and x such that
Lemma 5 (Concentration on M
c â‰¥ 20 and x â‰¥ 100c log c, we have:
h
h
ii
h
i 10c2
c(v) Â· 1 L(v)
b
c(v) â‰¥ x âˆ§ L(v)
b
=j .
P M
=j+1 â‰¤ 2 Â·E M
x

(18)

b
c(v) are defined in Eq. (10) and Eq. (16), respectively.
Where L(v)
and M
c
The full proof of Lemma 5 is deferred to Appendix A. Next, we show certain basic properties of M
that are derived from Lemma 5.
Corollary 1. For any vertex v, c â‰¥ 20, and x â‰¥ 100c log c, we have:
h
i
h
i
2
c(v) â‰¥ x â‰¤ 10c Â· E M
c(v) ,
P M
x2
c(v) is defined in Eq. (16).
where M
17

b
Proof. We simply sum Eq. (18) from Lemma 5 over j = 1, . . . , J. The term corresponding to L(v)
= J +1
is missing from the right hand side, but this only makes the inequality stronger. The term corresponding
b
to L(v)
= 1 is missing from the left hand side, but the corresponding probability is in fact 0. Indeed,
b
if L(v)
= 1, each edge adjacent to v has only 1/d weight on it, and there are at most d such edges.
c(v) â‰¤ 1 < x.
M
The following corollary enables us to bound the contribution of the high degree vertices to the
fractional matching. This is a key ingredient to proving a lower bound on the estimated matching size.
c(v) â‰¥ Î», where we think of
Namely, in the proof of Lemma 4 we ignore all the vertices such that M
c. The
Î» being some large constant. Ignoring those vertices reduces the matching mass contained in M
following corollary essentially bounds the matching mass lost in this process.
Corollary 2. For any vertex v, c â‰¥ 20, and Î» â‰¥ 100c2 :
h
h
ii
h
i
c(v) Â· 1 M
c(v) â‰¥ Î» â‰¤ 1 E M
c(v) ,
E M
4
c(v) is defined in Eq. (16).
where M
Proof. We prove this corollary by applying Corollary 1 for different values of x.
âˆž
h
h
ii X
i
h
c(v) Â· 1 M
c(v) â‰¥ Î» â‰¤
c(v) â‰¥ Î»2i
E M
Î»2i+1 P M
i=0
âˆž
by Corollary 1 X

â‰¤

10c2 h c i
E M (v)
Î»2 22i
i=0
h
i
c(v) X
âˆž
20c2 E M
2âˆ’i
=
Î»
i=0
1 hc i
â‰¤ E M (v)
4
Î»2i+1 Â·

Since Î» > 80c2 .
We now have all necessary tools to prove Lemma 4.
def

Proof of Lemma 4. We prove this lemma by constructing such a matching, M . To that end, let Î» =
c(v) â‰¥ Î» we say that v is a violating vertex. Similarly, any edge adjacent to at least one
100c2 . If M
c to M . Moreover, we
violating vertex we also call violating. We add all the non-violating edges of M
reduce the weight of each edge in fractional matching M by the factor 1/Î». Then, M is a feasible
fractional matching since the summation of the weights of the edges connected to each vertex is at most
one.
We now compute the expected weight of M . Recall that, in any fractional matching, the summation
of the weights of the edges is half the summation of the weights of the vertices, since each edge has two
endpoints. Therefore
1X
E [|M (v)|]
2
vâˆˆV
h
h
ii
1 X  hc i
c(v) Â· 1 M
c(v) â‰¥ Î»
=
E M (v) âˆ’ 2E M
2Î»
vâˆˆV
i
by Corollary 2 1 X 1 h
c(v)
â‰¥
E M
2Î»
2
vâˆˆV
h
i
1
c .
â‰¥ E M
4Î»
E [|M |] =

Since Î» is a constant by definition, this completes the proof.

18

5.3

Upper bound: Constructing a Near-Optimal Vertex Cover

c that our algorithm (implicitly)
Lemma 4 essentially states that the size of the pseudo-matching M
constructs does not exceed by more than a constant factor the size of a maximum matching. By the
c. Namely, we show that the size of M
c is
next lemma, we also provide a lower-bound on the size of M
only by a constant factor smaller than a vertex cover of the input graph. Since from duality theory the
size of a vertex cover is an upper-bound on the size of a maximum matching, the next lemma together
c is a Î˜(1)-approximate maximum matching.
with Lemma 4 shows that M
c|). For sufficiently small Î´ > 0 and large enough c the following holds.
Lemma 6 (Upper-bound on |M
h
i
c| = â„¦(|C|), where |M
c|
For any graph G = (V, E), there exists a feasible vertex cover C such that E |M
is defined in Eq. (15).
In our proof, we choose C to be the set of vertices that with constant probability do not pass to the
very last level, i.e., to the level T + 1. (C corresponds to the set that we defined in Eq. (8).) Then, the
high-level approach in our proof of Lemma 6 is to choose a vertex v âˆˆ C and show that with constant
probability the matching incident to v is sufficiently large.
Observe that v is added to C if the algorithm estimates that at some level the matching weight
of v is at least Î´. So, in light of our approach, we aim to show that if v has expected matching at
least Î´ then it is unlikely that its actual matching weight is much smaller than Î´ in a realization. So,
for every j independently we first upper-bound the probability that v gets added to C at level j if its
actual matching mass by level j is much smaller than Î´. To provide this type of upper-bound across
all the levels simultaneously, one standard approach would be to take a union bound over all the levels.
Unfortunately, the union bound would result in a loose upper-bound for our needs.
Instead we will show that over the levels the algorithmâ€™s estimates of the weight adjacent to a specific
vertex becomes more and more accurate (since it takes more and more samples). This allows us to
show that the likelihood of misclassifying a vertex by peeling it too early is small even across potentially
â„¦(log n) levels. The matching weight that the algorithm estimates while testing level j can be decomposed
into two parts:
â€¢ Aj â€“ the weight coming from the neighbors up to level j âˆ’ 1.
â€¢ Bj â€“ the weight coming from the neighbors that pass to level j.
Now, compared to the weight estimate while performing the test for level j âˆ’1, to obtain Aj the algorithm
performs c times more tests and takes their average. This means that Aj is a more precise version of
a test the algorithm has already done. Since this is the case, we can amortize the error coming from
estimating Aj to the tests that the algorithms has already applied, and bound only the error coming
from estimating Bj . This observation enables us to provide a more precise analysis than just applying a
union bound.
The next lemma makes formal our discussion of relating Aj and the weight estimate the algorithm
performs while testing level j âˆ’ 1. In this lemma, it is instructive to think of X as of Aj , of each X as a
single instance of level j âˆ’ 1 testing (that corresponds to Ajâˆ’1 + Bjâˆ’1 ), and of Yk as the test performed
on a single sampled edge.
Lemma 1 (Oversampling lemma). For sufficiently small Î´ > 0 and large enough c the following holds.
PK
def
Let
X =
and X =
k=1 Yk be a sum of independent random variables Yk taking values in [0, 1],

P
c
1
i=1 Xi where Xi are iid copies of X. If E [X] â‰¤ Î´/3 and P [X â‰¥ Î´] = p, then P X â‰¥ Î´ â‰¤ p/2.
c
This lemma formalizes the intuition that given a random variable X, taking the average of many
independent copies gives a more accurate estimate of the mean then X itself, and it should therefore
be less likely to exceed a threshold significantly above the mean. In the general case however, this is
not true. Indeed consider a variable X such that X > cÎ´ with some extremely small probability and
X < Î´ the rest of the time. In this case if even one instance of the independent samples exceeds Î´
then the average of all the samples (X) will as well; therefore the probability of exceeding Î´ actually
increases. To be able to prove the lemma we need to use an additional characteristic of X: namely that
it is the independent sum of bounded variables, and therefore it concentrates reasonably well around its
expectation. This characteristic will hold for the particular random variables to which we want to apply
the oversampling lemma in the proof of Lemma 6, specifically Aj .
We present the proof of this lemma in Appendix B. We are now ready to prove Lemma 6.

19

Proof of Lemma 6. Define the following set of vertices


h
i
1
def
b
C = v âˆˆ V P v âˆˆ VJ+1 â‰¤ 1 âˆ’ 2 .
3c

(19)

Notice that this set is deterministic and does not depend on the outcome of VbJ+1 , only its distribution.
First we prove that C is indeed a vertex cover. Suppose toward contradiction that both u, v 6âˆˆ C for
some edge e = (u, v). Suppose u has already made it to VbJ . Then in the course of deciding whether u
makes it further into VbJ+1 we take mcJ /n â‰¥ m/c2 iid edge-samples. Hence the probability of sampling
the edge e at least one of those times is

mcJ /d
m/c2

1
1
1
â‰¥1âˆ’ 1âˆ’
â‰¥ 2,
1âˆ’ 1âˆ’
m
m
2c
for c â‰¥ 1. If e is sampled, then the probability that v makes it into VbJ is at least 1 âˆ’ 1/(3c2 ) which is
strictly greater than 2/3, since v 6âˆˆ C. This means that with more than 1/(3c2 ) probability u would fail
at level J even if it made it that far and therefore must be in C. By contradiction C is a vertex cover.
Consider a vertex v âˆˆ C. Our goal is to show that the fractional matching adjacent to v is small with
small probability. To that end, we upper-bound the probability that the matching adjacent to v is less
than Î³, for some positive constant Î³  Î´. Indeed we will see that Î³ â‰¤ 1/(12c2 ) works.
Let j ? be the largest j âˆˆ [0, J + 1] such that E [Sj ? (v)] < Î³. We prove that the combined probability
of v failing any test up to j ? is at most 1/(6c2 ). Before proving this, let us explain the rest of the proof,
assuming we get such guarantee.
b
cj (v) and
Note that the random variable L(v)
is independent from the sequence of random variables M
c(v) = M
cb (v) by Item a and Item b of Observation 1. Thus the expected size of M
c is sufficiently
M
L(v)
?
b > j . This does not happen in two cases. Either v
large (at least Î³) conditioned on the event that L
fails a test at a level lower than or equal to j ? , or v fails no test, but j ? = T + 1, so v reaches the last
c(v) is too small in expectation regardless. The first
level but expected size of the incident matching M
event is bounded by 1/(6c2 ) by the above guarantee; the second event is bounded by 1 âˆ’ 1/(3c2 ) since
v âˆˆ C. Therefore v must reach a level greater than j ? with probability at least 1/(6c2 ). Note that this
also shows that j ? < T + 1, which is not clear from definition. Hence,
+1 h
h
i TX
i h
i
c(v) â‰¥
b
cj (v)
E M
P L(v)
=j E M
j=0

â‰¥

T
+1
X

h
i h
i
b
cj ? +1 (v)
P L(v)
=j E M

j=j ? +1

h
i
Î³
b
â‰¥P L(v)
> j? Â· Î³ > 2 ,
6c
h
i
c| â‰¥ Î³2 Â· |C|.
and consequently by linearity of expectation follows E |M
6c
In the rest of the proof we upper-bound the probability that v fails before or at the j ? th level.
j?
h
i X
h
i
?
b
b
P L(v) â‰¤ j =
P L(v)
=j
j=0
j?
h
i
X
b
â‰¤
P L(v)
= j|v âˆˆ Vbj

(20)

j=0
?

=

j
X

P [Sj (v) â‰¥ Î´] .

(21)

j=0

b
Eq. (20) follows from the fact that L(v)
= j implies that v âˆˆ Vbj (while the other direction does not

20

necessarily hold). We next rewrite P [Sj (v) â‰¥ Î´]. By definition Eq. (13), we have
ï£¹
ï£®
ï£º
ï£¯ mcj /d
min(L(e\v),j)
ï£º
ï£¯
X
X
ï£º
ï£¯
iâˆ’j
1 [v âˆˆ ek ]
c
â‰¥ Î´ï£º
P [Sj (v) â‰¥ Î´] = P ï£¯
ï£º
ï£¯
i=0
ï£»
ï£° k=1
ek âˆ¼ U (E)
We split the contribution to Sj (v) into two parts: the weight coming from the sampled edges incident to
v up to level j âˆ’ 1 (defined as the sum Aj below); and, to contribution coming from the sampled edges
incident to v that passed to level j (corresponding to the sum Bj below). More precisely, for each j, the
sums Aj and Bj are defined as follows
mcj /d

min(L(e\v),(jâˆ’1))

1 [v âˆˆ e]

X

def

Aj =

X

ciâˆ’j ,

i=0

k=1
ek âˆ¼ UE
mcj /d

1 [v âˆˆ e] 1 [L(e\v) â‰¥ j] ,

X

def

Bj =

k=1
ek âˆ¼ UE
where we use the notation v âˆˆ e for v being an endpoint of e; in this case e\v denotes the other endpoint.
Observe that if Sj (v) â‰¥ Î´, then either Aj â‰¥ Î´, or Aj < Î´ and Bj â‰¥ Î´ âˆ’ Aj > 0. If Bj > 0, it means
that at least one edge incident to v was sampled and it passed to level j. This sample contributes 1 to
Bj , and hence if Bj > 0 it implies Bj â‰¥ 1. Then we can write
P [Sj (v) â‰¥ Î´] = P [Aj + Bj â‰¥ Î´] = P [Aj â‰¥ Î´ âˆ¨ Bj â‰¥ 1] â‰¤ Î±j + Î²j ,
def

(22)

def

where we define Î±j = P [Aj â‰¥ Î´] and Î²j = P [Bj â‰¥ 1]. To upper-bound P [Sj (v) â‰¥ Î´], we upper-bound
Î±j â€™s and Î²j â€™s separately.
Upper-bounding Î±j and Î²j . We first upper-bound Î±j by (Î±jâˆ’1 + Î²jâˆ’1 )/2 by applying Lemma 1.
We begin by defining X, Y and X that correspond to the setup of Lemma 1. Let Yk be the following
random variable
min(L(e\v),(jâˆ’1))
X
Y = 1 [v âˆˆ e]
ciâˆ’(jâˆ’1) ,
i=0

Pmcjâˆ’1 /n
where e is an edge sampled uniformly at random. Then, Ajâˆ’1 + B
= k=1
Yk , where Yk is a
Pjâˆ’1
c
copy of Y . Let X = Ajâˆ’1 + Bjâˆ’1 and X = Aj . Observe that X = i=1 Xi /c. Then, for j > 0, it holds
Î±j

P [Aj â‰¥ Î´]

=
by Lemma 1

â‰¤

P [Ajâˆ’1 + Bjâˆ’1 â‰¥ Î´] /2

by Eq. (22)

â‰¤

(Î±jâˆ’1 + Î²jâˆ’1 )/2.

(23)

jâˆ’1
X
1
Î².
jâˆ’i i
2
i=0

(24)

In the case of j = 0, we have Î±0 = 0.
Applying Section 5.3 recursively, we derive
Î±j â‰¤

Now we upper-bound the sum of Î²j â€™s by using Markovâ€™s inequality: since for every j = 0, . . . , j ?
mcj /d

E [Bj ] =

X

E [1 [v âˆˆ e] 1 [L(e\v) â‰¥ j]] = E [|Nj (v)|] cj /d,

k=1
ek âˆ¼UE

21

we get
?

j
X

j?
h
i
X
Î²j â‰¤
E |N (v) âˆ© Vbj | Â· cj /d

j=0

j=0
j?
h
i
X X
P w âˆˆ Vbj Â· cj /d

=

wâˆˆN (v) j=0

= E [Sj ? (v)]
â‰¤Î³

(25)

Finalizing the proof. Combining the above inequalities together, we derive
?

h
i
b
P L(v)
â‰¤ j?

from Eq. (21) and Eq. (22)

â‰¤

j
X

(Î±j + Î²j )

j=0
?

from Eq. (24)

=

j
X
j=0

jâˆ’1
X
1
Î²
Î²j +
jâˆ’i i
2
i=0

!

?

j
X

â‰¤

2Î²j

j=0
from Eq. (25)

â‰¤

2Î³

â‰¤

1/(6c2 ),

for Î³ â‰¤ 1/(12c2 ), as desired

6

LCA

Local computational algorithms (or LCAâ€™s) have been introduced in [RTVX11] and have since been
studied extensively, particularly in context of graph algorithms [ARVX12, MRVX12, MV13, EMR14,
LRY17, GU19]. The LCA model is designed to deal with algorithms on massive data, such that both
the input and the output are too large to store in memory. Instead we deal with both via query access.
In the setting of graphs, we have access to a graph G via queries which can return the neighbors of a
particular vertex. We must then construct our output (in our case a constant factor maximum matching)
implicitly, such that we can answer queries about it consistently. That is, for any edge we must be able
to say whether or not it is in the matching and for any vertex we must be able to say whether or not
any edge adjacent to it is in the matching.
In this section we show that our approach, detailed in the previous sections, can be implemented in
the LCA model as well. Specifically, we prove Theorem 2.
Theorem 2. Let G be a graph with n vertices and maximum degree d. Then there exists a random
matching M , such that E [|M |] = Î˜(MM(G)), and an algorithm that with high probability:
â€¢ Given an edge e of G, the algorithm reports whether e is in M or not by using O(d log n) queries.
â€¢ Given a vertex v of G, the algorithm reports whether v is in M or not by using O(d log n) queries.
Moreover, this algorithm can be executed by using O(d log3 n) bits of memory.
Remark 6. It can also be shown with more careful analysis that if d = O((n/ log n)1/4 ) then |M | =
Î˜(MM(G)) with high probability. A proof sketch of this claim can be found in Section 6.7.
The proof of this theorem is organized as follows. First, in Section 6.3, we state our LCA algorithms.
Then, in Section 6.4 we analyze the query complexity of the provided algorithms, essentially proving the
two bullets of Theorem 2. In Section 6.5 we show that the matching fixed by our algorithms is Î˜(1)
approximation of MM (G). In Section 6.6 we discuss about the memory requirement of our approach
and the implementation of consistent randomness. These conclusions are combined in Section 6.7 into a
proof of Theorem 2.
22

6.1

Overview of Our Approach

Our main LCA algorithm simulates Algorithm 3, i.e., it simulates methods Level-(j + 1)-Test and
Edge-Level-Test provided in Section 3.2. However, instead of taking cj m/d random edge-samples
from the entire graph (as done on Line 4 of Level-j-Test (v)), we first sample the number of edges D
incident to a given vertex v, where D is drawn from binomial distribution B(cj m/d, d(v)/m). Then, we
query D random neighbors of v. This simulation is given as Algorithm 5.
In our analysis, we tie the fractional matching weight of an edge to the query complexity. Essentially,
we show that a matching weight w of an edge is computed by performing O(w Â· d) queries. As we will
see, this allows us to transform a fractional to an integral matching by using only O(d log n) queries per
an edge.
From fractional to integral matching. Given an edge e = {u, v}, LCA-Edge-Level-Test outputs
the fractional matching weight we of e. However, our goal is to implement an oracle corresponding to an
integral matching. To that end, we round those fractional to 0/1 weights as follows. First, each edge e is
marked with probability we /10Î», for some large constant Î», specifically the constant from Corollary 2,
the result of which we will be relying on heavily. Then, each edge that is the only one marked in its
neighborhood is added to the matching. We show that in expectation this rounding procedure outputs
a Î˜(1)-approximate maximum matching.
Consistency of the oracles. Our oracles are randomized. Nevertheless, they are designed in such a
way that if the oracle is invoked on an edge e multiple times, each time it provides the same output. We
first present our algorithms by ignoring this property, and then in Section 6.6 describe how to obtain
these consistent outputs.

6.2

Related Work

Parnas and Ron [PR07] initiated the question of estimating the minimum vertex cover and the
maximum matching size in sublinear time. First, they propose a general reduction scheme that takes a
k-round distributed algorithm and design an algorithm that for graphs of maximum degree d has O(dk )
query complexity. Second, they show how to instantiate this reduction with some known distributed
algorithms to estimate the maximum matching size with a constant multiplicative and n additive
factor with dO(log (d/)) queries. An algorithm with better dependence on , but worse dependence on d,
was developed by Nguyen and Onak [NO08] who showed how to obtain the same approximation result
by using 2O(d) /2 queries. Significantly stronger query complexity, i.e., O(d4 /2 ), was obtained by
Yoshida et al. [YYI09]. Both [NO08] and [YYI09] analyze the following randomized greedy algorithm:
choose a random permutation Ï€ of the edges; visit the edges sequentially in the order as given by Ï€; add
the current edge to matching if none of its incident edge is already in the matching. We analyze this
algorithm in great detail in Section 7. As their main result, assuming that the edges are sorted with
respect to Ï€, [YYI09] show that this randomized greedy algorithm in expectation requires O(d) queries
to output whether a given edge is in the matching fixed by Ï€ or not. When the edges are not sorted,
their algorithm in expectation requires O(d2 ) queries to simulate the randomized greedy algorithm.
The result of [YYI09] was improved by Onak el al. [ORRR12], who showed how to estimate the
maximum matching size by using OÌƒ(dÂ¯ Â· poly(1/) queries, where dÂ¯ is the average degree of the graph.
Instead of querying randomly chosen edges, [ORRR12] query randomly chosen vertices, which in turn
allows them to choose a sample of Î˜(1/2 ) vertices rather than a sample of Î˜(d2 /2 ) edges. Then, given
a vertex v, the approach of [ORRR12] calls the randomized greedy algorithm on (some of) the edges
incident to v. By adapting the analysis of [YYI09], [ORRR12] are able to show that the expected vertexquery complexity of their algorithm is O(d). As noted, these results estimate the maximum matching
size up to a constant multiplicative and n additive factor. Hence, assuming that the graph does not
have isolated vertices, to turn this additive to a constant multiplicative factor it suffices to set  = 1/d.
To approximate the maximum matching size, the aforementioned results design oracles that given
an edge e outputs whether e is in some fixed Î˜(1)-approximate maximum matching, e.g., a maximal
matching, or not. Then, they query a small number of edges chosen randomly, and use the oracle-outputs
on those edges to estimate the matching size. Concerning the query complexity, the usual strategy here
is to show that running the oracle on most of the edges requires a â€œsmallâ€ number of queries, leading
to the desired query complexity in expectation. When those oracles are queried on arbitrary chosen
edge, their query complexity might be significantly higher than the complexity needed to estimate the

23

maximum matching size. We devote Section 7 to analyzing the randomized greedy algorithm mentioned
above, and show that in some cases it requires at least â„¦(d2âˆ’ ) queries, for arbitrary small constant .
This is in stark contrast with the expected query complexity of O(d).
Recently, [GU19] showed that there exists an oracle that given an arbitrary chosen vertex v outputs
whether v is in some fixed maximal independent set or not by performing dO(log log d) Â· poly log n queries,
which improves on the prior work obtaining dO(poly log d) Â· poly log n complexity [RTVX11, ARVX12,
LRY17, Gha16]. When this oracle is applied to the line graph, then it reports whether a given edge is in
a fixed maximal matching or not.

6.3

Algorithms

Algorithm 5 is an LCA simulation of Algorithm 3. In LCA-Level-(j + 1)-Test, that is an LCA
simulation of Level-(j + 1)-Test, we can not choose a random edge-sample from the entire graph as it
is done on Line 4 of Level-(j +1)-Test. So, instead, we first sample from the distribution corresponding
to how many of those random edge-samples will be incident to a given vertex v. In this way we obtain
a number D (see Line 3 of LCA-Level-(j + 1)-Test). Then, our algorithm samples D random edges
incident to v and performs computation on them. Note that this is equivalent to the iid variant, as we
would ignore all edges not adjacent to v.
Algorithm 5 This is an LCA simulation of Algorithm 3 for a graph of maximum degree d. Given an
edge e, this algorithm returns a fractional matching-weight of e.
procedure LCA-Edge-Level-Test(e = (u, v))
for i = 1 to J + 1 do
if LCA-Level-i-Test(u) and LCA-Level-i-Test(v) then
w â† w + ci /n
5:
else
6:
return w
7:
return w
1:
2:
3:
4:

Algorithm 6 This is an LCA simulation of Algorithm 4. Given a vertex v, this algorithm returns true
if it belongs to level j + 1 and false otherwise.
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:

procedure LCA-Level-(j + 1)-Test(v)
Sâ†0

Sample D from binomial distribution B cj Â· m
d , d(v)/m
for k = 1 to D do
Let e = {v, w} be a random edge incident to v.
iâ†0
while i â‰¤ j and LCA-Level-i-Test(w) do . LCA-Level-0-Test(w) returns true by
definition.
S â† S + ciâˆ’j
if S â‰¥ Î´ then
return false
iâ†i+1
return true

We now build on LCA-Edge-Level-Test and LCA-Level-(j + 1)-Test to design an oracle that
for some fixed Î˜(1)-approximate maximum matching returns whether a given edge is in this matching
or not.
Our final LCA algorithm is Oracle-Edge (see Algorithm 8). Given an edge e = {u, v}, this
algorithm reports whether e is in some fixed matching M or not. This matching M is fixed for all
the queries. Oracle-Edge performs rounding of a fractional matching as outlined above. As a helper
method, it uses Matching-Candidate that as a parameter gets an edge e, and returns 0 and 1 randomly
chosen with respect to LCA-Edge-Level-Test(e). We note that if Matching-Candidate(e) returns
1 that it does not necessarily mean that e is included in M . It only means that e is a candidate for being
added to M . In fact, e is added to M if e is the only matching candidate in its 1-hop neighborhood.

24

Algorithm 7 Given an edge e, this method rounds fractional matching mass returned by
LCA-Edge-Level-Test(e) (that is first scaled by 10Î») to an integral one.
procedure Matching-Candidate(e)
Let Î» be the constant from Corollary 2.
Let Xe be 1 with probability LCA-Edge-Level-Test(e)/(10Î»), and be 0 otherwise.
4:
return Xe

1:
2:
3:

Algorithm 8 An oracle that returns true if a given edge e is in the matching, and returns false
otherwise.
1: procedure Oracle-Edge(e = (u, v))
2:
Xe â† Matching-Candidate(e)
3:
if Xe = 0 then
4:
return false
5:
for each edge e0 incident to e do
6:
Xe0 â† Matching-Candidate(e0 )
7:
if Xe0 = 1 then
8:
return false . e is in the matching only if it is the only candidate in its neighborhood
9:
return true
We also design a vertex-oracle (see Algorithm 9) that for a given vertex v reports whether an edge
incident to v is in the matching M or not. This oracle iterates over all the edges incident to v, and on
each invokes Matching-Candidate. If any invocation of Matching-Candidate(e) returns 1 it means
that there is at least one matching candidate in the neighborhood of v. Recall that having two or more
matching candidates in the neighborhood of v would result in none of those candidates being added to
M . Hence, v is in M only if Oracle-Edge(e) return true. Otherwise, v is not in M .
Algorithm 9 An oracle that returns true if a given vertex v is in the matching, and returns false
otherwise.
1: procedure Oracle-Vertex(v)
2:
for each edge e incident to v do
3:
Xe â† Matching-Candidate(e)
4:
if Xe = 1 then
5:
return Oracle-Edge(e)
6:
return false
. None of the edges incident to v is a matching candidate.

6.4

Query Complexity

We begin by analyzing the query complexity of LCA-Level-j-Test. Statement of the next lemma is
an adapted version of Lemma 2 to LCA.
Lemma 7. For every c â‰¥ 2, 0 < Î´ â‰¤ 1/2, and graph G = (V, E) with the maximum degree at most d, let
Ï„j be the maximum query complexity of LCA-Level-j-Test defined in Algorithm 5, for j âˆˆ [1, J + 1].
Then, with probability one we have:
Ï„j â‰¤ 2cjâˆ’1 .
(26)
Proof. We prove this lemma by induction that Eq. (26) holds for each j. This proof follows the lines of
Lemma 2.
Base of induction For j = 1 the bound Eq. (26) holds directly as
Ï„1 â‰¤ 1
by definition of the algorithm. Indeed, as soon as we sample a single neighbor the algorithm returns
false.

25

Inductive step. Assume that Eq. (26) holds for j. We now show that Eq. (26) holds for j + 1 as well.
Consider any vertex v âˆˆ V and LCA-Level-(j + 1)-Test(v). Let Î±i be the number of recursive
LCA-Level-i-Test calls invoked. Then, Ï„j+1 can be upper-bounded as
Ï„j+1 â‰¤ Î±0 +

j
X

Î±i Ï„i .

i=1

For Î´ < 1, we have Î±0 â‰¤ cj . Moreover, from Eq. (26) and our inductive hypothesis, it holds
Ï„j+1 â‰¤ cj +

j
X

Î±i Â· 2ciâˆ’1

i=1
j

=c Â·

1+2

j
X

!
iâˆ’1âˆ’j

c

Î±i

.

i=1

The rest of the proof now follows in the same way as in Lemma 2.
The next claim is an LCA variant of Lemma 3. The proof is almost identical.
Lemma 8. For every c â‰¥ 2, 0 < Î´ â‰¤ 1/2, and graph G = (V, E), for any edge e = (u, v) âˆˆ E, if Me
is the output of an invocation of LCA-Edge-Level-Test(e), then with probability one this invocation
used at most 4Me Â· d queries.
Proof. As before, let Ï„j be the maximum possible number of samples required by LCA-Level-j-Test.
From Lemma 7 we have Ï„j â‰¤ 2cjâˆ’1 .
Let I be the last value of i, at which the algorithm LCA-Edge-Level-Test(e) exits the while loop
in Line 6. Alternately if the algorithm exits in Line 7, let I = J. This P
means that the variable w has
Iâˆ’1
been incremented for all values of i from 0 to I âˆ’ 1, making w equal to i=0 ci /n. On the other hand,
in the worst case scenario, LCA-Level-i-Test has been called on both u and v for values of i from 1 to
I. Therefore, the number of queries used by this invocation of LCA-Edge-Level-Test (e) is at most
2

I
X
i=1

Ï„i â‰¤ 2

I
X

2ciâˆ’1 = 4

Iâˆ’1
X

ci = 4Me Â· d,

i=0

i=1

where we used the fact that I â‰¤ T .
We are now ready to prove the query complexity of our oracles, and at the same time prove the query
complexity part of Theorem 2.
Lemma 9. For every c â‰¥ 2, 0 < Î´ â‰¤ 1/2 and G = (V, E) be a graph of maximum degree at most d.
Then, for any edge e âˆˆ E and with probability at least 1 âˆ’ nâˆ’5 , Oracle-Edge(e) requires O(d log n)
queries.
Proof. Each loop of Oracle-Edge(e = {u, v}) queries one of the edges incident to u or v. Hence,
obtaining these incident edges takes O(d) queries.
Let E 0 be the set of edges on which LCA-Edge-Level-Test is called from
Matching-Candidate as a result of running Oracle-Edge(e) of Line 6. Let W be the sum of
outputs of LCA-Edge-Level-Test on these edges. First, by Lemma 8, the query complexity of all
the tests performed on Line 6 is O(W Â· d). Second, following that Xe0 is obtained by rounding
LCA-Edge-Level-Test(e0 )/(10Î») in Matching-Candidate, we have that
"
#
X
W
E
Xe0 =
.
10Î»
0
0
e âˆˆE

As soon as Xe0 = 1 for any e0 âˆˆ E 0 , the algorithm terminates and returns false on Line 8. Since Xe0 s
are independent random variables, by Chernoff bound (Theorem 7 (b)), with probability at least 1 âˆ’ nâˆ’5
we have W/(10Î») â‰¤ 20 log n. Hence, we conclude that the loop of Oracle-Edge requires O(d log n)
queries with probability at least 1 âˆ’ nâˆ’5 .
Lemma 10. For every c â‰¥ 2, 0 < Î´ â‰¤ 1/2 and G = (V, E) be a graph of maximum degree at most d.
Then, for any vertex v âˆˆ V , with high probability Oracle-Vertex(v) requires O(d log n) queries.
26

Proof. Each loop of Oracle-Vertex(v) queries one of the edges incident to v. Hence, obtaining these
incident edges takes O(d) queries.
Following the same arguments as in Lemma 9 we have that with probability at least 1 âˆ’ nâˆ’5 the total
query complexity of all invocations of Matching-Candidate on Line 3 is O(d log n). In addition, the
algorithm invokes Oracle-Edge at most once. Hence, from Lemma 9, the total query complexity of
Oracle-Vertex is O(d log n) with high probability.

6.5

Approximation Guarantee

We now prove that outputs of Oracle-Edge correspond to a Î˜(1)-approximate maximum matching of
G.
Lemma 11. For sufficiently small Î´ > 0 and large enough c the following holds. Let G = (V, E) be a
graph whose maximum degree is at most d. Let M be the set of edges for which Oracle-Edge outputs
true. Then, M is a matching and E [|M |] = Î˜(MM (G)).
Proof. We first argue that M is a matching.
For an edge e, let Xe be the output of
Matching-Candidate(e). The edge e is a candidate to be a matching edge (but e will not necessarily
be added to the matching M ) only if Xe = 1. Hence, if Xe = 0, then the Oracle-Edge(e) returns
false on Line 4. Otherwise, Line 7 verifies whether Xe0 = 1 for any e0 incident to e. If for at least one
such edge Xe0 = 1, then e and e0 are both candidates to be matching edges. However, adding both e
and e0 would lead to a collision and hence not a valid matching. This collision is resolved by adding
neither e nor e0 to M , implying that the set of edges added to M indeed forms a matching.
We now argue that E [M ] = Î˜(MM (G)). We use the fact that LCA-Level-j-Test and LCA-EdgeLevel-Test are perfect simulations of Level-j-Test and Edge-Level-Test respectively. Therefore,
the fractional pseudo-matching defined by the return values of LCA-Edge-Level-Test is identical to
c defined in Eq. (16) of Section 5, and obeys the same properties.
M
If a matching weight incident to a vertex v is at least Î», (recall that Î» is the constant from Corollary 2),
then we say v is heavy. An edge is incident to a heavy vertex if at least one of its endpoints is heavy. Hence,
by Corollary 2, at most 1/2 of the matching mass is incident to heavy vertices in expectation. Let e be an
edge neither of whose endpoints are heavy. The edge e is in the matching if Xe = 1 and Xe0 = 0 for each
other edge e0 incident to e. Recall that Xe0 = 1 with probability LCA-Edge-Level-Test(e0 )/(10Î»).
Since no endpoint of e is heavy, it implies that Xe0 = 0 for every e0 incident to e with probability

Y 
LCA-Edge-Level-Test(e0 )
1âˆ’
10Î»
0
e âˆˆÎ´(e)

â‰¥ 1âˆ’

X LCA-Edge-Level-Test(e0 )
10Î»
0

e âˆˆÎ´(e)

â‰¥ 1âˆ’

2Î»
4
= .
10Î»
5

Hence, if e is not incident to a heavy vertex, Oracle-Edge(e) returns true and hence adds e to M
with probability at least 45 LCA-Edge-Level-Test(e)/(10Î»). Also, by Theorem 5 and our discussion
about the matching mass of the edges incident to heavy vertices, we have that
X
LCA-Edge-Level-Test(e) = Î˜(MM (G)).
e = {u, v} : u and v are not heavy

This together with the fact that Î» is a constant implies that E [|M |] = Î˜(MM (G)), as desired.

6.6

Memory Complexity and Consistent Oracles

In this section we discuss about the memory requirement of our LCA algorithms, and also describe how
to obtain oracles that provide consistent outputs.
Each of our methods maintains O(1) variables. Observe that by definition the depth of our recursive
method LCA-Level-(j + 1)-Test is O(log d). Hence, the total number of variables that our algorithms
have to maintain at any point of execution is O(log d) requiring O(log d Â· log n) bits.
The way we described Oracle-Edge above, when it is invoked with an edge e two times, it could
potentially provide different outputs. This is the case for two reasons: D on Line 3 and e on Line 5 of
27

LCA-Level-(j +1)-Test are chosen randomly, and this choice may vary from iteration to iteration; and
the random variable Xe drawn by Matching-Candidate may be different in different invocations of
Matching-Candidate(e). It is tempting to resolve this by memorizing the output of Oracle-Edge(e)
and the corresponding invocations of LCA-Level-(j + 1)-Test (as we will see shortly, not all the
invocations of LCA-Level-(j + 1)-Test should be memorized). Then, when Oracle-Edge(e) is
invoked the next time we simply output the stored value. Unfortunately, in this way our algorithm
would potentially require Î˜(Q) memory to execute Q oracle queries, while our goal is to implement the
oracles using O(d Â· poly log n) memory. To that end, instead of memorizing outputs, we will use k-wise
independent hash functions.
Lemma 12. For k, b, N âˆˆ N, there is a hash family H of k-wise independent hash functions such that
all h âˆˆ H maps {0, 1}N to {0, 1}b . Any hash function in the family H can be stored using O(k Â· (N + b))
bits of space.
Lemma 13 (Nisanâ€™s PRG, [Nis90]). For every s, R > 0, there exists a PRG that given a seed of s log R
truly random bits can produce â„¦(R) pseudo random bits such that any algorithm of space at most s
requiring O(R) random bits will succeed using the pseudo random bits with probability at least 2âˆ’â„¦(s) .
Each bit can be extracted in O(s log R) time.
Randomness and consistency in the algorithms. Before we explain how to apply Lemmas 12
and 13 to obtain consistency of our methods, we recall a part of our analysis and recall how
LCA-Level-j-Test is used in our algorithms. Our analysis crucially depends on Corollary 2 (see
Lemma 11) that provides a statement about heavy vertices, i.e., about vertices whose incident edges
c, while M
c variables
have the matching mass of at least Î». Heavy vertices are defined with respect to M
b
b
are defined with respect to L (see Eqs. (14) and (16)). Finally, L(v) is defined/sampled by repeatedly
invoking LCA-Level-j-Test(v) for j = 0, 1, 2, . . . while the tests return true (see the discussion
b for its endpoints by Line 3. Since we
above Eq. (9)). LCA-Edge-Level-Test(e) effectively samples L
b
provide our analysis by assuming that L(v)
is defined consistently, the invocations of
LCA-Level-j-Test directly from LCA-Edge-Level-Test have to be consistent (Line 3). We call
this invocation as the top invocation of LCA-Level-j-Test.
Based on this discussion, the top invocation of LCA-Level-j-Test(v) will use a fixed sequence
B(v) of random bits to execute this call (including all the recursive invocations of LCA-Level-j-Test
performed therein). We emphasize that the output of the top invocation of LCA-Level-j-Test(v)
does not have to be the same as the output of LCA-Level-j-Test(v) invoked recursively via some
other top invocation. That is, for example, if a top level invocation LCA-Level-j-Test(v) recursively
calls LCA-Level-i-Test(w) (for some w neighbor of v and some i < j) then the recursive call
LCA-Level-i-Test(w) continues to use B(v) for its randomness.
Next, recall that by Lemma 7 LCA-Level-j-Test has query complexity O(d) even at the highest
level. Each query is a random edge-sample. Also, recursively via Line 7, each query requires sampling
O(d) times variable D on Line 3 of LCA-Level-j-Test. Hence, the total number of random bits
required for the execution of LCA-Level-(j)-Test is O(d log2 n). However, the test itself uses only
log(d) Â· log(n) space, therefore, by Lemma 13, a seed of O(log3 n) truly random bits suffice, that is
|B(v)| = O(log3 n). Furthermore, our runtime is only increased by a factor of log3 n from using Nisanâ€™s
PRG.
The rounding of the fractional matching by Matching-Candidate(e) should also be consistent
across queries and should never depend on where the call to Matching-Candidate(e) came from,
(unlike with LCA-Level-j-Test). To that end, each edge e should have its own random seed B(e) to
use in Matching-Candidate(e). Here |B(e)| = O(log n) suffices.
Independence. We have shown that our LCA algorithm would work correctly if all seeds, B(v) and
B(e) for v âˆˆ V and e âˆˆ E, were truly independent. However, storing â„¦(m) random seeds would be
extremely inefficient. Instead we will use an k-wise independent hash family, H, mapping V âˆª E to
3
{0, 1}O(log n) . That is we will sample a hash function h âˆˆ H up front and calculate B(v) = h(v) during
queries. Consider an edge e âˆˆ E. Note that whether or not e is in the integral matching depends only
on the levels of vertices in the 1-hop neighborhood of e, as well as the rounding of edges in its 1-hop
neighborhood. This is O(d) vertices and edges in total, and so an O(d)-wise independent hash family
3
mapping V âˆª E to {0, 1}O(log n) with uniform marginal distributions on each vertex and edge would
suffice to guarantee that e is in the integral matching with exactly the same probability as in the truly

28

independent case. This shows that E[|M |] = MM (G) would still hold. By Lemma 12 such a family
exists, and any element of it can be stored in space O(d log3 n) space.

6.7

Proof of Theorem 2

Proof of Theorem 2. We are now ready to prove the main result of this section. In Section 6.3 we
provided two oracles, Oracle-Edge and Oracle-Vertex. Lemmas 9 and 10 show that these oracles
have the desired query complexity. Lemma 11 proves that Oracle-Edge outputs a Î˜(1)-approximate
maximum matching. Observe that Oracle-Vertex is consistent with Oracle-Edge. That is, for any
vertex v, Oracle-Vertex(v) output true iff there is an edge incident to v for which Oracle-Edge(e)
outputs true. This implies that the outputs of Oracle-Vertex also correspond to a Î˜(1)-approximate
maximum matching. Finally, in Section 6.6 we discussed how these oracles can be implemented by using
space O(d log3 n).
Proof sketch of Remark 6. We observe that the random variable |M | concentrates around its
expectation since it is the sum of many bounded variables:
X
|M | =
1(e âˆˆ M ).
eâˆˆE

Although these variables are not independent, their dependence graph has bounded degree, as observed
in Section 6.6 under the heading Independence. Indeed, for any specific edge e = (u, v) âˆˆ E, the
variable 1(e âˆˆ M ) depends on the levels of the vertices in the one-hop neighborhood of e, as well as the
level and rounding of edges in the one-hope neighborhood of e. Overall, random bits that influence the
rounding of e include B(w) and B(f ) for all vertices w and edges f in the neighborhood of e. If e and e0
are at least distance 3 away, they are completely independent (disregarding the dependences introduced
by the hash functions we use), therefore the dependence graph of the variables 1(e âˆˆ M ) has maximum
degree d0 = O(d3 ).
Theorem 1. of [Pem01] states that the when the independence graph of Bernoulli variables Xi has
degree bounded by d0 , then
"
#
X

4(d0 + 1)
exp âˆ’Âµ2 /2(d0 + 1) ,
P
Xi â‰¥ (1 âˆ’ )Âµ â‰¤

i
P
where Âµ = E [ i Xi ]. By applying this with  = 1/2, Âµ = O(MM(G)) = O(n/d), d0 = O(d3 ) and
d = O((n/ log n)1/4 ), we get that indeed |M | is at least half of its expectation. It is also not hard to
see that the same bound follows even if the variables 1(e âˆˆ M ) for edges more than distance 2 away are
merely log n-wise independent instead of being truly independent.

7

e 2 ) for Simulation of Randomized Greedy
Lower Bound of â„¦(d

In this section we analyze the result of Yoshida at al. [YYI09] for constructing a constant fraction
approximate maximum matching in the LOCAL model. It was proven in [YYI09] that one can return
whether some edge is in a maximal matching or not in time only Î˜(d) when expectation is taken over
both the randomness of the algorithm and the choice of edge. If it could be proven that this (or even a
slightly weaker) bound holds for a worst case edge, the algorithm could be simply transformed into an
LCA algorithm more efficient than the one we present in Section 6. However, we proceed to prove that
this is not the case.
The algorithm we consider is a simulation of the greedy algorithm for maximal matching in LCA.
Given a graph G = (V, E) and a permutation Ï€ of E, a natural way to define a maximal matching of
G with respect to Ï€ is as follows: process the edges of E in the ordering as given by Ï€; when edge e is
processed, add e to the matching is none of its incident edges has been already added. Motivated by
this greedy approach, Yoshida et al. proposed and analyzed algorithm YYI-Maximal-Matching (see
Algorithm 10) that tests whether a given edge e is in the greedy maximal matching defined with respect
to Ï€.

29

Algorithm 10 Implementation of the greedy algorithm for maximal matching in LCA.
1:
2:
3:
4:
5:

procedure YYI-Maximal-Matching (e, Ï€)
for f âˆˆ Î´(e) such that f precedes e, in order of Ï€ do
. Î´(e) is the edge-neighbourhood of e.
if YYI-Maximal-Matching(f, Ï€) returns true then
return false
return true

Pictorially, YYI-Maximal-Matching can be viewed as a process of walking along neighboring edges
(as defined via the recursive calls), and hence exploring the graph adaptively based on Ï€. Yoshida et
al. showed that the size of this exploration graph of YYI-Maximal-Matching(e, Ï€) is in expectation
at most d, where the expectation is taken over all the starting edges e and all possible permutations Ï€.
It remained an open question whether it was necessary to take the expectation over the starting edge. If
the size of the exploration tree could be shown to be O(d) (or O(d log n)) for even the worst case edge,
this would yield an extremely efficient LCA algorithm for approximate maximum matching.
However, the main result of the section is that this is not the case:
âˆš
Theorem 3. There exists an absolute constant b > 0 such that for every n, d âˆˆ [5, exp(b log n)] and
 âˆˆ [1/d, 1/2] there exists a graph G with n vertices and maximum degree d + 1, and an edge e such that
running YYI-Maximal-Matching(e, Ï€) from Algorithm 10 results in an exploration tree of size at least
1
Â·Â·
8

 2âˆ’
d
,
2

in expectation.
âˆš
Notice that potentially d =âˆšexp(c log n)  log n. Therefore the O(d log n) bound that we achieve in
n)
Theorem 2 is a factor O( exp(clog log
) better.
n
Overview of Our Approach We first construct a simple infinite tree (Definition 1): a tree in which
each vertex has exactly d children with one extra special edge connected to the root. Then we prove
that the number of queries made by YYI-Maximal-Matching is bigger than d for the special edge.
Afterwards, we extend the graph by merging the end points of the special edges of d independent copies
of these trees which creates another infinite tree. In this tree, beside the root that has d children,
the rest of vertices has d children (Definition 3). We also add an edge to the root and show that
YYI-Maximal-Matching uses almost d2 queries for this edge. Infinite trees ease the computations
due to the fact that each subtree is isomorphic to the main tree. In Section 7.2, we carefully analyze the
probability and variance of YYI-Maximal-Matching reaching high depths and show that it is unlikely
that it passes depth O(log n). Later, we use that to truncate the tree after depth O(log n). This enables
us to get the graph with desired bounds in Section 7.3. Notice that, throughout this section for the sake
of simplicity, we assume that d is an integer.

7.1

Lower Bound for Infinite Graphs

We begin by analyzing the behavior of YYI-Maximal-Matching on infinite d-regular trees. We
implement the random permutation Ï€ be assigning to each edge e a rank r(e) chosen independently
and uniformly at random from the interval [0, 1]. Edges are then implicitly ordered by increasing rank.
Then, Line 2 of Algorithm 10 can be thought of as being â€for f âˆˆ Î´(e) such that r(f ) < r(e), in
increasing order of rank doâ€. The behavior of the algorithm on any edge can be described by the two
functions pe (Î») and te (Î»), where Î» âˆˆ [0, 1]. The function pe (Î») denotes the probability that e is in the
matching, given only that r(e) = Î». Therefore, pe (Î») âˆˆ [0, 1] and pe (0) = 1. The function te (Î») denotes
the expected size of the exploration tree when exploring from e, given only that r(e) = Î». Therefore,
we have te (Î») â‰¥ 1, with equality only if Î» = 0.
Definition 1 (Graph H d , see Fig. 1 for illustration). For an integer d â‰¥ 1, the graph H d is defined as
an infinite d-regular tree rooted in an edge e0 = (u0 , v0 ). Let u0 have no neighbor other than v0 , and let
v0 have d neighbors other than u0 . In general, let all vertices other than u0 have d + 1 neighbors. For
an edge e, level of e is defined as its distance from e0 and denoted by `(e). In particular, `(e0 ) = 0 and
the level of any other edge adjacent to v0 is 1. Every edge e = {u, v} =
6 e0 has exactly 2d edge-neighbors
(d incident to v and d incident to u); d of these neighbors have a higher level than e, we call them eâ€™s
30

u0
u0

u0
v0
Degree d + 1

Figure 1: Construction of H d .
children; d âˆ’ 1 have equal level to e, we call them eâ€™s siblings; exactly 1 has lower level than e, we call it
eâ€™s parent.
Definition 2 (Graph He ). For every edge e âˆˆ H d let He be the set of edges whose unique path to e0
goes through e (including e itself ).
In the rest of this section, we analyze the behavior of YYI-Maximal-Matching (e0 , Â·). Specifically,
we calculate the expectation and the variance of its size, and upper-bound its depth over the randomness
of the ranks. It will be convenient to consider a slightly more efficient version of Algorithm 10, one in
which the algorithm memorizes the results of queries across the recursive tree. That is, if in the tree of
recursive calls from YYI-Maximal-Matching (e0 , Ï€) some edge e appears multiple times, it is counted
only once in the size of the exploration tree. This memorization can lead to a great saving in the query
complexity. For instance, suppose that e is the parent of f and g, which are therefore siblings. Let their
ranks be r(e) = Î», r(f ) = Âµ and r(g) = Î½ with Î» > Âµ > Î½. If the algorithm queries e, it first explores
the subtree Hg . Then, if YYI-Maximal-Matching (g, Ï€) returns false, the algorithm proceeds to
querying f , only to immediately return to g and explore the same subtree of Hg , as g âˆˆ Î´(f ). Since the
output of YYI-Maximal-Matching (g, Ï€) is memorized, the algorithm does not have to explore Hg
again.
Thanks to memorization, in Line 2 we can ignore the parent of e, as well as all its siblings. We can
ignore the parent p, since e must have been recursively queried from p, therefore r(p) > r(e). As for
any sibling f of e, either r(f ) > r(e), in which case f can be safely ignored, or r(f ) < r(e), in which
case f must have already been queried from p and can be ignored due to memorization. This alteration
to Algorithm 10 can only reduce the size of the exploration tree Te0 (Î»). Since in this section we are
concerned with a lower-bound on the size of a specific exploration tree, we will analyze this altered
version of Algorithm 10.
Lemma 14. Let e0 be the root edge of the graph H d , as defined in Definition 1. Then
d

pe0 (Î») = x(Î») 1âˆ’d
d

te0 (Î») = x(Î») dâˆ’1 ,
where x(Î») = 1 + (d âˆ’ 1)Î».
Proof. Let p(Î») = pe0 (Î») and t(Î») =
YYI-Maximal-Matching(e, Ï€) by MM(e, Ï€).
A closed form expression for p(Î»).

te0 (Î»).

For

ease

of

notation

we

denote

We first derive a recursive formula for p(Î»):

p(Î») = P [MM(e0 , Ï€) returns true|r(e0 ) = Î»)]
= P [âˆ€e âˆˆ Î´(e0 ) : r(e) > Î» or MM(e, Ï€) returns false on He ]
!
Z Î»
Y
=
1âˆ’
P [MM(e, Ï€) returns true on He |r(e) = Âµ] dÂµ
0

eâˆˆÎ´(e0 )

Z
=

1âˆ’

!d

Î»

p(Âµ)dÂµ

,

0

31

(27)

since He is isomorphic to H d (as per Definitions 1 and 2).
We can now solve this recursion and get a closed form formula for p(Î»). By raising both sides of
RÎ»
Eq. (27) to power 1/d, we derive that p1/d (Î») = 1âˆ’ 0 p(Âµ)dÂµ. Differentiating both sides of this equation,
we get d1 Â· p(1/d)âˆ’1 (Î») Â· p0 (Î») = p(Î»), which implies that p(1/d)âˆ’2 (Î») Â· p0 (Î») = d and hence
d

d

p(Î») = (C + (d âˆ’ 1)Î») 1âˆ’d = x 1âˆ’d (Î»),
as claimed, where C = 1 due to the initial condition of p(0) = 1.
A closed form expression for t(Î»). We now derive a recursive formula for t(Î»). Let Te be a random
variable denoting the size of the exploration tree when running Algorithm 10 from e in He . Note that
Te is distributed identically for all e, and E(Te |r(e) = Î») = t(Î»), since He is always isomorphic to H d .
Let T = Te0 . Furthermore, let Ie denote the indicator variable of e being explored through the recursive
calls, when the algorithm is originally initiated from e0 . Then T satisfies
X
Ie Â· Te ,
T =1+
eâˆˆÎ´(e0 )

and hence,
X

t(Î») = E [T |r(e0 ) = Î»] = 1 +

E [Ie Â· Te |r(e0 ) = Î»]

eâˆˆÎ´(e0 )

The expression E [Ie Â· Te |r(e0 ) = Î»] can be nicely taken apart if we condition on the rank of e, as long
as it is less than Î». (If it is more than Î», Ie = 0.) Indeed, note that Ie depends only on the ranks and
outcomes of the siblings of e, as well as the rank of e itself. Meanwhile, Te depends only on the ranks
in He . The only intersection between these is the rank of e, meaning that Ie and Te are independent
conditioned on r(e). These observations lead to
t(Î») = 1 +

Î»

X Z
eâˆˆÎ´(e0 )

P [Ie = 1|r(e0 ) = Î», r(e) = Âµ] Â· E [Te |r(e) = Âµ] dÂµ.

0

The expected size of the exploration tree from e is simply t(Âµ), again since He is isomorphic to H d .
Consider the probability that e is explored at all. This happens exactly when for any sibling of e,
f , either r(f ) > r(e) or MM(f, Ï€) returns false. This condition is very similar to the condition for
MM(e0 , Ï€) returning true (recall Eq. (27)). The only difference is that the condition must hold only
for Î´(e0 )\e as opposed to Î´(e0 ). Hence we have


Z

dâˆ’1

Âµ

1âˆ’

P [I0 = 1|r(e0 ) = Î», r(e) = Âµ] =

p(Î½)dÎ½

d

= p dâˆ’1 (Âµ) = xâˆ’1 (Âµ).

0

In particular, the rhs does not depend on Î». Therefore,
Î»

Z

xâˆ’1 (Âµ)t(Âµ)dÂµ.

t(Î») = 1 + d
0

We can now solve this recursion and get a closed form formula for t(Î»).
Î»

Z

xâˆ’1 (Âµ)t(Âµ)dÂµ

t(Î») = 1 + d
0

t0 (Î») = dxâˆ’1 (Î»)t(Î»)
d
t0 (Î»)
=
t(Î»)
x(Î»)
log (t(Î»)) =

d
Â· log (x(Î»)) + C1
dâˆ’1
d

d

t(Î») = C2 Â· x dâˆ’1 (Î») = x dâˆ’1 (Î»),
as claimed, due to the initial condition of t(0) = 1.
32

w0
u0

Degree d + 1

Degree d + 1

Degree d + 1

Figure 2: Construction of H d, .
Corollary 3. Let e0 be the root edge of the graph H d , as defined in Definition 1. Then,
d

d

E [Te0 ] â‰¤ EÎ» [te0 (Î»)] â‰¤ te0 (1) = x dâˆ’1 (1) = d dâˆ’1 â‰¤ 2d,
for d â‰¥ 5.
We next construct an infinite graph with an edge whose expected exploration tree size has nearly
quadratic dependence on d.
Definition 3 (Graph H d, , see Fig. 2 for illustration). Fix some small positive number . Take d
disjoint copies of H d , call them H (1) , H (2) , . . . , H (d) . Let the root edge of H (i) be e(i) = (u(i) , v (i) ). We
merge u(1) , u(2) , . . . , u(d) into a supernode u0 , and add a new node w0 along with an edge e0 = (w0 , u0 ).
This creates the infinite graph H d, ; we call the edge e0 the root edge.
We will now show that querying e0 in H d, with Algorithm 10 produces an exploration tree of nearly
quadratic size in expectation.
Lemma 15. For every  âˆˆ (0, 1), every integer d â‰¥ 5, in the graph H d, (see Definition 3), where e0 is
the root edge, it holds
te0 (Î») â‰¥  Â· x2âˆ’ (Î»)/2.
Proof. Recall the definitions of Ie and Te from the proof of Lemma 14: Let Ie(i) be the indicator variable
of e(i) being explored when Algorithm 10 is called from e0 ; let Te(i) be the size of the exploration tree
from e(i) in H (i) . For simplicity let Ti = Te(i) and Ii = Ie(i) . We can derive a formula for te0 (Î») similarly
to Lemma 14:
te0 (Î») = 1 +

d Z
X
i=1

Î»

h
i
h
i
P Ii |r(e(i) ) = Âµ Â· E Ti |r(e(i) ) = Âµ dÂµ.

0



d
E Ti |r(e(i) ) = Âµ is simply t(Âµ) = x dâˆ’1 (Âµ) by Lemma 14, since the subtree of e(i) in H d, is isomorphic
to H d . Similarly to Lemma 14, the probability that e(i) is explored is the probability that for any sibling
e(j) of e(i) we have that either r(e(j) ) > r(e(i) ) or MM(e(j) , Ï€) returns false. e(i) has d âˆ’ 1 neighbors,
each of whose subtree is isomorphic to H d , hence we have
Z
d 
i Y
P Ii |r(e ) = Âµ =
1âˆ’
h

(i)



Z

dâˆ’1

(Âµ)

= x 1âˆ’d (Âµ).

33

dâˆ’1
p(Î½)dÎ½

0
dâˆ’1
d

pe(i) (Î½)dÎ½

Âµ

1âˆ’

=p



0

i=1

=

Âµ

Therefore,
Î»

Z

dâˆ’1

d

x 1âˆ’d Â· x dâˆ’1 (Âµ)dÂµ

te0 (Î») = 1 + d
0
Î»

Z

x1âˆ’ (Âµ)dÂµ

â‰¥ 1 + d
0

=1+


d
Â· x2âˆ’ (Î») âˆ’ 1
(2 âˆ’ )(d âˆ’ 1)

â‰¥  Â· x2âˆ’ (Î»)/2,
as claimed.
Corollary 4. For every  âˆˆ (0, 1), every integer d â‰¥ 5, for the root e0 of the graph H d, (see Definition 3)
we have:
 2âˆ’
1
1
d
E [Te0 ] = EÎ» [te0 (Î»)] â‰¥ Â·  Â· te0 (1/2) â‰¥  Â· Â·
.
2
4
2
Therefore this is a construction in which an edge has expected exploration tree size which is nearly
quadratic in d. However, the graph H d, is infinite, so we proceed to finding a finite graph that has
an edge whose exploration tree is also near quadratic. To obtain such a finite graph, we perform the
following natural modification of H d, : we cut off the H d, graphs at some depth `, that is we discard
all edges e such that `(e) > `, along with vertices that become isolated as a result. (Recall that `(e) is
the level of the edge e.) We will prove that the exploration tree usually does not explore edges beyond
a depth of O(log d), and so intuitively we should be able to cut the graph at that depth. We make this
precise in the next section.

7.2

Depth and Variance Analysis for Infinite Graphs

We will now study the depth of the exploration tree, that is the highest level of any edge in it. Let the
depth of the exploration tree from e0 be D.
Lemma 16. For every ` â‰¥ 1, if D is the depth of the exploration tree in the graph H d (see Definition 1),
one has
P [D â‰¥ `] â‰¤ 21âˆ’` d2 .
Proof. Consider the weight of the exploration tree, defined as follows: For each edge e in the exploration
tree we count it with weight 2`(e) . Let the expected weighted size of an exploration tree from e0 ,
conditioned on e0 = Î» be T2 (Î»). We can derive a recursive formula for t2 (Î») with the same technique as
was used to derive a recursive formula for t(Î») in Lemma 14, the only difference is the additional factor
of 2 on the right hand side (we do not repeat the almost identical proof here). We get
Z Âµ
t2 (Î») = 1 + d
xâˆ’1 (Âµ)(2t2 (Âµ))dÂµ.
0

We can then solve this recursion in a similar manner to the one in Lemma 14 (again, the only
difference is the extra factor of 2 in the exponent):
Î»

Z

xâˆ’1 (Âµ)t2 (Âµ)dÂµ

t2 (Î») = 1 + 2d
0

t02 (Î») = 2dxâˆ’1 (Î»)t2 (Î»)
2d
t02 (Î»)
=
t2 (Î»)
x(Î»)
log (t2 (Î»)) =

2d
Â· log (x(Î»)) + C1
dâˆ’1
2d

2d

t2 (Î») = C2 Â· x dâˆ’1 (Î») = x dâˆ’1 (Î»),
2d

due to the initial condition of t2 (0) = 1. Specifically t2 (Î») < t2 (1) = x dâˆ’1 â‰¤ 2d2 for d â‰¥ 5.
34

We can now complete the proof of the lemma. Note that if the depth D of the exploration tree in
H d satisfies D â‰¥ ` then the tree contains at least an edge of level `, the weight of the tree must be at
least 2` . Therefore:
 
2d2 â‰¥ t2 (1) â‰¥ E 2D â‰¥ 2` Â· P [D â‰¥ `]
P [D â‰¥ `] â‰¤ 21âˆ’` d2 ,
as claimed.
Corollary 5. In the graph H d, (see Definition 3),
P [D â‰¥ ` + 1] â‰¤ 21âˆ’` d3 .
Proof. Indeed, in order for Te0 in H d, to have depth ` + 1, Te(i) in H (i) must have depth at least ` for
at least on of the iâ€™s. We know from Lemma 16 that the probability of this is at most 21âˆ’` d2 as H (i) is
isomorphic to H d . By simple union bound over all values of i we get that P [D â‰¥ ` + 1] â‰¤ 21âˆ’` d3 .
We have shown in Corollary 5 that the depth of the exploration tree from the root of H d or H d,
doesnâ€™t exceed O(log d) with high probability. It would be intuitive to truncate these graphs at depth
Î˜(log d) to get a finite example for a graph with an edge e0 from which exploration takes quadratic time.
However, Corollary 5 does not by itself rule out the possibility that Te0 concentrated extremely badly
around its expectation, i.e. the exploration tree is extremely large with very small probability, and most
of the work is done beyond the O(log d)
of the tree. We rule this out by exhibiting a dO(1)
 2first
 levels
d
upper bound on the second moment E Te0 in H and H d, . Afterwards, we show that combining these
second moment bounds with Corollary 5 and Lemma 15 shows that truncating H d, indeed yields a hard
instance. Our variance bound is given by:
 
Lemma 17. In the graph H d (see Definition 1) for the root edge e0 one has E Te20 â‰¤ 10d5 .
 
Corollary 6. In the graph H d, (see Definition 3) for the root edge e0 one has E Te20 â‰¤ 11d6 .
The proofs follow along the lines of previous analysis, but are more technical and hence both are
deferred to Appendix C.

7.3

The Lower Bound Instance (Truncated H d, )

We are now ready to truncate our graph H d, :
Definition 4 (Truncated graph H`d, ). Define H`d, as the graph H d, reduced to only edges of level at
most `.
Note that H`d, is a finite graph, specifically with approximately n = d` vertices. We now show that
for some ` = O(log d) the size of the exploration tree from the root edge e0 in the graph H`d, is essentially
the same as in the graph H d, (see Corollary 4 below). This yields our final lower bound instance.
Lemma 18. For every integer d â‰¥ 5, every  âˆˆ [1/d, 1/2] and ` â‰¥ 7 log2 (3d) + 1, in graph H`d, one has
2âˆ’
, where T` is the size of the exploration tree started at the root edge e0 in H`d, .
E [T` ] â‰¥ 81 Â·  Â· d2
Proof. Consider a graph H d, and its truncated version H`d, for ` â‰¥ 7 log2 (3d). Let T = Te0 in H d, and
T` = Te0 in H`d, . We consider the ranks of edges in H d, to be identical to the ranks of the corresponding
edges in H d, (this is in a way a coupling of the ranks of the two graphs). Consider the events S and
D referring to a shallow or a deep exploration tree respectively. Specifically, S refers to the event that
the exploration tree in H d, does not exceed a depth of `; D is the compliment of S. Note that given S,
T = T` thanks to the coupling of the ranks.
We know from Corollary 4 that
Â·

1
Â·
4

 2âˆ’
d
â‰¤ E [T ] = E [T Â· 1(S)] + E [T Â· 1(D)] = E [T` Â· 1(S)] + E [T Â· 1(D)] .
2

35

Thus, in order to prove that E [T` Â· 1(S)] â‰¥

1
8

Â·Â·


d 2âˆ’
2

1
E [T Â· 1(D)] â‰¤ Â·  Â·
8

it suffices to show that
 2âˆ’
d
.
2

(28)

Let p = P(D); by Corollary 5 and our choice of ` â‰¥ 7 log2 (3d) we know that this is at most 2 Â·
3âˆ’7 Â· dâˆ’4 â‰¤ (11 Â· 64)âˆ’1 Â· dâˆ’4 , for  â‰¤ 1. Let us upper bound E [T Â· 1(D)] = p Â· E [T |D]: We know from
Corollary 6 that
 




(p Â· E [T |D])2
2
11d6 â‰¥ E T 2 â‰¥ E 1(D) Â· T 2 = p Â· E T 2 |D â‰¥ p Â· E [T |D] =
,
p
and thus, rearranging, we get
p Â· E [T |D] â‰¤

p

11d6 p â‰¤

1
Â·Â·
8

 2âˆ’
d
,
2

since d â‰¥ 5 and p â‰¤ (11 Â· 64)âˆ’1 Â· dâˆ’4 by assumption of the lemma and setting of parameters.
We can now prove the main theorem of this section.
Proof of Theorem 3: Indeed, let G = H`d, with âˆš
e being the root edge and ` = Î˜(logd n). Then the
theorem holds by Lemma 18 as long as d â‰¤ exp(b log n) for a sufficiently small absolute constant b.

8

Lower-bound on the Number of Sampled Edges

8.1

Overview

In this section we prove that our algorithm is nearly optimal with respect to sample complexity, even
disregarding the constraint on space. That is, we show that it is impossible to obtain a constant-factor
approximation of the maximum matching size with polynomially fewer than n2 samples.
Theorem 8. There exists a graph G consisting of Î˜(n2 ) edges such that no algorithm can compute a
constant-factor approximation of MM (G) with probability more than 6/10 while using iid edge stream of
length n2âˆ’ . More generally, for every constant C, every m between n1+o(1) and â„¦(n2 ) it is information
theoretically impossible to compute a C-approximation to maximum matching size in a graph with high
constant probability using fewer than m1âˆ’o(1) iid samples from the edge set of G, even if the algorithm
is not space bounded.
Theorem 8 follows directly from the following result.
Theorem 9. For any  > 0, any C > 0, any m between n1+o(1) and â„¦(n2 ), and large enough n there
exists a pair of distributions of graphs on n vertices, DYES and DNO , such that the sizes of the maximum
matchings of all graphs in DNO are M and the sizes of the maximum matchings of all graphs in DYES
are at least CM . However, the total variation distance between an iid edge stream of length m1âˆ’ of a
random graph in DYES and one in DNO is at most 1/10.
Overview of the approach. Our lower bound is based on a construction of two graphs G and H on
n vertices such that for a parameter k (a) matching size in G is smaller than matching size in H by a
factor of nâ„¦(1)/k but (b) there exists a bijection from vertices of G to vertices of H that preserves k-depth
neighborhoods up to isomorphism. To the best of our knowledge, this construction is novel. Related
constructions have been shown in the literature (e.g., cluster trees of [KMW16]), but these constructions
would not suffice for our lower bound, since they do not provide a property as strong as (b) above.
For example, the construction of [KMW16] only produces one graph G with a large matching together
with two subsets of vertices S, S 0 of G whose neighborhoods are isomorphic. This suffices for proving
strong lower bounds on finding near-optimal matchings in a distributed setting [KMW16], but not for
our purpose. Indeed, it is crucial for us to have a gap (i.e., two graphs G and H) and have the strong
indistinguishability property provided by (b).
Our construction proceeds in two steps. We first construct two graphs G0 and H 0 that have identical
k-level degrees (see Section 8.3). This produces two graphs G0 and H 0 that are indistinguishable based
36

on k-level degrees (but whose neighborhoods are not isomorphic due to cycles) but whose matching size
differs by an nâ„¦(1/k) factor. These graphs have n2âˆ’O(1/k) edges and provide nearly tight instances for
peeling algorithms that we hope may be useful in other contexts. We note that a similar step is used
in the construction of cluster trees of [KMW16], but, as mentioned above, these graphs provide neither
the indistinguishability property for all vertices nor a gap in matching size. Furthermore, the number of
e 3/2 ), i.e., the graphs do not get denser with large
edges in the corresponding instances of [KMW16] is O(n
k, whereas our construction appears to have the optimal behaviour. The second step of our construction
is a lifting map (see Theorem 11) that relies on high girth Cayley graphs and allows us to convert graphs
with identical k-level vertex degrees to graphs with isomorphic depth-k neighborhoods without changing
matching size by much. The details are provided in Section 8.4.
Finally, the proof of the sampling lower bound proceeds as follows. To rule out factor C approximation
in m1âˆ’o(1) space, take a pair of constant (rather, mo(1) ) size graphs G and H such that (a) matching
size in G is smaller than matching size in H by a factor of C and (b) for some large k one has that
k-depth neighborhoods in G are isomorphic to k-depth neighborhoods in H. Then the actual hard input
distribution consists of a large number of disjoint copies of G in the NO case and a large number of
copies of H in the YES case, possibly with a small disjoint clique added in both cases to increase the
number of edges appropriately. Since the vertices are assigned uniformly random labels in both cases,
the only way to distinguish between the YES and the NO case is to ensure that at least k edge-samples
land in one of the small copies of H or G. Since k is small, the result follows.
We now give the details. Formally, our main tool will be a pair of constant sized graphs that are
indistinguishable if only some given constant number of edges are sampled from either. This is guaranteed
by the following theorem, proved in Section 8.5.
Theorem 10. For every Î» > 1 and every k, there exist graphs G and H such that MM (G) â‰¥ Î»Â·MM (H),
but for every graph K with at most k edges, the number of subgraphs of G and H isomorphic to K are
equal.
Defining distributions DYES and DNO . All the graphs from our distributions will have the same
def
vertex set V = [n]. Let G = (VG , EG ) and H = (VH , EH ) be the two graphs provided by Theorem 10
def
invoked with parameters Î» = 2C and k = 2/. Let q = max(|VG |, |VH |) (our construction in fact
def
guarantees that |VG | = |VH |). Let s = MM (H), and hence MM (G) â‰¥ Î» Â· s = 2C Â· s.
Partition V into the following:
1. r =

n
2q

sets of size q, denoted by V1 , . . . , Vr ;

2. a set VK consisting of w vertices, for w âˆˆ [0, ns/q]; and
3. set I containing the remaining vertices.
The sets V1 , . . . , Vr will serve as the vertex sets of copies of G or H, where Vi equals VG or equals VH
depending on whether we are constructing DYES or DNO . The set VK will be a clique, while I will be a
set of isolated vertices in the construction. The distributions DYES and DNO are now defined as follows:
DYES : Take r independently uniformly random permutations Ï€1 , . . . , Ï€r on V1 , . . . , Vr respectively, and
construct a copy Gi of G embedded into Vi via Ï€i . Then construct a clique Kw on VK .
DNO : Take r independently uniformly random permutations Ï€1 , . . . , Ï€r on V1 , . . . , Vr respectively and
construct a copy Hi of H embedded into Vi via Ï€i . Then construct a clique Kw on VK .
We will refer to copies of G and H in the two distributions above as gadgets. We now give an outline
of the proof of Theorem 9 assuming Theorem 10. The full proof follows the same steps, but is more
involved, and is deferred to Appendix D.
Proof outline (of Theorem 9). Naturally, our distribution-pair will be DYES and DNO as defined
above. Note that the maximum matching size of any element of the support of DYES is at least r Â· 2Cs as
it contains r copies of G. On the other hand, any element of the support of DNO contains r copies of H as
well as a clique of size w â‰¤ ns/q which means its maximum matching size is at most r Â· s + ns/2q â‰¤ 2r Â· s.
Hence, the sizes of maximum matchings in DYES and maximum matchings in DNO differ by at least
factor C, as desired.



Note that the number of edges in the construction is at least w2 and at most w2 + r Â· 2q â‰¤ w2 + qn.
Thus the number of edges can be set to be (within a constant factor of) anything from n1+o(1) to â„¦(n2 ).

37

Next, we compute the total variation distance between iid edge streams of length m1âˆ’ of a graph
sampled from DYES and DNO respectively. Denote these random iid edge streams by C1 and C2 ,
respectively. Consider the following event, that we call bad,
def

E = {âˆƒi âˆˆ [r] : edges between vertices of Vi appear more than k times in the stream}.
We show below that distributions of C1 conditioned on EÌ„ is identical to the distribution of C2 conditioned
on EÌ„. Then the total variation distance is bounded by the probability of E. We first bound this probability,
and then prove the claim above.
Upper bounding the probability of E. Consider a realization of DYES or a realization of DNO . Since
we have m1âˆ’ edge samples each chosen uniformly at random from a possible m edges, each specific edge,
e, appears exactly mâˆ’ times throughout the stream in expectation. Therefore, by Markovâ€™s inequality
âˆ’
the probability that e ever appears in the stream is at most 3nÂµ2 . Also, the event that an edge e1 appears
in the stream and the event that an edge e2 6= e1 appears in the stream are negatively associated.
Fix a single gadget of the realized graph; this gadget has at most q 2 edges. Therefore, by union
bound and from the negative association outlined above, the probability that at least k + 1 edges of the
gadget will be sampled is at most
 2 
 2 

q
q
âˆ’ k+1
Â· m
â‰¤
Â· mâˆ’2 ,
k+1
k+1
where we used the assuption that k = 2/. Thus, again by union bound, the probability that any of the
n
gadgets has at least k + 1 of its edges occurring in the stream is at most
r = 2q
 2 
q
n
Â·
Â· mâˆ’2 â‰¤ 1/10,
2q
k+1
for large enough n and m > n. So, P [E] â‰¤ 1/10 under both distributions.
Analyzing conditional distributions. It remains to show that C1 and C2 are identically distributed
when conditioned on E. We now give an overview of this proof, and defer details to Appendix D.
Since the cliques are identical across the two distributions, edges sampled from them are no help in
distinguishing between DYES and DNO . Consider a single gadget. (As a reminder, a gadget is a copy of
G or H.) By conditioning on E we have that only at most k distinct edges of this gadget are observed.
Since the gadgets are randomly permuted in both DY ES and DN O , only the isomorphism-class of the
sampled subgraph of the gadget provides any information. However, each isomorphism-classâ€™s probability
is proportional to the number of times such a subgraph appears in the gadget. This is equal across the
YES and NO cases thanks to the guarantee of Theorem 10 on G and H.
From here it remains to prove Theorem 10. We organize the rest of this section as follows. In
Section 8.3 we define and analyze our main construction, which is a pair of graphs isomorphic with
respect to k-level degrees while having greatly different matching numbers. That is to say, we produce
two graphs and a bijection between them such tha the bijection preserves degree structure up to a depth
of k, disregarding cycles. (For the definition of k-level degree see Section 8.2.) This is the main technical
result of our lower bound. Then in Section 8.4 we use a graph lifting construction to increase the girths
of our graphs, resulting in a pair of graphs that have truly isomorphic k-depth neighborhoods. Finally
in Section 8.5 we prove Theorem 10, concluding the lower bound.

8.2

Preliminaries

We begin by stating a few definition which will be used in the rest of Section 8. First, we recall a few
basic graph theoretical definitions:
Definition 5 (c-star). We call a graph with a single degree c vertex connected to c degree 1 vertices a
c-star. We call the degree c vertex the center and degree 1 vertices petals.
Definition 6 (girth of a graph). The girth of a graph is the length of its shortest cycle.

38

Definition 7 (permutation group of V ). Let G = (V, E) be a graph. We call the subgroup of SV (the
permutation group of V ) containing all permutations that preserve edges the automorphism group of G.
That is, a permutation Ï€ âˆˆ SV is in the automorphism group if the following holds:
âˆ€v, w âˆˆ V : (v, w) âˆˆ E â‡â‡’ (Ï€(v), Ï€(w)) âˆˆ E.
We denote it Aut(G).
We now define two local property of a vertex, i.e., k-hop neighborhood and k-level degree.
Definition 8 (k-hop neighbourhood of a vertex). Let the k-hop neighborhood of a vertex v in graph G
be defined as the subgraph induced by vertices of G with distance at most k from v.
Definition 9 (k-level degree of a vertex). Let the k-level degree of a vertex v in a graph G denoted by
dG
k (v) be a multiset defined recursively as follows:
def

â€¢ dG
1 (v) = d(v), the degree of v in G.
def  G
â€¢ For k > 1, dG
k (v) = dkâˆ’1 (w)|w âˆˆ N (v) , where this is a multiset.
For ease of presentation, in the future we will use the following less intuitive but more explicit formulation:
]
dG
{dG
k (v) =
kâˆ’1 (w)},
wâˆˆN (v)

U

where
denotes multiset union. Moreover, if G is clear from context, we will omit the superscript and
write dk (v) instead of dG
k (v).
Note that the above two definitions are similar but distinct, with the k-hop neighborhood containing
the more information of the two. Imagine a vertex v of a C3 cycle and a vertex w of a C4 cycle. Their
arbitrarily high level degrees are identical, but their 2-hop neighborhoods contain their respective graphs
fully, and so they are clearly different.
Observation 3. The following conditions are sufficient for the k-hop neighborhoods of vertices v and w
(from graphs G and H respectively) to be isomorphic:
H
1. dG
k (v) = dk (w)

2. G and H both have girth at least 2k + 2, that is neither graph contains a cycle shorter than 2k + 2.
This observation will be crucial to our construction as our main goal will be to construct two graphs
G and H such that a bijection between their vertex-sets preserves k-depth neighborhoods. We achieve
this by first guaranteeing condition 1. above, then improving our construction to guarantee condition 2.
as well.

8.3

Constructing Graphs with Identical k-Level Degrees

As stated before, our first goal is to design a pair of graphs with greatly differing maximum matching
sizes such that, nonetheless, there exists a bijection between them preserving k-level degrees for some
large constant k. We will later improve this construction so that the bijection also preserves k-hop
neighborhoods.
Definition 10 (Degree padding GÌ„ of a graph G and special vertices). For every graph G = (V, E),
let dl and dh be minimum and maximum vertex degrees in G respectively. Define the degree padding
GÌ„ = (VÌ„ , EÌ„) of G as the graph obtained from G by adding a bipartite clique between vertices of degree dl
and a new set of dh âˆ’ dl vertices. More formally, let S be a set of dh âˆ’ dl nodes disjoint from V , let
VÌ„ := V âˆª S, and EÌ„ := E âˆª (S Ã— {v âˆˆ V |d(v) = dl }) (see Fig. 4). We refer to the set S in GÌ„ as the set
of special vertices.
Definition 11 (Recursive construction of k-depth similar graphs G(k) and H (k) ). For every integer k â‰¥ 1
and integer c â‰¥ 1, define graphs G(k) and H (k) recursively as follows. For k = 1, let G(1) be a graph that
is the disjoint union of a c + 1-clique and (c + 1)c/2 isolated edges. Let H (1) be c + 1 disjoint copies of
c-stars (see Fig. 3). For k > 1, let GÌ„(kâˆ’1) denote a degree padding of G(kâˆ’1) as per Definition 16, and
(kâˆ’1)
(kâˆ’1)
let G(k) = GÌ„1
âˆª . . . âˆª GÌ„c
denote the disjoint union of c copies of GÌ„(kâˆ’1) . Similarly let HÌ„ (k) denote
(kâˆ’1)
(kâˆ’1)
(kâˆ’1)
the degree padding of H
, and let H (k) = HÌ„1
âˆª . . . âˆª HÌ„c
denote the disjoint union of c copies
(kâˆ’1)
of HÌ„
.
39

G(1)

G

V`
(c + 1)-clique

(c + 1)c/2 isolated edges

H (1)

dh âˆ’ d`
new vertices
G\Vh

c + 1 copies of c-star

Figure 4: Padding of a graph G.

Figure 3: Construction of G(1) and H (1) .

We first prove some simple structural claims about G(j) and H (j) .
(j)

(j)

Lemma 19. For graphs G(j) and H (j) as defined in Definition 11 there exist numbers Nh , Nl and
(j)
(j)
(j)
(j)
(j)
dh > dl such that both G(j) and H (j) contain exactly Nh vertices of degree dh , Nl vertices of
(j)
dl and no other vertices. Let V (j) and W (j) denote the vertex-sets of G(j) and H (j) , respectively.
(j)
(j)
(j)
(j)
Furthermore, let Vh âŠ† V (j) and Wh âŠ† W (j) be the vertices of degree dh ; let Vl
âŠ† V (j) and
(j)
(j)
Wl âŠ† W (j) be vertices of degree dl .
Proof. We the prove the lemma by induction on j. Clearly, for j = 1, the claim of the lemma is satisfied
(1)
(1)
(1)
(1)
with Nh = c + 1, Nl = (c + 1)c, dh = c and dl = 1.
Inductive step: j âˆ’ 1 â†’ j. Consider GÌ„(jâˆ’1) and HÌ„ (jâˆ’1) . It is clear that the vertices of both of these
graphs fall into two categories: the old vertices of G(jâˆ’1) and H (jâˆ’1) respectively as well as the newly
(jâˆ’1)
(jâˆ’1)
introduced special vertices. The old vertices used to be of degree either dh
or dl
according to
(jâˆ’1)
the inductive hypothesis. After the padding, the degree of those vertices is uniform and equal to dh
.
Since the special vertices are connected to all the old vertices that used to have low degree in G(jâˆ’1) and
(jâˆ’1)
H (jâˆ’1) respectively, the degrees of each special vertex is Nl
. By definition, the number of special
(jâˆ’1)
(jâˆ’1)
(jâˆ’1)
(jâˆ’1)
vertices in each of GÌ„
and HÌ„
is dh
âˆ’ dl
.
All that remains to be proven is that the special vertices have strictly higher degree than the old
(jâˆ’1)
vertices in, say GÌ„(jâˆ’1) , that is Nl
> djâˆ’1
h . For j = 2 one can simply verify that this is true
from the base construction of Definition 11. For j > 2 consider the structure of G(jâˆ’1) : G(jâˆ’1) =
(jâˆ’2)
(jâˆ’2)
(jâˆ’2)
GÌ„1
âˆª . . . âˆª GÌ„c
where GÌ„i
are disjoint copies of the degree padded version of G(jâˆ’2) . Hence, any
(jâˆ’1)
high degree vertex of G
corresponds to a special vertex used in the padding of G(jâˆ’2) and so, no
(jâˆ’1)
two high degree vertices of G
are connected to each other. So a high degre vertex of G(jâˆ’1) is only
connected to its low degree vertices, and not even all of them, since G(jâˆ’1) falls into c disjoint copies.
(jâˆ’1)
Therefore its degree, dh
must be smaller than the number of low degree vertices in the same graph,
(jâˆ’1)
Nl
.
In conclusion, we have proved the equivalent of the lemmaâ€™s statement for GÌ„(jâˆ’1) and HÌ„ (jâˆ’1) . It is
easy to see that duplicating this c times will not disrupt this.
Lemma 20. For graphs G(j) and H (j) as defined in Definition 11 and c â‰¥ 2k, the following inequalities
hold for the quantities from Lemma 19:
(j)

â€¢ Nh â‰¤ cj + 2jcjâˆ’1
(j)

â€¢ Nl

â‰¤ cj+1 + 2jcj
40

(j)

â€¢ dh â‰¤ cj + 2jcjâˆ’1
(1)

(1)

Proof. We prove the claims by induction. As mentioned before Nh = c + 1, Nl
which satisfies the inequalities.

(1)

= (c + 1)c and dh = c

Inductive step: j âˆ’ 1 â†’ j. Since these quantities can be derived equivalently from either G(j) or
H (j) by Lemma 19, we will be looking at G(j) for simplicity. The set of high degree vertices in G(j)
(jâˆ’1)
(jâˆ’1)
are the copies
of the special
vertices from degree padding G(jâˆ’1) , which number dh
âˆ’ dl
. So


(j)

(jâˆ’1)

Nh = c dh

(jâˆ’1)

âˆ’ dl

(jâˆ’1)

â‰¤ cdh

â‰¤ cj + 2(j âˆ’ 1)cjâˆ’1 â‰¤ cj + 2jcjâˆ’1 as claimed.

(jâˆ’1)
The set of low degree vertices in G(j) are copies
, which
 of the old vertices
 from the unpadded G
(jâˆ’1)
(jâˆ’1)
(j)
(jâˆ’1)
(jâˆ’1)
(jâˆ’1)
j
jâˆ’1
j+1
number |V
| = Nh
+ Nl
. So Nl = c Nh
+ Nl
â‰¤ c + 2(j âˆ’ 1)c
+c
+ 2(j âˆ’

1)cj â‰¤ cj+1 + 2jcj as claimed, since 2(j âˆ’ 1) â‰¤ 2k â‰¤ c.
The high degree vertices in G(j) are copies of the special vertices added during the degree padding
(jâˆ’1)
(j)
(jâˆ’1)
of G(jâˆ’1) ; their degree is Nl
as prescribed in Definition 16. So dh = Nl
= cj + 2(j âˆ’ 1)c(jâˆ’1) â‰¤
j
jâˆ’1
c + 2jc
as claimed.
We will now show that the discrepancy in the matching numbers of G(j) and H (j) persists throughout
the recursive construction.
Lemma 21. For c â‰¥ 2k the graph G(j) as constructed in Definition 11 has maximum matching size at
least (c + 1)cj /2 while H (j) from the same construction has maximum matching size at most 2jcj .
Proof. We prove the following claim by induction: G(j) has a feasible matching of size (c + 1)cj /2 while
H (j) has a feasible vertex cover of size 2jcj . For the case of j = 1 this is clear: the set of isolated edges
in G(1) constitutes a matching and the centers of the stars in H (1) constitute a vertex cover.
Inductive step: j âˆ’ 1 â†’ j. Degree padding does not affect the feasibility of a matching. Hence, when
constructing G(j) by duplicating GÌ„(jâˆ’1) c times, to construct a matching in G(j) we can simply duplicate
the matchings from GÌ„(jâˆ’1) as well. This results in a matching in G(j) that is c times larger than the one
in GÌ„(jâˆ’1) . Therefore, by the inductive hypothesis G(j) has a sufficiently large feasible matching.
Now we discuss the size of minimum vertex cover in H (j) . Upon degree padding H (jâˆ’1) we add the
(jâˆ’1)
(jâˆ’1)
special vertices to the vertex cover, increasing its size by dh
âˆ’ dl
â‰¤ cjâˆ’1 + 2(j âˆ’ 1)cjâˆ’2 . When
(jâˆ’1)
duplicating HÌ„
c times we duplicate the vertex cover as well. This makes the size of our feasible
vertex cover of H (j) at most 2(j âˆ’ 1)cj + cj + 2(j âˆ’ 1)cjâˆ’1 â‰¤ 2jcj as claimed, since 2(j âˆ’ 1) â‰¤ 2k â‰¤ c.
Next we define a sequence of bijections between vertex-sets of G(j) and H (j) . We begin by giving the
following definitions.
Each of the bijections between the vertex-sets of G(j) and H (j) that we define preserves j-level degree.
e def
e def
Lemma 22. For c â‰¥ 2k, let G
= G(j) and H
= H (j) be defined as in Definition 11. Then, there exists
e
e
G
(j)
a bijection Î¦(j) : V (j) â†’ W (j) such that for every v âˆˆ W (j) it holds dH
j (v) = dj (Î¦ (v)).
Proof. We prove this lemma by induction on j.
Base case: j = 1. Consider the following bijection Î¦(1) that maps the vertices from G(1) to H (1) : Î¦(1)
maps vertices of degree c (vertices in the clique in G(1) ) to the centers of stars in H (1) , and endpoints
of isolated edges in G(1) to petals in H (1) . Î¦(1) can be arbitrary as long as it satisfies this constraint
(and remains a bijection). Observe that Î¦(1) is a bijection such that the vertices of G(1) are mapped to
vertices of the same degree in H (1) .
Inductive step: j âˆ’ 1 â†’ j. We first define a bijection Î¦(j) and then argue that it maps the vertices
of G(j) to the vertices of H (j) with the same j-level degree.
We define Î¦(j) inductively on j as follows. Recall that G(j) is a disjoint union of c copies of the
degree padding GÌ„(jâˆ’1) of G(jâˆ’1) , and H (j) is a disjoint union of c copies of the degree padding HÌ„ (jâˆ’1)
of H (jâˆ’1) . To define Î¦(j) , we â€œreuseâ€ Î¦(jâˆ’1) (which exists by the inductive hypothesis) and add the
mapping for the vertices not included by Î¦(jâˆ’1) ; these vertices are called special (see Definition 16). By
41

(jâˆ’1)

(jâˆ’1)

Lemma 19 the number of special vertices in GÌ„i
equals the number of special vertices in HÌ„i
, for
(jâˆ’1)
(j)
every i = 1, . . . , c. So, we define Î¦ to map the special set Ai of vertices in GÌ„i
bijectively (and
(jâˆ’1)
arbitrarily) to the special set Bi of vertices in HÌ„i
.
(jâˆ’1)
(jâˆ’1)
def
def
For the sake of brevity, let dÎ¹ (v) = dÎ¹G
for vertices v of G(jâˆ’1) and dÎ¹ (v) = dH
for vertices
Î¹
(jâˆ’1)
def
(jâˆ’1)
HÌ„ (jâˆ’1)
Â¯Î¹ (v) def
of H (jâˆ’1) . Similarly, let dÂ¯Î¹ (v) = dGÌ„
for
vertices
v
of
GÌ„
and
d
=
d
(v)
for
vertices of
Î¹
Î¹
HÌ„ (jâˆ’1) . Furthermore, let N (v) and NÌ„ (v) denote the neighborhood of vertex v in G(jâˆ’1) âˆª H (jâˆ’1) and in
GÌ„(jâˆ’1) âˆª HÌ„ (jâˆ’1) , respectively.
e and H
e are c disjoint copies of GÌ„(jâˆ’1) and HÌ„ (jâˆ’1) , respectively, it suffices to show that Î¦(j)
Since G
maps a vertex of GÌ„(jâˆ’1) to a vertx of HÌ„ (jâˆ’1) with the same j-level degree. Recall that V (GÌ„(jâˆ’1) ) =
V (jâˆ’1) âˆª A, where V (jâˆ’1) = V (G(jâˆ’1) ) and A refers to the special vertices added to G(jâˆ’1) . Similarly,
recall that V (HÌ„ (jâˆ’1) ) = W (jâˆ’1) âˆª B. For the rest of our proof, we show the following claim.
Claim 1. We have that:
(A) For any u âˆˆ V (jâˆ’1) and u0 âˆˆ W (jâˆ’1) (not necessarily mapped to each other by the bijection) if
dÎ¹âˆ’1 (u) = dÎ¹âˆ’1 (u0 ), then dÂ¯Î¹ (u) = dÂ¯Î¹ (u0 ). (For the purposes of this statement, let dG
0 (v) = âˆ… for every
v.)
(B) For any u âˆˆ A and u0 âˆˆ B it holds dÂ¯Î¹ (u) = dÂ¯Î¹ (u0 ).
Proof. We prove this claim by induction.
Base case: Î¹ = 1.
Proof of (A) By construction of GÌ„(jâˆ’1) and HÌ„ (jâˆ’1) , all vertices in either V (jâˆ’1) or W (jâˆ’1) have degree
(jâˆ’1)
exactly dh
, so their 1-level degrees are equal.
Proof of (B) By construction of GÌ„(jâˆ’1) and HÌ„ (jâˆ’1) , all vertices in either A or B have degree exactly
(jâˆ’1)
dl
, so their 1-level degrees are equal.
Inductive step: Î¹ âˆ’ 1 â†’ Î¹.
Proof of (A) Let u and u0 be two vertices satisfying condition (A) for Î¹ âˆ’ 1, i.e., dÎ¹âˆ’1 (u) = dÎ¹âˆ’1 (u0 ).
Then, by definition
]
]
{dÎ¹âˆ’2 (Ï‰)} =
{dÎ¹âˆ’2 (Ï‰ 0 )}.
Ï‰ 0 âˆˆN (u0 )

Ï‰âˆˆN (u)

(jâˆ’1)

This means that there exists a bijection between the Gi
-neighborhoods of u and u0 , denoted by
âˆ—
0
Î¦u : N (u) â†’ N (u ) that preserves Î¹ âˆ’ 2-level degrees. For any Ï‰ âˆˆ N (u) and Ï‰ 0 âˆˆ N (u0 ) such that
Î¦âˆ—u (Ï‰) = Ï‰ 0 , by (A) of the inductive hypothesis it is also true that dÂ¯Î¹âˆ’1 (Ï‰) = dÂ¯Î¹âˆ’1 (Ï‰ 0 ). Therefore
]
]
{dÂ¯Î¹âˆ’1 (Ï‰)} =
{dÂ¯Î¹âˆ’1 (Ï‰ 0 )}.
(29)
Ï‰ 0 âˆˆN (u0 )

Ï‰âˆˆN (u)

To prove the inductive step for (A), we will show that
]
]
{dÂ¯Î¹âˆ’1 (Ï‰)} =

{dÂ¯Î¹âˆ’1 (Ï‰ 0 )}

(30)

Ï‰ 0 âˆˆNÌ„ (u0 )

Ï‰âˆˆNÌ„ (u)

as follows. Note that in (30) the neighbors w iterate over NÌ„ , while in (29) they iterate over N .
Now we consider two cases.
(jâˆ’1)

(jâˆ’1)

Case u âˆˆ Vh
: It follows that u0 âˆˆ Wh
. Then we are done, since u is not connected to A
(jâˆ’1)
(j)
and its neighborhoods in G1
and GÌ„1 are identical. Similarly for u0 .
(jâˆ’1)

(jâˆ’1)

Case u âˆˆ Vl
: It follows that u0 âˆˆ Wl
. Then NÌ„ (u)\N (u) = A. Similarly NÌ„ (u0 )\N (u0 ) = B.
It holds that |A| = |B| and, by (B) of the inductive hypothesis, any vertex of A and any vertex of
B have identical dÂ¯Î¹âˆ’1 -degrees. Therefore, extending the multiset-union from N (u) and N (u0 ) to
NÌ„ (u) and NÌ„ (u0 ), respectively, preserves the equality of dÂ¯Î¹ -degrees.
Hence, in both cases it holds that dÂ¯Î¹ (u) = dÂ¯Î¹ (u0 ), as claimed.

42

(jâˆ’1)

Proof of (B) Consider vertices u âˆˆ A and u0 âˆˆ B. In this case NÌ„ (u) = Vl
our goal is to prove that
]
]
{dÂ¯Î¹âˆ’1 (Ï‰ 0 )}
{dÂ¯Î¹âˆ’1 (Ï‰)} =

(jâˆ’1)

and NÌ„ (u0 ) = Wl

, so

(jâˆ’1)

(jâˆ’1)

Ï‰ 0 âˆˆWl

Ï‰âˆˆVl

(jâˆ’1)

or, equivalently, we aim to show that there exists a bijection between Vl
preserves dÂ¯Î¹âˆ’1 -degree.

(jâˆ’1)

and Wl

that

By the claim of the outer inductive hypothesis, i.e., by Lemma 22, there exists a bijection Î¦(jâˆ’1)
that preserves the djâˆ’1 -degree between V (jâˆ’1) and W (jâˆ’1) . Since Î¦(jâˆ’1) |V (jâˆ’1) preserves djâˆ’1
l
degree it also preserves dÎ¹âˆ’2 -degree and by (A) it also preserves dÂ¯Î¹âˆ’1 -degree. Thus, (B) holds.

To conclude the proof of Lemma 22 consider again a vertex v âˆˆ V (jâˆ’1) âˆª A to which Î¦(j) can be
applied. If v âˆˆ V (jâˆ’1) , then the claim holds by Claim 1 (A); if it is in A, then the claim holds by
Claim 1 (B).
Corollary 7. For large enough c â‰¥ 2k there exists a bijection Î¦(k) : V (k) â†’ W (k) such that for any
v âˆˆ V (k) dk (v) = dk (Î¦(k) (v)). However, MM (G1 ) and MM (G2 ) differ by at least a factor c+1
4k .
Proof. By Lemma 21 and Lemma 22 the graphs G(k) and H (k) constructed in Definition 11 satisfy these
requirements when c â‰¥ 2k

8.4

Increasing Girth via Graph Lifting

Let us start this section by introducing Cayley graphs, we will later use them in order to increase the
girth. Girth refers to the minimum length of a cycle within a graph.
Definition 12. Let G be a group and S a generating set of elements. The Cayley graph associated with
G and S is defined as follows: Let the vertex set of the graph be G. Let any two elements of the vertex
set, g1 and g2 be connected by an edge if and only if g1 Â· s = g2 or g1 Â· sâˆ’1 = g2 for some element s âˆˆ S.
We can think of the edges of a Cayley graph as being directed and labeled by generator elements from S.
Remark 7. Consider traversing a (undirected) walk in a Cayley graph. Let the sequence of edge-labels
of the walk be s1 , s2 , . . . , sl . Further, let 1 , 2 , . . . , l be Â±1 variables, where i corresponds to whether
we have crossed the ith edge in the direction associated with right-multiplication by si (i = 1) or in the
direction associated with right-multiplication by sâˆ’1
(i = âˆ’1). Then traversing the walk corresponds to
i
multiplication by s11 Â· s22 Â· . . . Â· sl l , that is the final vertex of the walk corresponds to the starting vertex
multiplied by this sequence.
Definition 13. Let G be a group. A sequence of elements s11 Â· s22 Â· . . . Â· sl l , from some generator set S,
is considered irreducible if no two consecutive elements cancel out. That is
i+1
@i : si i Â· si+1
= 1.



Remark 8. By the previous remark, circuits of a Cayley graph associated with G and S correspond to
irreducible sequences from S multiplying to 1. A circuit is a closed walk with no repeating edges. (Note
that the other direction is not true: Not all irreducible sequences from S multiplying to 1 correspond to
circuits in the Cayley graph, as they could have repeating edges.)
(k)

(k)

Taking (G1 , G2 ) from the previous section we now have a pair of graphs that differ greatly in
their maximum matching size but are identical with respect to their k-level degree composition. In order
to turn this result into a hard instance for approximating the maximum matching size, we need the
additional property that both graphs are high girth (particularly â‰¥ 2k + 2).
Theorem 11. For every graph G = (V, E), |V | = n, every integer g â‰¥ 1, there exists an integer
R = R(n, g) = nO(g) and a graph L = (VL , EL ), VL = V Ã— [R], such that the following conditions hold.
(1) Size of the maximum matching of L is multiplicatively close to that of G: R Â· MM (G) â‰¤ MM (L) â‰¤
2R Â· MM (G);
43

(2) L contains no cycle shorter than g (i.e., L has high girth);
G
(3) For every k âˆˆ N and every v âˆˆ V one has, for all r âˆˆ R that dH
k ((v, r)) = dk (v).

We refer to L as the lift of G.
Before proving this theorem, let us state a key lemma that we use in proving it. The proof of this
lemma is deferred to Appendix D.2.
Lemma 23. For any parameters g and l, there exists a group G of size lO(g) along with a set of generator
elements S of size at least l, such that the associated Cayley graph (Definition 12) has girth at least g.
Equivalently, this means that no irreducible sequence of elements from S and their inverses, shorter
than g, equates to the identity. We are now ready to prove Theorem 11.
Proof of Theorem 11. Let G be a group according to Lemma 23 with parameter l = |E|, and let S âŠ† G
denote the set of elements of G whose Cayley graph has girth at least g, as guaranteed by Lemma 23.
We think of the elements of S as indexed by the edges of the graph G, and write S = (se )eâˆˆE . We direct
the edges of G arbitrarily, and define the edge-set EL of L as
def

EL = {((v1 , g1 ), (v2 , g2 )) âˆˆ VL Ã— VL |e = (v1 , v2 ) âˆˆ E and g1 Â· se = g2 } .
That is we connect vertices (v1 , g1 ), (v2 , g2 ) âˆˆ VL by an edge if and only if e = (v1 , v2 ) is an (directed)
edge in E and g1 Â· se = g2 in G. In this construction R = |G| = mO(g) = nO(g) as stated in the theorem.
Note that every vertex v in the original graph G corresponds to an independent set of R vertices v Ã—G
in L. For any pair of vertices v1 and v2 in the original graph if (v1 , v2 ) âˆˆ E, then the subgraph induced
by (v1 Ã— G) âˆª (v2 Ã— G) is a perfect matching; if not, the union (v1 Ã— G) âˆª (v2 Ã— G) forms an independent
set. Overall, every edge e = (v1 , v2 ) âˆˆ E can be naturally mapped to R edges among the edges of L,
namely
{((v1 , g1 ), (v2 , g1 Â· se )) âˆˆ VL Ã— VL |g1 âˆˆ G} .
Property (1). Any matching M of G can be converted into a matching M L of size R|M | in L, for
instance M L = {((v1 , g1 ), (v2 , g2 )) âˆˆ EL |(v1 , v2 ) âˆˆ M and g1 âˆˆ G}. As mentioned above, g2 is uniquely
defined here. The same statement is true for vertex covers: If V is a vertex cover in G, then V Ã— G is a
vertex cover in L. Therefore,
R Â· MM (G) â‰¤ MM (L) â‰¤ VC (L) â‰¤ R Â· VC (G) â‰¤ 2R Â· MM (G) ,
where the last inequality is due to the fact that the minimum vertex cover of a graph is at most twice
the size of its maximum matching.
Property (2).
C be

Toward contradiction, suppose C is a short cycle in L, that is it has length f < g. Let
((v1 , g1 )(v2 , g2 ), . . . , (vh , gh )(vf +1 , gf +1 ) = (v1 , g1 )).

Let ei := ((vi , gi ), (vi+1 , gi+1 )) âˆˆ EL . Let i be a Â±1 variable indicating whether the direction of the
edge ei is towards vi+1 (i = 1) or towards vi (i = âˆ’1). (Recall that the edges of G were arbitrarily
directed during the construction of L.)

If C as described above is indeed a cycle, this means that g1 Â· se11 Â· se22 Â· . . . Â· seff = g1 . Therefore
f
1 2
se1 Â·se2 Â·. . .Â·sef is an irreducible sequence of elements from S and their inverses shorter than g that equates
to unity. Indeed, if it was not irrdeducible, that is an element and its inverse appeared consecutively,
then that would mean C crossed an edge twice consecutively (ei = ei+1 for some i) and therefore it
would not be a true cycle. This is a contradiction of theorem Lemma 23, so L must have girth at least
g.
Property (3). The third statement is proven by induction on k. The base case is provided by k = 1.
Fix v and h âˆˆ G. Every neighbor of v in G corresponds to exactly one neighbor of (v, h) in L. Indeed,
w âˆˆ N (v) (such that e = (v, w) âˆˆ E) corresponds to (w, h Â· se ) where  indicates the direction of e.
Therefore dG (v) = dL ((v, h)).

44

We now show the inductive step (k âˆ’ 1 â†’ k). Again, fix v and h. Similarly to the base case, every
neighbor w âˆˆ N (v) corresponds to a single neighbor of (v, h) in L: (w, hw ) for some hw âˆˆ G. By the
L
inductive hypothesis dG
kâˆ’1 (w) = dkâˆ’1 ((w, hw )). So
]
]
L
dG
{dG
{dL
k (v) =
kâˆ’1 (w)} =
kâˆ’1 ((w, hw ))} = dk ((v, h))
wâˆˆN (v)

wâˆˆN (v)

Thus, there exists a pair of graphs G1 and G2 such that there is a bijection between their vertex-sets
that preserves high level degrees up to level k and such that neither G1 nor G2 contains a cycle shorter
than 2k + 2. Furthermore, MM (G1 ) and MM (G2 ) differ by a factor of at least c+1
8k , which can be set to
be arbitrarily high by the choice of c.
Corollary 8. For c â‰¥ 2k, there exists a pair of graphs G and H with vertex sets V and W , respectively,
such that there is a bijection Î¦ : V â†’ W with the property that the k-depth neighborhoods of v and Î¦(v)
are isomorphic. Also, MM (G) â‰¥ c+1
8k MM (H).
Proof. This follows directly from Corollary 7, Theorem 11 and Observation 3. Indeed, consider graphs
G0 and H 0 guaranteed by Corollary 7 with the same parameters. Apply to each Theorem 11 to get the
lifted graphs G and H respectively. The bijection Î¦ guaranteed in Corollary 7 extends naturally to G
and H. By the guarantee of Theorem 11 this bijection still preserves k-level degrees, and furthermore,
both G and H have girth at least 2k + 2. By Observation 3 this is sufficient to show Corollary 8.

8.5

k-Edge Subgraph Statistics in G and H

The main result of this section is the equality of numbers of subgraphs in G and H. This will result in
proving Theorem 10.
Definition 14 (Subgraph counts). For a graph G = (V, E) and any graph K we let
#(K : G) = |{U âŠ† E|U âˆ¼
= K}| ,
where we write U âˆ¼
= K to denote the condition that U is isomorphic to K.
Lemma 24. Let k â‰¥ 1 be an integer and let G = (VG , EG ) and H = (VH , EH ) be two graphs such that a
bijection Î¦ : VG â†’ VH between their vertex-sets preserves the k-depth neighborhoods. That is, for every
v âˆˆ VG , the k-depth neighborhood of v is isomorphic to that of Î¦(v). Then for any graph K = (VK , EK )
of at most k edges #(K : G) = #(K : H).
Proof. We prove below that if
Î±K := |{Î¨ : VK ,â†’ VG |âˆ€(u, w) âˆˆ EK : (Î¨(u), Î¨(w)) âˆˆ EG }|
and
Î²K := |{Î¨ : VK ,â†’ VH |âˆ€(u, w) âˆˆ EK : (Î¨(u), Î¨(w)) âˆˆ EH }| ,
then Î±K = Î²K . Since Î±K = #(K : G) Â· |Aut(K)| and Î²K = #(K : H) Â· |Aut(K)|, where |Aut(K)| is the
number of automorphisms of K, then result then follows.
We proceed by induction on the number of connected components q in K. We start with the base
case (q = 1), which is when K is connected, i.e. K has one connected component. Select arbitrarily a
root r âˆˆ VK of K. Define further, for all v âˆˆ VG and v âˆˆ VH respectively
#(K : G|v) = |{Î¨ : VK ,â†’ VG |âˆ€(u, w) âˆˆ EK : (Î¨(u), Î¨(w)) âˆˆ EG âˆ§ Î¨(r) = v}|
and
#(K : H|v) = |{Î¨ : VK ,â†’ VH |âˆ€(u, w) âˆˆ EK : (Î¨(u), Î¨(w)) âˆˆ EH âˆ§ Î¨(r) = v}|
P
P
so that #(K : G) = vâˆˆVG #(K : G|v) and #(K : H) = vâˆˆVH #(K : H|v). Recall that there exists a
bijection Î¦ : VG â†’ VH such that for every v âˆˆ VG the k-depth neighborhood of v in G is identical to the
k-depth neighborhood of Î¦(v) in H. Since the number of edges in K is at most k, and K is connected,
we get that #(K : G|v) = #(K : H|Î¦(v)), and hence
X
X
X
#(K : G) =
#(K : G|v) =
#(K : H|Î¦(v)) =
#(K : H|v) = #(K : H),
vâˆˆVG

vâˆˆVG

vâˆˆVH

45

as required.
We now provide the inductive step (q âˆ’ 1 â†’ q). Since q â‰¥ 2, we let K1 = (VK1 , EK1 ) and
K2 = (VK2 , EK2 ) be a bipartition of K into two disjoint non-empty subgraphs, i.e., VK is the disjoint
union of VK1 and VK2 and EK is the disjoint union of EK1 and EK2 . Since the number of components of
K1 and K2 are both smaller than q, we have by the inductive hypothesis that Î±K1 = Î²K1 and Î±K2 = Î²K2 .
We will write the number of embeddings of K into G (resp. H) in terms of the number of embeddings
of K1 , K2 into G (resp. H), as well as embeddings of natural derived other graphs. Indeed, every pair
of embeddings (Î¨1 , Î¨2 ), where Î¨i : VKi ,â†’ VG , i âˆˆ {1, 2}, naturally defines a mapping Î¨ : VF â†’ VG .
However, this mapping is not necessarily injective. Indeed Î¨1 (v1 ) might clash with Î¨2 (v2 ) for some pairs
(v1 , v2 ) âˆˆ VK1 Ã— VK2 . In this case Î¦ defines different graph K 0 which we get by merging all clashing
pairs from VK1 Ã— VK2 , aling with an embedding of K 0 into G. Note that K 0 has strictly fewer than q
components. We call such a pair (Î¨1 , Î¨2 ) an K 0 -clashing pair. We now get
Î±K = |{embedding Î¨ of K into G}|
= |{(Î¨1 , Î¨2 )|embeddings of Ki into G, i âˆˆ {1, 2}}|
X
âˆ’
|{(Î¨1 , Î¨2 )|K 0 âˆ’ clashing embeddings of Ki , i âˆˆ {1, 2} into G}|
K 0 graph with
< q components

= |{(Î¨1 , Î¨2 )|embeddings of Ki into H, i âˆˆ {1, 2}}|
X
|{(Î¨1 , Î¨2 )|K 0 âˆ’ clashing embeddings of Ki , i âˆˆ {1, 2} into H}|
âˆ’
K 0 graph with
< q components

= Î²K ,
where in the third transition above we used the fact that for any K 0 with < q connected components
one has
|{(Î¨1 , Î¨2 )|K 0 âˆ’ clashing embeddings of Ki , i âˆˆ {1, 2} into G}|
=|{Î¨0 |embeddings of K 0 into G}|
=|{Î¨0 |embeddings of K 0 into H}| (by inductive hypothesis)
|{(Î¨1 , Î¨2 )|K 0 âˆ’ clashing embeddings of Ki , i âˆˆ {1, 2} into H}|.
This completes the proof.
Corollary 9 (Theorem 10). For every Î» > 1 and every k, there exist graphs G and H such that
MM (G) â‰¥ Î» Â· MM (H), but for every graph K with at most k edges, the number of subgraphs of G and
H isomorphic to K are equal.
Proof. By Lemma 24 the pair of graphs satisfying the guarantee of Corollary 8 will satisfy the guarantees
c+1
of Theorem 10 as well. We just need to set c such that c â‰¥ 2k and 8(k+1)
â‰¥ Î».

9
9.1

Analysis of the algorithm on a random permutation stream
Introduction and Technical Overview

In this section we focus on the setting in which the set of elements, e.g., edges, is presented as a random
permutation. This has been a popular model of computation for graph algorithms in recent years with
many results, including for matching size approximation [KMM12, KKS14, MMPS17, PS18, ABB+ 19].
As mentioned before, our goal is to show that Algorithm 2 is robust to the correlations introduced by
replacing independent samples with a random permutation. This results in algorithm for approximating
matching size to within a factor of O(log2 n), in polylogarythmic space.
Theorem 4. There exists an algorithm that given access to a random permutation edge stream of a graph
G = (V, E), with n vertices and m â‰¥ 3n edges, produces an O(log2 n) factor approximation to maximum
matching size in G using O(log2 n) bits of memory in a single pass over the stream with probability at
least 3/4.

46

Recall that this improves the previous best-known approximation ratio ([KKS14]) by at least a factor
O(log6 n).
Our overall strategy consists of showing that the algorithm behaves identically when applied to iid
samples or a permutation. That is, we show that distribution of the state of the algorithm at each moment
is similar under these two settings. To this end, we use total variation distance and KL-divergence as
a measure of similarity of distributions. Furthermore, we break down the algorithm to the level of
Level-j-Test tests to show that these behave similarly.
More specifically, consider an invocation of Level-j-Test(v) in either the iid or the permutation
stream. In the permutation stream, we have already seen edges pass and therefore know that they will
not reappear during the test; this biases the output of the test compared to the iid version which is
oblivious to the prefix of the stream. However, we are able to prove in Section 9.4 that KL-divergence
between the output-distribution of these two versions is proportional to the number of samples used,
with a factor of O(log2 n/m), see Lemma 30. Since this is true for all tests, intuitively, the algorithm
should still work in the permutation setting as long as it uses O(m/ log2 n) samples. Conversely, our
original algorithm uses Î˜(m) samples; however, we can reduce the number of samples used by slightly
altering it, at the expense of a O(log2 n) factor in the approximation ratio.
In fact the algorithm we use in permutation stream setting is nearly identical to the one defined in
Section 3; the one difference is that in the subroutine Edge-Level-Test we truncate the number of
tests to J âˆ’ â„¦(log2 n) to reduce the number of edges used, (see Algorithm 16 in Algorithm 16).
One technical issue that arises in carrying out this approach is the fact that KL-divergence does
not satisfy the triangle inequality, not even an approximate one, when the distributions of the random
variables in question can be very concentrated. (Or, equivalently, we can assume very small values, since
we are thinking of Bernoulli variables). Essentially triangle inequality is not satisfied even approximately
when some of the random variables in question are nearly deterministic (see Remark 2 in Section 9.2).
We circumvent this problem using a mixture of total variation and KL-divergence bounds.
Furthermore, we develop a weaker version of triangle inequality for KL-divergence, see Lemma 27. This
both loses a constant factor in the inequality and is only true under the condition that the variables
involved are bounded away from deterministic. However, it suffices for our proofs in Sections 9.3
and 9.4.
Our proof strategy consists of two steps: we first modify the tests somewhat to ensure that all relevant
variables are not too close to deterministic (see the padded tests presented in Section 9.3), at the same
time ensuring that the total variation distance to the original tests is very small (see Lemma 28 in
Section 9.3). We then bound the KL-divergence between the padded tests on the iid and permutation
stream in Section 9.4. An application of Pinskerâ€™s inequality then completes the proof.

9.2

Preliminaries

Throughout this section, for sake of simplicity, we use n as an upperbound on the maximum degree d of
G. This does not affect the guarantees that we provide.
Next we provide some notations that we will use in the sequel. Let Î  be the random variable
describing the permutation stream. Let the residual graph at time t be Gt , that is the original graph,
but with the edges that have already appeared in the stream up until time t deleted, for 0 â‰¤ t â‰¤ m. In
this case G0 is simply the original input graph and Gm is the empty graph of n isolated vertices. Let
the residual degree of v at time T be
t
dG
def
.
dt =
1 âˆ’ t/m
Let the outcome of a j th level test on vertex v (recall Level-j-Test from Algorithm 3) be TjIID (v).
Let of the same test on the permutation stream, performed at time t will be denoted TjÏ€,t (v).
Definition 15 (KL-divergence). For two distributions P and Q over X , the KL-divergence of P and Q
is


X
Q(x)
DKL (P kQ) =
âˆ’P (x) log
.
P (x)
xâˆˆX

Remark 1. Throughout this document, log is used to denote the natural logarithm in the definition of
KL-divergence, even though it is conventionally the base 2 logarithm. This only scales down DKL (P kQ)
by a factor of log 2 and does not affect any of the proofs.

47

Lemma 25 (Chain rule). For random vectors P = (Pi )i and Q = (Qi )i ,
X
DKL (P kQ) =
Ex1 ,...,xi1 [DKL (Pi kQi |x1 , . . . , xiâˆ’1 )]
i

Lemma 26. For every p,  âˆˆ [0, 1] such that p +  âˆˆ [0, 1] one has
DKL (Ber (p + )kBer (p)) =

162
p(1 âˆ’ p)

A proof of Lemma 26 is provided in Appendix E.
Remark 2. The triangle inequality does not hold for KL-divergence, not even with a constant
factor

loss, as the following example shows. For sufficiently small , let A = Ber (), B = Ber 2 and C =

Ber 1/ . One can verify that DKL (AkB) â‰¤ , DKL (BkC) â‰¤ , but DKL (AkC) â‰¥ Ï‰(). Nevertheless,
we provide a restricted version of triangle inequality in Lemma 27 that forms the basis of our analysis.
Definition 16 (Î¸-padding of a Bernoulli random variable). We define the padding operation as follows.
Given a Bernoulli random variable X and a threshold Î¸ âˆˆ (0, 1) we let
ï£±
if E[X] âˆˆ [Î¸, 1 âˆ’ Î¸]
ï£² X
def
Ber (Î¸)
if E[X] < Î¸
Padding(X, Î¸) =
ï£³
Ber (1 âˆ’ Î¸) if E[X] > 1 âˆ’ Î¸
Lemma 27 (Triangle inequality for padded KL-divergence). Suppose p, q, r âˆˆ [0, 1] and consider the
KL-divergence between Bernoulli variables Ber (p), Ber (q) and Ber (r). Then for some absolute constant
C, any  âˆˆ [0, 1/32], if DKL (Ber (p)kBer (q)) â‰¤  and DKL (Ber (q)kBer (r)) â‰¤ , then
DKL (Ber (p)kBer (Padding(r, ))) â‰¤ C.
The proof is provided in Appendix E.
Remark 9. We note that the padding is crucial, due to the example in Remark 2.

9.3

Padding and total variation distance

In this and the next section we compare the behavior of the Level-j-Testâ€™s on the iid and permutation
streams. Recall the definition of Level-j + 1-Test from Algorithm 4 in Section 3. We call this the
IID
Tj+1
test and restate it here for completeness:
Algorithm 11 Vertex test in the i.i.d. stream
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:

IID
procedure Tj+1
(v)
Sâ†0
for k = 1 to cj Â· m
n do
e â† next edge in the iid stream
if e is adjacent to v then
w â† the other endpoint of e
iâ†0
while i â‰¤ j and TiIID (w) do
S â† S + ciâˆ’j
if S â‰¥ Î´ then
return false
iâ†i+1

. Equivalent to sampling and iid edge

Similarly, define the version of the Tj+1 tests on the permutation stream, starting at position t, which
Ï€,t
we call Tj+1
as follows:

48

Algorithm 12 Vertex test in the permutation stream
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:

Ï€,t
procedure Tj+1
(v)
Sâ†0
for k = 1 to cj Â· m
n do
e â† next edge in the permutation stream . We start using the stream from the t + 1th edge
if e is adjacent to v then
w â† the other endpoint of e
iâ†0
while i â‰¤ j and TiÏ€ (w) do
S â† S + ciâˆ’j
if S â‰¥ Î´ then
return false
iâ†i+1
return True

IID
We also define recursively padded versions of Tj+1
define recursively

Algorithm 13 Padded T1 test
1:
2:

procedure Te1IID (v)
return T1IID (v).

Algorithm 14 Recursively padded Tj+1 tests
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:

IID

procedure T j+1 (v)
Sâ†0
for cj m/n edges e of the iid stream do
if e is adjacent to v then
w â† the other endpoint of e.
iâ†0
while i â‰¤ j and TeiIID (w) do
S â† S + ciâˆ’j
if S â‰¥ Î´ then
return false
iâ†i+1
return True
and, using Definition 16,

Algorithm 15 Recursively padded Tj+1 tests
1:
2:

IID
procedure Tej+1
(v)
IID

j

return Padding(T j+1 (v), 200c nlog

2

n

)

Note that these alternate tests are not implementable and merely serve as a tool to proving that the
T Ï€ tests work similarly to the T IID tests.
We begin by proving that padding the T IID tests, even recursively, only changes the output with
probability proportional to the fraction of the stream (of length m) consumed by the test.
Lemma 28. For sufficiently small Î´ > 0 and large enough c the following holds. For all v âˆˆ V and
j = [0, J] one has
400 Â· cj log2 n
IID
IID
â‰¤
.
Tj+1
(v) âˆ’ Tej+1
(v)
n
TV
Proof. The proof follows by triangle inequality of total variation distance: We will prove the following

49

bounds:
IID

IID
Tj+1
(v) âˆ’ T j+1 (v)

TV

IID

IID
T j+1 (v) âˆ’ Tej+1
(v)

200 Â· cj log2 n
n
200 Â· cj log2 n
â‰¤
.
n
â‰¤

TV

(31)
(32)
IID

IID
Eq. (32) follows easily from definitions, since Tej+1
(v) is 200cj log2 (n)/n-padded version of T j+1 (v).
We proceed to proving Eq. (31).
Consider the following 0, 1 vector Wj+1 (v) describing the process of a Tj+1 (v) test: The first cj m/n
coordinates denote the search phase; specifically we write a 1 if a neighbor was found and a 0 if not. The
following coordinates denote the outcomes of the recursive tests, 1 for pass 0 for fail, in the order they
were performed. There could be at most cj recursive tests performed (if they were all T1 â€™s) so Wj+1 (v)
has length cj m/n + cj in total. However, often much fewer recursive tests are performed due to the early
stopping rule, or simply because too few neighbors of v were found. In this case, tests not performed are
represented by a 0 in Wj+1 (v).
IID

IID
(v) and W j+1 (v). Since
Like with Tj+1 (v), we will define different versions of Wj+1 (v), namely Wj+1
IID

IID
(v) âˆ’ W j+1 (v)
Wj+1 (v) determines Tj+1 (v) we can simply bound Wj+1
IID

.
TV

The first cj m/n coordinates of W IID and W
are distributed identically and contribute nothing to
the divergence. Consider the test corresponding to the ith coordinate of the recursive phase of Wj+1 (v).
Let the level of the test be `i âˆˆ [0, j] (with 0 representing no test) and let the vertex of the test be ui .
These are random variables determined by Î . Furthermore, let ti be the position in the stream where
the recursive T`i test is called on ui . Then we have
IID

IID
Tj+1
(v) âˆ’ T j+1 (v)

IID

TV

IID
â‰¤ Wj+1
(v) âˆ’ W j+1 (v)
TV

X
  IID
IID
=
E`i ,ui Wj+1,i
(v) `i , ui âˆ’ W j+1,i (v) `i , ui
i

=

X

E`i ,ui T`IID
(ui ) âˆ’ Te`IID
(ui )
i
i

i

â‰¤

X


E`i

i

400c`i âˆ’1 log2 n
n

TV

TV


,

by the inductive hypothesis. Here (Wj+1,i (v)|`i , ui ) denotes conditional distribution of Wj+1,i (v) on `i
and ui .
Note that the term in the sum is proportional to the number of edges used by the corresponding
recursive test. Indeed, recall by Lemma 2 that a T`i test runs for at most c`i âˆ’1 Â· m
n Â· (1 + 2Î´) samples
with probability one. It is crucial that this result holds with probability one, and therefore extends to
not just the iid stream it was originally proven on, but any arbitrary stream of edges.
Therefore, the term in the sum above is at most 200C Â· c log2 (n)/m times the number of edges used
by the corresponding recursive test. So the entire sum is at most 400 log2 (n)/m times the length of the
recursive phase of the original Tj+1 test. It was also derived in Lemma 2 that the recursive phase itself
takes at most cj Â· m
n Â· 2Î´ edges. (Again, the result holds with probability one.) Therefore,
IID
IID
Tj+1
(v) âˆ’ Tej+1
(v)

â‰¤
TV

400 log2 n 2Î´cj m
400 Â· cj log2 n
Â·
â‰¤
,
m
n
n

for small enough absolute constant Î´. This shows Eq. (31) and concludes the proof.

9.4

Bounding KL-divergence

In this section we will bound the KL-divergence between tests on the permutation stream and padded
tests on the iid stream. We will use the padded triangle inequality of Lemma 27 as well as the data
processing inequality for kl-divergence:
Lemma 29. (Data Processing Inequality) For any random variables X, Y and any function f one has
DKL (f (X)||f (Y )) â‰¤ D(X||Y ).
50

Lemma 30. For sufficiently small Î´ > 0 and large enough c the following holds. With high probability
over Î , for all v âˆˆ V , j â‰¤ J, t â‰¤ m/2 âˆ’ 2cj Â· m
n,
 200C Â· cj log2 n

Ï€,t
IID
(v) Tej+1
DKL Tj+1
(v) Gt â‰¤
,
n

(33)

where C is the constant from Lemma 27.
Note that the choice of t is such that we guarantee the Tj+1 tests finishing before half of the stream
is up, by Lemma 2.
Naturally, we will prove the above lemma by induction on j. Specifically, our inductive hypothesis
will be that Eq. (33) holds up to some threshold j. Furthermore, recall that Algorithm 15 satisfies


200 Â· cj log2 n
IID
IID
.
Tej+1
(v) = Padding T j+1 (v),
n


Ï€,t
IID
(v) Gt we can apply Lemma 27 directly. By setting Ber (p) to
Thus, to bound DKL Tj+1
(v) Tej+1

IID
Ï€,t
IID
(v) Gt and Ber (r) to T j+1 (v) we get that Tej+1
Tj+1
(v) âˆ¼ Ber (e
r). So in order to bound
DKL (Ber (p)kBer (e
r)) as required by the lemma, we need only to bound DKL (Ber (p)kBer (q)) and
DKL (Ber (q)kBer (r)).
Our q, the midpoint of our triangle inequality, will be the outcome of a newly defined hybrid test
using both the permutation and iid streams.
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:

Ï‡,t
procedure Tj+1
(v)
Sâ†0
for k = 1 to cj Â· m
n do
e â† next edge in the permutation stream . We start using the stream from the t + 1th edge
if e is adjacent to v then
w â† the other endpoint of e
iâ†0
. Represents the last level that w passes
while i â‰¤ j and TeiIID (w) do
S â† S + ciâˆ’j
if S â‰¥ Î´ then
return false
iâ†i+1
return True



IID
Ï‡,t
We will begin by bounding DKL Tj+1
(v) T j+1 (v) Gt
Lemma 31. For sufficiently small Î´ > 0 and large enough c the following holds. With probability 1âˆ’nâˆ’5 ,
for all, v âˆˆ V , j â‰¤ J, t â‰¤ m/2 âˆ’ 2cj Â· m
n

 200 Â· cj Â· log2 n
IID
Ï‡,t
.
(34)
DKL Tj+1
(v) T j+1 (v) Gt â‰¤
n
Proof. We will deconstruct Tj+1 (v) a little differently than before, in the proof of Lemma 28. Consider
the following integer vector, Yj+1 (v), describing the process of Tj+1 (v): There are cj m/n coordinates
in total, with the ith coordinate corresponding to the ith edge sampled from the stream. If this edge is
not adjacent to v the coordinate is 0. If it is adjacent, with its other endpoint being ui , the algorithm
performs higher and higher level tests on ui until it fails or the level exceeds j. So let the ith coordinate
of Yj+1 (v) simply denote the level `i at which T`i (ui ) failed, or j + 1 if the vertex never failed.
IID

Ï‡,t
Like with Tj+1 (v), we will define different versions of Yj+1 (v), namely Yj+1
(v) and Y j+1 (v). Note that


IID
Ï‡
Yj+1 (v) determines Tj+1 (v) and it suffices to bound DKL Yj+1 (v) Y j+1 (v) Gt by the data processing

inequality (Lemma 29).




IID
IID
Ï‡,t
Ï‡,t
DKL Tj+1
(v) T j+1 (v) Gt â‰¤ DKL Yj+1
(v) Y j+1 (v) Gt


X
IID
Ï‡,t
=
EGti DKL Yj+1,i
(v) Y j+1,i (v) Gti ,
i

51

where Gti represents the residual graph right after we sample the ith edge for the Tj+1 test (that is
IID

Ï‡,t
(v) and Y j+1,i (v). In
ith excluding edges sampled during recursion). Consider the distribution of Yj+1,i
both cases the recursive tests run would use the iid stream. Consider for each neighbor of v running all
iid tests Te`IID for ` from 1 to j.
Let Ïˆ : E â†’ [0, j +1] be the following random mapping: If e is not adjacent on v then Ïˆ(e) = 0. If e =
IID
IID
(u, v), Ïˆ(e) is distributed as the smallest level ` at which u would fail when running T 1 (u), . . . , T j (u),
IID

Ï‡
(v)
and j + 1 if it never fails. As described above, Y j+1,i (v) is distributed as Ïˆ(e) : e âˆ¼ U(E) and Yj+1,i
ti
ti
is distributed as Ïˆ(e) : e âˆ¼ U(E ). Here U represents the uniform distribution and E represents the
residual edge-set.



IID
Ï‡
EGti DKL Yj+1,i
(v) Y j+1,i (v) Gti = EGti ,Ïˆ DKL Ïˆ(e) : e âˆ¼ U(E ti ) Ïˆ(e) : e âˆ¼ U(E) Gti

We will now bound the right hand side with high probability over Gti . Fix v and j, thereby fixing
the distribution of Ïˆ. Define for every k âˆˆ [0, j + 1]
pIID
k (v) := Peâˆ¼U(E) [Ïˆ(e) = k]

(35)

i
pÏ‡,t
k (v) := Peâˆ¼U(E ti ) [Ïˆ(e) = k].

We know by Chernoff bound that for every fixed choice of v âˆˆ V , k âˆˆ [0, j +1] and ti â‰¤ m/2âˆ’2ckâˆ’1 Â· m
n
r
100pk log n
Ï‡,ti
(36)
|pIID
k (v) âˆ’ pk (v)| â‰¤
m
with probability at least 1 âˆ’ nâˆ’10 over the randomness of Î . Indeed
i
pÏ‡,t
k (v) =

X 1(e âˆˆ Gti ) Â· P (Ïˆ(e) = k)
,
m âˆ’ ti

eâˆˆE

Ï‡,ti
i
IID
so EpÏ‡,t
is a sum of independent variables bounded by 2/m. Therefore, by
k (v) = pk (v) and pk
Chernoff bounds, specifically Items a and c of Theorem 7,
#
"
r


100pIID
100pIID
Ï‡,ti
IID
k log n
k log n
â‰¤ 2 exp
â‰¤ nâˆ’10 .
P |pk (v) âˆ’ pk (v)| â‰¥
m
3mpIID
Â·
(2/m)
k

Therefore, with probability at least 1 âˆ’ nâˆ’5 this bound holds for all choices of v, k and ti simultaneously.
Let us restrict our analysis to this event from now on.
i
In the following calculation we denote pÏ‡,t
ek and pIID
k (v) by pk for simplicity of notation. We
k (v) by p
have
  X


j+1
j+1
 X
pk
pk âˆ’ pek
DKL Ïˆ(e) : e âˆ¼ U(E ti ) Ïˆ(e) : e âˆ¼ U(E) Ïˆ, Gti =
âˆ’e
pk log
=
âˆ’e
pk log 1 +
.
pek
pek
k=0

k=0

Note that because ti â‰¤ m/2, pe âˆˆ [0, 2pk ], so (pk âˆ’ pek )/e
pk is in the range [âˆ’1/2, âˆž]. In the range
x âˆˆ [âˆ’1/2, âˆž], log(1 + x) can be lower bounded by x âˆ’ x2 . Therefore, the above sum can be upper
bounded as follows:


j+1
 X
(pk âˆ’ pek )2
pk âˆ’ pek
DKL Ïˆ(e) : e âˆ¼ U(E ti ) Ïˆ(e) : e âˆ¼ U(E) Ïˆ, Gti â‰¤
âˆ’
âˆ’e
pk Â·
pek
pe2k
k=0

j+1 
X
(pk âˆ’ pek )2
=
pek âˆ’ pk +
pek
k=0
p
2
j+1
100e
pk log n/m
X
â‰¤
due to Eq. (36),
pek
k=0

â‰¤

j+1
X
100 log n
k=0

m
2

â‰¤

200 log n
.
m

52



IID
Ï‡
(v) T j+1 (v) .
With this we can bound the whole sum, and thus DKL Tj+1

 cj m 200 log2 n
IID
Ï‡
(v) T j+1 (v) =
DKL Tj+1
Â·
n
m
200 Â· cj log2 n
=
,
n
as claimed.
We will now proceed to bound the divergence between T1Ï€,t (v) and T1IID (v). This will serve as the
base case of the induction in the proof of Lemma 30.
Lemma 32. For sufficiently small Î´ > 0 and large enough c the following holds. With probability 1âˆ’nâˆ’5 ,
for all v âˆˆ V , t â‰¤ m/2 âˆ’ 2m/n
 200C Â· log2 n
,
DKL T1Ï€,t (v) T1IID (v) Gt â‰¤
n
where C is the constant from Lemma 27.

(37)

Proof. We will deconstruct the tests similarly to the previous proof. For both types of T1 (v) test consider
the random boolean vector Y âˆˆ {0, 1}m/n denoting whether the next m/n edges in the appropriate stream
are adjacent to v or not. That is Yi = 1 if and only if the ith edge in the appropriate stream is adjacent
to v. Let Y Ï€ and Y IID denote such random variables for T1Ï€,t (v) and T1IID (v) respectively.
Because Y determines the outcome of T1 (v), by data processing inequality (Lemma 29) one has


DKL T1Ï€,t (v) T1IID (v) Gt â‰¤ DKL Y Ï€ Y IID Gt .
(38)
m/n
 X

Ï€
DKL YiÏ€ YiIID Gt , Y1Ï€ , . . . , Yiâˆ’1
DKL Y Ï€ Y IID Gt =
i=1
m/n

â‰¤

X

DKL YiÏ€ YiIID Gt+iâˆ’1



(39)

i=1
m/n

=

X

DKL Ber dt+iâˆ’1 (v)/m




Ber (d(v)/m) .

i=1
t

Recall that d (v) is the residual degree of v at time t. By Lemma 26 one has
 t+iâˆ’1
2
d
(v)
d(v)
16
âˆ’


m
m


DKL Ber dt+iâˆ’1 (v)/m Ber (d(v)/m) Gt+iâˆ’1 â‰¤
,
d(v)
d(v)
1âˆ’ m
m

(40)

and hence it suffices to upper bound the squared deviation of the residual degree dt+iâˆ’1 (v) from d(v).
Note that we have E[dt (v)] = d(v) for every v and t, and by Chernoff bounds the residual degree
concentrates around its expectation. More specifically:


p
P |dt (v) âˆ’ d(v)| â‰¥ 100d(v) log(n) â‰¤ nâˆ’10 .
(41)
Let us constrain ourselves to the event of probability
at least 1 âˆ’ nâˆ’5 where this bound is satisfied
p
t
for every t and v. That is one has |d (v) âˆ’ d(v)| â‰¤ 100d(v) log(n).
We thus have, using Eqs. (40) and (41) together with the fact that 1 â‰¤ d(v) â‰¤ m/3
p
2
16
100d(v) log n




DKL Ber dt+iâˆ’1 (v)/m Ber (d(v)/m) Gt+iâˆ’1 â‰¤
m Â· d(v) 1 âˆ’ d(v)
m
(42)
1600 Â· d(v) log n
â‰¤
m Â· d(v) Â· 2/3
2400 log n
=
.
m
53

(Note that we assumed d(v) â‰¤ n â‰¤ m/3.)
Therefore,
 2400 log n m
2400 log n
DKL Y Ï€ Y IID Gt â‰¤
Â·
=
,
m
n
n
which is stronger than the desired bound of Eq. (37).
We are finally ready to prove Lemma 30.
Proof of Lemma 30. First, let us constrain ourselves to the high probability event where Eqs. (34)
and (37) hold from Lemmas 31 and 32. As mentioned before, we will proceed by induction on j. Our
base case is simply Lemma 32 with a slight modification. Indeed, Lemma 32 bounds the divergence of
T1Ï€,t (v) and T1IID (v), whereas we need to bound the divergence of T1Ï€,t (v) and Te1IID (v). Note however,
IID
that Te1IID (v) is simply the padding of T 1 (v) by 200 log2 (n)/n, which is itself identical to T1IID (v) by
definition.
IID
If T 1 (v) is not close to deterministic, the padding does nothing and the base case holds. If T1IID (v)
is close enough to deterministic that it gets padded, the divergence from T1Ï€,t (v) either decreases or
increases to at most DKL (Ber (0)kBer ()), where  = 200 log2 (n)/n is the padding parameter. In this
case





200 log2 n
Ï€,t
t
IID
e
DKL T1 (v) T1 (v) G â‰¤ DKL Ber (0) Ber
n

2 
200 log n
= âˆ’ log 1 âˆ’
n
â‰¤

400 log2 n
,
n

which suffices. (See Fact 3 from the proof of Lemma 27 in Appendix E.)
Ï‡,t
For, the inductive step, we will use the triangle inequality of Lemma 27, pivoting on Tj+1
(v).
Specifically, we invoke Lemma 27 with

Ï€,t
p = P Tj+1
(v) succeeds

Ï‡,t
q = P Tj+1
(v) succeeds

 IID
r = P T j+1 (v) succeeds .
2
j
and
  = 200c log n/n. Since the -padding of r, denoted by re = Padding(r, ) is exactly
Ï€,t
P Tej+1 (v) succeeds , by Lemma 27 it suffices to prove

 200cj log2 n
Ï€,t
Ï‡,t
DKL Tj+1
(v) Tj+1
(v) Gt â‰¤
n
and

(43)


 200cj log2 n
IID
Ï‡,t
DKL Tj+1
(v) T (v) Gt â‰¤
.
(44)
n
Eq. (44) holds by Lemma 31.
Ï€,t
Ï‡,t
Comparing Tj+1
(v) and Tj+1
(v) (establishing Eq. (43)): We will deconstruct Tj+1 identically
to what was done in the proof of Lemma 28, and we will use techniques for bounding the divergence
similar to the ones used in the proof of Lemma 28 for bounding the total variation distance. Recall the
0, 1 vector Wj+1 (v) describing the process of a Tj+1 (v) test: The first cj m/n coordinates denote the
search phase; specifically we write a 1 if a neighbor was found and a 0 if not. The following coordinates
denote the outcomes of the recursive tests, 1 for pass 0 for fail, in the order they were performed. There
could be at most cj recursive tests performed (if they were all T1 â€™s) so Wj+1 (v) has length cj m/n + cj in
total. However, often much fewer recursive tests are performed due to the early stopping rule, or simply
because too few neighbors of v were found. In this case, tests not performed are represented by a 0 in
Wj+1 (v).

Ï€,t
Ï‡,t
Since Wj+1 (v) determines Tj+1 (v) we can simply bound DKL Wj+1
(v) Wj+1
(v) Gt by the data
processing inequality of Lemma 29.
54

The first cj m/n coordinates of W Ï€ and W Ï‡ are distributed identically and contribute nothing to
the divergence. Consider the test corresponding to the ith coordinate of the recursive phase of Wj+1 (v).
Let the level of the test be `i âˆˆ [0, j] (with 0 representing no test) and let the vertex of the test be ui .
These are random variable determined by Î . Furthermore, let ti be the position in the stream where
the recursive T`i test is called on ui . Then we have


Ï€,t
Ï‡,t
Ï€,t
Ï‡,t
DKL Tj+1
(v) Tj+1
(v) Gt â‰¤ DKL Wj+1
(v) Wj+1
(v) Gt
X

Ï€,t
Ï‡,t
Ï€,t
Ï€,t
=
DKL Wj+1,i
(v) Wj+1,i
(v) Gt , Wj+1,1
(v), . . . , Wj+1,iâˆ’1
(v)
i

â‰¤

X

=

X

â‰¤

X

Ï‡,t
Ï€,t
E`i ,ui DKL Wj+1,i
(v) Gti , `i , ui
(v) Wj+1,i



i



i
(ui ) Gti
E`i ,ui DKL T`Ï€,t
(ui ) Te`IID
i
i

i

i


E`i

200C Â· c`i âˆ’1 log2 n
n


,

by the inductive hypothesis. Here in the third line we condition on `i and ui , thereby only increasing
the divergence.
Note that the term in the sum is proportional to the number of edges used by the corresponding
recursive test. Indeed, recall by Lemma 2 that a T`i test runs for at most c`i âˆ’1 Â· m
n Â· (1 + 2Î´) samples
with probability one. It is crucial that this result holds with probability one, and therefore extends to
not just the iid stream it was originally proven on, but any arbitrary stream of edges.
Therefore, the term in the sum above is at most 200C log2 (n)/m times the number of edges used by
the corresponding recursive test. So the entire sum is at most 200C log2 (n)/m times the length of the
recursive phase of the original Tj+1 test. It was also derived in Lemma 2 that the recursive phase itself
takes at most cj Â· m
n Â· 2Î´ edges. (Again, the result holds with probability one.) Therefore,
 200C log2 (n) Î´cj m
200 Â· cj log2 n
Ï‡,t
Ï€,t
Â·
â‰¤
,
DKL Tj+1
(v) Gt â‰¤
(v) Tj+1
m
n
n
for small enough absolute constant Î´. This shows Eq. (43) and concludes the proof.

9.5

The full algorithm

We proceed to derive an algorithm for approximating the matching size of a graph in a random
permutation stream. The following well-known theorem will be useful for proving correctness:
Theorem 12 (Pinskerâ€™s Inequality). For two distributions P and Q,
p
kP âˆ’ QkTV â‰¤ 2 log2 e Â· DKL (P kQ).
Recall the Edge-Level-Test from Algorithm 3 in Section 3. We will run a similar edge test on
the permutation stream, with one crucial difference: Our original algorithm from Section 3 uses Î˜(m)
samples from the stream. Since our bound on the divergence between the iid and permutation variants
of Level-j-Test scales with log2 n times the sample complexity, it would grow past constant if we were
to adapt our iid algorithm as is. Therefore, we will constrain the algorithm to only use a log2 n fraction
of the available stream. Specifically Edge-Level-Test will only calculate the level of an edge up to
J âˆ’ 2 logc (log n), as opposed to J. We will call this edge test E IID (e).
Algorithm 16 Given an edge e, this algorithm returns a fractional matching-weight of e.
1:
2:
3:
4:
5:
6:
7:
8:

procedure E IID (e = (u, v))
w â† 1/n
for i = 1 to J âˆ’ 2 logc (log n) do
if Level-i-Test(u) and Level-i-Test(v) then
w â† w + ci /n
else
return w
return w

55

. Recall that J = blogc nc âˆ’ 1

As in the previous section we define the edge level test E(e) for the permutation stream as well,
e IID using the padded TeIID tests in place of Level-i-Test.
namely E Ï€,t (e). We also define E
i
Although the truncated edge test E IID is not as powerful as the untruncated version, it is a Î˜(log2 n)factor estimator for the matching number of the input graph, MM (G).
Corollary 10. For all c large enough, there exists Î´ > 0 such that the following holds. For all G = (V, E)
and an edge e âˆˆ E, let E IID (e) denote the value returned by Algorithm 16. Then,
X
E IID (e) = O(MM (G))
eâˆˆE

X

E IID (e) = â„¦

eâˆˆE



MM (G)
log2 n


.

Proof. Recall that Theorem 5 guarantees that
X
Me = Î˜(MM (G)),
eâˆˆE

where Me is the is the output of the untruncated edge level test Edge-Level-Test from Algorithm 3.
Note that, by stopping 2 logc (log n) levels early, E IID may misclassify edges by assigning them to levels
up to 2 logc (log n) levels lower, but never misclassifies them by putting them higher. Therefore, E IID (e)
is no greater than Me and at most Î˜(log2 n) factor lower.
Finally, recall Algorithm 2, our main algorithm for estimating the matching size.
Algorithm 17 Algorithm 2 estimating the value of MM (G).
1:
2:
3:
4:
5:
6:
7:
8:

procedure IID-Peeling(G = (V, E))
M0 â† 0
sâ†1
while samples lasts do
M 0 â† Sample(G, s)
s â† 2s
return M 0

procedure Sample(G = (V, E), s)
M0 â† 0
for k = 1 to s do
e â† iid edge from the stream
M 0 â† M 0 + Edge-Level-Test(e)
0
14:
return m Â· Ms

9:
10:
11:
12:
13:

Note that the variable denoting the number of edges sampled in Sample, originally t, has been
changed to s here so as to not be confused with the variable used to denote our position in the stream.
We will now describe the permutation variant, which can be used to approximate the maximum
matching size to an O(log2 n) factor, in a random permutation stream with O(log2 n) bits of space.
Definition 17. Let Permutation-Peeling(G = (V, E)) be a variant of IID-Peeling that uses the
permutation stream in Line 12 and the subroutine E Ï€,t (e) in Line 13. Furthermore, it should only
continue until a Î˜(1/ log2 n) fraction of the stream is exhausted, as opposed to Algorithm 2 which exhausts
the entire Î˜(m) sized stream in Line 4.
We define further variants of Algorithm 17 that will be useful in the proof of correctness of
Permutation-Peeling.
Definition 18. We define the following three variants of Algorithm 17:
â€¢ Truncated-IID-Peeling(G = (V, E)) uses the iid stream in Line 12 and E IID in Line 13.
â€¢ Hybrid-Peeling(G = (V, E)) uses the permutation stream in Line 12 and E IID in Line 13.

56

e IID in Line 13.
â€¢ Padded-Peeling(G = (V, E)) uses the permutation stream in Line 12 and E
All three algorithms terminate after exhausting a Î˜(1/ log2 n) fraction of the stream, like PermutationPeeling.
Theorem 13. For sufficiently small Î´ > 0 and large enough c the following holds. For any graph
G = (V, E), Permutation-Peeling (Algorithm 17) with 3/4 probability outputs a O(log2 n) factor
approximation of MM (G) by using a single pass over a randomly permuted stream of edges.
Proof. Correctness of Truncated-IID-Peeling. We will first prove the same claim for TruncatedIID-Peeling. Recall first the proof of Theorem 6 from Section 4. Let Me denote the outcome of
Edge-Level-Test(e) and Âµ = Eeâˆ¼U (E) [Me ]. Recall that the proof hinges on showing that Âµ is within
a constant factor of MM (G). Also Me : e âˆ¼ U (E) has variance at most 2Âµ/c, so Î˜(1/Âµc) independent
edge tests suffice to be able to bound the deviation from the mean powerfully enough using Chebyshevâ€™s
inequality. Furthermore, we know that in expectation, Edge-Level-Test(e) : e âˆ¼ U (E) takes samples
proportional to the number Âµ Â· m (see Lemma 3). Putting all this together we get that with constant
probability the output of Sample(G, s) is within a constant factor of MM (G) for large enough s, and
such a call of Sample fits into O(m) edges from the stream.
We will have a very similar proof for the correctness of Truncated-IID-Peeling. Indeed, consider
the random variable S = E IID (e) : e âˆ¼ U (E). Let ES = Âµ. In Corollary 10 we have shown that Âµ is
a Î˜(log2 n)-factor approximation of MM (G). Furthermore, S is bounded by c log2 2 n , so its variance is
2Âµ
.
c log2 n

1
By Chebyshevâ€™s inequality s = Î˜( cÂµ log
2 n ) edge tests suffice to get an accurate enough


m
samples, as desired. If the algorithm
empirical mean. This many edge tests take s Â· Âµ Â· m = Î˜ c log
2n

at most

mR
doesnâ€™t terminate within c log
2 n we consider it to have failed. This happens with probability less than
1/10 for some large absolute constant R.
Correctness of Hybrid-Peeling. The only difference between Truncated-IID-Peeling and
Hybrid-Peeling is that the latter we use the permutation stream for sampling edges to run E IID on.
This guarantees no repetitions which only improves our variance bound and does not hurt the previous
proof.
Comparing Hybrid-Peeling and Padded-Peeling. Note that these two algorithms differ only in
the type of vertex level tests they use: Specifically, Hybrid-Peeling uses T IID while Padded-Peeling
uses TeIID . By Lemma 28 for all v âˆˆ V and j âˆˆ [0, J]
IID
IID
Tj+1
(v) âˆ’ Tej+1
(v)

â‰¤
TV

400 Â· cj log2 n
.
n
2

n
Note that this is proportional (with multiplicative factor 200 log
) to the sample complexity of the
m
corresponding test. The output of the main peeling algorithm depends only on the outputs of vertex
level tests called directly from edge level tests, which are disjoint (in the samples they use). Furthermore,
mR
the entire algorithm takes at most c log
2 n samples, so the total variation distance between the outputs
of Hybrid-Peeling and Padded-Peeling is at most

200 log2 n
mR
200R
Â·
=
.
2
m
c
c log n
For more details on this proof technique see the proofs of Lemmas 28 and 30.
Comparing Padded-Peeling and Permutation-Peeling. Note that these two algorithms again
differ only in the type of vertex level test they use: Specifically, Padded-Peeling uses TeIID while
Permutation-Peeling uses T Ï€,t . By Lemma 30, with high probability, for all v âˆˆ V , j â‰¤ J, t â‰¤
m/2 âˆ’ 2cj Â· m
n,

 200C Â· cj log2 n
Ï€,t
IID
DKL Tj+1
(v) Tej+1
,
(45)
(v) Gt â‰¤
n
where C is the constant from Lemma 27. Note that this is proportional (with multiplicative factor
100C log2 n
) to the sample complexity of the corresponding test. Again we note that the output of the
m
main peeling algorithm is a function of the outputs of the vertex level tests directly called from edge level
tests, which are disjoint (in the samples they use). Furthermore, since the entire algorithm takes at most
mR
samples, the divergence between the outputs of Padded-Peeling and Permutation-Peeling
c log2 n
is at most
100 log2 n
mR
100R
Â·
=
.
2
m
c
c log n
57

q
This translates to a total variation distance of at most 200Rclog2 e by Pinskerâ€™s inequality. Again, for
more details on this proof technique see the proofs of Lemmas 28 and 30.
In conclusion, Permutation-Peeling
returns a log2 n-factor approximation of MM (G) with
q
probability at least 4/5 âˆ’

200R
c

âˆ’

200R log2 e
c

â‰¥ 3/4 for large enough c.

From here the proof of the main theorem follows.
Proof of Theorem 4. The proof follows from Theorem 13 and the fact that Algorithm 17 has recursion
depth of O(log n), where each procedure in the recursion maintains O(1) variables, hence requiring
O(log n) bits of space. Therefore, the total memory is O(log2 n).

Acknowledgments
Michael Kapralov is supported in part by ERC Starting Grant 759471. Slobodan MitrovicÌ was supported
by the Swiss NSF grant P2ELP2 181772 and MIT-IBM Watson AI Lab. Jakab Tardos is supported by
ERC Starting Grant 759471.

References
[AB19]

Sepehr Assadi and Aaron Bernstein. Towards a unified theory of sparsification for matching
problems. In Fineman and Mitzenmacher [FM18], pages 11:1â€“11:20.

[ABB+ 19] Sepehr Assadi, MohammadHossein Bateni, Aaron Bernstein, Vahab S. Mirrokni, and Cliff
Stein. Coresets meet EDCS: algorithms for matching and vertex cover on massive graphs.
In Timothy M. Chan, editor, Proceedings of the Thirtieth Annual ACM-SIAM Symposium
on Discrete Algorithms, SODA 2019, San Diego, California, USA, January 6-9, 2019, pages
1616â€“1635. SIAM, 2019.
[AG11a]

Kook Jin Ahn and Sudipto Guha. Laminar families and metric embeddings: Non-bipartite
maximum matching problem in the semi-streaming model. arXiv preprint arXiv:1104.4058,
2011.

[AG11b]

Kook Jin Ahn and Sudipto Guha. Linear programming in the semi-streaming model with
application to the maximum matching problem. In International Colloquium on Automata,
Languages, and Programming, pages 526â€“538. Springer, 2011.

[AK17]

Sepehr Assadi and Sanjeev Khanna. Randomized composable coresets for matching and
vertex cover. In Christian Scheideler and Mohammad Taghi Hajiaghayi, editors, Proceedings
of the 29th ACM Symposium on Parallelism in Algorithms and Architectures, SPAA 2017,
Washington DC, USA, July 24-26, 2017, pages 3â€“12. ACM, 2017.

[AKL17]

Sepehr Assadi, Sanjeev Khanna, and Yang Li. On estimating maximum matching size in
graph streams. In Klein [Kle17], pages 1723â€“1742.

[AKLY16] Sepehr Assadi, Sanjeev Khanna, Yang Li, and Grigory Yaroslavtsev. Maximum matchings
in dynamic graph streams and the simultaneous communication model. In Krauthgamer
[Kra16], pages 1345â€“1364.
[ARVX12] Noga Alon, Ronitt Rubinfeld, Shai Vardi, and Ning Xie. Space-efficient local computation
algorithms. In Proceedings of the twenty-third annual ACM-SIAM symposium on Discrete
Algorithms, pages 1132â€“1139. Society for Industrial and Applied Mathematics, 2012.
[BGM+ 19] Marc Bury, Elena Grigorescu, Andrew McGregor, Morteza Monemizadeh, Chris
Schwiegelshohn, Sofya Vorotnikova, and Samson Zhou. Structural results on matching
estimation with applications to streaming. Algorithmica, 81(1):367â€“392, 2019.
[BGY18]

Paul Beame, Shayan Oveis Gharan, and Xin Yang. Time-space tradeoffs for learning finite
functions from random evaluations, with applications to polynomials. In SeÌbastien Bubeck,
Vianney Perchet, and Philippe Rigollet, editors, Conference On Learning Theory, COLT
2018, Stockholm, Sweden, 6-9 July 2018., volume 75 of Proceedings of Machine Learning
Research, pages 843â€“856. PMLR, 2018.
58

[BS15]

Marc Bury and Chris Schwiegelshohn. Sublinear estimation of weighted matchings in dynamic
data streams. In Nikhil Bansal and Irene Finocchi, editors, Algorithms - ESA 2015 - 23rd
Annual European Symposium, Patras, Greece, September 14-16, 2015, Proceedings, volume
9294 of Lecture Notes in Computer Science, pages 263â€“274. Springer, 2015.

[CCE+ 16] Rajesh Chitnis, Graham Cormode, Hossein Esfandiari, MohammadTaghi Hajiaghayi,
Andrew McGregor, Morteza Monemizadeh, and Sofya Vorotnikova. Kernelization via
sampling with applications to finding matchings and related problems in dynamic graph
streams. In Krauthgamer [Kra16], pages 1326â€“1344.
[CH12]

John Cullinan and Farshid Hajir. Primes of prescribed congruence class in short intervals.
Integers, 12:A56, 2012.

[CJMM17] Graham Cormode, Hossein Jowhari, Morteza Monemizadeh, and S. Muthukrishnan. The
sparse awakens: Streaming algorithms for matching size estimation in sparse graphs. In Kirk
Pruhs and Christian Sohler, editors, 25th Annual European Symposium on Algorithms, ESA
2017, September 4-6, 2017, Vienna, Austria, volume 87 of LIPIcs, pages 29:1â€“29:15. Schloss
Dagstuhl - Leibniz-Zentrum fuer Informatik, 2017.
[EHL+ 15] Hossein Esfandiari, Mohammad Taghi Hajiaghayi, Vahid Liaghat, Morteza Monemizadeh,
and Krzysztof Onak. Streaming algorithms for estimating the matching size in planar graphs
and beyond. In Piotr Indyk, editor, Proceedings of the Twenty-Sixth Annual ACM-SIAM
Symposium on Discrete Algorithms, SODA 2015, San Diego, CA, USA, January 4-6, 2015,
pages 1217â€“1233. SIAM, 2015.
[EHM16]

Hossein Esfandiari, MohammadTaghi Hajiaghayi, and Morteza Monemizadeh. Finding
large matchings in semi-streaming. In Carlotta Domeniconi, Francesco Gullo, Francesco
Bonchi, Josep Domingo-Ferrer, Ricardo A. Baeza-Yates, Zhi-Hua Zhou, and Xindong Wu,
editors, IEEE International Conference on Data Mining Workshops, ICDM Workshops 2016,
December 12-15, 2016, Barcelona, Spain., pages 608â€“614. IEEE Computer Society, 2016.

[EKS09]

Sebastian Eggert, Lasse Kliemann, and Anand Srivastav. Bipartite graph matchings in the
semi-streaming model. In European Symposium on Algorithms, pages 492â€“503. Springer,
2009.

[ELMS11] Leah Epstein, Asaf Levin, JuliaÌn Mestre, and Danny Segev. Improved approximation
guarantees for weighted matching in the semi-streaming model. SIAM Journal on Discrete
Mathematics, 25(3):1251â€“1265, 2011.
[EMR14]

Guy Even, Moti Medina, and Dana Ron. Deterministic stateless centralized local algorithms
for bounded degree graphs. In European Symposium on Algorithms, pages 394â€“405. Springer,
2014.

[FKM+ 05] Joan Feigenbaum, Sampath Kannan, Andrew McGregor, Siddharth Suri, and Jian Zhang. On
graph problems in a semi-streaming model. Theoretical Computer Science, 348(2-3):207â€“216,
2005.
[FM18]

Jeremy T. Fineman and Michael Mitzenmacher, editors. 2nd Symposium on Simplicity in
Algorithms, SOSA@SODA 2019, January 8-9, 2019 - San Diego, CA, USA, volume 69 of
OASICS. Schloss Dagstuhl - Leibniz-Zentrum fuer Informatik, 2018.

[Gha16]

Mohsen Ghaffari. An improved distributed algorithm for maximal independent set. In
Proceedings of the twenty-seventh annual ACM-SIAM symposium on Discrete algorithms,
pages 270â€“277. Society for Industrial and Applied Mathematics, 2016.

[GKK12]

Ashish Goel, Michael Kapralov, and Sanjeev Khanna. On the communication and streaming
complexity of maximum bipartite matching. In Yuval Rabani, editor, Proceedings of the
Twenty-Third Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2012, Kyoto,
Japan, January 17-19, 2012, pages 468â€“485. SIAM, 2012.

[GKMS18] Buddhima Gamlath, Sagar Kale, Slobodan MitrovicÌ, and Ola Svensson. Weighted matchings
via unweighted augmentations. arXiv preprint arXiv:1811.02760, 2018.

59

[GRT18]

Sumegha Garg, Ran Raz, and Avishay Tal. Extractor-based time-space lower bounds for
learning. In Ilias Diakonikolas, David Kempe, and Monika Henzinger, editors, Proceedings
of the 50th Annual ACM SIGACT Symposium on Theory of Computing, STOC 2018, Los
Angeles, CA, USA, June 25-29, 2018, pages 990â€“1002. ACM, 2018.

[GU19]

Mohsen Ghaffari and Jara Uitto. Sparsifying distributed algorithms with ramifications in
massively parallel computation and centralized local computation. In Proceedings of the
Thirtieth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 1636â€“1653. SIAM,
2019.

[GW19]

Mohsen Ghaffari and David Wajc. Simplified and space-optimal semi-streaming (2+epsilon)approximate matching. In Fineman and Mitzenmacher [FM18], pages 13:1â€“13:8.

[Kap13]

Michael Kapralov. Better bounds for matchings in the streaming model. In Proceedings of
the twenty-fourth annual ACM-SIAM symposium on Discrete algorithms, pages 1679â€“1697.
SIAM, 2013.

[KKS14]

Michael Kapralov, Sanjeev Khanna, and Madhu Sudan. Approximating matching size
from random streams. In Proceedings of the twenty-fifth annual ACM-SIAM symposium
on Discrete algorithms, pages 734â€“751. SIAM, 2014.

[Kle17]

Philip N. Klein, editor. Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium
on Discrete Algorithms, SODA 2017, Barcelona, Spain, Hotel Porta Fira, January 16-19.
SIAM, 2017.

[KMM12] Christian Konrad, FreÌdeÌric Magniez, and Claire Mathieu.
Maximum matching in
semi-streaming with few passes. In Approximation, Randomization, and Combinatorial
Optimization. Algorithms and Techniques, pages 231â€“242. Springer, 2012.
[KMW16] Fabian Kuhn, Thomas Moscibroda, and Roger Wattenhofer. Local computation: Lower and
upper bounds. Journal of the ACM (JACM), 63(2):17, 2016.
[Kon18]

Christian Konrad. A simple augmentation method for matchings with applications to
streaming algorithms. In 43rd International Symposium on Mathematical Foundations of
Computer Science (MFCS 2018). Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik, 2018.

[Kra16]

Robert Krauthgamer, editor. Proceedings of the Twenty-Seventh Annual ACM-SIAM
Symposium on Discrete Algorithms, SODA 2016, Arlington, VA, USA, January 10-12, 2016.
SIAM, 2016.

[KRT17]

Gillat Kol, Ran Raz, and Avishay Tal. Time-space hardness of learning sparse parities. In
Hamed Hatami, Pierre McKenzie, and Valerie King, editors, Proceedings of the 49th Annual
ACM SIGACT Symposium on Theory of Computing, STOC 2017, Montreal, QC, Canada,
June 19-23, 2017, pages 1067â€“1080. ACM, 2017.

[LPS88]

Alexander Lubotzky, Ralph Phillips, and Peter Sarnak. Ramanujan graphs. Combinatorica,
8(3):261â€“277, 1988.

[LRY17]

Reut Levi, Ronitt Rubinfeld, and Anak Yodpinyanee. Local computation algorithms for
graphs of non-constant degrees. Algorithmica, 77(4):971â€“994, 2017.

[McG05]

Andrew McGregor. Finding graph matchings in data streams. In Approximation,
Randomization and Combinatorial Optimization. Algorithms and Techniques, pages 170â€“181.
Springer, 2005.

[MMPS17] Morteza Monemizadeh, S. Muthukrishnan, Pan Peng, and Christian Sohler. Testable
bounded degree graph properties are random order streamable. In Ioannis Chatzigiannakis,
Piotr Indyk, Fabian Kuhn, and Anca Muscholl, editors, 44th International Colloquium on
Automata, Languages, and Programming, ICALP 2017, July 10-14, 2017, Warsaw, Poland,
volume 80 of LIPIcs, pages 131:1â€“131:14. Schloss Dagstuhl - Leibniz-Zentrum fuer Informatik,
2017.

60

[MRVX12] Yishay Mansour, Aviad Rubinstein, Shai Vardi, and Ning Xie. Converting online algorithms
to local computation algorithms. In International Colloquium on Automata, Languages, and
Programming, pages 653â€“664. Springer, 2012.
[MV13]

Yishay Mansour and Shai Vardi. A local computation approximation scheme to maximum
matching. In Approximation, Randomization, and Combinatorial Optimization. Algorithms
and Techniques, pages 260â€“273. Springer, 2013.

[MV16]

Andrew McGregor and Sofya Vorotnikova.
Planar matching in streams revisited.
In Klaus Jansen, Claire Mathieu, JoseÌ D. P. Rolim, and Chris Umans, editors,
Approximation, Randomization, and Combinatorial Optimization. Algorithms and
Techniques, APPROX/RANDOM 2016, September 7-9, 2016, Paris, France, volume 60 of
LIPIcs, pages 17:1â€“17:12. Schloss Dagstuhl - Leibniz-Zentrum fuer Informatik, 2016.

[MV18]

Andrew McGregor and Sofya Vorotnikova. A simple, space-efficient, streaming algorithm for
matchings in low arboricity graphs. In Raimund Seidel, editor, 1st Symposium on Simplicity
in Algorithms, SOSA 2018, January 7-10, 2018, New Orleans, LA, USA, volume 61 of
OASICS, pages 14:1â€“14:4. Schloss Dagstuhl - Leibniz-Zentrum fuer Informatik, 2018.

[Nis90]

Noam Nisan. Pseudorandom generators for space-bounded computation. In Proceedings of
the 22nd Annual ACM Symposium on Theory of Computing, May 13-17, 1990, Baltimore,
Maryland, USA, pages 204â€“212, 1990.

[NO08]

Huy N Nguyen and Krzysztof Onak. Constant-time approximation algorithms via local
improvements. In 2008 49th Annual IEEE Symposium on Foundations of Computer Science,
pages 327â€“336. IEEE, 2008.

[ORRR12] Krzysztof Onak, Dana Ron, Michal Rosen, and Ronitt Rubinfeld. A near-optimal sublineartime algorithm for approximating the minimum vertex cover size. In Proceedings of
the twenty-third annual ACM-SIAM symposium on Discrete Algorithms, pages 1123â€“1131.
Society for Industrial and Applied Mathematics, 2012.
[Pem01]

Sriram V Pemmaraju.
Equitable coloring extends chernoff-hoeffding bounds.
In
Approximation, Randomization, and Combinatorial Optimization:
Algorithms and
Techniques, pages 285â€“296. Springer, 2001.

[PR07]

Michal Parnas and Dana Ron. Approximating the minimum vertex cover in sublinear time
and a connection to distributed algorithms. Theoretical Computer Science, 381(1-3):183â€“196,
2007.

[PS17]

Ami Paz and Gregory Schwartzman. A (2 + )-approximation for maximum weight matching
in the semi-streaming model. In Klein [Kle17], pages 2153â€“2161.

[PS18]

Pan Peng and Christian Sohler. Estimating graph parameters from random order streams.
In Artur Czumaj, editor, Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium
on Discrete Algorithms, SODA 2018, New Orleans, LA, USA, January 7-10, 2018, pages
2449â€“2466. SIAM, 2018.

[Raz16]

Ran Raz. Fast learning requires good memory: A time-space lower bound for parity learning.
In Irit Dinur, editor, IEEE 57th Annual Symposium on Foundations of Computer Science,
FOCS 2016, 9-11 October 2016, Hyatt Regency, New Brunswick, New Jersey, USA, pages
266â€“275. IEEE Computer Society, 2016.

[Raz17]

Ran Raz. A time-space lower bound for a large class of learning problems. In Chris Umans,
editor, 58th IEEE Annual Symposium on Foundations of Computer Science, FOCS 2017,
Berkeley, CA, USA, October 15-17, 2017, pages 732â€“742. IEEE Computer Society, 2017.

[RTVX11] Ronitt Rubinfeld, Gil Tamir, Shai Vardi, and Ning Xie. Fast local computation algorithms.
arXiv preprint arXiv:1104.1377, 2011.
[YYI09]

Yuichi Yoshida, Masaki Yamamoto, and Hiro Ito. An improved constant-time approximation
algorithm for maximum. In Proceedings of the forty-first annual ACM symposium on Theory
of computing, pages 225â€“234. ACM, 2009.

[Zel12]

Mariano Zelke. Weighted matching in the semi-streaming model. Algorithmica, 62(1-2):1â€“20,
2012.
61

A

Proof of Lemma 5

In this section we prove the following result, stated in Section 5.2.
c(v)). For any vertex v, any j > 0, and constants c and x such that
Lemma 5 (Concentration on M
c â‰¥ 20 and x â‰¥ 100c log c, we have:
h
h
ii
h
i 10c2
c(v) Â· 1 L(v)
b
c(v) â‰¥ x âˆ§ L(v)
b
=j .
P M
=j+1 â‰¤ 2 Â·E M
x

(18)

b
c(v) are defined in Eq. (10) and Eq. (16), respectively.
Where L(v)
and M
Proof. We begin by rewriting the LHS and the RHS of Eq. (18).
Rewriting the LHS of Eq. (18) Observe that
h
i Observation 1 (b) h
i
c(v) â‰¥ x |L(v)
b
cj+1 (v) â‰¥ x |L(v)
b
P M
=j+1
=
P M
=j+1 .
h
i
h
i
c(v) â‰¥ x |L(v)
b
cj+1 (v) â‰¥ x |L(v)
b
cj+1 (v) = M
c(v)
First observe that P M
=j+1 =P M
= j + 1 , since M
b
conditioned on L(v)
= j + 1 by definition. We thus have
h
i
h
i
c(v) â‰¥ x âˆ§ L(v)
b
cj+1 (v) â‰¥ x âˆ§ L(v)
b
P M
= j + 1 =P M
=j+1
h
i h
i
Observation 1 (a)
cj+1 (v) â‰¥ x P L(v)
b
=
P M
=j+1
h
i h
i h
i h
i
cj+1 (v) â‰¥ x P v âˆˆ Vbj P v âˆˆ Vbj+1 |v âˆˆ Vbj P v 6âˆˆ Vbj+2 |v âˆˆ Vbj+1
=P M
h
i h
i
cj+1 (v) â‰¥ x P v âˆˆ Vbj P [Sj (v) < Î´] P [Sj+1 (v) â‰¥ Î´] ,
=P M
(46)
h

i

where Sj (v) is as defined in Eq. (13). (When j = J, P v 6âˆˆ Vbj+2 is simply 1.)
Rewriting the RHS of Eq. (18) We have
h
h
ii
h
i h
i h
i
c(v) Â· 1 L(v)
b
b
cj (v) .
E M
= j = P v âˆˆ Vbj P L(v)
= j|v âˆˆ Vbj E M
h
i
b
By definition P L(v)
= j|v âˆˆ Vbj = P [Sj â‰¥ Î´], and hence
ii
h
i
h
i
h
h
c(v) Â· 1 L(v)
b
cj (v) .
= j = P v âˆˆ Vbj P [Sj (v) â‰¥ Î´] E M
E M

(47)

The proof strategy In the rest of the proof, to establish Eq. (18) we upper-bound the ratio of the
LHS of Eq. (46) and the RHS of Eq. (47) by 10c2 /x2 . At a high level, the RHS of Eq. (46) is small
cj+1 (v) and Sj (v) (see Eq. (17) and Eq. (13)
when x  Î´. This is the case since the random variables M
respectively) have similar expectations and they both concentrate well around their expectations. Hence,
cj+1 (v) is large and Sj (v) is small.
it is unlikely that at the same time M
h
i
cj+1 (v) . First, when
To implement this intuition, we consider two cases with respect to E M
h
i
cj+1 (v) is relatively large, i.e., at least x/2, we show that P [Sj (v) < Î´] is small. On the other
E M
h
i
cj+1 (v) implies large
hand, for the terms appearing on the RHS of Eq. (47) we have: large E M
h
i
cj (v) , and small P [Sj (v) < Î´] implies that P [Sj (v) â‰¥ Î´] â‰¥ 1/2.
E M
h
i
h
i
cj+1 (v) < x/2, we show that P M
cj+1 (v) â‰¥ x is very small.
Second, when E M
We complete the proof by balancing the two cases.

62

h
i
cj+1 (v) â‰¥ x/2.
Case 1: E M

In this case we have

Observation 2

â‰¥

E [Sj (v)]

E [Sj+1 (v)]
c+1

Observation 1 (c)

=

h
i
cj+1 (v)
E M
c+1

â‰¥

x
.
2(c + 1)

(48)

This further implies
h
i
Eq. (48)
1 (c)
cj (v) Observation
E M
=
E [Sj (v)] â‰¥

x
.
2(c + 1)

(49)

Let us define a random Zk for iid edge ek as follows
min{L(w),j}

Zk = 1 [ek equals {w, v}]

X

ciâˆ’j .

i=0

Notice that Sj (v) =
Sj (v):

Pcj m/n
k=1

Zk and Zk âˆˆ [0, 2], therefore by applying Chernoff bound Theorem 7 (c) to
P [Sj (v) < Î´] â‰¤ P [Sj (v) â‰¤ (1 âˆ’ 1/2)E [Sj (v)]]


(1/2)2 E [Sj (v)]
â‰¤ exp âˆ’
2Â·2
!
x
2
Eq. (48)
(1/2) 2(c+1)
â‰¤ exp âˆ’
2Â·2


x
â‰¤ exp âˆ’
32(c + 1)
 x 
â‰¤ exp âˆ’
,
40c

(50)

where the first inequality follows as Î´ â‰¤ 1/2 and E [Sj (v)] â‰¥ 1 from Eq. (48) and the definition of x. The
last inequality of Eq. (50) follows since c â‰¥ 20 by the assumption of the lemma. Therefore, substituting
Eq. (50) into Eq. (46), we get
h
i
h
i
 x 
c(v) â‰¥ x âˆ§ L(v)
b
.
(51)
P M
= j + 1 â‰¤ P v âˆˆ Vbj exp âˆ’
40c
Furthermore, from Eq. (50) and for x â‰¥ 100c log c we have P [Sj â‰¥ Î´] â‰¥ 1/2. Substituting this bound
and Eq. (49) into Eq. (47) leads to
ii 1 h
h
h
i
x
c(v) Â· 1 L(v)
b
= j â‰¥ P v âˆˆ Vbj Â·
E M
.
2
2(c + 1)
Combining the last inequality with Eq. (51) leads to
h
i
c(v) â‰¥ x âˆ§ L(v)
b
x
P M
=j+1
)
4(c + 1) Â· exp(âˆ’ 40c
h
h
ii
.
â‰¤
x
c(v) Â· 1 L(v)
b
E M
=j
h
i
cj+1 (v) < x/2.
Case 2: E M

(52)

h
i
cj+1 (v) = tx for some t < 1/2. To apply Chernoff bound,
Let E M

similar to previous case we define Zw for any vertex w âˆˆ N (v) as follows
b
min{L(w),j}
def

Zw =

X

ci /n.

i=0

cj (v) =
Therefore we have M

P

wâˆˆN (v)

Zw . Observe that Zw âˆˆ [0, 2]. Since t < 1/2, by applying

63

Chernoff bound Theorem 7 (b) we get
h
i
h
h
ii
cj+1 (v) â‰¥ x = P M
cj+1 (v) â‰¥ (1 + (1/t âˆ’ 1)) Â· E M
cj+1 (v)
P M
h
iï£¶
ï£«
cj+1 (v)
1/t Â· log 1/t Â· E M
ï£¸
â‰¤ exp ï£­âˆ’
3Â·2


x log 1/t
â‰¤ exp âˆ’
.
6
Substituting this bound into Eq. (46) we obtain


h
i
h
i
x log 1/t
c(v) â‰¥ x âˆ§ L(v)
b
.
P M
= j + 1 â‰¤ P v âˆˆ Vbj P [Sj+1 â‰¥ Î´] exp âˆ’
6

(53)

Eq. (47) and Eq. (53) imply


h
i
c(v) â‰¥ x âˆ§ L(v)
b
2(c + 1) Â· exp âˆ’ x log6 1/t
P M
= j + 1)
P [Sj+1 (v) â‰¥ Î´]
h
h
ii
â‰¤
Â·
tx
P [Sj (v) â‰¥ Î´]
c(v) Â· 1 L(v)
b
E M
=j
2(c + 1) x/6âˆ’1 P [Sj+1 (v) â‰¥ Î´]
Â·t
Â·
x
P [Sj (v) â‰¥ Î´]
2(c + 1) âˆ’x/6+1 P [Sj+1 (v) â‰¥ Î´]
â‰¤
Â·2
Â·
x
P [Sj (v) â‰¥ Î´]
=

Now we upper bound

P[Sj+1 (v)â‰¥Î´]
P[Sj (v)â‰¥Î´] .

Consider the definition of Sj (v) from Eq. (13)

cj m/n
def

Sj (v) =

min(L(w),j)

1 [ek equals {w, v}]

X

X

ciâˆ’j

i=0

k=1
ek âˆ¼ UE

Let us split this definition into two parts: one corresponding to the last term of the second sum and one
corresponding to all other terms:
Sj (v) = Aj (v) + Bj (v)
cj m/n

Aj (v) =

X

min(L(w),jâˆ’1)

1 [ek equals {w, v}]

X

ciâˆ’j

i=0

k=1
ek âˆ¼ UE
cj m/n

Bj (v) =

X

h

1 [ek equals {w, v}] 1 w âˆˆ Vbj

i

k=1
ek âˆ¼ UE
Note that P [Sj (v) â‰¥ Î´] = P [Aj (v) â‰¥ Î´ âˆ¨ Bj â‰¥ 1] â‰¤ P [Aj (v) â‰¥ Î´] + P [Bj (v) â‰¥ 1]. We must bound
P [Aj+1 â‰¥ Î´] + P [Bj+1 (v) â‰¥ 1]
.
P [Sj (v) â‰¥ Î´]
(i)

Notice that Aj+1 (v) is the average of c independently sampled copies of Sj (v), say Sj (v). In order
(i)

for Aj+1 (v) to be greater than Î´ at least one of the Sj (v)â€™s must be greater than Î´, therefore by union
bound P [Aj+1 â‰¥ Î´] â‰¤ cP [Sj (v) â‰¥ Î´]. Notice now that Bj+1 (v) is at most the sum of c independent
(i)
copies of Bj (v), say Bj (v). Since Bj (v) is integral, in order for Bj+1 (v) to be greater than 1 at least
(i)

one of the Bj (v)â€™s need to be greater than 1, therefore by union bound P [Bj+1 (v)] â‰¤ cP [Bj (v) â‰¥ 1] â‰¤
cP [Sj (v) â‰¥ Î´]. So in conclusion
P [Sj+1 (v) â‰¥ Î´]
â‰¤ 2c.
P [Sj (v) â‰¥ Î´]
64

This finalizes the bound of this case as well
h
i
c(v) â‰¥ x âˆ§ L(v)
b
P M
= j + 1)
2(c + 1) âˆ’x/6+1
h
i
â‰¤
Â·2
Â· 2c.
x
c(v)1(L(v)
b
E M
= j)

(54)

Finalizing

Combining the two cases, from Eq. (52) and Eq. (54) we conclude
h
i

c(v) â‰¥ x âˆ§ L(v)
b
x 
P M
=j+1
)
4c(c + 1) âˆ’x/6+1 4(c + 1) Â· exp(âˆ’ 40c
h
i
â‰¤ max
Â·2
,
x
x
c(v)1L(v)
b
E M
=j

The RHS of the inequality above is upper-bounded by 10c2 /x2 for c â‰¥ 20 and x â‰¥ 100c log c.

B

Oversampling Lemma

In this section we formally proof the following lemma.
Lemma 1 (Oversampling lemma). For sufficiently small Î´ > 0 and large enough c the following holds.
PK
def
Let
X =
and X =
k=1 Yk be a sum of independent random variables Yk taking values in [0, 1],

P
c
1
i=1 Xi where Xi are iid copies of X. If E [X] â‰¤ Î´/3 and P [X â‰¥ Î´] = p, then P X â‰¥ Î´ â‰¤ p/2.
c
Pc
Proof. Let Z = i=1 Xi . Notice that Z is a sum of independent random variables each in the range
[0, 1]. Also, P X â‰¥ Î´ = P [Z â‰¥ cÎ´]. From the definition of Xi and the linearity of expectation, we have
E [Z] â‰¤ cÎ´/3. This, in compination with Chernoff bound (Theorem 7(b)), further implies






2 Â· cÎ´/3
cÎ´
P X â‰¥ Î´ = P [Z â‰¥ cÎ´] â‰¤ exp âˆ’
â‰¤ exp âˆ’
.
(55)
3
9
We now consider two cases depending on the value of p.

Case 1: p â‰¥ 2 exp âˆ’ cÎ´
9 .

The proof follows directly from Eq. (55).


Case 2: p < 2 exp âˆ’ cÎ´
9 .

In this case we consider the following three events which we call bad.

â€¢ Event E1 : At least two of Xi â€™s have value at least Î´.
â€¢ Event E2 : At least one Xi has value more than t, for a threshold t := Î´c/30  Î´.
â€¢ Event E3 : At least one Xi has value more than Î´ and less than 0.1 Â· c of the Xi â€™s have value below
2Î´/3.
If none of the bad events happen, then X â‰¤ Î´. To see that, observe that EÌ„1 and EÌ„2 imply that at
most one Xi has value more than Î´, and that the same Xi has value at most t. Note that EÌ„3 is the
event that either none of the Xi â€™s has value more than Î´ or more than 0.1c of the Xi â€™s have value below
2Î´/3. In the former case, X â‰¤ Î´ is clearly satisfied. Consider now the latter case intersected with EÌ„1
and EÌ„2 ; denote the Xi larger than Î´ by Xlarge . At least 0.1 Â· c values of Xi are less than 2Î´/3 and the
rest, excluding Xlarge , are less than Î´. Therefore, the average of Xlarge and the elements having value
less than 2Î´/3 is at most 0.1Â·cÂ·2Î´/3+t
= 2Î´/3 + 10t/c. This is less than Î´ as long as t â‰¤ Î´c/30. All other
0.1Â·c
elements are below Î´ as well.
In the rest of the proof we upper-bound the probability
that each of the bad events occurs. Then, by

taking union bound we will upper-bound P X â‰¥ Î´ .
Upper-bound on P [E1 ].

By union bound we have

P [E1 ] = P [âˆƒi1 6= i2 : Xi1 â‰¥ Î´ âˆ§ Xi2

65

 


c 2
cÎ´
2
â‰¥ Î´] â‰¤
p â‰¤ c p exp âˆ’
2
9

(56)

Upper-bound on P [E2 ].

Again by union bound we derive
P [E2 ] = P [âˆƒi : Xi â‰¥ t]
â‰¤ c Â· P [X â‰¥ Î´] Â· P [X â‰¥ t|X â‰¥ Î´]
= cp Â· P [X â‰¥ t|X â‰¥ Î´] .

(57)

To upper-bound P [X â‰¥ t|X â‰¥ Î´], consider the random variable L defined as the lowest integer such that
PL
the partial sum k=1 Yk is already at least Î´. Then
P [X â‰¥ t|X â‰¥ Î´] =

K
X

P [X â‰¥ t|L = l] P [L = l|X â‰¥ Î´]

l=1

â‰¤ max P [X â‰¥ t|L = l]
l
" lâˆ’1
#
K
X
X
= max P
Yk + Yl +
Yk â‰¥ t|L = l .
l

k=1

Recall that each Yk âˆˆ [0, 1]. Also, for L = l, by the definition we have
lâˆ’1
X

(58)

k=l+1

Plâˆ’1

k=1

Yk < Î´ < 1. Hence,

Yk + Yl â‰¤ 2.

(59)

k=1

This together with Eq. (58) implies
"

from Eq. (58)

P [X â‰¥ t|X â‰¥ Î´]

â‰¤

max P
l

â‰¤

Yk â‰¥ t âˆ’

k=l+1

"

from Eq. (59)

â‰¤

K
X

max P
l

K
X

lâˆ’1
X

#
Yk âˆ’ Yl |L = l

k=1

#
Yk â‰¥ t âˆ’ 2|L = l

k=l+1

P [X â‰¥ t âˆ’ 2] .

(60)

From the assumption given in the statement of the lemma, it holds that E [X] â‰¤ Î´/3 < 1 (we may
contrain Î´ to be less than 3). By Chernoff bound (Theorem 7(b)) and taking into account that X is a
sum of random variables in [0, 1], we obtain


from E [X] < 1
tâˆ’3
.
P [X â‰¥ t âˆ’ 2]
â‰¤
P [X â‰¥ E [X] + t âˆ’ 3] â‰¤ exp âˆ’
3
From the last chain of inequalities and Eq. (57) we derive


tâˆ’3
P [E2 ] â‰¤ cp Â· exp âˆ’
.
3

(61)

Upper-bound on P [E3 ]. Consider E3 as the union of the subevents E3 (iâˆ— ) when Xiâˆ— is specifically
greater than Î´ and less than 0.1 Â· c of the rest of the Xi â€™s are below 2Î´/3.
P [E3 ] â‰¤

c
X

P [E3 (iâˆ—)] = cP [E3 (1)] = cpP [|{i > 1 : Xi â‰¤ 2Î´/3}| < 0.1 Â· c] .

(62)

iâˆ—=1

Note that by Markovâ€™s inequality we have P [Xi â‰¤ 2Î´/3] â‰¥ 1/2 (since P [Xi â‰¥ 2Î´/3] â‰¤ 1/2). Therefore,
E [|{i > 1 : Xi â‰¤ 2Î´/3}|] â‰¥ (c âˆ’ 1)/2.
Hence, by Chernoff bound (Theorem 7(c)) we derive

P [|{i > 1 : Xi â‰¤ 2Î´/3}| â‰¤ 0.1 Â· c] â‰¤ exp

(3/4)2 (c âˆ’ 1)/2
2

assuming that c â‰¥ 10. This bound together with Eq. (62) implies
 c
P [E3 ] â‰¤ cp exp âˆ’
.
8
66



 c
â‰¤ exp âˆ’
.
8

(63)

Combining all the bounds. From Eq. (56), Eq. (61) and Eq. (63) we conclude


P X â‰¥ Î´ â‰¤ P [E1 ] + P [E2 ] + P [E3 ]




 c
cÎ´
tâˆ’3
â‰¤ c2 p exp âˆ’
+ cp exp âˆ’
+ cp exp âˆ’
9
3
8
â‰¤ p/2,
when Î´ and c are set appropriately. Indeed recalling that t =
this goal. Notice that these bounds are not tight.

C

cÎ´
30

and set c â‰¥ 2000 log(1/Î´)/Î´ to achieve

Proofs omitted from Section 7

Proof of Lemma 17: Recall the definitions of Ie and Te from the proof of Lemma 14: Let Ie be the
indicator variable of e being explored when Algorithm 10 is called from e0 ; let Te be the size of the
exploration tree from e in Hi . Let T = Te0 , t(Î») = te0 (Î»). Let v(Î») = E(T 2 |r(e0 ) = Î»); we will derive
a recursive formula for v(Î») and prove that supÎ» v(Î») â‰¤ 10d5 , thus proving the lemma. Recall further
from the proof of Lemma 14 our formula for T
X
T =1+
Ie Â· Te .
eâˆˆÎ´(e0 )

Therefore,
ï£«

ï£¶2
X

v(Î») = E ï£­1 +

Ie Â· Te r(e0 ) = Î»ï£¸

eâˆˆÎ´(e0 )

ï£«
= 1 + 2E ï£­

ï£¶
X

ï£«

Ie Â· Te r(e0 ) = Î»ï£¸ + E ï£­

ï£«
â‰¤ 2E ï£­1 +

X

Ie Â· If Â· Te Â· Tf r(e0 ) = Î»ï£¸

eâˆˆÎ´(e0 ) f âˆˆÎ´(e0 )

eâˆˆÎ´(e0 )

ï£¶
X

ï£¶
X

ï£«

Ie Â· Te r(e0 ) = Î»ï£¸ + E ï£­

ï£¶
X

Ie Â· If Â· Te Â· Tf r(e0 ) = Î»ï£¸ + E ï£­

e6=f

eâˆˆÎ´(e0 )

ï£«

ï£¶
X

Ie Â· Te2 r(e0 ) = Î»ï£¸ .

eâˆˆÎ´(e0 )

The first term is simply 2E(T |r(e0 ) = Î») = 2t(Î») and is therefore bounded by 4d, due to Corollary 3.
To bound the second term, we drop the Ie and If . Then we note that Te and Tf are independent, as
they depend only on He and Hf respectively.
ï£«
ï£¶
X
X
Eï£­
Ie Â· If Â· Te Â· Tf r(e0 ) = Î»ï£¸ â‰¤
E(Te Â· Tf |r(e0 ) = Î»)
e6=f

e6=f

=

X

ETe Â· ETf

e6=f

â‰¤ d(d âˆ’ 1) Â· (ET )2
â‰¤ 4d4 ,
again by Corollary 3.
The third term does not admit to an outright bound. However we can express it recursively in terms
of v(Âµ). Note, as in the proof of Lemma 14, that Ie and Te are independent when conditioned on the
rank of e.
ï£«
ï£¶
X
X Z Î»
Eï£­
Ie Â· Te2 r(e0 ) = Î»ï£¸ =
E(Ie Â· Te2 |r(e) = Âµ)dÂµ
eâˆˆÎ´(e0 )

eâˆˆÎ´(e0 )

0

X Z

=

eâˆˆÎ´(e0 )

Z
=d

Î»

E(Ie |r(e) = Âµ) Â· E(Te2 |r(e) = Âµ)dÂµ

0

Î»

xâˆ’1 (Âµ)v(Âµ)dÂµ.

0

67

Therefore, the full recursive inequality for v(Î») is
v(Î») â‰¤ 4d + 4d4 + d

Î»

Z

xâˆ’1 (Âµ)v(Âµ)dÂµ â‰¤ 5d4 + d

Z

Î»

xâˆ’1 (Âµ)v(Âµ)dÂµ,

0

0

for d â‰¥ 5. This is very similar for to the recursive formula for t(Î») seen in the proof of Lemma 14. Let
vÌƒ(Î») = v(Î»)/(5d4 ). Then
Z Î»
v(Î») â‰¤ 1 + d
xâˆ’1 (Âµ)v(Âµ)dÂµ.
0

This is now identical to the formula for t(Î») (with an inequality instead of the equality), so vÌƒ(Î») â‰¤ t(Î»)
by GroÌˆnwallâ€™s inequality, since xâˆ’1 (Âµ) â‰¥ 0. So v(Î») â‰¤ 5d4 t(Î») â‰¤ 10d5 as claimed.
Proof of Corollary 6: Let T = Te0 and Ti = Te(i) .
"
#
d
X
 2
2
E T â‰¤ E (1 +
Ti )
i=1

"
= 1 + 2E

d
X

#
Ti

ï£®
ï£¹
" d
#
d
X
X
2
+ Eï£°
Ti Â· Tj ï£» + E
Ti

i=1

i=1

i6=j
2

2

 
â‰¤ 1 + 2d Â· E [T1 ] + (d) E [T1 ] + d Â· E T12
â‰¤ 1 + 4d2 + 42 d4 + 10d6 ,
by Corollary 3 and Lemma 17. This can then be upper bounded by 11d6 for d â‰¥ 5.

D
D.1

Details omitted from Section 8
Proof of Theorem 9

We now provide the formal analysis of the total variation distance between m1âˆ’ edge-samples from
graphs sampled from our hard distributions DY ES and DN O .
Proof of Theorem 9: We begin by defining random variables A1 , A2 , B1 , and B2 that contain partial
information about the iid stream under the YES and NO cases respectively. Let Ai be a random variable
def
m1âˆ’
m1âˆ’
in A = ([r] âˆª {?})
Ã— Nr , where the j th coordinate of the first half of Ai (the part in ([r] âˆª {?})
)
signifies which gadget (if any) the j th edge of the stream belongs to, the coordinate being ? if it belongs
to the clique. The j th coordinate of the second half of Ai (the part in Nr ) signifies the number of distinct
edges from Vj Ã— Vj sampled throughout the stream. Furthermore, let Bi be a vector of length r + 1,
where the j th coordinate signifies the isomorphism class of sampled edges of the j th gadget and the last
coordinate signifies the isomorphism class of the subsampled clique. Let the support of Bi be B
With slight abuse of notation, for i âˆˆ {1, 2} let
pi (a, b, c) := P [Ai = a âˆ§ Bi = b âˆ§ Ci = c]
pi (a) := P [Ai = a]
pi (b) := P [Bi = b]
pi (c) := P [Ci = c]
pi (b|a) := P [Bi = b|Ai = a]
pi (c|a, b) := P [Ci = c|Ai = a âˆ§ Bi = b] .
Again, we are interested in the total variation distance between C1 and C2 , which satisfies
kC1 âˆ’ C2 kTV â‰¤ k(A1 , B1 , C1 ) âˆ’ (A2 , B2 , C2 )kTV
X
1
=
|p1 (a, b, c) âˆ’ p2 (a, b, c)|
2
(a,b,c)âˆˆAÃ—BÃ—C

=

1
2

X

|p1 (a)p1 (b|a)p1 (c|a, b) âˆ’ p2 (a)p2 (b|a)p2 (c|a, b)|

(a,b,c)âˆˆAÃ—BÃ—C

68

First, observe that there is no discrepancy between p1 (a) and p2 (a) as the distributions of A1 and A2
are identical. Notice that the probability of a given iid edge being in a specific gadget or in the clique
depends only
 on the number of edges of that gadget or the number of edges of the cliques. The clique
contains w2 edges in both the YES and NO cases. Also G and H have the same number of edges
(simply apply the guarantee of Theorem 10 with K being a single edge), so all gadgets have the same
number of edges as well. p1 (a) = p2 (a) =: p(a).
X
1X
p(a)
|p1 (b|a)p1 (c|a, b) âˆ’ p2 (b|a)p2 (c|a, b)|
kC1 âˆ’ C2 kTV =
2
aâˆˆA

(b,c)âˆˆBÃ—C

1 X
â‰¤ P [E] +
p(a)
2
0
aâˆˆA

X

|p1 (b|a)p1 (c|a, b) âˆ’ p2 (b|a)p2 (c|a, b)|

(b,c)âˆˆBÃ—C

where A0 is the set of outcomes of Ai in accordance with E. Recall that
def

E = {âˆƒi âˆˆ [r] : edges between vertices of Vi appear more than k times in the stream}.
Consider now the discrepancy between p1 (b|a) and p2 (b|a). Again, we will prove that the two
0
distributions
are equivalent,
 as long as the value of Ai being conditioned on is in A . Consider Bi to be

(j)
(1)
(2)
(r)
Bi , Bi , . . . , Bi , Biâˆ— , where Bi represents the isomorphism class of the sampled version of the
j th gadget and Biâˆ— is the isomorphism class of the sampled version of the clique. Note that the
coordinates of Bi are independent conditioned on an outcome of Ai . Clearly, the distributions of B1âˆ—
(j)
(j)
and B2âˆ— are identical.
Consider now the distributions of B1
and B2
conditioned on
0
0
A1 = A2 = a âˆˆ A . Conditioning on an outcome in A fixes the size of the sampled subgraph to some
l â‰¤ k, which means the support of pi (b|a) is some set of graphs of size l. For any specific graph K in
the support, we know that the number of subgraphs of G and H isomorphic to K are equal (by the
guarantee
of Theoremi 10); let
h
h this number be X.
i Also let the number of edges in a gadget be Y . Then
(j)
(j)
P B1 = [K]|A1 = a = P B2 = [K]|A2 = a = X/ Yl . Thus p1 (b|a) = p2 (b|a) =: p(b|a) for every
a âˆˆ A0 .
kC1 âˆ’ C2 kTV â‰¤

1
1
+
10 2

X

p(a)p(b|a)

(a,b)âˆˆA0 Ã—B

X

|p1 (c|a, b) âˆ’ p2 (c|a, b)|

câˆˆC

Finally, consider the discrepancy between p1 (c|a, b) and p2 (c|a, b). We will, yet again, prove that the
two distributions are identical when conditioned on any (a, b) âˆˆ A0 Ã— B. Having conditioned on Ai = a
and Bi = b the following are set about the stream: for every gadget, as well as the clique, we know the
placement and number of the edges in the stream, and we know the isomorphidm class of the subsampled
gadget (or clique). For every gadget (or clique) with subsampled isomorphism-class [K], we donâ€™t know
the particular embedding of K into Vj (or VK ) that produces the subsampled gadget (or clique), and we
also donâ€™t know the order and multiplicity with which these edges arrive. Thanks to the fact that all
gadgets were uniformly randomly permuted in their embedding into V in the construction of DYES and
DNO , the embedding of K into Vj is also uniformly random. (The clique is completely symmetric and
need not be permuted.) Furthermore, since the stream is iid, conditioned on the set of edges in Vj Ã— Vj
that must appear, their order and multiplicity is drawn from the same distribution, regardless of whether
we are in the YES or NO case. Therefore, for any (a, b, c) âˆˆ A0 Ã—B Ã—C, p1 (c|a, b) = p2 (c|a, b) =: p(c|a, b).
kC1 âˆ’ C2 kTV â‰¤

D.2

1
1
+
10 2

X

p(a)p(b|a)

(a,b)âˆˆA0 Ã—B

X
câˆˆC

|p(c|a, b) âˆ’ p(c|a, b)| =

1
10

Proof of Lemma 23

Our proof of Lemma 23 is built on Theorem 3.4. of [LPS88] and a result from [CH12]. We next restate
the first result.
Theorem 14 ([LPS88]). For any distinct primes p and q congruent to 1 modulo 4, there exists a group
G p,q with a set S of generator elements with the following properties: |G p,q | âˆˆ [q(q 2 âˆ’ 1)/2, q(q 2 âˆ’ 1)];
|S| = p + 1; and, G p,q has girth at least 2 logp (q/4).
69

Theorem 15 ([CH12]). For any x â‰¥ 7, the interval (x, 2x] contains a prime number congruent 1 modulo
4.
Lemma 23. For any parameters g and l, there exists a group G of size lO(g) along with a set of generator
elements S of size at least l, such that the associated Cayley graph (Definition 12) has girth at least g.
Proof. If l < 7, let p = 13. Otherwise, if l â‰¥ 7, let p be a prime number congruent 1 modulo 4 from the
interval [l, 2l]. By Theorem 15, such p exists. Let q be a prime number congruent 1 modulo 4 from the
interval [4pg , 8pg ]. Again by Theorem 15 and recalling that p â‰¥ 2, such q exists. The statement now
follows by Theorem 14.

E

Proofs omitted from Section 9

Proof of Lemma 26: By symmetry we may assume that p â‰¤ 1/2. We consider 3 cases:
Case 1:  â‰¤ âˆ’p/3.

With this constraint


DKL (Ber (p + )kBer (p)) â‰¤ DKL (Ber (0)kBer (p)) = log

1
1âˆ’p


â‰¤

p
.
1âˆ’p

Therefore the lemma statement is always satisfied.
Case 2:  â‰¥ 1/4.
1
p.

With this constraint DKL (Ber (p + )kBer (p)) â‰¤ DKL (Ber (1)kBer (p)) = log

 
1
p

â‰¤

Therefore, the lemma statement is always satisfied.

Case 3:  âˆˆ [âˆ’p/3, 1/4].

In that case we have


1âˆ’p
âˆ’ (1 âˆ’ p âˆ’ ) log
(64)
DKL (Ber (p + )kBer (p)) = âˆ’(p + ) log
1âˆ’pâˆ’






= âˆ’(p + ) log 1 âˆ’
âˆ’ (1 âˆ’ p âˆ’ ) log 1 +
(65)
p+
1âˆ’pâˆ’





42

42
â‰¤ âˆ’(p + ) âˆ’
âˆ’
âˆ’ (1 âˆ’ p âˆ’ )
âˆ’
p +  (p + )2
1 âˆ’ p âˆ’  (1 âˆ’ p âˆ’ )2
(66)


p
p+



42
(p + )(1 âˆ’ p âˆ’ )
16
â‰¤
p(1 âˆ’ p)
=



(67)
(68)

Here Eq. (66) follows from Taylorâ€™s theorem. Indeed, By the restriction on the range of , both
âˆ’/(p + ) and /(1 âˆ’ p âˆ’ ) are in the interval [âˆ’1/2, âˆž). On this interval the function log(1 + x) is
twice differentiable and the absolute value of its second derivative is bounded by 4, therefore
x âˆ’ 4x2 â‰¤ log(1 + x) â‰¤ x + 4x2 .
Proof of Lemma 27: We assume without loss of generality that r â‰¤ 1/2. Let re := Padding(r, ).
Then re is also less than half and in fact re = max(r, ). Let Î·1 = |p âˆ’ q|, Î·2 = |q âˆ’ re| and Î·3 = |p âˆ’ re|.
For simplicity we will denote DKL (Ber (x)kBer (y)) as DKL (xky) during this proof. By Lemma 26, in
order to establish the result of the lemma it suffices to show that
Î·32 â‰¤ O()e
r(1 âˆ’ re).
Note that the term (1 âˆ’ re) is in [1/2, 1] and can be disregarded.
We will use the following facts throughout the proof:
Fact 1. For all x âˆˆ R,
log(1 + x) â‰¤ x.
70

(69)

Fact 2. For all x â‰¤ 1,
log(1 + x) â‰¤ x âˆ’

x2
.
4

Fact 3. For all x âˆˆ [0, 1/2],
DKL (0kx) â‰¤ 2x.
Fact 4. For all x âˆˆ [0, 1/2],
DKL (xk2x) â‰¥

x
.
4

Indeed,

DKL (xk2x) = âˆ’x log

2x
x




âˆ’ (1 âˆ’ x) log 1 âˆ’

x
1âˆ’x


â‰¥ âˆ’x log 2 + (1 âˆ’ x) Â·

x
x
= x Â· (1 âˆ’ log 2) â‰¥ ,
1âˆ’x
4

by Fact 1.
We will differentiate six cases depending on the ordering of p, q and re. However, four of these, the
ones where q is not in the middle, will be very simple.
Case 1: p â‰¤ re â‰¤ q.

Then,

Case 2: re â‰¤ p â‰¤ q.

Then,

DKL (pke
r) â‰¤ DKL (pkq) â‰¤ .

DKL (pke
r) â‰¤ DKL (qke
r) â‰¤ DKL (qkr) â‰¤ .

Case 3: qe â‰¤ p â‰¤ re.

Then, if re = r,
DKL (pke
r) â‰¤ DKL (qke
r) = DKL (qkr) â‰¤ .

On the other hand, if re = ,
DKL (pke
r) â‰¤ DKL (0k) = âˆ’ log(1 âˆ’ ) â‰¤ 2,
by Fact 3 since  â‰¤ 1/2.
Case 4: qe â‰¤ re â‰¤ p.

Then,

Case 5: p â‰¤ q â‰¤ re.

We consider two subcases.

(a.) p â‰¤ 4.

DKL (pke
r) â‰¤ DKL (pkq) â‰¤ .

Then q cannot be greater than 8. Indeed this would mean by Fact 4 that
DKL (pkq) > DKL (4k8) â‰¥ ,

which is a contradiction. Similarly, r cannot be greater than 16. Indeed this would mean by Fact 4 that
DKL (qkr) > DKL (8k16) â‰¥ 2,
which is also a contradiction. Ultaminately, re â‰¤ 16, so
DKL (pke
r) â‰¤ DKL (0k16) â‰¤ 32
by Fact 3 since 16 â‰¤ 1/2.

71

(b.) p â‰¥ 4. Note that re = r. Let us bound Î·1 . First note that Î·1 cannot be greater than p due to
âˆš
Fact 4. We will further show that Î·1 in fact cannot be greater than 2 p.
 â‰¥ DKL (pkq)




Î·1
Î·1
âˆ’ (1 âˆ’ p) log 1 âˆ’
= âˆ’p log 1 +
p
1âˆ’p




Î·1
Î·1
Î·12
â‰¥ âˆ’p
âˆ’ 2 âˆ’ (1 âˆ’ p) âˆ’
p
4p
1âˆ’p
Î·12
=
.
4p
âˆš
An identical calculation shows that Î·2 â‰¤ 2 q. Ultimately,

By Facts 1 and 2, since Î·1 /p â‰¤ 1,

Î·3 = Î·1 + Î·2
âˆš
âˆš
â‰¤ 2 p + 2 q
q
âˆš
âˆš
â‰¤ 2 p + 2 (p + 2 p) Â· 
âˆš
â‰¤ 6 p
âˆš
â‰¤ 6 re.

Since p â‰¥ ,

From here (69) follows immediately.
Case 6: re â‰¤ q â‰¤ p.
âˆš
2 q.

In this case, let us first bound Î·2 . We will show that Î·2 cannot be greater than

 â‰¥ DKL (qkr)
â‰¥ DKL (qke
r)

= âˆ’q log 1 âˆ’

Î·2
â‰¥ âˆ’q âˆ’ âˆ’
q
2
Î·
= 2.
4q




Î·2
Î·2
âˆ’ (1 âˆ’ q) log 1 +
q
1âˆ’q



2
Î·2
Î·2
âˆ’ (1 âˆ’ q)
4q 2
1âˆ’q

By Facts 1 and 2,

This also implies that q is at most 6e
r. Indeed, suppose q = Î³e
r. Then
re = q âˆ’ Î·2
âˆš
â‰¥ q âˆ’ 2 q
p
= Î³e
r âˆ’ 2 Î³e
r
âˆš
â‰¥ (Î³ âˆ’ 2 Î³) Â· re,
âˆš
âˆš
since re â‰¥ . Therefore, 1 â‰¥ Î³ âˆ’ 2 Î³, so Î³ â‰¤ 6. We conclude that Î·2 â‰¤ 2 6e
r. An identical calculation
âˆš
âˆš
shows that Î·1 â‰¤ 2 6q â‰¤ 12 re. Ultimately,
Î·3 = Î·1 + Î·2
âˆš
âˆš
â‰¤ 2 6e
r + 12 re
âˆš
â‰¤ 18 re.
From here (69) follows immediately.
This concludes the proof of the lemma under all possible orderings of p, q and re.

72

