Proceedings of Machine Learning Research vol xxx:1‚Äì39, 2020

Scalable Reinforcement Learning of Localized Policies for
Multi-Agent Networked Systems

arXiv:1912.02906v2 [math.OC] 18 Feb 2020

Guannan Qu
Adam Wierman

GQU @ CALTECH . EDU
ADAMW @ CALTECH . EDU

Department of Computing and Mathematical Sciences
California Institute of Technology
Pasadena, CA 91125, USA

Na Li

NALI @ SEAS . HARVARD . EDU

School of Engineering and Applied Sciences
Harvard University
Cambridge, MA 02138, USA

Abstract
We study reinforcement learning (RL) in a setting with a network of agents whose states and actions
interact in a local manner where the objective is to find localized policies such that the (discounted)
global reward is maximized. A fundamental challenge in this setting is that the state-action space
size scales exponentially in the number of agents, rendering the problem intractable for large networks. In this paper, we propose a Scalable Actor Critic (SAC) framework that exploits the network
structure and finds a localized policy that is an O(œÅŒ∫+1 )-approximation of a stationary point of the
objective for some œÅ ‚àà (0, 1), with complexity that scales with the local state-action space size of
the largest Œ∫-hop neighborhood of the network.
Keywords: Multi-agent reinforcement learning, networked systems, actor-critic methods.

1. Introduction
Having demonstrated impressive performance in a wide array of domains such as game play (Silver
et al., 2016; Mnih et al., 2015), robotics (Duan et al., 2016; Levine et al., 2016), autonomous driving
(Li et al., 2019), Reinforcement Learning (RL) has emerged as a promising tool for decision and
control. However, in order to use RL in the context of control of large scale networked systems, such
as those in cyber-physical systems, it is necessary to develop scalable RL algorithms for networked
systems.
In this paper, we consider a RL problem for a network of n agents, each with state si and action
ai , both taking values from finite sets. The agents are associated with an underlying dependence
graph G and interact locally, i.e, the distribution of si (t + 1) only depends on the current states
of the local neighborhood of i as well as the local ai (t). Further, each agent is associated with
stage reward ri that is a function of si , ai , and the global stage reward is the average of ri . In
this setting, the design goal is to find a decision policy that maximizes the (discounted) global
reward. This setting captures a wide range of applications. For example, such models have been
used in the literature on epidemics (Mei et al., 2017), social networks (Chakrabarti et al., 2008;
Llas et al., 2003), communication networks (Zocca, 2019; Vogels et al., 2003), queueing networks

c 2020 G. Qu, A. Wierman & N. Li.

S CALABLE R EINFORCEMENT L EARNING OF L OCALIZED P OLICIES FOR M ULTI -AGENT N ETWORKED S YSTEMS

(Papadimitriou and Tsitsiklis, 1999), smart transportation (Zhang and Pavone, 2016), smart building
systems (Wu et al., 2016; Zhang et al., 2017), and multi-agent game play (Borovikov et al., 2019).
A fundamental difficulty when applying RL to such networked systems is that, even if individual state and action spaces are small, the entire state profile (s1 , . . . , sn ) and the action profile
(a1 , . . . , an ) can take values from a set of size exponentially large in n. This ‚Äúcurse of dimensionality‚Äù renders the problem unscalable. For example, most RL algorithms like temporal difference
(TD) learning or Q-learning require storage of a value function or Q-function (Bertsekas and Tsitsiklis, 1996) whose size is the same as the state space (or state-action space), which in our problem
is exponentially large in n. Such scalability issues have indeed been observed in previous research
on variants of the problem we study, e.g. in multi-agent RL (Littman, 1994; Bu et al., 2008) and
factored Markov Decision Proccess (MDP) (Kearns and Koller, 1999; Guestrin et al., 2003). A variety of approaches have been proposed to manage this issue, e.g. the idea of ‚Äúindependent learners‚Äù
in Claus and Boutilier (1998); or function approximation schemes (Tsitsiklis and Van Roy, 1997).
However, such approaches lack rigorous optimality guarantees. In fact, it has been suggested that
such MDPs with exponentially large state spaces may be fundamentally intractable in general, e.g.,
see Papadimitriou and Tsitsiklis (1999); Blondel and Tsitsiklis (2000).
In addition to the challenges posed by the scalability issue, another issue is that, even if an
optimal policy that maps a global state (s1 , . . . , sn ) profile to a global action (a1 , . . . , an ) can be
found, it is usually impractical to implement such a policy for real-world networked systems because
of the limited information and communication among agents. For example, in large scale networks,
each agent i may only be able to to implement localized policies, where its action ai only depends
on its own state si . Designing such localized polices with global network performance guarantee
can also be challenging, see e.g. Rotkowitz and Lall (2005).
The challenges described above highlight the difficulty of applying RL to control large scale
networked systems. However, the network itself provides some structure that can potentially be
exploited. The question that motivates this paper is: Can the network structure be utilized to develop
scalable RL algorithms that provably find a (near-)optimal localized policy?
Contributions. In this work we propose a framework that exploits properties of the network
structure to develop RL to learn localized policies for large-scale networked systems in a scalable
manner. Specifically, our main result (Theorem 5) shows that our algorithm, Scalable Actor Critic
(SAC), finds a localized policy that is a O(œÅŒ∫+1 )-approximation of a stationary point of the objective function, with complexity that scales with the local state-action space size of the largest
Œ∫-hop neighborhood. To the best of our knowledge, our results are perhaps the first to provide such
provable guarantee for scalable RL of localized policies in multi-agent network settings.
The key technique underlying our results is the observation that, when the size of Œ∫-hop neighborhood is bounded, the network structure implies that the Q-function satisfies an exponential decay
property (Definition 2), which leads to a tractable approximation of the policy gradient. In particular, despite the policy gradient itself being intractable to compute due to the large state-action space
size, we introduce a truncated policy gradient (see Lemma 4) that can be computed efficiently and
can be used in an actor-critic framework which yields an O(œÅŒ∫+1 )-approximation. This technique is
novel and is a contribution in its own right. It can be used broadly to develop RL in network settings
beyond the specific actor-critic algorithm we propose in this paper.
Related Literature. Our problem falls under category of the ‚Äúsuccinctly described‚Äù MDPs in
Blondel and Tsitsiklis (2000, Section 5.2), where the state and/or action space is a product space
formed by the individual state and/or action space of multiple agents. As the state/action space is
2

S CALABLE R EINFORCEMENT L EARNING OF L OCALIZED P OLICIES FOR M ULTI -AGENT N ETWORKED S YSTEMS

exponentially large, such problems are unscalable in general, even when the problem has structure
(Blondel and Tsitsiklis, 2000; Whittle, 1988; Papadimitriou and Tsitsiklis, 1999). Despite this, there
is a large literature on RL and MDPs in multi-agent settings under various structural assumptions.
Multi-agent RL dates back to the early work of Littman (1994); Claus and Boutilier (1998);
Littman (2001); Hu and Wellman (2003) (see Bu et al. (2008) for a review) and has been actively
studied, e.g. Zhang et al. (2018); Kar et al. (2013); Macua et al. (2015); Mathkar and Borkar (2017);
Wai et al. (2018), see a more recent review in Zhang et al. (2019). Multi-agent RL encompasses
a broad range of settings including competitive agents and Markov games. The case most relevant
to ours is the cooperative multi-agent RL where typically, the agents can take their own actions but
they share a common global state and maximize a global reward (Bu et al., 2008). This is contrast
to the model we study, in which each agent has its own state and acts upon its own state. Despite the
existence of a global state, multi-agent RL still faces scalability issues since the joint-action space
is exponentially large. A number of techniques have been proposed to deal with this, including
independent learners (Claus and Boutilier, 1998; Matignon et al., 2012), where each agent employs
a single-agent RL method. While successful in some cases, the independent learner approach can
suffer from instability (Matignon et al., 2012). Alternatively, one can use function approximation
schemes to approximate the large Q-table, e.g., linear function approximation (Zhang et al., 2018) or
neuro networks (Lowe et al., 2017). Such methods can reduce computation complexity significantly,
but it is unclear whether the performance loss caused by the function approximation is small. In
contrast, our technique not only reduces computation but also guarantees small performance loss.
Factored MDPs are problems where every agent has its own state and the state transition factorizes in a way similar to our model (Kearns and Koller, 1999; Guestrin et al., 2003; Osband and
Van Roy, 2014). However, they differ from the model we consider in that each agent does not have
its own action. Instead, there is a global action affecting every agent. Despite the difference, Factored MDPs still suffer from scalability issues. Similar approaches as in the case of Multi-agent RL
are used, e.g., Guestrin et al. (2003) proposes a class of ‚Äúfactored‚Äù linear function approximators;
however, it is unclear whether the loss caused by the approximation is small.
Other Related Work. Beyond the above, our work is also connected to a few other classes of
problems. The first is the class of weakly coupled MDPs, where every agent has its own state and
action but their transition is decoupled (Meuleau et al., 1998). While similar to our model, our model
differs in that the transition probability is coupled among the agents. Additionally, our model shares
some similarity with the work of control of dynamical systems over graphs, e.g., the epidemics
(Cator and Van Mieghem, 2012; Sahneh et al., 2013; Mei et al., 2017) and Glauber dynamics in
physics (Lokhov et al., 2015; Mezard and Montanari, 2009), though our focus is very different from
these works. Finally, this work is related to Qu and Li (2019), which assumes the full knowledge
of MDP model (not RL) and imposes strong assumptions on the graph. In contrast, our work here
does not need knowledge of the MDP and significantly relaxes the network assumptions.

2. Preliminaries
We consider a network of n agents that are associated with an underlying undirected graph G =
(N , E), where N = {1, . . . , n} is the set of agents and E ‚äÇ N √ó N is the set of edges. Each agent i
is associated with state si ‚àà Si , ai ‚àà Ai where Si and Ai are finite sets. The global state is denoted
as s = (s1 , . . . , sn ) ‚àà S := S1 √ó ¬∑ ¬∑ ¬∑ √ó Sn and similarly the global action a = (a1 , . . . , an ) ‚àà A :=
A1 √ó ¬∑ ¬∑ ¬∑ √ó An . At time t, given current state s(t) and action a(t), the next individual state si (t + 1)
3

S CALABLE R EINFORCEMENT L EARNING OF L OCALIZED P OLICIES FOR M ULTI -AGENT N ETWORKED S YSTEMS

is independently generated and is only dependent on neighbors:
P (s(t + 1)|s(t), a(t)) =

n
Y

P (si (t + 1)|sNi (t), ai (t)),

(1)

i=1

where notation Ni means the neighborhood of i (including i itself) and sNi is the states of i‚Äôs
neighbors. In addition, for integer Œ∫ ‚â• 1, we let NiŒ∫ denote the Œ∫-hop neighborhood of i, i.e. the
nodes whose graph distance to i is less than or equal to Œ∫, including i itself. We also let f (Œ∫) =
supi |NiŒ∫ |.
Each agent is associated with a class of localized policies Œ∂iŒ∏i parameterized by Œ∏i . The localized
policy Œ∂iŒ∏i (ai |si ) is a distribution on the local action ai conditioned on the local state si , and each
agent, conditioned on observing si (t), takes an action ai (t) independently drawn from Œ∂iŒ∏i (¬∑|si (t)).
We use Œ∏ = (Œ∏1 , . . . , Œ∏n ) to denote the tuple of the localized policies Œ∂iŒ∏i , and also use Œ∂ Œ∏ (a|s) =
Qn
Œ∏i
i=1 Œ∂i (ai |si ) to denote the joint policy, which is a product distribution of the localized policies
as each agent acts independently.
Further, each agent is associated with a stage reward function
P ri (si , ai ) that depends on the local
state and action, and the global stage reward is r(s, a) = n1 ni=1 ri (si , ai ). The objective is to find
localized policy tuple Œ∏ such that the discounted global stage reward is maximized, starting from
some initial state distribution œÄ0 ,
X

‚àû
t
max J(Œ∏) := Es‚àºœÄ0 Ea(t)‚àºŒ∂ Œ∏ (¬∑|s(t))
Œ≥ r(s(t), a(t)) s(0) = s .
(2)
Œ∏

t=0

To provide context for what follows, we review a few key concepts in RL. First, fixing a localized
policy tuple Œ∏ = (Œ∏1 , . . . , Œ∏n ), the Q-function for this policy Œ∏ is:
Œ∏

Q (s, a) = Ea(t)‚àºŒ∂ Œ∏ (¬∑|s(t))

X
‚àû


Œ≥ r(s(t), a(t)) s(0) = s, a(0) = a
t

t=0


X
‚àû
n
n
1X Œ∏
1X
Œ≥ t ri (si (t), ai (t)) s(0) = s, a(0) = a :=
=
Ea(t)‚àºŒ∂ Œ∏ (¬∑|s(t))
Q (s, a).
n i=1
n i=1 i
t=0

(3)

In the last step, we have defined QŒ∏i (s, a) which is the Q function for the individual reward ri . Both
QŒ∏ and QŒ∏i are exponentially large tables and, therefore, are intractable to compute and store.
Finally, we recall the policy gradient theorem, which is the basis of many algorithmic results in
RL. We emphasize that the lemma shows that the gradient of J(Œ∏) depends on QŒ∏ and, therefore, is
intractable to compute using the form in Lemma 1.
LemmaP
1 (Sutton et al. (2000)) Let œÄ Œ∏ be a distribution on the state space given by œÄ Œ∏ (s) =
t Œ∏
Œ∏
(1 ‚àí Œ≥) ‚àû
t=0 Œ≥ œÄt (s), where œÄt is the distribution of s(t) under fixed policy Œ∏ when s(0) is drawn
from œÄ0 . Then
1
‚àáJ(Œ∏) =
E Œ∏
QŒ∏ (s, a)‚àá log Œ∂ Œ∏ (a|s).
(4)
Œ∏
1 ‚àí Œ≥ s‚àºœÄ ,a‚àºŒ∂ (¬∑|s)

3. Algorithm Design and Results
In this paper we propose an algorithm, Scalable Actor Critic (SAC), which provably finds an
O(œÅŒ∫+1 )-stationary point of the objective J(Œ∏) for some œÅ ‚â§ Œ≥,1 with complexity scaling in the
1. In this paper, a Œµ-stationary point of J(Œ∏) refers to a Œ∏ s.t. k‚àáJ(Œ∏)k2 ‚â§ Œµ.

4

S CALABLE R EINFORCEMENT L EARNING OF L OCALIZED P OLICIES FOR M ULTI -AGENT N ETWORKED S YSTEMS

size of the local state-action space of the largest Œ∫-hop neighborhood. We state our main result
formally in Theorem 5 after introducing the details of SAC and the key idea underlying its design.
3.1. Key Idea: Exponential Decay of Q-function Leads to Efficient Gradient Approximation
Recall that the policy gradient in Lemma 1 is intractable to compute due to the dimension of the
Q-function. Our key idea is that exponential decay of the Q function allows efficient approximation
of the policy gradient via truncation. To illustrate this, we start with the definition of the exponential
Œ∫ = N /N Œ∫ ,
decay property. Recall that NiŒ∫ is the set of Œ∫-hop neighborhood of node i and define N‚àíi
i
Œ∫ ),
i.e. the set of agents that are outside of i‚Äôth Œ∫-hop neighborhood. We write state s as (sNiŒ∫ , sN‚àíi
i.e. the states of agents that are in the Œ∫-hop neighborhood of i and outside of Œ∫-hop neighborhood
Œ∫ ). The exponential decay property is then defined
respectively. Similarly, we write a as (aNiŒ∫ , aN‚àíi
as follows.
Definition 2 The (c, œÅ)-exponential decay property holds if, for any localized policy Œ∏, for any
Œ∫ , s0 Œ∫ ‚àà SN Œ∫ , aN Œ∫ ‚àà AN Œ∫ , aN Œ∫ , a0 Œ∫ ‚àà AN Œ∫ , QŒ∏ satisfies,
i ‚àà N , sNiŒ∫ ‚àà SNiŒ∫ , sN‚àíi
i
N
N
‚àíi
‚àíi
i
i
‚àíi
‚àíi

‚àíi

Œ∫ , aN Œ∫ , aN Œ∫ )
|QŒ∏i (sNiŒ∫ , sN‚àíi
i
‚àíi

‚àí

0
Œ∫ , aN Œ∫ , aN Œ∫ )|
QŒ∏i (sNiŒ∫ , s0N‚àíi
i
‚àíi

‚â§ cœÅŒ∫+1 .

It may not be immediately clear when the exponential decay property holds. Lemma 3 highlights
that the exponential decay property holds generally, with œÅ = Œ≥. Further, under some mixing time
assumptions, the exponential decay property holds with œÅ < Œ≥. For more details on the generality
of the exponential decay property, see Appendix A.
rÃÑ
Lemma 3 If ‚àÄi, ri is upper bounded by rÃÑ, then the ( 1‚àíŒ≥
, Œ≥)-exponential decay property holds.

The power of the exponential decay property is that it guarantees that the dependence of QŒ∏i on
other agents shrinks quickly as the distance between them grows. This motivates us to consider the
following class of truncated Q-functions,
X
Œ∏
Œ∫ , aN Œ∫ ; sN Œ∫ , aN Œ∫ )Q (sN Œ∫ , sN Œ∫ , aN Œ∫ , aN Œ∫ ),
QÃÇŒ∏i (sNiŒ∫ , aNiŒ∫ ) =
wi (sN‚àíi
(5)
i
‚àíi
i
i
i
‚àíi
i
‚àíi
sN Œ∫ ,aN Œ∫
‚àíi

‚àíi

Œ∫ , aN Œ∫ ; sN Œ∫ , aN Œ∫ ) are any non-negative weights satisfying
where wi (sN‚àíi
i
i
‚àíi
X
Œ∫ , aN Œ∫ ; sN Œ∫ , aN Œ∫ ) = 1, ‚àÄ(sN Œ∫ , aN Œ∫ ) ‚àà S k √ó A k .
wi (sN‚àíi
N
N
‚àíi
i
i
i
i
i

i

(6)

sN Œ∫ ‚ààSN Œ∫ ,aN Œ∫ ‚ààAN Œ∫
‚àíi
‚àíi
‚àíi
‚àíi

Finally, our key insight is the following Lemma 4, which says when the exponential decay property holds, the truncated Q-function (5) can be used to accurately approximate the policy gradient.
The proof of Lemma 4 is postponed to Appendix B.
Lemma 4 (Truncated Policy Gradient) Given i, define the following truncated policy gradient
h1 X
i
1
hÃÇi (Œ∏) =
Es‚àºœÄŒ∏ ,a‚àºŒ∂ Œ∏ (¬∑|s)
QÃÇŒ∏j (sNjŒ∫ , aNjŒ∫ ) ‚àáŒ∏i log Œ∂iŒ∏i (ai |si ),
(7)
1‚àíŒ≥
n
Œ∫
j‚ààNi

where QÃÇŒ∏j can be any truncated Q-function in the form of (5). Then, if (c, œÅ)-exponential decay
property holds and if k‚àáŒ∏i log Œ∂iŒ∏i (ai |si )k ‚â§ Li for any ai , si , we have khÃÇi (Œ∏) ‚àí ‚àáŒ∏i J(Œ∏)k ‚â§
cLi Œ∫+1
.
1‚àíŒ≥ œÅ
5

S CALABLE R EINFORCEMENT L EARNING OF L OCALIZED P OLICIES FOR M ULTI -AGENT N ETWORKED S YSTEMS

The power of this lemma is that the truncated Q function has much smaller dimension than the
true Q function, and is thus scalable. However, despite the reduction in dimension, the error of
the approximated gradient (7) is small. In the next section, we use this idea to design a scalable
algorithm.
3.2. Algorithm Design: Scalable Actor Critic (SAC)
The good properties of the truncated Q-function open many possibilities for algorithm design. For
instance, one can first obtain the truncated Q-function in some way (which could be much easier
than directly computing the full Q-function) and then do a policy gradient step using the Lemma 4.
In this subsection, we propose one particular approach using the actor-critic framework. Our approach, Scalable Actor Critic (SAC), uses temporal difference (TD) learning to obtain the truncated
Q-function and then uses policy gradient for policy improvement. Psuedocode of the proposed
algorithm is given in Algorithm 1.
Overall structure. The overall structure of SAC is a for-loop from line 1 to line 13. Inside the
outer loop, there is an inner loop (line 4 through line 9) that uses temporal difference learning to get
the truncated Q-function, which is followed by a policy gradient step that does policy improvement.
The Critic: TD-inner loop. Line 4 through line 9 is the policy evaluation inner loop that obtains
the truncated Q function, where line 7 and 8 are the temporal difference update. We note that steps 7
and 8 use the same update equation as TD learning, except that it ‚Äúpretends‚Äù (sNiŒ∫ , aNiŒ∫ ) is the true
state-action pair while the true state-action pair should be (s, a). As will be shown in the theoretic
analysis in Appendix C, such a TD update implicitly gives an estimate of a truncated Q function.
The Actor: Policy Gradient. Steps 10 through 12 define the the actor actions. Here, each agent
calculates an estimate of the truncated gradient based on (7), and then conducts a gradient step.
Discussion. Our algorithm serves as an initial concrete demonstration of how to make use of the
truncated policy gradient to develop a scalable RL method for networked systems. There are many
extensions and other approaches that could be pursued, either within the actor-critic framework or
beyond. One immediate extension is to do a warm start, i.e., initialize QÃÇ0i as the final estimate QÃÇTi
in the previous outer-loop. Additionally, one can use the TD-Œª variant of TD learning with variance
reduction schemes like the advantage function. Further, beyond the actor-critic framework, another
direction is to develop Q-learning/SARSA type algorithms based on the truncated Q-functions.
These are interesting topics for future work.
3.3. Approximation Bound
In this section we state and discuss the formal approximation guarantee for SAC. Before stating the
theorem, we first state the assumptions we use. The first assumption is standard in the RL literature
and bounds the reward and state/action space size.
Assumption 1 (Bounded reward and state/action space size) The reward is upper bounded as
0 ‚â§ ri (si , ai ) ‚â§ rÃÑ, ‚àÄi, si , ai . The individual state and action space size are upper bounded as
|Si | ‚â§ S, |Ai | ‚â§ A, ‚àÄi.
Assumption 2 (Exponential Decay) The (c, œÅ) exponential decay property holds for some œÅ ‚â§ Œ≥.
Note that under Assumption 1, Assumption 2 automatically holds with œÅ = Œ≥, cf. Lemma 3.
However, we state the exponential decay property as an assumption to account for the more general
case that œÅ could be strictly less than Œ≥, as detailed in Appendix A.
6

S CALABLE R EINFORCEMENT L EARNING OF L OCALIZED P OLICIES FOR M ULTI -AGENT N ETWORKED S YSTEMS

1
2

3
4
5
6
7

8
9
10
11
12
13

Algorithm 1: SAC: Scalable Actor Critic
Input: Œ∏i (0); parameter Œ∫; T , length of each episode; step size parameters h, t0 , Œ∑.
for m = 0, 1, 2, . . . do
Œ∏ (m)
Sample initial state s(0) ‚àº œÄ0 , each agent i takes action ai (0) ‚àº Œ∂i i (¬∑|si (0)), receives
reward ri (0) = ri (si (0), ai (0)).
S

Œ∫ √óA

Œ∫

N
i to be the all zero vector.
Initialize QÃÇ0i ‚àà R Ni
for t = 1 to T do
Œ∏ (m)
Get state si (t), take action ai (t) ‚àº Œ∂i i (¬∑|si (t)), get reward ri (t) = ri (si (t), ai (t)).
h
Update the truncated Q function with step size Œ±t‚àí1 = t‚àí1+t
,
0
t
QÃÇi (sNiŒ∫ (t ‚àí 1), aNiŒ∫ (t ‚àí 1)) =
Œ∫
Œ∫
(1 ‚àí Œ±t‚àí1 )QÃÇit‚àí1 (sNiŒ∫ (t ‚àí 1), aNiŒ∫ (t ‚àí 1)) + Œ±t‚àí1 (ri (t ‚àí 1) + Œ≥ QÃÇt‚àí1
i (sNi (t), aNi (t))),
t‚àí1
t
QÃÇi (sNiŒ∫ , aNiŒ∫ ) = QÃÇi (sNiŒ∫ , aNiŒ∫ ) for (sNiŒ∫ , aNiŒ∫ ) 6= (sNiŒ∫ (t ‚àí 1), aNiŒ∫ (t ‚àí 1)).
end
Each agent i calculates approximated gradient,
P
P
Œ∏ (m)
gÃÇi (m) = Tt=0 Œ≥ t n1 j‚ààN Œ∫ QÃÇTj (sNjŒ∫ (t), aNjŒ∫ (t))‚àáŒ∏i log Œ∂i i (ai (t)|si (t)).
i
Œ∑
Each agent i conducts gradient step Œ∏i (m + 1) = Œ∏i (m) + Œ∑m gÃÇi (m) with Œ∑m = ‚àöm+1
.

end

Our third assumption can be interpreted as an ergodicity condition which ensures that the stateaction pairs are sufficiently visited.
Assumption 3 (Sufficient Local exploration) There exists positive integer œÑ and œÉ ‚àà (0, 1) s.t.
under any fixed policy Œ∏ and any initial state-action (s, a) ‚àà S √ó A, ‚àÄi ‚àà N , ‚àÄ(s0N Œ∫ , a0N Œ∫ ) ‚àà
i
i
SNiŒ∫ √ó ANiŒ∫ , we have P ((sNiŒ∫ (œÑ ), aNiŒ∫ (œÑ )) = (s0N Œ∫ , a0N Œ∫ )|(s(1), a(1)) = (s, a)) ‚â• œÉ.
i

i

Assumption 3 requires that every state action pair in the Œ∫-hop neighborhood must be visited with
some positive probability after some time. This type of assumption is common for finite time
convergence results in RL. For example, in Srikant and Ying (2019), it is assumed that every stateaction pair is visited with positive probability in the stationary distribution and the state-action
distribution converges to the stationary distribution with some rate. This implies our assumption
which is weaker in the sense that we only require local state-action pair (sNiŒ∫ , aNiŒ∫ ) to be visited as
opposed to the full state-action pair (s, a).
Finally, we assume boundedness and Lipschitz continuity of the gradients, which is standard in
the RL literature.
Assumption 4 (Bounded and Lipschitz continuous gradient) Forq
any i, ai , si and Œ∏i , we assume
Pn
Œ∏i
Œ∏
2
k‚àáŒ∏i log Œ∂i (ai |si )k ‚â§ Li . As a result, k‚àáŒ∏ log Œ∂ (a|s)k ‚â§ L =
i=1 Li . Further, assume
‚àáJ(Œ∏) is L0 -Lipschitz continuous in Œ∏.
Theorem 5 Under Assumption 1, 2, 3 and 4, for any Œ¥ ‚àà (0, 1), M ‚â• 3, suppose the critic
h
step size Œ±t = t+t
satisfies h ‚â• œÉ1 max(2, 1‚àí1‚àöŒ≥ ), t0 ‚â• max(2h, 4œÉh, œÑ ); and the actor step
0

7

S CALABLE R EINFORCEMENT L EARNING OF L OCALIZED P OLICIES FOR M ULTI -AGENT N ETWORKED S YSTEMS

size satisfies Œ∑m =
T +1‚â•

logŒ≥ c(1‚àíŒ≥)
rÃÑ

‚àö Œ∑
m+1

with Œ∑ ‚â§

1
4L0 .

Further, if the inner loop length T is large enough s.t.

+ (Œ∫ + 1) logŒ≥ œÅ and
Œ¥
,T)
Ca ( 2nM
Ca0
2cœÅŒ∫+1
‚àö
+
,
‚â§
T + t0
(1 ‚àí Œ≥)2
T + t0

(8)

where
6¬Ø

Ca (Œ¥, T ) =
‚àö
1‚àí Œ≥

r

œÑh
2œÑ T 2
2
16¬Ø
hœÑ 2rÃÑ
[log(
) + f (Œ∫) log SA], Ca0 =
,
(œÑ + t0 )),
‚àö max(
œÉ
Œ¥
1‚àí Œ≥
œÉ
1‚àíŒ≥

rÃÑ
with ¬Ø = 4 1‚àíŒ≥
+ 2rÃÑ and we recall that f (Œ∫) = maxi |NiŒ∫ | is the size of the largest Œ∫-neighborhood.
Then, with probability at least 1 ‚àí Œ¥,

PM ‚àí1
m=0

2

Œ∑m k‚àáJ(Œ∏(m))k
‚â§
PM ‚àí1
m=0 Œ∑m

2rÃÑ
Œ∑(1‚àíŒ≥)

+

8rÃÑ 2 L2
(1‚àíŒ≥)4

q

log M log 4Œ¥ +
‚àö
M +1

96rÃÑ 2 L0 L2
(1‚àíŒ≥)4 Œ∑ log M

+

12L2 crÃÑ Œ∫+1
œÅ
.
(1 ‚àí Œ≥)5

(9)

The proof of Theorem 5 can be found in Appendix-D. To interpret the result, note that the first
term in (9) converges to 0 in the order of OÃÉ( ‚àö1M ) and the second term, which we denote as ŒµŒ∫ , is the
bias caused by the truncation of the Q-function and it scales in the order of O(œÅŒ∫+1 ). As such, our
method SAC will eventually find an O(œÅŒ∫+1 )-approximation of a stationary point of the objective
function J(Œ∏), which could be very close to a true stationary point even for small Œ∫ as ŒµŒ∫ decays
exponentially in Œ∫.
In terms of complexity, (9) gives that, to reach a O(ŒµŒ∫ )-approximate stationary point, the num1
ber of outer-loop iterations required is M ‚â• ‚Ñ¶ÃÉ( ŒµŒ∫12 poly(rÃÑ, L, L0 , (1‚àíŒ≥)
)), which scales polynomially with the parameters of the problem. We emphasize that it does not scale exponentially
with n. Further, since the left hand side of (8) decays to 0 as T increases in the order of OÃÉ( ‚àö1T )
and the right hand side of (8) is in the same order as O(ŒµŒ∫ ), the inner-loop length required is
1
T ‚â• ‚Ñ¶ÃÉ( Œµ12 poly(œÑ, œÉ1 , 1‚àíŒ≥
, rÃÑ, f (Œ∫))). Parameters œÑ and œÉ1 are from Assumption 3 and they scale
k
with the local state-action space size of the largest Œ∫-hop neighborhood. Therefore, the inner-loop
length required scale with the size of the local state-action space of the largest Œ∫-neighborhood,
which is much smaller than the full state-action space size when the graph is sparse.2

4. Experimental Results
In this section, we conduct numerical experiments to verify our results. We first run a small case
n = 8 nodes interacting on a line. We set the individual state and action space as Si = {0, 1}
and Ai = {0, 1} for all i. We draw the local transition probabilities P (¬∑|sNi , ai ) in (1) uniformly
random for all i, sNi ‚àà SNi , ai ‚àà Ai . For the rewards, for each i we first pick a state action
pair (si , ai ) ‚àà Si √ó Ai and set ri (si , ai ) as 5; then we draw all other entries of ri (¬∑, ¬∑) uniformly
randomly from [0, 1]. The discounting factor is set as Œ≥ = 0.7 and the initial state distribution œÄ0
is set to be the Dirac measure concentrated on s = (0, . . . , 0). On this problem instance, we run
our SAC algorithm with Œ∫ = 0, . . . , 7, with Œ∫ = 7 giving the full actor critic method (no truncation
2. This requirement on T could potentially be further reduced if we do a warm start for the inner-loop, as the Q-estimate
from the previous outer-loop should be already a good estimate for the current outer-loop. We leave the finite time
analysis of the warm start variant as future work.

8

S CALABLE R EINFORCEMENT L EARNING OF L OCALIZED P OLICIES FOR M ULTI -AGENT N ETWORKED S YSTEMS

Figure 1: Simulation results for a small test case (n = 8). The left figure shows the approximated
objective function value versus the number of outer-loop iterations for different values of Œ∫. The
right figure shows the approximated optimality gap for different values of Œ∫.
of Q-table or policy gradient). For all values of Œ∫, we run M = 2000 outer-loop iterations and we
do a warm start for the TD inner-loop with inner-loop length set as T = 10. In each outer-loop
iteration m, we evaluate the performance of the current policy by estimating the objective function
(2), through sampling 20 trajectories with length T = 10 using the current policy and calculate the
discounted reward.
In Figure 1 (left), we plot the approximated objective function throughout the training procedure
for different values of Œ∫. It shows that when Œ∫ increases, the objective function increases throughout
the entire training procedure. We then use the final approximated objective function value achieved
by Œ∫ = 7 as the benchmark and plot the optimality gap of our algorithm with different values of Œ∫ in
Figure 1 (right), where the optimality gap is calculated as the difference between the benchmark and
the final objective function value achieved by the algorithm with the respective Œ∫. Figure 1 (right)
shows the optimality gap decays exponentially in Œ∫ up to Œ∫ = 4, confirming our theoretical result.
We note that the optimality gap stops decaying for Œ∫ > 4, which we believe is due to the fact that
both the benchmark and the final objective function achieved by the algorithm are sampled values
and are therefore noisy.
We also run the experiment on a larger example with n = 50 nodes, keeping all other settings the
same as the previous case. We run our algorithm up to Œ∫ = 5, and show the approximated objective
value throughout the training process (number of outer-loop iterations) for different values of Œ∫.
These results show that when Œ∫ increases, the objective function increases throughout the entire
training procedure, again consistent with the theoretical results.

5. Conclusion and Discussion
This paper proposes a SAC algorithm that provably finds a close-to-stationary point of J(Œ∏) in time
that scales with the local state-action space size of the largest Œ∫-hop neighbor, which can be much
smaller than the full state-action space size when the graph is sparse. This perhaps represents the
first scalable RL method for localized control of multi-agent networked systems with such provable
guarantee. In addition, the framework underlying SAC, including the truncated Q-function (5) and

9

S CALABLE R EINFORCEMENT L EARNING OF L OCALIZED P OLICIES FOR M ULTI -AGENT N ETWORKED S YSTEMS

Figure 2: Simulation results for a large test case (n = 50), showing the approximated objective
value versus the number of outer loop iterations.
truncated policy gradient (Lemma 7), is a contribution in its own right and could potentially lead
to other scalable RL methods for networked systems, including the warm start, TD-Œª variants and
Q-learning/SARSA type methods. We leave these directions as future work.

References
Dimitri P Bertsekas and John N Tsitsiklis. Neuro-dynamic programming, volume 5. Athena Scientific Belmont, MA, 1996.
Vincent D Blondel and John N Tsitsiklis. A survey of computational complexity results in systems
and control. Automatica, 36(9):1249‚Äì1274, 2000.
Igor Borovikov, Yunqi Zhao, Ahmad Beirami, Jesse Harder, John Kolen, James Pestrak, Jervis
Pinto, Reza Pourabolghasem, Harold Chaput, Mohsen Sardari, Long Lin, Navid Aghdaie, and
Kazi Zaman. Winning isn‚Äôt everything: Training agents to playtest modern games. In AAAI 2019
Workshop on Reinforcement Learning in Games, 01 2019.
Lucian Bu, Robert Babu, Bart De Schutter, et al. A comprehensive survey of multiagent reinforcement learning. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and
Reviews), 38(2):156‚Äì172, 2008.
Eric Cator and Piet Van Mieghem. Second-order mean-field susceptible-infected-susceptible epidemic threshold. Physical review E, 85(5):056111, 2012.
Deepayan Chakrabarti, Yang Wang, Chenxi Wang, Jurij Leskovec, and Christos Faloutsos. Epidemic thresholds in real networks. ACM Transactions on Information and System Security (TISSEC), 10(4):1, 2008.
Caroline Claus and Craig Boutilier. The dynamics of reinforcement learning in cooperative multiagent systems. AAAI/IAAI, 1998:746‚Äì752, 1998.

10

S CALABLE R EINFORCEMENT L EARNING OF L OCALIZED P OLICIES FOR M ULTI -AGENT N ETWORKED S YSTEMS

Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking deep
reinforcement learning for continuous control. In International Conference on Machine Learning,
pages 1329‚Äì1338, 2016.
Carlos Guestrin, Daphne Koller, Ronald Parr, and Shobha Venkataraman. Efficient solution algorithms for factored mdps. Journal of Artificial Intelligence Research, 19:399‚Äì468, 2003.
Junling Hu and Michael P Wellman. Nash q-learning for general-sum stochastic games. Journal of
machine learning research, 4(Nov):1039‚Äì1069, 2003.
Soummya Kar, JoseÃÅ MF Moura, and H Vincent Poor. Qd-learning: A collaborative distributed strategy for multi-agent reinforcement learning through consensus + innovations. IEEE Transactions
on Signal Processing, 61(7):1848‚Äì1862, 2013.
Michael Kearns and Daphne Koller. Efficient reinforcement learning in factored mdps. In IJCAI,
volume 16, pages 740‚Äì747, 1999.
Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visuomotor policies. The Journal of Machine Learning Research, 17(1):1334‚Äì1373, 2016.
Dong Li, Dongbin Zhao, Qichao Zhang, and Yaran Chen. Reinforcement learning and deep learning based lateral control for autonomous driving [application notes]. IEEE Computational Intelligence Magazine, 14(2):83‚Äì98, 2019.
Michael L Littman. Markov games as a framework for multi-agent reinforcement learning. In
Machine learning proceedings 1994, pages 157‚Äì163. Elsevier, 1994.
Michael L Littman. Value-function reinforcement learning in markov games. Cognitive Systems
Research, 2(1):55‚Äì66, 2001.
Mateu Llas, Pablo M Gleiser, Juan M LoÃÅpez, and Albert Dƒ±ÃÅaz-Guilera. Nonequilibrium phase
transition in a model for the propagation of innovations among economic agents. Physical Review
E, 68(6):066101, 2003.
Andrey Y. Lokhov, Marc MeÃÅzard, and Lenka ZdeborovaÃÅ. Dynamic message-passing equations
for models with unidirectional dynamics. Phys. Rev. E, 91:012811, Jan 2015. doi: 10.1103/
PhysRevE.91.012811. URL https://link.aps.org/doi/10.1103/PhysRevE.91.
012811.
Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-agent
actor-critic for mixed cooperative-competitive environments. In Advances in Neural Information
Processing Systems, pages 6379‚Äì6390, 2017.
Sergio Valcarcel Macua, Jianshu Chen, Santiago Zazo, and Ali H Sayed. Distributed policy evaluation under multiple behavior strategies. IEEE Transactions on Automatic Control, 60(5):1260‚Äì
1274, 2015.
Adwaitvedant Mathkar and Vivek S Borkar. Distributed reinforcement learning via gossip. IEEE
Transactions on Automatic Control, 62(3):1465‚Äì1470, 2017.

11

S CALABLE R EINFORCEMENT L EARNING OF L OCALIZED P OLICIES FOR M ULTI -AGENT N ETWORKED S YSTEMS

Laetitia Matignon, Guillaume J Laurent, and Nadine Le Fort-Piat. Independent reinforcement learners in cooperative markov games: a survey regarding coordination problems. The Knowledge
Engineering Review, 27(1):1‚Äì31, 2012.
Wenjun Mei, Shadi Mohagheghi, Sandro Zampieri, and Francesco Bullo. On the dynamics of
deterministic epidemic propagation over networks. Annual Reviews in Control, 44:116‚Äì128,
2017.
Nicolas Meuleau, Milos Hauskrecht, Kee-Eung Kim, Leonid Peshkin, Leslie Pack Kaelbling,
Thomas L Dean, and Craig Boutilier. Solving very large weakly coupled markov decision processes. In AAAI/IAAI, pages 165‚Äì172, 1998.
Marc Mezard and Andrea Montanari. Information, physics, and computation. Oxford University
Press, 2009.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. Nature, 518(7540):529, 2015.
Ian Osband and Benjamin Van Roy. Near-optimal reinforcement learning in factored mdps. In
Advances in Neural Information Processing Systems, pages 604‚Äì612, 2014.
Christos H Papadimitriou and John N Tsitsiklis. The complexity of optimal queuing network control. Mathematics of Operations Research, 24(2):293‚Äì305, 1999.
Guannan Qu and Na Li. Exploiting fast decaying and locality in multi-agent mdp with tree dependence structure. arXiv preprint arXiv:1909.06900, 2019.
Michael Rotkowitz and Sanjay Lall. A characterization of convex problems in decentralized control.
IEEE transactions on Automatic Control, 50(12):1984‚Äì1996, 2005.
Faryad Darabi Sahneh, Caterina Scoglio, and Piet Van Mieghem. Generalized epidemic mean-field
model for spreading processes over multilayer complex networks. IEEE/ACM Transactions on
Networking (TON), 21(5):1609‚Äì1620, 2013.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. nature, 529(7587):484, 2016.
R Srikant and Lei Ying. Finite-time error bounds for linear stochastic approximation and td learning.
arXiv preprint arXiv:1902.00923, 2019.
Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient
methods for reinforcement learning with function approximation. In Advances in neural information processing systems, pages 1057‚Äì1063, 2000.
John N Tsitsiklis and Benjamin Van Roy. Analysis of temporal-diffference learning with function
approximation. In Advances in neural information processing systems, pages 1075‚Äì1081, 1997.

12

S CALABLE R EINFORCEMENT L EARNING OF L OCALIZED P OLICIES FOR M ULTI -AGENT N ETWORKED S YSTEMS

Werner Vogels, Robbert van Renesse, and Ken Birman. The power of epidemics: Robust communication for large-scale distributed systems. SIGCOMM Comput. Commun. Rev., 33(1):131‚Äì135,
January 2003. ISSN 0146-4833. doi: 10.1145/774763.774784. URL http://doi.acm.
org/10.1145/774763.774784.
Hoi-To Wai, Zhuoran Yang, Zhaoran Wang, and Mingyi Hong. Multi-agent reinforcement learning
via double averaging primal-dual optimization. arXiv preprint arXiv:1806.00877, 2018.
Peter Whittle. Restless bandits: Activity allocation in a changing world. Journal of applied probability, 25(A):287‚Äì298, 1988.
Zijian Wu, Qing-Shan Jia, and Xiaohong Guan. Optimal control of multiroom hvac system: An
event-based approach. IEEE Transactions on Control Systems Technology, 24(2):662‚Äì669, 2016.
Kaiqing Zhang, Zhuoran Yang, Han Liu, Tong Zhang, and Tamer BasÃßar. Fully decentralized multiagent reinforcement learning with networked agents. arXiv preprint arXiv:1802.08757, 2018.
Kaiqing Zhang, Zhuoran Yang, and Tamer BasÃßar. Multi-agent reinforcement learning: A selective
overview of theories and algorithms. arXiv preprint arXiv:1911.10635, 2019.
Rick Zhang and Marco Pavone. Control of robotic mobility-on-demand systems: a queueingtheoretical perspective. The International Journal of Robotics Research, 35(1-3):186‚Äì203, 2016.
Xuan Zhang, Wenbo Shi, Bin Yan, Ali Malkawi, and Na Li. Decentralized and distributed temperature control via hvac systems in energy efficient buildings. arXiv preprint arXiv:1702.03308,
2017.
Alessandro Zocca. Temporal starvation in multi-channel csma networks: an analytical framework.
Queueing Systems, 91(3-4):241‚Äì263, 2019.
Shaofeng Zou, Tengyu Xu, and Yingbin Liang. Finite-sample analysis for sarsa with linear function
approximation. In Advances in Neural Information Processing Systems, pages 8665‚Äì8675, 2019.

Appendix A. The Exponential Decay Property
Our main results depend on the (c, œÅ)-exponential decay of the Q-function (cf. Definition 2), which
Œ∫ and s0 Œ∫ , aN Œ∫ , aN Œ∫ and a0 Œ∫ ,
means that for any i, any sNiŒ∫ , sN‚àíi
N
N
i
‚àíi
‚àíi

‚àíi

Œ∏
0
0
Œ∫+1
Œ∫ , aN Œ∫ , aN Œ∫ ) ‚àí Q (sN Œ∫ , s Œ∫ , aN Œ∫ , a Œ∫ )| ‚â§ cœÅ
|QŒ∏i (sNiŒ∫ , sN‚àíi
.
i
N‚àíi
N‚àíi
i
‚àíi
i
i

In Section 3.1, we have pointed out in Lemma 3 that the (c, œÅ)-exponential decay property always holds with œÅ being set to the discounting factor Œ≥, assuming the rewards ri are upper bounded.
We now provide the proof of Lemma 3.
Œ∫ ), a = (aN Œ∫ , aN Œ∫ ); s0 =
Proof of Lemma 3. For notational simplicity, denote s = (sNiŒ∫ , sN‚àíi
i
‚àíi
(sNiŒ∫ , s0N Œ∫ ) and a0 = (aNiŒ∫ , a0N Œ∫ ). Let œÄt,i be the distribution of (si (t), ai (t)) conditioned on
‚àíi
‚àíi
0 be the distribution of (s (t), a (t)) conditioned
(s(0), a(0)) = (s, a) under policy Œ∏, and let œÄt,i
i
i
0
0
0 for all t ‚â§ Œ∫. The
on (s(0), a(0)) = (s , a ) under policy Œ∏. Then, we must have œÄt,i = œÄt,i
13

S CALABLE R EINFORCEMENT L EARNING OF L OCALIZED P OLICIES FOR M ULTI -AGENT N ETWORKED S YSTEMS

reason is that, due to the local dependence structure (1) and the localized policy structure, œÄt,i
only depends on (sNit , aNit ) (the initial state-action of agent i‚Äôth t-hop neighborhood) which is the
same as (s0N t , s0N t ) when t ‚â§ Œ∫ per the way the initial state (s, a), (s0 , a0 ) are chosen. With these
i

i

definitions, we expand the definition of QŒ∏i in (3),
|QŒ∏i (s, a) ‚àí QŒ∏i (s0 , a0 )|
‚àû
X




‚â§
E Œ≥ t ri (si (t), ai (t)) (s(0), a(0)) = (s, a) ‚àí E Œ≥ t ri (si (t), ai (t)) (s(0), a(0)) = (s0 , a0 )
t=0

=
=
‚â§

‚àû
X

0 ri (si , ai )
Œ≥ t E(si ,ai )‚àºœÄt,i ri (si , ai ) ‚àí Œ≥ t E(si ,ai )‚àºœÄt,i

t=0
‚àû
X
t=Œ∫+1
‚àû
X
t=Œ∫+1

0 ri (si , ai )
Œ≥ t E(si ,ai )‚àºœÄt,i ri (si , ai ) ‚àí Œ≥ t E(si ,ai )‚àºœÄt,i

0
Œ≥ t rÃÑTV(œÄt,i , œÄt,i
)‚â§

rÃÑ
Œ≥ Œ∫+1 ,
1‚àíŒ≥

(10)

0 ) is the total variation distance between œÄ and œÄ 0 which is upper bounded by
where TV(œÄt,i , œÄt,i
t,i
t,i
rÃÑ
1. The above inequality shows that the ( 1‚àíŒ≥
, Œ≥)-exponential decay property holds and concludes
the proof of Lemma 3.

Lemma 3 shows that the (c, œÅ)-exponential decay property automatically holds with œÅ being the
discounting factor Œ≥, without any assumption on the transition probabilities except for the factorization structure (1) and the localized policy structure. However, in practice, typically the Markov
chain is ergodic and has fast mixing property. The following Lemma 6 shows that when some fast
mixing holds, then the (c, œÅ)-exponential decay property holds for some œÅ < Œ≥.

Lemma 6 Suppose ri is upper bounded by rÃÑ for all i, and assume there exists c0 > 0 and ¬µ ‚àà
(0, 1) s.t. under any policy Œ∏, the Markov chain is ergodic and starting from any initial state,
TV(œÄt,i , œÄ‚àû,i ) ‚â§ c0 ¬µt where œÄt,i is the distribution of (si (t), ai (t)) and œÄ‚àû,i is the distribution for
2c0 rÃÑ
(si , ai ) in stationarity. Then, the ( 1‚àíŒ≥¬µ
, Œ≥¬µ)-exponential decay property holds.
Proof The proof is almost identical to that of Lemma 3. The only change is that in step (10), we
0 ) ‚â§ 2c0 ¬µt .
use TV(œÄt,i , œÄt,i
The condition on mixing rate in Lemma 6 is similar to those used in the literature on finite-time analysis of RL methods, e.g. Zou et al. (2019). In fact, our condition is weaker than the common mixing
rate condition in that we only require the distribution of the local state-action pair (si (t), ai (t)) to
mix, instead of the full state-action pair (s(t), a(t)). We leave it as future work to study such ‚Äúlocal‚Äù
mixing behavior and its relation to the local transition probabilities (1).

14

S CALABLE R EINFORCEMENT L EARNING OF L OCALIZED P OLICIES FOR M ULTI -AGENT N ETWORKED S YSTEMS

Appendix B. Proof of Lemma 4
We first show that the truncated Q function is a good approximation of the true Q function. To see
that, we have for any (s, a) ‚àà S √ó A, by (5) and (6),
|QÃÇŒ∏i (sNiŒ∫ , aNiŒ∫ ) ‚àí QŒ∏i (s, a)|
X
0
Œ∏
0
Œ∏
0
Œ∫ , aN Œ∫ ; sN Œ∫ , aN Œ∫ )Qi (sN Œ∫ , sN Œ∫ , aN Œ∫ , aN Œ∫ ) ‚àí Qi (sN Œ∫ , sN Œ∫ , aN Œ∫ , aN Œ∫ )
=
wi (s0N‚àíi
‚àíi
i
‚àíi
i
i
i
i
i
‚àíi
‚àíi
‚àíi
s0N Œ∫ ,a0N Œ∫
‚àíi

‚àíi

X

‚â§

0
Œ∏
0
0
Œ∏
Œ∫ , aN Œ∫ ; sN Œ∫ , aN Œ∫ ) Qi (sN Œ∫ , sN Œ∫ , aN Œ∫ , aN Œ∫ ) ‚àí Qi (sN Œ∫ , sN Œ∫ , aN Œ∫ , aN Œ∫ )
wi (s0N‚àíi
i
i
i
i
i
‚àíi
i
‚àíi
‚àíi
‚àíi
‚àíi

s0N Œ∫ ,a0N Œ∫
‚àíi

Œ∫+1

‚â§ cœÅ

‚àíi

,

(11)

where in the last step, we have used the (c, œÅ) exponential decay property, cf. Definition 2.
Next, recall by the policy gradient theorem (Lemma 1),
1
E Œ∏
QŒ∏ (s, a)‚àáŒ∏i log Œ∂ Œ∏ (a|s)
Œ∏
1 ‚àí Œ≥ s‚àºœÄ ,a‚àºŒ∂ (¬∑|s)
1
=
E Œ∏
QŒ∏ (s, a)‚àáŒ∏i log Œ∂iŒ∏i (ai |si ),
Œ∏
1 ‚àí Œ≥ s‚àºœÄ ,a‚àºŒ∂ (¬∑|s)
P
Œ∏
where we have used ‚àáŒ∏i log Œ∂ Œ∏ (a|s) = ‚àáŒ∏i j‚ààN log Œ∂j j (aj |sj ) = ‚àáŒ∏i log Œ∂iŒ∏i (ai |si ) by the localized policy structure. With the above equation, we can compute hÃÇi (Œ∏) ‚àí ‚àáŒ∏i J(Œ∏),
‚àáŒ∏i J(Œ∏) =

hÃÇi (Œ∏) ‚àí ‚àáŒ∏i J(Œ∏)
i
h1 X
1
=
QÃÇŒ∏j (sNjŒ∫ , aNjŒ∫ ) ‚àí QŒ∏ (s, a) ‚àáŒ∏i log Œ∂iŒ∏i (ai |si )
Es‚àºœÄŒ∏ ,a‚àºŒ∂ Œ∏ (¬∑|s)
1‚àíŒ≥
n
j‚ààNiŒ∫
h1 X
i
1
1 X Œ∏
=
Es‚àºœÄŒ∏ ,a‚àºŒ∂ Œ∏ (¬∑|s)
QÃÇŒ∏j (sNjŒ∫ , aNjŒ∫ ) ‚àí
Qj (s, a) ‚àáŒ∏i log Œ∂iŒ∏i (ai |si )
1‚àíŒ≥
n
n
j‚ààN

j‚ààN

1
1 X Œ∏
Es‚àºœÄŒ∏ ,a‚àºŒ∂ Œ∏ (¬∑|s)
‚àí
QÃÇj (sNjŒ∫ , aNjŒ∫ )‚àáŒ∏i log Œ∂iŒ∏i (ai |si )
1‚àíŒ≥
n
Œ∫
j‚ààN‚àíi

:= E1 ‚àí E2 .
Œ∫ ,
We claim that E2 = 0. To see this, consider for any j ‚àà N‚àíi

Es‚àºœÄŒ∏ ,a‚àºŒ∂ Œ∏ (¬∑|s) ‚àáŒ∏i log Œ∂iŒ∏i (ai |si )QÃÇŒ∏j (sNjŒ∫ , aNjŒ∫ )
=

X

=

X

œÄ Œ∏ (s)

s,a

s,a

n
Y

Œ∂`Œ∏` (a` |s` )

`=1
Œ∏

œÄ (s)

Y

Œ∂iŒ∏i (ai |si )

QÃÇŒ∏j (sNjŒ∫ , aNjŒ∫ )

Œ∂`Œ∏` (a` |s` )‚àáŒ∏i Œ∂iŒ∏i (ai |si )QÃÇŒ∏j (sNjŒ∫ , aNjŒ∫ )

`6=i

X

=

‚àáŒ∏i Œ∂iŒ∏i (ai |si )

s,a1 ,...,ai‚àí1 ,ai+1 ,...,an

œÄ Œ∏ (s)

Y

Œ∂`Œ∏` (a` |s` )QÃÇŒ∏j (sNjŒ∫ , aNjŒ∫ )

X
ai

`6=i

= 0,
15

‚àáŒ∏i Œ∂iŒ∏i (ai |si )

S CALABLE R EINFORCEMENT L EARNING OF L OCALIZED P OLICIES FOR M ULTI -AGENT N ETWORKED S YSTEMS

where in the last equality, we have used QÃÇŒ∏j (sNjŒ∫ , aNjŒ∫ ) does not depend on ai as i 6‚àà NjŒ∫ ; and
P
P Œ∏i
Œ∏i
ai Œ∂i (ai |si ) = ‚àáŒ∏i 1 = 0. Now that we have shown E2 = 0, we can
ai ‚àáŒ∏i Œ∂i (ai |si ) = ‚àáŒ∏i
bound E1 as follows
khÃÇi (Œ∏) ‚àí ‚àáŒ∏i J(Œ∏)k = kE1 k
1 X Œ∏
1
‚â§
Es‚àºœÄŒ∏ ,a‚àºŒ∂ Œ∏ (¬∑|s)
QÃÇj (sNjŒ∫ , aNjŒ∫ ) ‚àí QŒ∏j (s, a) k‚àáŒ∏i log Œ∂iŒ∏i (ai |si )k
1‚àíŒ≥
n
j‚ààN

1
cœÅŒ∫+1 Li ,
‚â§
1‚àíŒ≥
where in the last step, we have used (11) and the upper bound k‚àáŒ∏i log Œ∂iŒ∏i (ai |si )k ‚â§ Li . This
concludes the proof of Lemma 4.


Appendix C. Analysis of the Critic
In this section we provide an analysis of the error bound associated with the critic component of our
framework. More specifically, recall that within iteration m the inner loop update is
Œ∫
Œ∫
QÃÇti (sNiŒ∫ (t ‚àí 1), aNiŒ∫ (t ‚àí 1)) = (1 ‚àí Œ±t‚àí1 )QÃÇt‚àí1
i (sNi (t ‚àí 1), aNi (t ‚àí 1))
Œ∫
Œ∫
+ Œ±t‚àí1 (ri (si (t ‚àí 1), ai (t ‚àí 1)) + Œ≥ QÃÇt‚àí1
i (sNi (t), aNi (t))), (12)

QÃÇti (sNiŒ∫ , aNiŒ∫ ) = QÃÇit‚àí1 (sNiŒ∫ , aNiŒ∫ ) for (sNiŒ∫ , aNiŒ∫ ) 6= (sNiŒ∫ (t ‚àí 1), aNiŒ∫ (t ‚àí 1)),
(13)
S

Œ∫ √óA

Œ∫

h
N
i is initialized to be all zero vector, and Œ±t =
where QÃÇ0i ‚àà R Ni
t+t0 is the step size. We
note that when implementing (12) and (13) within outer loop iteration m, (s(t), a(t)) is a random
Œ∏(m)
trajectory generated by the agents taking a fixed policy Œ∏(m). Let Qi
‚àà RS√óA be the true
Q-function for reward ri under this fixed policy Œ∏(m) as defined in (3).
Given the above notation, the specific goal of this section is to prove the following theorem,
Œ∏(m)
which bounds the error between the approximation QÃÇTi generated by (12), (13) and the true Qi .

Theorem 7 Assume Assumption 1, 2, 3 are true and suppose t0 , h satisfies, h ‚â• œÉ1 max(2, 1‚àí1‚àöŒ≥ )
and t0 ‚â• max(2h, 4œÉh, œÑ ). Then, inside outer loop iteration m, for each i ‚àà N , with probability
at least 1 ‚àí Œ¥, we have the following error bound,
Œ∏(m)

sup
(s,a)‚ààS√óA

Qi

Ca
Ca0
2cœÅŒ∫+1
(s, a) ‚àí QÃÇTi (sNiŒ∫ , aNiŒ∫ ) ‚â§ ‚àö
+
+
,
T + t0 T + t0 (1 ‚àí Œ≥)2

where
6¬Ø

Ca =
‚àö
1‚àí Œ≥

r

œÑh
2œÑ T 2
2
16¬Ø
hœÑ 2rÃÑ
[log(
) + f (Œ∫) log SA], Ca0 =
,
(œÑ + t0 )),
‚àö max(
œÉ
Œ¥
1‚àí Œ≥
œÉ
1‚àíŒ≥

rÃÑ
with ¬Ø = 4 1‚àíŒ≥
+ 2rÃÑ.

16

S CALABLE R EINFORCEMENT L EARNING OF L OCALIZED P OLICIES FOR M ULTI -AGENT N ETWORKED S YSTEMS

C.1. Overview of the proof of Theorem 7
Since Theorem 7 is entirely about a particular outer-loop iteration m, inside which the policy is fixed
to be Œ∏(m), to simplify notation we drop the dependence on m and Œ∏(m) throughout this section.
Œ∏(m)
Particularly, we refer to Qi
as Q‚àói . Since Q‚àói is the true Q-function for reward ri under policy
Œ∏(m), it must satisfy the Bellman equation (Bertsekas and Tsitsiklis, 1996),
Q‚àói = TD(Q‚àói ) := ri + Œ≥P Q‚àói ,

(14)

where TD : RS√óA ‚Üí RS√óA is the standard Bellman operator for reward ri and P is the transition
probability from s(t), a(t) to s(t + 1), a(t + 1) under policy Œ∏(m). Note in (14), without causing
any confusion, ri is interpreted as a vector in RS√óA although ri only depends on (si , ai ).
Theorem 7 essentially says that the critic iterate QÃÇti in (12) (13) will become a good estimate
of Q‚àói as t increases. Our proof is divided into 5 steps. In Step 1, we rewrite (12) and (13) in
a linear update form (cf. (16)). Then, the averaged behavior of the linear update form will be
studied in Step 2 (cf. Lemma 8). In Step 3, we decompose the error into a recursive form (cf.
Lemma 11), and in Step 4, we bound a certain martingale difference-like sequence (cf. Lemma 12
and Lemma 13). Finally, in Step 5, we use the recursive error decomposition and the bound on the
martingale difference-like sequence to prove Theorem 7.
Step 1: Writing the critic update in linear form. To simplify notation, we use the following
definitions. We use z = (s, a) ‚àà Z = S√óA to represent a particular state action pair (s, a) ‚àà S√óA.
Similarly, we define zi = (si , ai ) ‚àà Zi = Si √ó Ai , and zNiŒ∫ = (sNiŒ∫ , aNiŒ∫ ) ‚àà ZNiŒ∫ = SNiŒ∫ √ó ANiŒ∫ .
Z

Œ∫

Also, define ezN Œ∫ to be the indicator vector in R Ni , i.e. the zNiŒ∫ ‚Äôth entry of ezN Œ∫ is 1 and other
i
i
entries are zero. Then, the critic update equations (12) and (13) can be written as,
t‚àí1
Œ∫
Œ∫
QÃÇti = QÃÇt‚àí1
+ Œ±t‚àí1 [ri (zi (t ‚àí 1)) + Œ≥ QÃÇt‚àí1
i
i (zNi (t)) ‚àí QÃÇi (zNi (t ‚àí 1))]ezN Œ∫ (t‚àí1) ,

(15)

i

with QÃÇ0i being the all zero vector in R
following definition

ZN Œ∫
i

t‚àí1
>
Œ∫
. Notice that QÃÇt‚àí1
i (zNi ) = ezN Œ∫ QÃÇi , we can make the
i

A(z, z 0 ) = ezN Œ∫ [Œ≥e>
z0
i

NŒ∫
i

‚àí e>
zN Œ∫ ] ‚àà R

b(z) = ezN Œ∫ ri (zi ) ‚àà R

ZN Œ∫ √óZN Œ∫
i

i

,

i

ZN Œ∫
i

,

i

and rewrite (15) in a linear form


QÃÇti = QÃÇt‚àí1
+ Œ±t‚àí1 A(z(t ‚àí 1), z(t))QÃÇt‚àí1
+ b(z(t ‚àí 1)) .
i
i

(16)

Step 2: Analyze the average behavior of A, b. Recall that P is transition matrix from z(t ‚àí 1)
to z(t). We define,
AÃÉ(z) = Ez 0 ‚àºP (¬∑|z) A(z, z 0 ) = ezN Œ∫ [Œ≥P (¬∑|z)Œ¶ ‚àí eTzN Œ∫ ],
i

(17)

i

where P (¬∑|z) is understood as the z‚Äôth row of P and is treated as a row vector. Also, we have
Z√óZN Œ∫
i to be a matrix with each row indexed by z ‚àà Z and each column indexed by
defined Œ¶ ‚àà R
>
0
0
zN
Œ∫ ‚àà ZN Œ∫ . Further, the z‚Äôth row of Œ¶ is the indicator vector ez Œ∫ , in other words Œ¶(z, zN Œ∫ ) = 1
i
N
i

i

17

i

S CALABLE R EINFORCEMENT L EARNING OF L OCALIZED P OLICIES FOR M ULTI -AGENT N ETWORKED S YSTEMS

0
0
if zN
Œ∫ = zN Œ∫ and Œ¶(z, zN Œ∫ ) = 0 elsewhere. We further define, given any distribution d on the
i
i
i
state-action pair z, the ‚Äúaveraged‚Äù A and b,

AÃÑd = Ez‚àºd AÃÉ(z)
X
=
d(z)ezN Œ∫ [Œ≥P (¬∑|z)Œ¶ ‚àí e>
zN Œ∫ ]
i

z‚ààZ
>

i



= Œ¶ diag(d) Œ≥P Œ¶ ‚àí Œ¶ ,
>

d

bÃÑ = Ez‚àºd b(z) = Œ¶ diag(d)ri ,

(18)
(19)

where diag(d) ‚àà RZ√óZ is a diagonal matrix with the z‚Äôth diagonal entry being d(z); in the last
equation, ri is understood as a vector over the entire state-action space Z, though it only depends on
zi . The goal of this step is to show the following lemma, which shows a certain contraction property
for the ‚Äúaveraged‚Äù A and b. The proof is postponed to Section C.2.
Lemma 8 Given distribution d on state-action pair z whose marginalization onto zNiŒ∫ is non-zero
for every zNiŒ∫ , we have AÃÑd QÃÇi + bÃÑd can be written as
AÃÑd QÃÇi + bÃÑd = ‚àíDQÃÇi + Dg d (QÃÇi ),
Z

Œ∫ √óZ

Œ∫

where D = Œ¶> diag(d)Œ¶ ‚àà R Ni Ni is a diagonal matrix, with the zNiŒ∫ ‚Äôth entry being the
marginalized distribution of zNiŒ∫ under distribution d; g d (¬∑) is given by g d (QÃÇi ) = Œ†d TDŒ¶QÃÇi ,
where Œ†d = (Œ¶> diag(d)Œ¶)‚àí1 Œ¶> diag(d) and TD(Qi ) = ri + Œ≥P Qi is the Bellman operator in
(14).
Z Œ∫
Further, g d (¬∑) is Œ≥ contractive in infinity norm, and has a unique fixed point QÃÇdi ‚àà R Ni
depending on d, and the fixed point satisfies
kŒ¶QÃÇdi ‚àí Q‚àói k‚àû ‚â§

cœÅŒ∫+1
.
1‚àíŒ≥

Step 3: Decomposition of the error. Recall the update for QÃÇti is

QÃÇti = QÃÇit‚àí1 + Œ±t‚àí1 A(z(t ‚àí 1), z(t))QÃÇt‚àí1
+ b(z(t ‚àí 1))].
i

(20)

(21)

We define the following simplifying notations,
At‚àí1 = A(z(t ‚àí 1), z(t)),
bt‚àí1 = b(z(t ‚àí 1)).
Let Ft be the œÉ-algebra generated by z(0), . . . , z(t). Then, clearly At‚àí1 is Ft -measurable and bt‚àí1
is Ft‚àí1 measurable. As a result, QÃÇti is Ft -measurable. Let œÑ > 0 to be the integer in Assumption 3.
Let dt‚àí1 be the distribution of z(t ‚àí 1) conditioned on Ft‚àíœÑ . Further define,
AÃÑt‚àí1 = AÃÑdt‚àí1 ,

18

bÃÑt‚àí1 = bÃÑdt‚àí1 ,

S CALABLE R EINFORCEMENT L EARNING OF L OCALIZED P OLICIES FOR M ULTI -AGENT N ETWORKED S YSTEMS

i.e. the ‚Äúaveraged‚Äù A and b under distribution dt‚àí1 . It is clear that dt‚àí1 , AÃÑt‚àí1 , bÃÑt‚àí1 are all Ft‚àíœÑ
measurable random vectors (matrices). With these notations, (21) can be rewritten as,

QÃÇti = QÃÇt‚àí1
+ Œ±t‚àí1 At‚àí1 QÃÇit‚àí1 + bt‚àí1 ]
i

= QÃÇt‚àí1
+ Œ±t‚àí1 AÃÑt‚àí1 QÃÇit‚àí1 + bÃÑt‚àí1 ] + Œ±t‚àí1 [(At‚àí1 ‚àí AÃÑt‚àí1 )QÃÇt‚àí1
+ bt‚àí1 ‚àí bÃÑt‚àí1 ]
i
i

= QÃÇit‚àí1 + Œ±t‚àí1 AÃÑt‚àí1 QÃÇit‚àí1 + bÃÑt‚àí1 ]
‚àí QÃÇt‚àíœÑ
+ Œ±t‚àí1 [(At‚àí1 ‚àí AÃÑt‚àí1 )QÃÇt‚àíœÑ
+ bt‚àí1 ‚àí bÃÑt‚àí1 ] +Œ±t‚àí1 (At‚àí1 ‚àí AÃÑt‚àí1 )(QÃÇt‚àí1
i ), (22)
i
{z
}
{z i
}
|
|
:=t‚àí1

:=œÜt‚àí1

where in the last step, we have defined sequence t‚àí1 and œÜt‚àí1 . We have the following auxiliary
lemma that provides upper bounds for QÃÇti , t and œÜt , which will be frequently used in the rest of the
proof. The proof of Lemma 9 is postponed to Section C.3.
Lemma 9 We have the following upper bounds.
(a) kQÃÇti k‚àû ‚â§

rÃÑ
1‚àíŒ≥

almost surely.

rÃÑ
(b) kt k‚àû ‚â§ ¬Ø = 4 1‚àíŒ≥
+ 2rÃÑ almost surely.

(c) kœÜt k‚àû ‚â§ 2¬Ø


Pt‚àí1

k=t‚àíœÑ +1 Œ±k

almost surely.

By Lemma 8, we have for each t, there exists diagonal matrix Dt‚àí1 and operator gt‚àí1 s.t.
AÃÑt‚àí1 QÃÇit‚àí1 + bÃÑt‚àí1 = ‚àíDt‚àí1 QÃÇt‚àí1
+ Dt‚àí1 gt‚àí1 (QÃÇt‚àí1
i
i ),

(23)
d

where by Lemma 8, gt‚àí1 is a Œ≥-contraction in infinity norm, with unique fixed point QÃÇi t‚àí1 satifying
d

kŒ¶QÃÇi t‚àí1 ‚àí Q‚àói k‚àû ‚â§
Z

Œ∫ √óZ

cœÅŒ∫+1
.
1‚àíŒ≥

(24)

Œ∫

Further, by Lemma 8 Dt‚àí1 ‚àà R Ni Ni is a diagonal matrix, with the zNiŒ∫ ‚Äôth entry being
dt‚àí1 (zNiŒ∫ ), the marginalized distribution of zNiŒ∫ under dt‚àí1 . Since dt‚àí1 is the distribution of z(t‚àí1)
conditioned on Ft‚àíœÑ , by Assumption 3, we have almost surely,
Dt‚àí1  œÉI,

(25)

where œÉ > 0 is from Assumption 3.
With these preparations, we plug (23) into (22) and expand it recursively, getting,
QÃÇti = (I ‚àí Œ±t‚àí1 Dt‚àí1 )QÃÇt‚àí1
+ Œ±t‚àí1 Dt‚àí1 gt‚àí1 (QÃÇt‚àí1
i
i ) + Œ±t‚àí1 t‚àí1 + Œ±t‚àí1 œÜt‚àí1
=

t‚àí1
Y
k=œÑ

(I ‚àí Œ±k Dk )QÃÇœÑi +

t‚àí1
X

t‚àí1
Y

Œ±k Dk

t‚àí1
X

(I ‚àí Œ±` D` )gk (QÃÇki ) +

`=k+1

k=œÑ

k=œÑ

Œ±k

t‚àí1
Y

(I ‚àí Œ±` D` )(k + œÜk ).

`=k+1

(26)
We use the following notation:
Bk,t = Œ±k Dk

t‚àí1
Y

(I ‚àí Œ±` D` ), BÃÉk,t =

`=k+1

t‚àí1
Y

(I ‚àí Œ±` D` ).

`=k+1

19

S CALABLE R EINFORCEMENT L EARNING OF L OCALIZED P OLICIES FOR M ULTI -AGENT N ETWORKED S YSTEMS

It is then immediately clear that
BÃÉœÑ ‚àí1,t +

t‚àí1
X

Bk,t = I.

(27)

k=œÑ

We also define

t‚àí1
Y

Œ≤k,t = Œ±k

(1 ‚àí Œ±` œÉ),

Œ≤ÃÉk,t =

`=k+1

t‚àí1
Y

(1 ‚àí Œ±` œÉ).

`=k+1

Since every diagonal entry of D` is lower bounded by œÉ almost surely (cf. (25)), we have every
entry of Bk,t is upper bounded by Œ≤k,t and every entry of BÃÉk,t is upper bounded by Œ≤ÃÉk,t almost
surely. We have the following lemma on the Œ≤k,t , Œ≤ÃÉk,t sequence which we will frequently use later.
The proof of Lemma 10 is provided in Section C.3.
h
, where t0 ‚â• h > œÉ2 and t0 ‚â• 4œÉh, and t0 ‚â• œÑ , then Œ≤k,t , Œ≤ÃÉk,t satisfies the
Lemma 10 If Œ±t = t+t
0
following.

œÉh
œÉh

k+1+t0
k+1+t0
h
(a) Œ≤k,t ‚â§ k+t
,
Œ≤ÃÉ
‚â§
.
k,t
t+t0
t+t0
0

(b)

Pt‚àí1

(c)

Pt‚àí1

2
k=1 Œ≤k,t
k=œÑ

Œ≤k,t

‚â§

2h
1
œÉ (t+t0 ) .

Pk‚àí1

`=k‚àíœÑ +1 Œ±`

‚â§

8hœÑ 1
œÉ t+t0 .

Next, (26) can be rewritten as
QÃÇti = BÃÉœÑ ‚àí1,t QÃÇœÑi +

t‚àí1
X

Bk,t gk (QÃÇki ) +

k=œÑ

t‚àí1
X

Œ±k BÃÉk,t k +

t‚àí1
X

Œ±k BÃÉk,t œÜk .

(28)

k=œÑ

k=œÑ

The goal of this step is to decompose the error. Let at = kŒ¶QÃÇti ‚àí Q‚àói k‚àû = supz‚ààZ |QÃÇti (zNiŒ∫ ) ‚àí
be the error at time t. From (28), and also utilizing the Œ≥-contraction of gk as well as the
property of the fixed point of gk (24), we have the following Lemma, which decomposes the error
in a resursive form. The proof of Lemma 11 is postponed to Section C.4.

Q‚àói (z)|

Lemma 11 Let at = kŒ¶QÃÇti ‚àí Q‚àói k‚àû . The following recursion holds almost surely,
at ‚â§ Œ≤ÃÉœÑ ‚àí1,t aœÑ + Œ≥

sup

t‚àí1
X

zN Œ∫ ‚ààZN Œ∫ k=œÑ
i
i

bk,t (zNiŒ∫ )ak +

t‚àí1

t‚àí1

k=œÑ

k=œÑ

X
X
2cœÅŒ∫+1
+k
Œ±k BÃÉk,t k k‚àû + k
Œ±k BÃÉk,t œÜk k‚àû ,
1‚àíŒ≥

Q
where bk,t (zNiŒ∫ ) is the zNiŒ∫ ‚Äôth diagonal entry of Bk,t , and bk,t (zNiŒ∫ ) = Œ±k dk (zNiŒ∫ ) t‚àí1
`=k+1 (1 ‚àí
Œ±` d` (zNiŒ∫ )), where dk (zNiŒ∫ ) is the zNiŒ∫ ‚Äôth diagonal entry of Dk satisfying dk (zNiŒ∫ ) ‚â• œÉ.
P
From
Lemma 11, it is clear that to bound the error at , we need to bound k t‚àí1
k=œÑ Œ±k BÃÉk,t k k‚àû
Pt‚àí1
and k k=œÑ Œ±k BÃÉk,t œÜk k‚àû , which is the focus of the next step.
Pt‚àí1
Step
4:
Bound
the

and
the
œÜ
-sequence.
The
goal
of
this
step
is
to
bound
k
k
k
k=œÑ Œ±k BÃÉk,t k k‚àû
Pt‚àí1
and k k=œÑ Œ±k BÃÉk,t œÜk k‚àû . Recall that,
t‚àí1 = (At‚àí1 ‚àí AÃÑt‚àí1 )QÃÇt‚àíœÑ
+ bt‚àí1 ‚àí bÃÑt‚àí1 ,
i
œÜt‚àí1 = (At‚àí1 ‚àí AÃÑt‚àí1 )(QÃÇt‚àí1
‚àí QÃÇt‚àíœÑ
i
i ).
20

S CALABLE R EINFORCEMENT L EARNING OF L OCALIZED P OLICIES FOR M ULTI -AGENT N ETWORKED S YSTEMS

Clearly, t‚àí1 is Ft -measurable, and satisfies
Et‚àí1 |Ft‚àíœÑ = E[(At‚àí1 ‚àí AÃÑt‚àí1 )QÃÇt‚àíœÑ
+ bt‚àí1 ‚àí bÃÑt‚àí1 |Ft‚àíœÑ ]
i
= E[(At‚àí1 ‚àí AÃÑt‚àí1 )|Ft‚àíœÑ ]QÃÇt‚àíœÑ
+ E[bt‚àí1 ‚àí bÃÑt‚àí1 |Ft‚àíœÑ ]
i
= 0,

(29)

where in the last equality we have used
E[At‚àí1 |Ft‚àíœÑ ] = E[A(z(t ‚àí 1), z(t))|Ft‚àíœÑ ] = EAÃÉ(z(t ‚àí 1))|Ft‚àíœÑ = AÃÑdt‚àí1 = AÃÑt‚àí1 ,
E[bt‚àí1 |Ft‚àíœÑ ] = Eb(z(t ‚àí 1))|Ft‚àíœÑ = bÃÑdt‚àí1 = bÃÑt‚àí1 ,
per the definition of dt‚àí1 .
P
Equation (29) shows that t‚àí1 is a ‚Äúshifted‚Äù martingale difference sequence.3 Therefore, k t‚àí1
k=œÑ Œ±k BÃÉk,t k k‚àû
can be controlled by Azuma-Hoeffding type inequalities, as shown by Lemma 12. We comment that
BÃÉk,t is also random and BÃÉk,t k is no longer a martingale difference sequence. As a result, to prove
Lemma 12 requires more than direct application of the Azuma-Hoeffding bound. For more details,
see the full proof of Lemma 12 in Appendix C.5.
Lemma 12 We have with probability 1 ‚àí Œ¥,
s
t‚àí1
X
2œÑ t
œÑh
[log(
) + f (Œ∫) log SA].
Œ±k BÃÉk,t k
‚â§ 6¬Ø

œÉ(t + t0 )
Œ¥
‚àû
k=œÑ

Pt‚àí1
Œ±k BÃÉk,t œÜk k‚àû , primarily using the fact each œÜt‚àí1 = (At‚àí1 ‚àí
Finally we bound sequence k k=œÑ
t‚àí1
t‚àíœÑ
AÃÑt‚àí1 )(QÃÇi ‚àí QÃÇi ) can be bounded by the movement of the QÃÇti function after œÑ steps (i.e. kQÃÇt‚àí1
‚àí
i
QÃÇt‚àíœÑ
k
),
which
is
quite
small
due
to
the
step
size
selection.
The
proof
of
Lemma
13
can
also
be
‚àû
i
found in Section C.5.
Lemma 13 The following inequality holds almost surely.
k

t‚àí1
X

Œ±k BÃÉk,t œÜk k‚àû ‚â§

k=œÑ

16¬Ø
hœÑ 1
1
:= CœÜ
.
œÉ t + t0
t + t0

Step 5: bounding the critic error and proof of Theorem 7. We are now ready to use the error
decomposition in Lemma 11 as well as the bound on k , œÜk -sequences in Lemma 12 and Lemma 13
to bound the error of the critic. Recall that Theorem 7 states with probability 1 ‚àí Œ¥,
aT ‚â§ ‚àö
where C0 =

2cœÅŒ∫+1
1‚àíŒ≥ ,

6¬Ø

Ca =
‚àö
1‚àí Œ≥

r

Ca0
C0
Ca
+
+
,
T + t0 T + t0 1 ‚àí Œ≥

(30)

and

œÑh
2œÑ T 2
2
16¬Ø
hœÑ 2rÃÑ
[log(
) + f (Œ∫) log SA], Ca0 =
,
(œÑ + t0 )).
‚àö max(
œÉ
Œ¥
1‚àí Œ≥
œÉ
1‚àíŒ≥

3. It is not a standard martingale difference sequence, which would require Et‚àí1 |Ft‚àí1 = 0.

21

S CALABLE R EINFORCEMENT L EARNING OF L OCALIZED P OLICIES FOR M ULTI -AGENT N ETWORKED S YSTEMS

To prove (30), we start by applying Lemma 12 to t ‚â§ T with Œ¥ replaced by Œ¥/T . Then, using a
union bound, we get with probability 1 ‚àí Œ¥, for any t ‚â§ T ,
t‚àí1
X

Œ±k BÃÉk,t k

k=œÑ

‚àû

‚â§ C ‚àö

1
,
t + t0

q
2
where C = 6¬Ø
 œÑœÉh [log( 2œÑŒ¥T ) + f (Œ∫) log SA]. Combine the above with Lemma 11 and use
Lemma 13, we get with probability 1 ‚àí Œ¥, for all œÑ ‚â§ t ‚â§ T ,

at ‚â§ Œ≤ÃÉœÑ ‚àí1,t aœÑ + Œ≥ sup
zN Œ∫
i

t‚àí1
X

bk,t (zNiŒ∫ )ak + C ‚àö

k=œÑ

1
1
+ CœÜ
+ C0 .
t + t0
t + t0

(31)

We now condition on (31) is true and use induction to show (30). Eq. (30) is true for t = œÑ , as
2rÃÑ
2rÃÑ
> aœÑ , where we have used |aœÑ | ‚â§ kQ‚àói k‚àû + kQÃÇœÑi k‚àû ‚â§ 1‚àíŒ≥
. Then, assume (30)
‚â• 1‚àí2‚àöŒ≥ 1‚àíŒ≥
is true for up to k ‚â§ t ‚àí 1, we have by (31),
Ca0
œÑ +t0

at ‚â§ Œ≤ÃÉœÑ ‚àí1,t aœÑ + Œ≥ sup
zN Œ∫
i

t‚àí1
X

bk,t (zNiŒ∫ )[ ‚àö

k=œÑ
t‚àí1
X

‚â§ Œ≤ÃÉœÑ ‚àí1,t aœÑ + Œ≥Ca sup
zN Œ∫
i

+ C ‚àö

Ca
1
1
Ca0
C0
] + C ‚àö
+ CœÜ
+
+
+ C0
t + t0
t + t0
k + t0 k + t0 1 ‚àí Œ≥

bk,t (zNiŒ∫ ) ‚àö

k=œÑ

t‚àí1
X
1
1
bk,t (zNiŒ∫ )
+ Œ≥Ca0 sup
k + t0
k + t0
zN Œ∫
k=œÑ
i

C0
1
1
+
.
+ CœÜ
t + t0 1 ‚àí Œ≥
t + t0

We use the following auxiliary Lemma, whose proof is provided in Section C.6.
Q
h
Œ∫
Lemma 14 Recall Œ±k = k+t
, and bk,t (zNiŒ∫ ) = Œ±k dk (zNiŒ∫ ) t‚àí1
`=k+1 (1 ‚àí Œ±` d` (zNi )), here
0
‚àö
1
dk (zNiŒ∫ ) ‚â• œÉ. If œÉh(1 ‚àí Œ≥) ‚â• 1, t0 ‚â• 1, and Œ±0 ‚â§ 2 , then, for any zNiŒ∫ , and any 0 < œâ ‚â§ 1,
t‚àí1
X
k=œÑ

bk,t (zNiŒ∫ )

1
1
‚â§‚àö
.
(k + t0 )œâ
Œ≥(t + t0 )œâ

With Lemma 14, and using the bound on Œ≤ÃÉœÑ ‚àí1,t in Lemma 10 (a), we have
‚àö

1
1
1
1
C0
‚àö
+ Œ≥Ca0
+ C ‚àö
+ CœÜ
+
t + t0
t + t0 1 ‚àí Œ≥
t + t0
t + t0
 œÑ + t œÉh
1
1
1
C0
1
‚àö
‚àö
0
‚â§ Œ≥Ca ‚àö
+ Œ≥Ca0
+ CœÜ
+
aœÑ +
.
+ C ‚àö
t
+
t
t
+
t
t
+
t
1
‚àíŒ≥
t + t0
t + t0
0
0
{z 0
}
{z
} |
|

at ‚â§ Œ≤ÃÉœÑ ‚àí1,t aœÑ +

Œ≥Ca ‚àö

:=Ft0

:=Ft

0

Ca
a
and Ft0 ‚â§ t+t
. To see this,
To finish the induction, it suffices to show Ft ‚â§ ‚àöC
t+t0
0
‚àö
t + t0 ‚àö
C
Ft
= Œ≥+
,
Ca
Ca
CœÜ aœÑ (œÑ + t0 ) (œÑ + t0 )œÉh‚àí1
t + t0 ‚àö
Ft0
=
Œ≥
+
+
.
Ca0
Ca0
Ca0
(t + t0 )œÉh‚àí1

22

S CALABLE R EINFORCEMENT L EARNING OF L OCALIZED P OLICIES FOR M ULTI -AGENT N ETWORKED S YSTEMS

So, we can require Ca , Ca0 to be large enough such that
‚àö
‚àö
1‚àí Œ≥
1‚àí Œ≥
CœÜ
C
aœÑ (œÑ + t0 )
‚àö
‚â§ 1 ‚àí Œ≥,
‚â§
,
‚â§
.
Ca
Ca0
2
Ca0
2
2rÃÑ
, one can check our selection of Ca and Ca0 satisfies the above three inequalities,
Using aœÑ ‚â§ 1‚àíŒ≥
and so the induction is finished and the proof of Theorem 7 is concluded.

C.2. Proof of Lemma 8
Z

Œ∫ √óZ

Œ∫

It is easy to check that D = Œ¶> diag(d)Œ¶ ‚àà R Ni Ni is a diagonal matrix, and the zNiŒ∫ ‚Äôth diagonal entry is the marginal probability of zNiŒ∫ under d, which is non-zero by the assumption of the
Z

Œ∫ √óZ

Œ∫

lemma. Therefore, Œ¶> diag(d)Œ¶ ‚àà R Ni Ni is invertable and matrix Œ†d = (Œ¶> diag(d)Œ¶)‚àí1 Œ¶> diag(d)
is well defined. Further, the zNiŒ∫ ‚Äôth row of Œ†d is in fact the conditional distribution of the full state
z given zNiŒ∫ . So, Œ†d must be a stochastic matrix and is non-expansive in infinity norm.
By the definition of AÃÑd and bÃÑd , we have,


AÃÑd QÃÇi + bÃÑd = Œ¶> diag(d) Œ≥P Œ¶ ‚àí Œ¶ QÃÇi + Œ¶> diag(d)ri
= Œ¶> diag(d)[ri + Œ≥P Œ¶QÃÇi ] ‚àí Œ¶> diag(d)Œ¶QÃÇi
= Œ¶> diag(d)TD(Œ¶QÃÇi ) ‚àí Œ¶> diag(d)Œ¶QÃÇi
= ‚àíDQÃÇi + DŒ†d TD(Œ¶QÃÇi )
= ‚àíDQÃÇi + Dg d (QÃÇi ),
where TD is the Bellman operator for reward ri defined in (14), and operator g d is given by g d (QÃÇi ) =
Œ†d TDŒ¶QÃÇi .
Notice that Œ¶ is non-expansive in k ¬∑ k‚àû norm since each row of Œ¶ has precisely one entry being
1 and all others are zero. Also since Œ†d is non-expansive in k ¬∑ k‚àû norm and TD is a Œ≥-contraction
in k ¬∑ k‚àû norm, we have g d = Œ†d TDŒ¶ is a Œ≥ contraction in k ¬∑ k‚àû norm. As a result, g d has a unique
fixed point QÃÇdi .
Finally, we show (20), which bounds the distance between Œ¶QÃÇdi and Q‚àói , where Q‚àói is the true
Q-function for reward ri and it is the unique fixed point of TD operator (14). We have,
kŒ¶QÃÇdi ‚àí Q‚àói k‚àû ‚â§ kŒ¶QÃÇdi ‚àí Œ¶Œ†d Q‚àói k‚àû + kŒ¶Œ†d Q‚àói ‚àí Q‚àói k‚àû
= kŒ¶Œ†d TD(Œ¶QÃÇdi ) ‚àí Œ¶Œ†d TD(Q‚àói )k‚àû + kŒ¶Œ†d Q‚àói ‚àí Q‚àói k‚àû
‚â§ Œ≥kŒ¶QÃÇdi ‚àí Q‚àói k‚àû + kŒ¶Œ†d Q‚àói ‚àí Q‚àói k‚àû ,
where the equality follows from the fact that QÃÇdi is the fixed point of Œ†d TDŒ¶, Q‚àói is the fixed point
of TD; the last inequality is due to Œ¶Œ†d TD is a Œ≥ contration in infinity norm. Therefore,
kŒ¶QÃÇdi ‚àí Q‚àói k‚àû ‚â§

1
kŒ¶Œ†d Q‚àói ‚àí Q‚àói k‚àû .
1‚àíŒ≥

(32)

Next, recall that the zNiŒ∫ ‚Äôs row of Œ†d is the distribution of the state-action pair z conditioned on its
NiŒ∫ coordinates being fixed to be zNiŒ∫ . We denote this conditional distribution of the states outside

23

S CALABLE R EINFORCEMENT L EARNING OF L OCALIZED P OLICIES FOR M ULTI -AGENT N ETWORKED S YSTEMS

Œ∫ , given zN Œ∫ , as d(zN Œ∫ |zN Œ∫ ). With this notation,
of NiŒ∫ , zN‚àíi
i
‚àíi
i

X

(Œ†d Q‚àói )(zNiŒ∫ ) =

‚àó
Œ∫ |z k )Q (zN Œ∫ , zN Œ∫ ).
d(zN‚àíi
i
N
i
‚àíi
i

zN Œ∫

‚àíi

And therefore,
Œ∫ ) =
(Œ¶Œ†d Q‚àói )(zNiŒ∫ , zN‚àíi

X

0
‚àó
0
Œ∫ |zN Œ∫ )Qi (zN Œ∫ , zN Œ∫ ).
d(zN
i
i
‚àíi
‚àíi

0
zN
Œ∫

‚àíi

Further, we have
‚àó
Œ∫ ) ‚àí Q (zN Œ∫ , zN Œ∫ )|
|(Œ¶Œ†d Q‚àói )(zNiŒ∫ , zN‚àíi
i
‚àíi
i
X
X
‚àó
0
0
‚àó
0
Œ∫ |zN Œ∫ )Qi (zN Œ∫ , zN Œ∫ )
Œ∫ |zN Œ∫ )Qi (zN Œ∫ , zN Œ∫ ) ‚àí
d(zN
=
d(zN
‚àíi
i
i
i
i
‚àíi
‚àíi
‚àíi
0
zN
Œ∫

0
zN
Œ∫

‚àíi

‚àíi

‚â§

X

0
Œ∫ |zN Œ∫ )
d(zN
i
‚àíi

0
Œ∫ )
Q‚àói (zNiŒ∫ , zN
‚àíi

‚àí

Œ∫ )
Q‚àói (zNiŒ∫ , zN‚àíi

0
zN
Œ∫
‚àíi

‚â§ cœÅŒ∫+1 ,
where the last inequality is due to the exponential decay property (cf. Definition 2 and Assumption 2). Therefore,
kŒ¶Œ†d Q‚àói ‚àí Q‚àói k‚àû ‚â§ cœÅŒ∫+1 .
Combining the above with (32), we get the desired result
kŒ¶QÃÇdi ‚àí Q‚àói k‚àû ‚â§

cœÅŒ∫+1
.
1‚àíŒ≥


C.3. Proof of Lemma 9 and Lemma 10
In this section, we provide proofs of the two auxiliary lemmas, Lemma 9 and Lemma 10. We start
with the proof of Lemma 9.
Proof of Lemma 9. First, notice that A(z, z 0 ) = ezN Œ∫ [Œ≥eTz0 Œ∫ ‚àí eTzN Œ∫ ] and b(z) = ezN Œ∫ ri (zi ). As
i

such, kA(z, z 0 )k‚àû ‚â§ 1 + Œ≥ < 2, kb(z)k‚àû ‚â§ rÃÑ.

N

i

i

i

Part (a) can be proved by induction. Part (a) is true for t = 0 as QÃÇ0i = 0. Assume kQÃÇt‚àí1
i k‚àû ‚â§
rÃÑ
1‚àíŒ≥ . Recall the update equation (15),
t‚àí1
Œ∫
Œ∫
QÃÇti = QÃÇit‚àí1 + Œ±t‚àí1 [ri (zi (t ‚àí 1)) + Œ≥ QÃÇt‚àí1
i (zNi (t)) ‚àí QÃÇi (zNi (t ‚àí 1))]ezN Œ∫ (t‚àí1) ,
i

or in other words,
t‚àí1
t‚àí1
Œ∫
Œ∫
Œ∫
QÃÇti (zNiŒ∫ (t ‚àí 1)) = QÃÇt‚àí1
i (zNi (t ‚àí 1)) + Œ±t‚àí1 [ri (zi (t ‚àí 1)) + Œ≥ QÃÇi (zNi (t)) ‚àí QÃÇi (zNi (t ‚àí 1))]
t‚àí1
Œ∫
Œ∫
= (1 ‚àí Œ±t‚àí1 )QÃÇt‚àí1
i (zNi (t ‚àí 1)) + Œ±t‚àí1 [ri (zi (t ‚àí 1)) + Œ≥ QÃÇi (zNi (t))].

24

S CALABLE R EINFORCEMENT L EARNING OF L OCALIZED P OLICIES FOR M ULTI -AGENT N ETWORKED S YSTEMS

And for other entries of QÃÇti , it stays the same as QÃÇt‚àí1
i . For this reason,
t
Œ∫
kQÃÇti k‚àû ‚â§ max(kQÃÇt‚àí1
i k‚àû , |QÃÇi (zNi (t ‚àí 1))|).

Notice that
|QÃÇti (zNiŒ∫ (t ‚àí 1))| ‚â§ (1 ‚àí Œ±t‚àí1 )

rÃÑ
rÃÑ
rÃÑ
+ Œ±t‚àí1 (rÃÑ + Œ≥
)=
,
1‚àíŒ≥
1‚àíŒ≥
1‚àíŒ≥

which finishes the induction and the proof of part (a).
For part (b), notice that t = (At ‚àí AÃÑt )QÃÇt+1‚àíœÑ
+ bt ‚àí bÃÑt . Therefore, it is easy to check that by
i
rÃÑ
part (a), kt k‚àû ‚â§ 4 1‚àíŒ≥
+ 2rÃÑ = ¬Ø.
For part (c), notice that, for any k
+ bk‚àí1 k‚àû ‚â§ Œ±k‚àí1 [2
kQÃÇki ‚àí QÃÇk‚àí1
k‚àû = Œ±k‚àí1 kAk‚àí1 QÃÇk‚àí1
i
i

rÃÑ
+ rÃÑ].
1‚àíŒ≥

Therefore, by triangle inequality,
kQÃÇt‚àí1
‚àí QÃÇt‚àíœÑ
i
i k‚àû ‚â§ [2

t‚àí2
X
rÃÑ
Œ±k .
+ rÃÑ]
1‚àíŒ≥
k=t‚àíœÑ

As a consequence,
kœÜt k‚àû ‚â§ kAt ‚àí

AÃÑt k‚àû kQÃÇti

‚àí

+1
QÃÇt‚àíœÑ
k‚àû
i

rÃÑ
‚â§ [8
+ 4rÃÑ]
1‚àíŒ≥

t‚àí1
X
k=t‚àíœÑ +1

t‚àí1
X

Œ±k = 2¬Ø


Œ±k .

k=t‚àíœÑ +1


Proof of Lemma 10. Notice that log(1 ‚àí x) ‚â§ ‚àíx for all x < 1. Then,
(1 ‚àí œÉŒ±t ) = e

œÉh
log(1‚àí t+t
)
0

œÉh
‚àí t+t

‚â§e

0

.

Therefore,
t‚àí1
Y

(1 ‚àí œÉŒ±` ) ‚â§ e

‚àí

Pt‚àí1

‚àí

Rt

œÉh
`=k+1 `+t0

`=k+1

‚â§e

œÉh
`=k+1 `+t0 d`

‚àíœÉh log(

t+t0

)

k+1+t0
=e
 k + 1 + t œÉh
0
=
,
t + t0

which leads to the bound on Œ≤k,t and Œ≤ÃÉk,t .
For part (b),
2
Œ≤k,t
‚â§

h2
(k + 1 + t0 )2œÉh
2h2
‚â§
(k + t0 )2œÉh‚àí2 ,
(k + t0 )2
(t + t0 )2œÉh
(t + t0 )2œÉh
25

S CALABLE R EINFORCEMENT L EARNING OF L OCALIZED P OLICIES FOR M ULTI -AGENT N ETWORKED S YSTEMS

where we have used (k + 1 + t0 )2œÉh ‚â§ 2(k + t0 )2œÉh , which is true when t0 ‚â• 4œÉh. Then,
t‚àí1
X

2
Œ≤k,t

k=1

Z t
t‚àí1
X
2h2
2h2
2œÉh‚àí2
(y + t0 )2œÉh‚àí2 dy
‚â§
(k + t0 )
‚â§
(t + t0 )2œÉh
(t + t0 )2œÉh 1
k=1

2h2
1
1
2h
<
(t + t0 )2œÉh‚àí1 <
,
2œÉh
2œÉh ‚àí 1
œÉ (t + t0 )
(t + t0 )
where in the last inequality we have used 2œÉh ‚àí 1 > œÉh.
For part (c), notice that for k ‚àí œÑ + 1 ‚â§ ` ‚â§ k ‚àí 1 where k ‚â• œÑ , we have Œ±` ‚â§
(using t0 ‚â• œÑ ). Then,
t‚àí1
X
k=œÑ

Œ≤k,t

k‚àí1
X

Œ±` ‚â§

`=k‚àíœÑ +1

t‚àí1
X
k=œÑ

‚â§

t‚àí1
X
k=œÑ

h
k‚àíœÑ +t0

‚â§

2h
k+t0

t‚àí1

Œ≤k,t

X h  k + 1 + t0 œÉh 2hœÑ
2hœÑ
‚â§
k + t0
k + t0
t + t0
k + t0
k=œÑ

4h2 œÑ
(t + t0 )œÉh

(k + t0 )œÉh‚àí2

4h2 œÑ (t + t0 )œÉh‚àí1
(t + t0 )œÉh œÉh ‚àí 1
8hœÑ 1
‚â§
,
œÉ t + t0
‚â§

where we have used (k + 1 + t0 )œÉh ‚â§ 2(k + t0 )œÉh , and œÉh ‚àí 1 > 21 œÉh.



C.4. Proof of Lemma 11
Let the zNiŒ∫ ‚Äôth diagonal entry of Bk,t be bk,t (zNiŒ∫ ), and that of BÃÉk,t be bÃÉk,t (zNiŒ∫ ). Using these
notations, equation (28) can be written as,
:=G(zN Œ∫ )
i

z
QÃÇti (zNiŒ∫ ) = bÃÉœÑ ‚àí1,t (zNiŒ∫ )QÃÇœÑi (zNiŒ∫ ) +

}|
t‚àí1
X

{
bk,t (zNiŒ∫ )[gk (QÃÇki )](zNiŒ∫ )

k=œÑ

+

t‚àí1
X

Œ±k bÃÉk,t (zNiŒ∫ )(k (zNiŒ∫ ) + œÜk (zNiŒ∫ )).

k=œÑ

26

(33)

S CALABLE R EINFORCEMENT L EARNING OF L OCALIZED P OLICIES FOR M ULTI -AGENT N ETWORKED S YSTEMS

Notice that by (27), bÃÉœÑ ‚àí1,t (zNiŒ∫ ) +

Pt‚àí1

Œ∫
k=œÑ bk,t (zNi )

= 1. Then,

|G(zNiŒ∫ ) ‚àí Q‚àói (z)| ‚â§ bÃÉœÑ ‚àí1,t (zNiŒ∫ )|QÃÇœÑi (zNiŒ∫ ) ‚àí Q‚àói (z)| +
‚â§ bÃÉœÑ ‚àí1,t (zNiŒ∫ )|QÃÇœÑi (zNiŒ∫ ) ‚àí Q‚àói (z)| +

t‚àí1
X
k=œÑ
t‚àí1
X

bk,t (zNiŒ∫ )|[gk (QÃÇki )](zNiŒ∫ ) ‚àí Q‚àói (z)|
bk,t (zNiŒ∫ )|[gk (QÃÇki )](zNiŒ∫ ) ‚àí QÃÇdi k (zNiŒ∫ )|

k=œÑ

+

t‚àí1
X

bk,t (zNiŒ∫ )|Q‚àói (z) ‚àí QÃÇdi k (zNiŒ∫ )|

k=œÑ

‚â§ bÃÉœÑ ‚àí1,t (zNiŒ∫ )|QÃÇœÑi (zNiŒ∫ ) ‚àí Q‚àói (z)| + Œ≥

t‚àí1
X

bk,t (zNiŒ∫ )kQÃÇki ‚àí QÃÇdi k k‚àû

k=œÑ

+

t‚àí1
X

bk,t (zNiŒ∫ )kQ‚àói ‚àí Œ¶QÃÇdi k k‚àû

k=œÑ

‚â§ bÃÉœÑ ‚àí1,t (zNiŒ∫ )|QÃÇœÑi (zNiŒ∫ ) ‚àí Q‚àói (z)| + Œ≥

t‚àí1
X

bk,t (zNiŒ∫ )kŒ¶QÃÇki ‚àí Q‚àói k‚àû

k=œÑ

+2

t‚àí1
X

bk,t (zNiŒ∫ )kQ‚àói ‚àí Œ¶QÃÇdi k k‚àû

k=œÑ

‚â§ Œ≤ÃÉœÑ ‚àí1,t aœÑ + Œ≥

t‚àí1
X

bk,t (zNiŒ∫ )ak +

k=œÑ

2cœÅŒ∫+1
,
1‚àíŒ≥

(34)

where in the thrid inequality, we have used that gk is Œ≥-contraction in infinity norm with fixed point
QÃÇdi k , and in the last inequality, we have used (24). Combining the above with (33), we have
at = kŒ¶QÃÇti ‚àí Q‚àói k‚àû
‚â§ Œ≤ÃÉœÑ ‚àí1,t aœÑ + Œ≥ sup
zN Œ∫
i

t‚àí1
X

bk,t (z

NiŒ∫

k=œÑ

t‚àí1

t‚àí1

k=œÑ

k=œÑ

X
X
2cœÅŒ∫+1
)ak +
Œ±k BÃÉk,t œÜk k‚àû .
+k
Œ±k BÃÉk,t k k‚àû + k
1‚àíŒ≥


C.5. Proof of Lemma 12 and Lemma 13
Given
Pt‚àí1the work done above, notice that Lemma 9 (c) and Lemma 10 (c) imply the bound on
k k=œÑ Œ±k BÃÉk,t œÜk k‚àû in Lemma 13, and so the lemma follows directly. So, in this section, we
focus on the proof of Lemma 12. We start by stating a variant of the Azuma-Hoeffding bound that
handles our ‚Äúshifted‚Äù Martingale difference sequence.
Lemma 15 Let Xt be a Ft -adapted stochastic process, satisfying EXt |Ft‚àíœÑ = 0. Further, |Xt | ‚â§
XÃÑt almost surely. Then with probability 1 ‚àí Œ¥, we have,
v
u
t
t
X
u X
2œÑ
t
|
Xt | ‚â§ 2œÑ
XÃÑk2 log( ).
Œ¥
k=0

k=0

27

S CALABLE R EINFORCEMENT L EARNING OF L OCALIZED P OLICIES FOR M ULTI -AGENT N ETWORKED S YSTEMS

Proof Let ` be an integer between 0 and œÑ ‚àí 1. For each `, define process Yk` = XœÑ k+` , scalar
YÃÑk` = XÃÑkœÑ +` , and define Filtration FÃÉk` = FœÑ k+` . Then, Yk` is FÃÉk` -adapted, and satisfies
`
EYk` |FÃÉk‚àí1
= EXkœÑ +` |FkœÑ +`‚àíœÑ = 0.

Therefore, applying Azuma-Hoeffding bound on Yk` , we have
P (|

t2
),
Yk` | ‚â• t) ‚â§ 2 exp(‚àí P
2 k:kœÑ +`‚â§t (YÃÑk` )2
k:kœÑ +`‚â§t
X

i.e. with probability at least 1 ‚àí œÑŒ¥ ,
|

X

XkœÑ +` | = |

k:kœÑ +`‚â§t

X

Yk` |

s
‚â§ 2

k:kœÑ +`‚â§t

X

2
XÃÑkœÑ
+` log(

k:kœÑ +`‚â§t

2œÑ
).
Œ¥

Using the union bound for ` = 0, . . . , œÑ ‚àí 1, we get that with probability at least 1 ‚àí Œ¥,
v
u
t
t
œÑ ‚àí1
œÑ ‚àí1 s
X
X
X
X
X
u X
2œÑ
2œÑ
2
t
2
XÃÑkœÑ +` log( ) ‚â§ 2œÑ
XÃÑk2 log( ),
|
Xt | ‚â§
|
XkœÑ +` | ‚â§
Œ¥
Œ¥
k=0

`=0 k:kœÑ +`‚â§t

`=0

k:kœÑ +`‚â§t

k=0

where the last inequality is due to Cauchy-Schwarz.
Recall that Lemma 12 is an upper bound on k
Z Œ∫
random vector in R Ni , with its zNiŒ∫ ‚Äôth entry being
t‚àí1
X
k=œÑ

Œ±k k (zNiŒ∫ )

t‚àí1
Y

Pt‚àí1

k=œÑ

Œ±k BÃÉk,t k k, where

Pt‚àí1

k=œÑ

Œ±k BÃÉk,t k is a

(1 ‚àí Œ±` d` (zNiŒ∫ )),

(35)

`=k+1

with d` (zNiŒ∫ ) ‚â• œÉ almost surely, cf. (25). Fixing zNiŒ∫ , as have been shown in (29), k (zNiŒ∫ )
Q
is a Fk+1 adapted stochastic process satisfying Ek (zNiŒ∫ )|Fk+1‚àíœÑ = 0. However, t‚àí1
`=k+1 (1 ‚àí
Œ±` d` (zNiŒ∫ )) is not Fk+1‚àíœÑ -measurable, and as such we cannot directly apply the Azuma-Hoeffding
bound in Lemma 15 to quantity (35). In what follows, we first show in Lemma 16 that almost surely,
the absolute value of quantity (35) can be upper bounded by the sup of another quantity, to which
we can directly apply Lemma 15. With the help of Lemma 16, we can use the Azuma-Hoeffding
bound to control (35) and prove Lemma 12.
Lemma 16 For each zNiŒ∫ , we have almost surely,
t‚àí1
X
k=œÑ

Œ±k k (zNiŒ∫ )

t‚àí1
Y


(1 ‚àí Œ±` d` (zNiŒ∫ )) ‚â§

`=k+1

sup
œÑ ‚â§k0 ‚â§t‚àí1

t‚àí1
X

k (zNiŒ∫ )Œ≤k,t

k=k0 +1

Proof Let pk be a scalar sequence defined as follows. Set pœÑ = 0, and
pk = (1 ‚àí Œ±k‚àí1 dk‚àí1 (zNiŒ∫ ))pk‚àí1 + Œ±k‚àí1 k‚àí1 (zNiŒ∫ ).
28


+ 2¬Ø
Œ≤k0 ,t .

S CALABLE R EINFORCEMENT L EARNING OF L OCALIZED P OLICIES FOR M ULTI -AGENT N ETWORKED S YSTEMS

Then pt =
|pt |. Let

Pt‚àí1

k=œÑ

Œ±k k (zNiŒ∫ )

Qt‚àí1

`=k+1 (1

‚àí Œ±` d` (zNiŒ∫ )), and to prove Lemma 16 we need to bound

k0 = sup{k ‚â§ t ‚àí 1 : (1 ‚àí Œ±k dk (zNiŒ∫ ))|pk | ‚â§ Œ±k |k (zNiŒ∫ )|}.
We must have k0 ‚â• œÑ since |pœÑ | = 0. With k0 defined, we now define another scalar sequence pÃÉ s.t.
pÃÉk0 +1 = pk0 +1 and
pÃÉk = (1 ‚àí Œ±k‚àí1 œÉ)pÃÉk‚àí1 + Œ±k‚àí1 k‚àí1 (zNiŒ∫ ).
We claim that for all k ‚â• k0 + 1, pk and pÃÉk have the same sign, and |pk | ‚â§ |pÃÉk |. This is obviously
true for k = k0 + 1. Suppose it is true for for k ‚àí 1. Without loss of generality, suppose both pk‚àí1
and pÃÉk‚àí1 are non-negative. Since k ‚àí 1 > k0 and by the definition of k0 , we must have
(1 ‚àí Œ±k‚àí1 dk‚àí1 (zNiŒ∫ ))pk‚àí1 > |Œ±k‚àí1 k‚àí1 (zNiŒ∫ )|.
Therefore, pk > 0. Further, since dk‚àí1 (zNiŒ∫ ) ‚â• œÉ, we also have
(1 ‚àí Œ±k‚àí1 œÉ)pÃÉk‚àí1 ‚â• (1 ‚àí Œ±k‚àí1 dk‚àí1 (zNiŒ∫ ))pk‚àí1 > |Œ±k‚àí1 k‚àí1 (zNiŒ∫ )|.
These imply pÃÉk ‚â• pk > 0. The case where both pk‚àí1 and pÃÉk‚àí1 are negative are similar. This
finishes the induction, and as a result, |pt | ‚â§ |pÃÉt |.
Notice,
pÃÉt =

t‚àí1
X
k=k0 +1

Œ±k k (zNiŒ∫ )

t‚àí1
Y

(1‚àíŒ±` œÉ)+pÃÉk0 +1

t‚àí1
Y

k (zNiŒ∫ )Œ≤k,t +pÃÉk0 +1 Œ≤ÃÉk0 ,t .

k=k0 +1

`=k0 +1

`=k+1

t‚àí1
X

(1‚àíŒ±` œÉ) =

By the definition of k0 , we have
|pk0 +1 | ‚â§ (1 ‚àí Œ±k0 dk0 (zNiŒ∫ ))|pk0 | + Œ±k0 |k0 (zNiŒ∫ )| ‚â§ 2Œ±k0 |k0 (zNiŒ∫ )| ‚â§ 2Œ±k0 ¬Ø,
where in the last step, we have used the upper bound on kk0 k‚àû in Lemma 9 (b). As a result,
t‚àí1
X

|pt | ‚â§ |pÃÉt | ‚â§

k (zNiŒ∫ )Œ≤k,t + pÃÉk0 +1 Œ≤ÃÉk0 ,t

k=k0 +1

‚â§

t‚àí1
X

k (zNiŒ∫ )Œ≤k,t + 2Œ±k0 ¬ØŒ≤ÃÉk0 ,t

k=k0 +1

=

t‚àí1
X

k (zNiŒ∫ )Œ≤k,t + 2¬Ø
Œ≤k0 ,t .

k=k0 +1

With the above preparations, we are now ready to prove Lemma 12.
Proof of Lemma 12. Fix zNiŒ∫ and œÑ ‚â§ k0 ‚â§ t ‚àí 1. As have been shown in (29), k (zNiŒ∫ )Œ≤k,t is
a Fk+1 adapted stochastic process satisfying Ek (zNiŒ∫ )Œ≤k,t |Fk+1‚àíœÑ = 0. Also by Lemma 9(b),
|k (zNiŒ∫ )Œ≤k,t | ‚â§ ¬ØŒ≤k,t almost surely. As a result, we can use the Azuma-Hoeffding bound in
Lemma 15 to get with probability 1 ‚àí Œ¥,
v
u
t‚àí1
t‚àí1
X
X
u
2 log( 2œÑ ).
k (zNiŒ∫ )Œ≤k,t ‚â§ ¬Øt2œÑ
Œ≤k,t
Œ¥
k=k0 +1

k=k0 +1

29

S CALABLE R EINFORCEMENT L EARNING OF L OCALIZED P OLICIES FOR M ULTI -AGENT N ETWORKED S YSTEMS

By a union bound on œÑ ‚â§ k0 ‚â§ t ‚àí 1, we get with probability 1 ‚àí Œ¥,
v
v
u
u
t‚àí1
t‚àí1
t‚àí1
X
X
X
u
u
2œÑ
t
2
2 log( 2œÑ t ).
sup
) ‚â§ ¬Øt2œÑ
k (zNiŒ∫ )Œ≤k,t ‚â§ sup ¬Øt2œÑ
Œ≤k,t log(
Œ≤k,t
Œ¥
Œ¥
œÑ ‚â§k0 ‚â§t‚àí1
œÑ ‚â§k0 ‚â§t‚àí1
k=k0 +1

k=k0 +1

k=œÑ +1

Then, by Lemma 16, we have with probability 1 ‚àí Œ¥,
|

t‚àí1
X

Œ±k k (zNiŒ∫ )

k=œÑ

t‚àí1
Y


(1 ‚àí Œ±` d` (zNiŒ∫ ))| ‚â§

`=k+1

sup
œÑ ‚â§k0 ‚â§t‚àí1

t‚àí1
X


k (zNiŒ∫ )Œ≤k,t + 2¬Ø
Œ≤k0 ,t

k=k0 +1

v
u
t‚àí1
X
u
2 log( 2œÑ t ) +
t
sup 2¬Ø
Œ≤k0 ,t
Œ≤k,t
‚â§ ¬Ø 2œÑ
Œ¥
œÑ ‚â§k0 ‚â§t‚àí1
k=œÑ +1
s
œÑh
2œÑ t
h  k0 + 1 + t0 œÉh
log(
) + sup 2¬Ø

‚â§ 2¬Ø

œÉ(t + t0 )
Œ¥
k0 + t0
t + t0
œÑ ‚â§k0 ‚â§t‚àí1
s
2œÑ t
h
œÑh
log(
) + 2¬Ø

‚â§ 2¬Ø

œÉ(t + t0 )
Œ¥
t ‚àí 1 + t0
s
œÑh
2œÑ t
‚â§ 6¬Ø

log(
),
œÉ(t + t0 )
Œ¥
where in the third inequality, we have used the bounds on Œ≤k,t in Lemma 10. Finally, apply the union
bound over zNiŒ∫ ‚àà ZNiŒ∫ , and noticing that |NiŒ∫ | ‚â§ f (Œ∫) and |ZNiŒ∫ | ‚â§ (SA)f (Œ∫) by Assumption 1,
we have with probability 1 ‚àí Œ¥,
s
s
t‚àí1
X
œÑh
œÑh
2œÑ t(SA)f (Œ∫)
2œÑ t

Œ±k BÃÉk,t k k‚àû ‚â§ 6¬Ø
k
log(
) = 6¬Ø

[log(
) + f (Œ∫) log SA].
œÉ(t + t0 )
Œ¥
œÉ(t + t0 )
Œ¥
k=œÑ


C.6. Proof of Lemma 14
Throughout the proof, we fix zNiŒ∫ and prove the desired upper bound. For notational simplicity, we
drop the dependence on zNiŒ∫ and write bk,t and dk instead, and we will use the property dk ‚â• œÉ.
Define the sequence
t‚àí1
X
1
.
et =
bk,t
(k + t0 )œâ
k=œÑ

We use induction to show that et ‚â§
eœÑ +1 =

bœÑ,œÑ +1 (œÑ +t10 )œâ

=

Œ±œÑ dœÑ (œÑ +t10 )œâ

1
Œ≥(t+t0 )œâ . The statement is clearly
1
‚â§ ‚àöŒ≥(œÑ +1+t
œâ (last step needs Œ±œÑ
0)

‚àö

30

true for t = œÑ + 1, as
‚â§ 21 , (1 +

1 œâ
t0 )

‚â§

‚àö2 ,
Œ≥

S CALABLE R EINFORCEMENT L EARNING OF L OCALIZED P OLICIES FOR M ULTI -AGENT N ETWORKED S YSTEMS

implied by t0 ‚â• 1, œâ ‚â§ 1). Let the statement be true for t ‚àí 1. Then, notice that,
et =

t‚àí2
X

bk,t

k=œÑ

1
1
+ bt‚àí1,t
(k + t0 )œâ
(t ‚àí 1 + t0 )œâ

= (1 ‚àí Œ±t‚àí1 dt‚àí1 )

t‚àí2
X
k=œÑ

bk,t‚àí1

1
1
+ Œ±t‚àí1 dt‚àí1
œâ
(k + t0 )
(t ‚àí 1 + t0 )œâ

= (1 ‚àí Œ±t‚àí1 dt‚àí1 )et‚àí1 + Œ±t‚àí1 dt‚àí1

1
(t ‚àí 1 + t0 )œâ

1
1
+ Œ±t‚àí1 dt‚àí1
‚â§ (1 ‚àí Œ±t‚àí1 dt‚àí1 ) ‚àö
Œ≥(t ‚àí 1 + t0 )œâ
(t ‚àí 1 + t0 )œâ
h
1
‚àö i
= 1 ‚àí Œ±t‚àí1 dt‚àí1 (1 ‚àí Œ≥) ‚àö
,
Œ≥(t ‚àí 1 + t0 )œâ
where the inequality is based on induction assumption. Then, plug in Œ±t‚àí1 =
dt‚àí1 ‚â• œÉ, we have,

h
t‚àí1+t0

and use

œÉh
1
‚àö i
(1 ‚àí Œ≥) ‚àö
t ‚àí 1 + t0
Œ≥(t ‚àí 1 + t0 )œâ
h
œÉh
1
‚àö i t + t0 œâ
= 1‚àí
(1 ‚àí Œ≥)
‚àö
t ‚àí 1 + t0
t ‚àí 1 + t0
Œ≥(t + t0 )œâ
h
œâ
1
1
œÉh
‚àö i
.
= 1‚àí
(1 ‚àí Œ≥) 1 +
‚àö
t ‚àí 1 + t0
t ‚àí 1 + t0
Œ≥(t + t0 )œâ

h
et ‚â§ 1 ‚àí

Now using the inequality that for any x > ‚àí1, (1 + x) ‚â§ ex , we have,
œâ
‚àö
1
œÉh
1
‚àö i
‚àí œÉh (1‚àí Œ≥)+œâ t‚àí1+t
0 ‚â§ 1,
(1 ‚àí Œ≥) 1 +
‚â§ e t‚àí1+t0
t ‚àí 1 + t0
t ‚àí 1 + t0
‚àö
where in the last inequality, we have used œâ ‚â§ 1 and the condition on h s.t. œÉh(1 ‚àí Œ≥) ‚â• 1. This
1
shows et ‚â§ ‚àöŒ≥(t+t

œâ and finishes the induction.
0)
h
1‚àí

Appendix D. Analysis of the Actor and Proof of Theorem 5
In this section, we analyze the actor step. Recall that at iteration m,
Œ∏i (m + 1) = Œ∏i (m) + Œ∑m gÃÇi (m),
with Œ∑m =

‚àö Œ∑
m+1

and gÃÇi (m) is given by

gÃÇi (m) =

T
X
t=0

Œ≥t

1 X m,T
Œ∏ (m)
QÃÇj (sNjŒ∫ (t), aNjŒ∫ (t))‚àáŒ∏i log Œ∂i i (ai (t)|si (t)),
n
Œ∫

(36)

j‚ààNi

where QÃÇm,T
is the final estimate of the Q-function for ri at the end of the critic loop in iteration
i
m, where we have added an additional superscript m to QÃÇm,T
to indicate its dependence on m;
i
31

S CALABLE R EINFORCEMENT L EARNING OF L OCALIZED P OLICIES FOR M ULTI -AGENT N ETWORKED S YSTEMS

{s(t), a(t)}Tt=0 is the state-action trajectory with s(0) drawn from œÄ0 (the initial state distribution
defined in the objective function J(Œ∏), cf. (2)) and the agents taking policy Œ∏(m). Our goal is to
show that gÃÇi (m) is approximately the right gradient direction, ‚àáŒ∏i J(Œ∏(m)), which by Lemma 1 can
be written as,
‚àáŒ∏i J(Œ∏(m)) =

‚àû
X

Es‚àºœÄŒ∏(m) ,a‚àºŒ∂ Œ∏(m) (¬∑|s) Œ≥ t QŒ∏(m) (s, a)‚àáŒ∏i log Œ∂ Œ∏(m) (a|s),

t=0

(37)

t

Œ∏(m)

where œÄt
is the distribution of s(t) under fixed policy Œ∏(m) when the initial state is drawn from
Œ∏(m)
œÄ0 ; Q
is the true Q function for the global reward r under policy Œ∏(m), cf. (3).
To bound the difference between gÃÇi (m) and the true gradient ‚àáŒ∏i J(Œ∏(m)), we define the following additional sequences,
gi (m) =

T
X
t=0

hi (m) =

T
X
t=0

Œ≥t

1 X Œ∏(m)
Œ∏ (m)
Qj (s(t), a(t))‚àáŒ∏i log Œ∂i i (ai (t)|si (t)),
n
Œ∫

(38)

j‚ààNi

Es‚àºœÄŒ∏(m) ,a‚àºŒ∂ Œ∏(m) (¬∑|s) Œ≥ t
t

1 X Œ∏(m)
Œ∏ (m)
Qj (s, a)‚àáŒ∏i log Œ∂i i (ai |si ),
n
Œ∫

(39)

j‚ààNi

Œ∏(m)

where Qi
is the true Q function for ri under policy Œ∏(m). We also use notation h(m), g(m),
gÃÇ(m) to denote the respective hi (m), gi (m), gÃÇi (m) stacked into a larger vector. The following
result is an immediate consequence of Assumption 1 and Assumption 4, whose proof is postponed
to Appendix D.1.
Lemma 17 We have almost surely, ‚àÄm ‚â§ M ,
max(kgÃÇ(m)k, kg(m)k, kh(m)k, k‚àáJ(Œ∏(m))k) ‚â§

rÃÑL
.
(1 ‚àí Œ≥)2

Proof Overview. Our main proof idea is the following decomposition,
gÃÇ(m) = gÃÇ(m) ‚àí g(m) + g(m) ‚àí h(m) + h(m) ‚àí ‚àáJ(Œ∏(m)) +‚àáJ(Œ∏(m)),
|
{z
} |
{z
} |
{z
}
e1 (m)

e2 (m)

(40)

e3 (m)

where the error between the gradient estimator gÃÇ(m) and the true gradient ‚àáJ(Œ∏(m)) is decomposed into the sum of three terms. In Step 1, we bound the first term ke1 (m)k which is a direct
consequence of our result in the analysis of the critic, cf. Theorem 7 in Appendix C. In Step 2,
we study e2 (m), which turns out to be a martingale difference sequence and can be controlled by
Azuma-Hoeffding bound. In Step 3, we bound e3 (m), and finally in Step 4, we combine the bounds
on e1 (m), e2 (m) and e3 (m) to prove our main result Theorem 5.
Step 1: bounds on e1 (m). Notice that the difference between gÃÇi (m) and gi (m) is that the critic
Œ∏(m)
estimate QÃÇm,T
is replaced with the true Q-function Qj . By Theorem 7, we have QÃÇm,T
will
j
j
Œ∏(m)

be very close to Qj
with high probability when T is large enough, based on which we can
1
bound ke (m)k, which is formally provided in Lemma 18. The proof of Lemma 18 is postponed to
Appendix D.2.
32

S CALABLE R EINFORCEMENT L EARNING OF L OCALIZED P OLICIES FOR M ULTI -AGENT N ETWORKED S YSTEMS

Lemma 18 When T is large enough s.t.
6¬Ø

Ca (Œ¥, T ) =
‚àö
1‚àí Œ≥

r

Œ¥
Ca ( 2nM
,T )
‚àö
T +t0

+

Ca0
T +t0

‚â§

2cœÅŒ∫+1
,
(1‚àíŒ≥)2

where

œÑh
2œÑ T 2
2
16¬Ø
hœÑ 2rÃÑ
[log(
) + f (Œ∫) log SA], Ca0 =
,
(œÑ + t0 )),
‚àö max(
œÉ
Œ¥
1‚àí Œ≥
œÉ
1‚àíŒ≥

rÃÑ
with ¬Ø = 4 1‚àíŒ≥
+ 2rÃÑ, then we have with probability at least 1 ‚àí 2Œ¥ ,

ke1 (m)k ‚â§

sup
0‚â§m‚â§M ‚àí1

4cLœÅŒ∫+1
.
(1 ‚àí Œ≥)3

Step 2: bounds on e2 (m). Let Gm be the œÉ-algebra generated by the trajectories in the first m outerloop iterations. Then, Œ∏(m) is Gm‚àí1 measurable, and so is hi (m). Further, by the way that the trajectory {(s(t), a(t))}Tt=0 is generated, we have Eg(m)|Gm‚àí1 = h(m). As such, Œ∑m h‚àáJ(Œ∏(m)), e2 (m)i
is a martingale difference sequence w.r.t. Gm , and we have the following bound in Lemma 19 which
is a direct consequence of Azuma-Hoeffding bound. The proof of Lemma 19 is postponed to Section D.3.
Lemma 19 With probability at least 1 ‚àí Œ¥/2,
M
‚àí1
X
m=0

v
u M ‚àí1
u X
4
2
t2
2 log .
Œ∑m h‚àáJ(Œ∏(m)), e (m)i ‚â§
Œ∑m
4
(1 ‚àí Œ≥)
Œ¥
2rÃÑ2 L2

m=0

Step 3: bounds on e3 (m). We have the following Lemma 20 that bounds ke3 (m)k. Its proof is
quite similar to that of Lemma 4 and is postponed to Appendix D.4.
Lemma 20 When T + 1 ‚â•

log

c(1‚àíŒ≥)
+(Œ∫+1) log œÅ
rÃÑ

log Œ≥

, we have almost surely,

ke3 (m)k ‚â§ 2

Lc
œÅŒ∫+1 .
(1 ‚àí Œ≥)

Step 4: Proof of Theorem 5. With the above bounds on e1 (m), e2 (m) and e3 (m), we are now
ready to prove the main result Theorem 5. Since ‚àáJ(Œ∏) is L0 Lipschitz continuous, we have
J(Œ∏(m + 1)) ‚â• J(Œ∏(m)) + h‚àáJ(Œ∏(m)), Œ∏(m + 1) ‚àí Œ∏(m)i ‚àí
= J(Œ∏(m)) + Œ∑m h‚àáJ(Œ∏(m)), gÃÇ(m)i ‚àí

L0
kŒ∏(m + 1) ‚àí Œ∏(m)k2
2

2
L0 Œ∑m
kgÃÇ(m)k2 .
2

Recall the decomposition of gÃÇ(m),
gÃÇ(m) = gÃÇ(m) ‚àí g(m) + g(m) ‚àí h(m) + h(m) ‚àí ‚àáJ(Œ∏(m)) +‚àáJ(Œ∏(m)).
|
{z
} |
{z
} |
{z
}
e1 (m)

e2 (m)

e3 (m)

Then,
kgÃÇ(m)k2 ‚â§ 4ke1 (m)k2 + 4ke2 (m)k2 + 4ke3 (m)k2 + 4k‚àáJ(Œ∏(m))k2 .
33

(41)

S CALABLE R EINFORCEMENT L EARNING OF L OCALIZED P OLICIES FOR M ULTI -AGENT N ETWORKED S YSTEMS

Further, we can bound h‚àáJ(Œ∏(m)), gÃÇ(m)i,
h‚àáJ(Œ∏(m)), gÃÇ(m)i = k‚àáJ(Œ∏(m))k2 + h‚àáJ(Œ∏(m)), e1 (m) + e2 (m) + e3 (m)i
‚â• k‚àáJ(Œ∏(m))k2 + h‚àáJ(Œ∏(m)), e2 (m)i ‚àí k‚àáJ(Œ∏(m))k(ke1 (m)k + ke3 (m)k).
Plug the above bounds on kgÃÇ(m)k2 and h‚àáJ(Œ∏(m)), gÃÇ(m)i into (41), we have,
2
2
J(Œ∏(m + 1)) ‚â• J(Œ∏(m)) + (Œ∑m ‚àí 2L0 Œ∑m
)k‚àáJ(Œ∏(m))k2 + Œ∑m Œµm,0 ‚àí Œ∑m Œµm,1 ‚àí Œ∑m
Œµm,2 , (42)

where
Œµm,0 = h‚àáJ(Œ∏(m)), e2 (m)i,
Œµm,1 = k‚àáJ(Œ∏(m))k(ke1 (m)k + ke3 (m)k),
Œµm,2 = 2L0 (ke1 (m)k2 + ke2 (m)k2 + ke3 (m)k2 ).
Doing a telescope sum for (42), we get
J(Œ∏(M )) ‚â• J(Œ∏(0)) +

‚â• J(Œ∏(0)) +

M
‚àí1
X
m=0
M
‚àí1
X
m=0

2
(Œ∑m ‚àí 2L0 Œ∑m
)k‚àáJ(Œ∏(m))k2 +

M
‚àí1
X

Œ∑m Œµm,0 ‚àí

m=0

1
Œ∑m k‚àáJ(Œ∏(m))k2 +
2

M
‚àí1
X

Œ∑m Œµm,0 ‚àí

M
‚àí1
X

Œ∑m Œµm,1 ‚àí

m=0
M
‚àí1
X

m=0

Œ∑m Œµm,1 ‚àí

m=0

M
‚àí1
X

2
Œ∑m
Œµm,2

m=0
M
‚àí1
X

2
Œ∑m
Œµm,2 ,

m=0

(43)
2 = Œ∑ (1 ‚àí 2L0 Œ∑ ) ‚â• 1 Œ∑ , which is true because Œ∑ ‚â§ Œ∑ ‚â§
where we have used Œ∑m ‚àí 2L0 Œ∑m
m
m
m
2 m
After rearranging, we get
M
‚àí1
X
m=0

1
4L0 .

M
‚àí1
M
‚àí1
M
‚àí1
X
X
X
1
2
Œ∑m k‚àáJ(Œ∏(m))k2 ‚â§ J(Œ∏(M )) ‚àí J(Œ∏(0)) ‚àí
Œ∑m Œµm,0 +
Œ∑m Œµm,1 +
Œ∑m
Œµm,2 .
2
m=0

m=0

m=0

(44)
We now apply our results in the first three steps. By Lemma 19, we have with probability 1 ‚àí 2Œ¥ ,
M
‚àí1
X
m=0

Œ∑m Œµm,0

v
u M ‚àí1
u X
4
t2
2 log .
‚â§
Œ∑m
4
(1 ‚àí Œ≥)
Œ¥
2rÃÑ2 L2

(45)

m=0

By Lemma 18 and Lemma 20, we have with probability 1 ‚àí 2Œ¥ ,
sup Œµm,1 ‚â§
m‚â§M ‚àí1

rÃÑL
( sup ke1 (m)k + sup ke3 (m)k)
(1 ‚àí Œ≥)2 m‚â§M ‚àí1
m‚â§M ‚àí1

rÃÑL
4cLœÅŒ∫+1
Lc
(
+2
œÅŒ∫+1 )
2
3
(1 ‚àí Œ≥) (1 ‚àí Œ≥)
(1 ‚àí Œ≥)
6L2 crÃÑ Œ∫+1
‚â§
œÅ .
(1 ‚àí Œ≥)5
‚â§

34

(46)

S CALABLE R EINFORCEMENT L EARNING OF L OCALIZED P OLICIES FOR M ULTI -AGENT N ETWORKED S YSTEMS

rÃÑL
By Lemma 17, we have almost surely, max(ke1 (m)k, ke2 (m)k, ke3 (m)k) ‚â§ 2 (1‚àíŒ≥)
2 , and
hence almost surely,

sup Œµm,2 = 2L0 (ke1 (m)k2 + ke2 (m)k2 + ke3 (m)k2 )
m‚â§M ‚àí1

‚â§

24rÃÑ2 L0 L2
.
(1 ‚àí Œ≥)4

(47)

Using a union bound, we have with probability 1 ‚àí Œ¥, all three events (45), (46) and (47) hold,
which when combined with (44) implies
PM ‚àí1
2
m=0 Œ∑m k‚àáJ(Œ∏(m))k
PM ‚àí1
m=0 Œ∑m
PM ‚àí1
PM ‚àí1 2
2(J(Œ∏(M )) ‚àí J(Œ∏(0))) + 2
m=0 Œ∑m Œµm,0 + 2 supm‚â§M ‚àí1 Œµm,2
m=0 Œ∑m
‚â§
+ 2 sup Œµm,1
PM ‚àí1
m‚â§M ‚àí1
m=0 Œ∑m
q P
P
‚àí1 2
M ‚àí1 2
4
48rÃÑ2 L0 L2
4rÃÑ2 L2
2 M
2(J(Œ∏(M )) ‚àí J(Œ∏(0))) + (1‚àíŒ≥)
4
m=0 Œ∑m log Œ¥ + (1‚àíŒ≥)4
m=0 Œ∑m
‚â§
PM ‚àí1
m=0 Œ∑m
2
12L crÃÑ Œ∫+1
+
œÅ .
(48)
(1 ‚àí Œ≥)5
‚àö
‚àö
P ‚àí1 2
PM ‚àí1
Œ∑
Œ∑m > 2Œ∑( M + 1 ‚àí 1) ‚â• Œ∑ M + 1 and M
Since Œ∑m = ‚àöm+1
, we have, m=0
m=0 Œ∑m <
rÃÑ
2
2
Œ∑ (1 + log(M )) < 2Œ∑ log(M ) (using M ‚â• 3). Further we use the bound J(Œ∏(M )) ‚â§ 1‚àíŒ≥
and
J(Œ∏(0)) ‚â• 0 almost surely. Combining these results, we get with probability 1 ‚àí Œ¥,
q
2 L0 L2
PM ‚àí1
2rÃÑ
8rÃÑ2 L2
2
log M log 4Œ¥ + 96rÃÑ
Œ∑ log M
+
4
12L2 crÃÑ Œ∫+1
Œ∑(1‚àíŒ≥)
(1‚àíŒ≥)
(1‚àíŒ≥)4
m=0 Œ∑m k‚àáJ(Œ∏(m))k
‚àö
+
‚â§
œÅ .
PM ‚àí1
5
(1
‚àí
Œ≥)
M
+
1
Œ∑
m
m=0
This concludes the proof of the main Theorem 5.
D.1. Proof of Lemma 17
Recall that
gÃÇi (m) =

T
X

Œ≥t

t=0

1 X m,T
Œ∏ (m)
QÃÇj (sNjŒ∫ (t), aNjŒ∫ (t))‚àáŒ∏i log Œ∂i i (ai (t)|si (t)).
n
Œ∫
j‚ààNi

Therefore,
kgÃÇi (m)k ‚â§

T
X

Œ≥t

t=0

‚â§

T
X
t=0

1 X
Œ∏ (m)
|QÃÇm,T
(sNjŒ∫ (t), aNjŒ∫ (t))|k‚àáŒ∏i log Œ∂i i (ai (t)|si (t))k
j
n
Œ∫
j‚ààNi

Œ≥t

rÃÑ
rÃÑ
Li <
Li ,
1‚àíŒ≥
(1 ‚àí Œ≥)2

35



S CALABLE R EINFORCEMENT L EARNING OF L OCALIZED P OLICIES FOR M ULTI -AGENT N ETWORKED S YSTEMS

where we have used that kQÃÇm,T
k‚àû ‚â§
j

rÃÑ
1‚àíŒ≥

almost surely (cf. Lemma 9 (a)). As a result,

v
u n
uX
kgÃÇ(m)k = t
kgÃÇi (m)k2 <
i=1

rÃÑ
L.
(1 ‚àí Œ≥)2

The upper bounds for kg(m)k, kh(m)k and k‚àáJ(Œ∏(m))k can be obtained in an almost identical
way and their proof is therefore omitted.

D.2. Proof of Lemma 18
In this section, we prove Lemma 18.
Proof of Lemma 18 Let Gm be the œÉ-algebra generated by the trajectories in the first m outer-loop
iterations. Then, Theorem 7 implies that, fixing each m ‚â§ M and i ‚àà N , conditioned on Gm‚àí1 , the
following event happens with probability at least 1 ‚àí Œ¥:
Œ∏(m)

sup
(s,a)‚ààS√óA

Qi

Ca (Œ¥, T )
2cœÅŒ∫+1
Ca0
Œ∫
Œ∫
‚àö
‚â§
(s, a) ‚àí QÃÇm,T
(s
,
a
)
+
,
+
Ni
Ni
i
T + t0 T + t0 (1 ‚àí Œ≥)2

where
6¬Ø

Ca (Œ¥, T ) =
‚àö
1‚àí Œ≥

r

œÑh
2œÑ T 2
2
16¬Ø
hœÑ 2rÃÑ
[log(
) + f (Œ∫) log SA], Ca0 =
,
(œÑ + t0 )),
‚àö max(
œÉ
Œ¥
1‚àí Œ≥
œÉ
1‚àíŒ≥

rÃÑ
with ¬Ø = 4 1‚àíŒ≥
+ 2rÃÑ.
We can take expectation and average out Gm‚àí1 , and apply union bound over 0 ‚â§ m ‚â§ M ‚àí 1
and i ‚àà N , getting with probability at least 1 ‚àí 2Œ¥ ,

sup sup

sup

m‚â§M ‚àí1 i‚ààN (s,a)‚ààS√óA

Œ∏(m)

Qi

(s, a) ‚àí QÃÇm,T
(sNiŒ∫ , aNiŒ∫ ) ‚â§
i
‚â§

Œ¥
Ca ( 2nM
,T)
Ca0
2cœÅŒ∫+1
‚àö
+
+
T + t0 (1 ‚àí Œ≥)2
T + t0

4cœÅŒ∫+1
,
(1 ‚àí Œ≥)2
Ca (

(49)
Œ¥

,T )

0

where in the last step, we have used that our lower bound on T implies ‚àö2nM
+ TC+ta 0 ‚â§
T +t0
Therefore, conditioned on (49) being true, we have for any m ‚â§ M ‚àí 1 and any i ‚àà N ,

2cœÅŒ∫+1
.
(1‚àíŒ≥)2

kgÃÇi (m) ‚àí gi (m)k
‚â§

T
X

Œ≥t

t=0

‚â§

T
X

Œ≥t

t=0

‚â§

T
X
t=0


1 X  Œ∏(m)
Œ∏ (m)
Qj (s(t), a(t)) ‚àí QÃÇm,T
(sNjŒ∫ (t), aNjŒ∫ (t)) ‚àáŒ∏i log Œ∂i i (ai (t)|si (t))
j
n
Œ∫
j‚ààNi

1 X
Œ∏(m)
Œ∏ (m)
Qj (s(t), a(t)) ‚àí QÃÇm,T
(sNjŒ∫ (t), aNjŒ∫ (t)) ‚àáŒ∏i log Œ∂i i (ai (t)|si (t))
j
n
Œ∫
j‚ààNi

Œ≥t

4cœÅŒ∫+1
4cLi œÅŒ∫+1
L
<
.
i
(1 ‚àí Œ≥)2
(1 ‚àí Œ≥)3

36

S CALABLE R EINFORCEMENT L EARNING OF L OCALIZED P OLICIES FOR M ULTI -AGENT N ETWORKED S YSTEMS

As a result,
kgÃÇ(m) ‚àí g(m)k ‚â§

sup
0‚â§m‚â§M ‚àí1

4cLœÅŒ∫+1
,
(1 ‚àí Œ≥)3

which is true conditioned on event (49) is true that happens with probability at least 1 ‚àí 2Œ¥ .



D.3. Proof of Lemma 19
By Lemma 17, we have almost surely,
|Œ∑m h‚àáJ(Œ∏(m)), e2 (m)i| ‚â§ Œ∑m k‚àáJ(Œ∏(m))kkh(m) ‚àí g(m)k ‚â§ Œ∑m

2rÃÑ2 L2
.
(1 ‚àí Œ≥)4

As Œ∑m h‚àáJ(Œ∏(m)), e2 (m)i is a martingale difference sequence w.r.t. Gm , we have by Azuma Hoeffding bound, with probability at least 1 ‚àí 21 Œ¥,
v
u M ‚àí1
M
‚àí1
2
2
u X
X
4
2rÃÑ
L
t2
2 log .
Œ∑m
Œ∑m h‚àáJ(Œ∏(m)), e2 (m)i ‚â§
4
(1 ‚àí Œ≥)
Œ¥
m=0

m=0


D.4. Proof of Lemma 20
In this section, we provide the proof of Lemma 20.
Proof of Lemma 20 By (37), we have
‚àáŒ∏i J(Œ∏(m)) =
=

‚àû
X
t=0
‚àû
X

Es‚àºœÄŒ∏(m) ,a‚àºŒ∂ Œ∏(m) (¬∑|s) Œ≥ t QŒ∏(m) (s, a)‚àáŒ∏i log Œ∂ Œ∏(m) (a|s)
t

Œ∏ (m)

Es‚àºœÄŒ∏(m) ,a‚àºŒ∂ Œ∏(m) (¬∑|s) Œ≥ t QŒ∏(m) (s, a)‚àáŒ∏i log Œ∂i i

t=0

where we have used ‚àáŒ∏i log Œ∂ Œ∏(m) (a|s) = ‚àáŒ∏i
Also recall the definition of hi (Œ∏) in (39),
hi (m) =

T
X

Es‚àºœÄŒ∏(m) ,a‚àºŒ∂ Œ∏(m) (¬∑|s) Œ≥ t

t=0

(ai |si )

t

t

P

Œ∏ (m)

j‚ààN

log Œ∂j j

Œ∏ (m)

(aj |sj ) = ‚àáŒ∏i log Œ∂i i

(ai |si ).

1 X Œ∏(m)
Œ∏ (m)
Qj (s, a)‚àáŒ∏i log Œ∂i i (ai |si ).
n
Œ∫
j‚ààNi

Combining the above two equations, we have,
‚àáŒ∏i J(Œ∏(m)) ‚àí hi (m)
=

T
X

Es‚àºœÄŒ∏(m) ,a‚àºŒ∂ Œ∏(m) (¬∑|s) Œ≥
t

t=0

+

‚àû
X

t

Œ∏ (m)
‚àáŒ∏i log Œ∂i i (ai |si )

j‚ààNi

Œ∏ (m)

Es‚àºœÄŒ∏(m) ,a‚àºŒ∂ Œ∏(m) (¬∑|s) Œ≥ t ‚àáŒ∏i log Œ∂i i

t=T +1



1 X Œ∏(m)
Œ∏(m)
Q
(s, a) ‚àí
Qj (s, a)
n
Œ∫

t

:= E1 + E2 .
37

(ai |si )QŒ∏(m) (s, a)

S CALABLE R EINFORCEMENT L EARNING OF L OCALIZED P OLICIES FOR M ULTI -AGENT N ETWORKED S YSTEMS

Li rÃÑ
Œ≥ T +1 .
(1‚àíŒ≥)2

Clearly, the second term satisfies kE2 k ‚â§

E1 =

T
X

Œ∏ (m)

Es‚àºœÄŒ∏(m) ,a‚àºŒ∂ Œ∏(m) (¬∑|s) Œ≥ t ‚àáŒ∏i log Œ∂i i
t

t=0

=

T
X

+

t

t

T
X


 X
1
Œ∏(m)
Qj (s, a)
(ai |si )
n
Œ∫
j‚ààN‚àíi

Es‚àºœÄŒ∏(m) ,a‚àºŒ∂ Œ∏(m) (¬∑|s) Œ≥

t=0

For E1 , we have


X  Œ∏(m)
Œ∏(m)
Qj (s, a) ‚àí QÃÇj (sNjŒ∫ , aNjŒ∫ )

1
Œ∏ (m)
‚àáŒ∏i log Œ∂i i (ai |si )

n

Œ∏ (m)

Es‚àºœÄŒ∏(m) ,a‚àºŒ∂ Œ∏(m) (¬∑|s) Œ≥ t ‚àáŒ∏i log Œ∂i i

Œ∫
j‚ààN‚àíi

(ai |si )

t

t=0

1 X Œ∏(m)
QÃÇj (sNjŒ∫ , aNjŒ∫ )
n
Œ∫
j‚ààN‚àíi

:= E3 + E4 ,
Œ∏(m)

Œ∏(m)

is any truncated Q function for Qj
where QÃÇj
Œ∫ and any t,
this, consider for any j ‚àà N‚àíi
Œ∏ (m)

Es‚àºœÄŒ∏(m) ,a‚àºŒ∂ Œ∏(m) (¬∑|s) ‚àáŒ∏i log Œ∂i i
t

=
=

n
Y

as defined in (5). We claim E4 is zero. To see
Œ∏(m)

(ai |si )QÃÇj

(sNjŒ∫ , aNjŒ∫ )

Œ∏ (m)

‚àáŒ∏i Œ∂i i

(ai |si ) Œ∏(m)
QÃÇj (sNjŒ∫ , aNjŒ∫ )
Œ∏i (m)
Œ∂
(a
|s
)
i
i
s,a
`=1
i
X Œ∏(m) Y Œ∏ (m)
Œ∏ (m)
Œ∏(m)
`
œÄt (s)
Œ∂`
(a` |s` )‚àáŒ∏i Œ∂i i (ai |si )QÃÇj (sNjŒ∫ , aNjŒ∫ )
s,a
`6=i
X

Œ∏(m)

œÄt

(s)

X

=

Œ∏ (m)

Œ∂` `

Œ∏(m)

œÄt

(s)

s,a1:i‚àí1 ,ai+1:n

(a` |s` )

Y

Œ∏ (m)

Œ∂` `

Œ∏(m)

(a` |s` )QÃÇj

(sNjŒ∫ , aNjŒ∫ )

X

Œ∏ (m)

‚àáŒ∏i Œ∂i i

(ai |si )

ai

`6=i

= 0,
Œ∏(m)

where in the last equality, we have used QÃÇj (sNjŒ∫ , aNjŒ∫ ) does not depend on ai as i 6‚àà NjŒ∫ ; and
P Œ∏ (m)
P
Œ∏i (m)
(ai |si ) = ‚àáŒ∏i ai Œ∂i i (ai |si ) = ‚àáŒ∏i 1 = 0.
ai ‚àáŒ∏i Œ∂i
For E3 , by the exponential decay property, the truncated Q function has a small error, cf. (11),
Œ∏(m)

sup |Qj
s,a

Œ∏(m)

(s, a) ‚àí QÃÇj

(sNjŒ∫ , aNjŒ∫ )| ‚â§ cœÅŒ∫+1 ,

and as a result,
kE3 k ‚â§

1 ‚àí Œ≥ T +1
Li c Œ∫+1
Li cœÅŒ∫+1 <
œÅ .
1‚àíŒ≥
(1 ‚àí Œ≥)

Therefore,

k‚àáŒ∏i J(Œ∏(m)) ‚àí hi (m)k = kE2 + E3 k ‚â§
‚â§2

Li rÃÑ
Li c Œ∫+1
Œ≥ T +1 +
œÅ ,
(1 ‚àí Œ≥)2
(1 ‚àí Œ≥)

Li c Œ∫+1
œÅ ,
(1 ‚àí Œ≥)

38

S CALABLE R EINFORCEMENT L EARNING OF L OCALIZED P OLICIES FOR M ULTI -AGENT N ETWORKED S YSTEMS

where in the last step, we have used
T +1‚â•

log c(1‚àíŒ≥)
+ (Œ∫ + 1) log œÅ
rÃÑ
,
log Œ≥

Lc
œÅŒ∫+1 .
and as a result, k‚àáJ(Œ∏(m)) ‚àí h(m)k ‚â§ 2 (1‚àíŒ≥)

39



