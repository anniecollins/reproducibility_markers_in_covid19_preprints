High Dimensional Latent Panel Quantile Regression

arXiv:1912.02151v1 [econ.EM] 4 Dec 2019

with an Application to Asset Pricing âˆ—
Alexandre Belloni1 , Mingli Chen2 , Oscar Hernan Madrid Padilla3 , and Zixuan (Kevin) Wang4
1

Fuqua Business School, Duke University
Department of Economics, University of Warwick
3
Department of Statistics, University of California, Los Angeles
4
Harvard University
2

December 5, 2019

Abstract

We propose a generalization of the linear panel quantile regression model to accommodate
both sparse and dense parts: sparse means while the number of covariates available is large,
potentially only a much smaller number of them have a nonzero impact on each conditional
quantile of the response variable; while the dense part is represented by a low-rank matrix that
can be approximated by latent factors and their loadings. Such a structure poses problems for
traditional sparse estimators, such as the `1 -penalised Quantile Regression, and for traditional
latent factor estimator, such as PCA. We propose a new estimation procedure, based on the
ADMM algorithm, that consists of combining the quantile loss function with `1 and nuclear
norm regularization. We show, under general conditions, that our estimator can consistently
estimate both the nonzero coefficients of the covariates and the latent low-rank matrix.
Our proposed model has a â€œCharacteristics + Latent Factorsâ€ Asset Pricing Model interpretation: we apply our model and estimator with a large-dimensional panel of financial data and
find that (i) characteristics have sparser predictive power once latent factors were controlled (ii)
the factors and coefficients at upper and lower quantiles are different from the median.
Keywords: High-dimensional quantile regression; factor model; nuclear norm regularization; panel data; asset pricing; characteristic-based model
âˆ—

We are grateful to conversations with Victor Chernozhukov, IvaÌn FernaÌndez-Val, Bryan Graham, Hiroaki Kaido,
Anna Mikusheva, Whitney Newey, and Vasilis Syrgkanis.

1

1

Introduction

A central question in asset pricing is to explain the returns of stocks. According to the Arbitrage Pricing
Theory, when the asset returns are generated by a linear factor model, there exists a stochastic discount factor
linear in the factors that prices the returns (Ross (1976), Cochrane (2009)). However, from an empirical
perspective, testing the theory is difficult because the factors are not directly observed.
To overcome this challenge, one approach uses characteristic-sorted portfolio returns to mimic the unknown factors. This approach for understanding expected returns can be captured by the following panel
data model (Cochrane (2011)):
0
E(Yi,t |Xi,tâˆ’1 ) = Xi,tâˆ’1
Î¸.
(1)
The drawback of this approach is that it requires a list of pre-specified characteristics which are chosen
based on empirical experience and thus somewhat arbitrarily (Fama and French (1993)). In addition, the literature has documented a zoo of new characteristics, and the proliferation of characteristics in this â€œvariable
zooâ€ leads to the following questions: â€œwhich characteristics really provide independent information about
average returns, and which are subsumed by others?â€ (Cochrane (2011)). Another approach uses statistical factor analysis, e.g. Principal Component Analysis (PCA), to extract latent factors from asset returns
(Chamberlain and Rothschild (1983), Connor and Korajczyk (1988)):
E(Yi,t ) = Î»0i gt .

(2)

However, one main critique with this approach is that the latent factors estimated via PCA are purely statistical factors, thus lacks economic insight Campbell (2017).
We extend both modeling approaches and propose a â€œCharacteristics + Latent Factorsâ€ framework. By
incorporating the â€œCharacteristicsâ€ documented, we improve the economic interpretability and explanatory
power of the model. The characteristics can have a sparse structure, meaning although a large set of variables
is available, only a much smaller subset of them might have predictive power. We also incorporate â€œLatent
Factorsâ€, one benefit of having this part is that it might help alleviate the â€œomitted variable biasâ€ problem
(Giglio and Xiu (2018)). As in the literature typically these latent factors are estimated via PCA, which
means all possible latent explanatory variables (those are not included in the model) might be important for
prediction although their individual contribution might be small, we term this as the dense part. 1
Hence, our framework allows for â€œSparse + Denseâ€ modeling with the time series and cross-section of
asset returns. In addition, we focus on understanding the quantiles (hence the entire distribution) of returns
rather than just the mean, in line with recent interest in quantile factor models (e.g. Ando and Bai (2019),
Chen et al. (2018), Ma et al. (2019), Feng (2019), and Sagner (2019)). Specifically, we study the following
high dimensional latent panel quantile regression model with Y âˆˆ RnÃ—T and X âˆˆ RnÃ—T Ã—p satisfying
0
FYâˆ’1
(Ï„ ) = Xi,t
Î¸(Ï„ ) + Î»i (Ï„ )0 gt (Ï„ ),
i,t |Xi,t ;Î¸(Ï„ ),Î»i (Ï„ ),gt (Ï„ )
1

i = 1 . . . , n, t = 1, . . . , T,

(3)

More about sparse modeling and dense modeling can be found in Giannone et al. (2017). See also Chernozhukov et al. (2017).

2

where i denotes subjects (or assets in our asset pricing setting), t denotes time, Î¸(Ï„ ) âˆˆ Rp is the vector of
coefficients, Î»i (Ï„ ), gt (Ï„ ) âˆˆ RrÏ„ with rÏ„  min{n, T } (denote Î i,t (Ï„ ) = Î»i (Ï„ )0 gt (Ï„ ), then Î (Ï„ ) âˆˆ RnÃ—T
is a low-rank matrix with unknown rank rÏ„ ), Ï„ âˆˆ [0, 1] is the quantile index, and FYi,t |Xi,t ;Î¸(Ï„ ),Î»i (Ï„ ),gt (Ï„ ) (or
FYi,t |Xi,t ;Î¸(Ï„ ),Î i,t (Ï„ ) ) is the cumulative distribution function of Yi,t conditioning on Xi,t , Î¸(Ï„ ) and Î»i (Ï„ ), gt (Ï„ )
(or Î i,t (Ï„ )). Our framework also allows for the possibility of lagged dependent data. Thus, we model the
quantile function at level Ï„ as a linear combination of the predictors plus a low-rank matrix (or a factor
structure). Here, we allow for the number of covariates p, and the time horizon T , to grow to infinity as n
grows. Throughout the paper we mainly focus on the case where p is large, possibly much larger than nT ,
but for the true model Î¸(Ï„ ) is sparse and has only sÏ„  p non-zero components.
Our framework is flexible enough that allows us to jointly answer the following three questions in
asset pricing: (i) Which characteristics are important to explain the time series and cross-section of stock
returns, after controlling for the factors? (ii) How much would the latent factors explain stock returns
after controlling for firm characteristics? (iii) Does the relationship of stock returns and firm characteristics
change across quantiles? The first question is related to the recent literature on variable selection in asset
pricing using machine learning (Kozak et al. (2019); Feng et al. (2019); Han et al. (2018)). The second
question is related to an classical literature starting from 1980s on statistical factor models of stock returns
(Chamberlain and Rothschild (1983); Connor and Korajczyk (1988) and recently Lettau and Pelger (2018)).
The third question extends the literature in late 1990s on stock return and firm characteristics (Daniel and
Titman (1997, 1998)) and further asks whether the relationship is heterogenous across quantiles.
There are several key features of considering prediction problem at the panel quantile model in this
setting. First, stock returns are known to be asymmetric and exhibits heavy tail, thus modeling different
quantiles of return provides extra information in addition to models of first and second moments. Second,
quantile regression provides a richer characterization of the data, allowing heterogeneous relationship between stock returns and firm characteristics across the entire return distribution. Third, the latent factors
might also be different at different quantiles of stock returns. Finally, quantile regression is more robust
to the presence of outliers relative to other widely used mean-based approaches. Using a robust method is
crucial when estimating low-rank structures (see e.g. She and Chen (2017)). As our framework is based on
modeling the quantiles of the response variable, we do not put assumptions directly on the moments of the
dependent variable.
Our main goal is to consistently estimate both the sparse part and the low-rank matrix. Recovery of a
low-rank matrix, when there are additional high dimensional covariates, in a nonlinear model can be very
challenging. The rank constraint will result in the optimization problem NP-hard. In addition, estimation in
high dimensional regression is known to be a challenging task, which in our frameworks becomes even more
difficult due to the additional latent structure. We address the former challenge via nuclear norm regularization which is similar to CandeÌ€s and Recht (2009) in the matrix completion setting. Without covariates, the
estimation can be done via solving a convex problem, and similarly there are strong statistical guarantees of

3

recovery of the underlying low-rank structure. We address the latter challenge by imposing `1 regularization
on the vector of coefficients of the control variables, similarly to Belloni and Chernozhukov (2011) which
mainly focused on the cross-sectional data setting. Note that with regards to sparsity, we must be cautious,
specially when considering predictive models (She and Chen (2017)). Furthermore, we explore the performance of our procedure under settings where the vector of coefficients can be dense (due to the low-rank
matrix).
We also propose a novel Alternating Direction Method of Multipliers (ADMM) algorithm (Boyd et al.
(2011)) that allows us to estimate, at different quantile levels, both the vector of coefficients and the low-rank
matrix. Our proposed algorithm can easily be adjusted to other nonlinear models with a low-rank matrix
(with or without covariates).
We view our work as complementary to the low dimensional quantile regression with interactive fixed
effects framework as of the recent work of Feng (2019), and the mean estimation setting in Moon and Weidner (2018). However, unlike Moon and Weidner (2018) and Feng (2019), we allow the number of covariates
p to be large, perhaps p  nT . This comes with different significant challenges. On the computational
side, it requires us to develop novel estimation algorithms, which turns out can also be used for the contexts
in Moon and Weidner (2018) and Feng (2019). On the theoretical side, allowing p  nT requires a systematically different analysis as compared to Feng (2019), as it is known that ordinary quantile regression
is inconsistent in high dimensional settings (p  nT ), see Belloni and Chernozhukov (2011).

Related Literature. Our work contributes to the recent growing literature on panel quantile model. Abrevaya and Dahl (2008), Graham et al. (2018), Arellano and Bonhomme (2017), considered the fixed T asymptotic case. Kato et al. (2012) formally derived the asymptotic properties of the fixed effect quantile regression
estimator under large T asymptotics, and Galvao and Kato (2016) further proposed fixed effects smoothed
quantile regression estimator. Galvao (2011) works on dynamic panel. Koenker (2004) proposed a penalized estimation method where the individual effects are treated as pure location shift parameters common to
all quantiles, for other related literature see Lamarche (2010), Galvao and Montes-Rojas (2010). We refer
to Chapter 19 of Koenker et al. (2017) for a review.
Our work also contributes to the literature on nuclear norm penalisation, which has been widely studied
in the machine learning and statistical learning literature, Fazel (2002), Recht et al. (2010); Koltchinskii et al.
(2011); Rohde and Tsybakov (2011), Negahban and Wainwright (2011), Brahma et al. (2017). Recently, in
the econometrics literature Athey et al. (2018) proposes a framework of matrix completion for estimating
causal effects, Bai and Ng (2017) for estimating approximate factor model, Chernozhukov et al. (2018)
considered the heterogeneous coefficients version of the linear panel data interactive fixed model where
the main coefficients has a latent low-rank structure, Bai and Feng (2019) for robust principal component
analysis, and Bai and Ng (2019) for imputing counterfactual outcome.

4

Finally, our results contribute to a growing literature on high dimensional quantile regression. Wang et al.
(2012) considered quantile regression with concave penalties for ultra-high dimensional data; Zheng et al.
(2015) proposed an adaptively weighted `1 -penalty for globally concerned quantile regression. Screening
procedures based on moment conditions motivated by the quantile models have been proposed and analyzed
in He et al. (2013) and Wu and Yin (2015) in the high-dimensional regression setting. We refer to Koenker
et al. (2017) for a review.
To sum-up, our paper makes the following contributions. First, we propose a new class models that
consist of both high dimensional regressors and latent factor structures. We provide a scalable estimation
procedure, and show that the resulting estimator is consistent under suitable regularity conditions. Second,
the high dimensional and non-smooth objective function require innovative strategies to derive all the abovementioned results. This leads to the use in our proofs of some novel techniques from high dimensional
statistics/econometrics, spectral theory, empirical process, etc. Third, the proposed estimators inherit from
quantile regression certain robustness properties to the presence of outliers and heavy-tailed distributions in
the idiosyncratic component of a factor model. Finally, we apply our proposed model and estimator to a
large-dimensional panel of financial data in the US stock market and find that different return quantiles have
different selected firm characteristics and that the number of latent factors can be also be different.

Outline. The rest of the paper is organized as follows. Section 2 introduces the high dimensional latent
quantile regression model, and provides an overview of the main theoretical results. Section 3 presents the
estimator and our proposed ADMM algorithm. Section 4 discusses the statistical properties of the proposed
estimator. Section 5 provides simulation results. Section 6 consists of the empirical results of our model
applied to a real data set. The proofs of the main results are in the Appendix.

Notation. For m âˆˆ N, we write [m] = {1, . . . , m}. For a vector v âˆˆ Rp we define its `0 norm as
P
kvk0 = pj=1 1{vj 6= 0}, where 1{Â·} takes value 1 if the statement inside {} is true, and zero otherwise;
P
P
its `1 norm as kvk1 = pj=1 |vj |. We denote kvk1,n,T = pj ÏƒÌ‚j |vj | the `1 -norm weighted by ÏƒÌ‚j â€™s (details
qP
p
nÃ—T
2
can be found in eq (20)). The Euclidean norm is denoted by k Â· k, thus kvk =
j=1 vj . If A âˆˆ R
qP P
n
T
2
is a matrix, its Frobenius norm is denoted by kAkF =
i=1
t=1 Ai,t , its spectral norm by kAk2 =
âˆš
supx : kxk=1 x0 A0 Ax, its infinity norm by kAkâˆ = max{|Ai,j | : i âˆˆ [n], j âˆˆ [T ]} , its rank by rank(A),
âˆš
and its nuclear norm by kAkâˆ— = trace( A0 A) where A0 is the transpose of A. The jth column A is denoted
by AÂ·,j . Furthermore, the multiplication of a tensor X âˆˆ RI1 Ã—...Ã—Im with a vector a âˆˆ RIm is denoted by
Pm
Z := Xa âˆˆ RI1 Ã—...Ã—Imâˆ’1 , and, explicitly, Zi1 ,...imâˆ’1 = Ij=1
Xi1 ,...,imâˆ’1 ,j aj . We also use the notation
a âˆ¨ b = max{a, b}, a âˆ§ b = min{a, b}, (a)âˆ’ = max{âˆ’a, 0}. For a sequence of random variables {zj }âˆ
j=1
âˆ and
we denote by Ïƒ(z1 , z2 , . . .) the sigma algebra generated by {zj }âˆ
.
Finally,
for
sequences
{a
}
n
n=1
j=1
{bn }âˆ
we
write
a

b
if
there
exists
positive
constants
c
and
c
such
that
c
b
â‰¤
a
â‰¤
c
n
n
1
2
1
n
n
2 bn for
n=1
sufficiently large n.
5

2

The Estimator and Overview of Rate Results

2.1

Basic Setting

The setting of interest corresponds to a high dimension latent panel quantile regression model, where Y âˆˆ
RnÃ—T , and X âˆˆ RnÃ—T Ã—p satisfying
0
FYâˆ’1
(Ï„ ) = Xi,t
Î¸(Ï„ ) + Î i,t (Ï„ ),
i,t |Xi,t ;Î¸(Ï„ ),Î i,t (Ï„ )

i = 1 . . . , n, t = 1, . . . , T,

(4)

where i denotes subjects, t denotes time, Î¸(Ï„ ) âˆˆ Rp is the vector of coefficients, Î (Ï„ ) âˆˆ RnÃ—T is a low-rank
matrix with unknown rank rÏ„  min{n, T }, Ï„ âˆˆ [0, 1] is the quantile index, and FYi,t |Xi,t ;Î¸(Ï„ ),Î i,t (Ï„ ) is the
cumulative distribution function of Yi,t conditioning on Xi,t , Î¸(Ï„ ) and Î i,t (Ï„ ). Thus, we model the quantile
function at level Ï„ as a linear combination of the predictors plus a low-rank matrix. Here, we allow for
the number of covariates p, and the time horizon T , to grow to infinity as n grows. Throughout the paper
the quantile index Ï„ âˆˆ (0, 1) is fixed. We mainly focus on the case where p is large, possibly much larger
than nT , but for the true model Î¸(Ï„ ) is sparse and has only sÏ„  p non-zero components. Mathematically,
sÏ„ := kÎ¸(Ï„ )k0 .
When Î i,t (Ï„ ) = Î»i (Ï„ )0 gt (Ï„ ), with Î»i (Ï„ ), gt (Ï„ ) âˆˆ RrÏ„ , this immediately leads to the following setting
0
FYâˆ’1
(Ï„ ) = Xi,t
Î¸(Ï„ ) + Î»i (Ï„ )0 gt (Ï„ ).
i,t |Xi,t ;Î¸(Ï„ ),Î i,t (Ï„ )

(5)

where we model the quantile function at level Ï„ as a linear combination of the covariates (as predictors)
plus a latent factor structure. This is directly related to the panel data models with interactive fixed effects
literature in econometrics, e.g. linear panel data model (Bai (2009)), nonlinear panel data models (Chen
(2014); Chen et al. (2014)).
Note, for eq (5), additional identification restrictions are needed for estimating Î»i (Ï„ ) and gt (Ï„ ) (see Bai
and Ng (2013)). In addition, in nonlinear panel data models, this create additional difficulties in estimation,
as the latent factors and their loadings part are nonconvex. 2

2.2

The Penalized Estimator, and its Convex Relaxation

In this subsection, we describe the high dimensional latent quantile estimator. With the sparsity and low-rank
constraints in mind, a natural formulation for the estimation of (Î¸(Ï„ ), Î (Ï„ )) is
minimize
Î¸ÌƒâˆˆRp , Î ÌƒâˆˆRnÃ—T

subject to

T
n
1 XX
0
ÏÏ„ (Yi,t âˆ’ Xi,t
Î¸Ìƒ âˆ’ Î Ìƒi,t )
nT
t=1 i=1

rank(Î Ìƒ) â‰¤ rÏ„ ,
kÎ¸Ìƒk0 = sÏ„ ,

2

(6)

Different identification conditions might result in different estimation procedures for Î» and f , see Bai and Li (2012) and Chen
(2014).

6

where ÏÏ„ (t) = (Ï„ âˆ’ 1{t â‰¤ 0})t is the quantile loss function as in Koenker (2005), sÏ„ is a parameter that
directly controls the sparsity of Î¸Ìƒ, and rÏ„ controls the rank of the estimated latent matrix.
While the formulation in (6) seems appealing, as it enforces variable selection and low-rank matrix
estimation simultaneously, (6) is a non-convex problem due to the constraints posed by the k Â· k0 and rank(Â·)
functions. We propose a convex relaxation of (6). Inspired by the seminal works of Tibshirani (1996) and
CandeÌ€s and Recht (2009), we formulate the problem
min
Î¸ÌƒâˆˆRp , Î ÌƒâˆˆRnÃ—T

subject to

T
n
1 XX
0
ÏÏ„ (Yi,t âˆ’ Xi,t
Î¸Ìƒ âˆ’ Î Ìƒi,t )
nT
t=1 i=1

kÎ Ìƒkâˆ— â‰¤ Î½2 ,
p
X
wj |Î¸Ìƒj | â‰¤ Î½1 ,

(7)

j=1

where Î½1 > 0 and Î½2 > 0 are tuning parameters, and w1 , . . . , wp are user specified weights (more on this in
Section 4 ).
In principle, one can use any convex solver software to solve (7), since this is a convex optimization
problem. However, for large scale problems a more careful implementation might be needed. Section 3.1
presents a scheme for solving (7) that is based on the ADMM algorithm ( (Boyd et al., 2011)).

2.3

Summary of results

We now summarize our main results. For the model defined in (4):
â€¢ Under (4), sÏ„  min{n, T }, an assumption that implicitly requires rÏ„  min{n, T }, and other
regularity conditions defined in Section 4, we show that our estimator (Î¸Ì‚(Ï„ ), Î Ì‚(Ï„ )) defined in Section
3 is consistent for (Î¸(Ï„ ), Î (Ï„ )). Specifically, for the independent data case (across i and t), under
suitable regularity conditions that can be found in Section 4, we have



p
p
âˆš
âˆš
1
1
sÏ„ max{ log p, log n, rÏ„ } âˆš + âˆš
.
(8)
kÎ¸Ì‚(Ï„ ) âˆ’ Î¸(Ï„ )k = OP
n
T
and




1
1
1
2
kÎ Ì‚(Ï„ ) âˆ’ Î (Ï„ )kF = OP sÏ„ max{log p, log n, rÏ„ }
+
,
nT
n T

(9)

Importantly, the rates in (8) and (9), ignoring logarithmic factors, match those in previous works.
However, our setting allows for modeling at different quantile levels. We also complement our results
by allowing for the possibility of lagged dependent data. Specifically, under a Î²-mixing assumption,
Theorem 1 provides a statistical guarantee for estimating (Î¸(Ï„ ), Î (Ï„ )). This result can be thought as
a generalization of the statements in (8) and (9).
7

â€¢ An important aspect of our analysis is that we contrast the performance of our estimator in settings
where the possibility of a dense Î¸(Ï„ ) provided that the features are highly correlated. We show that
there exist choices of the tuning parameters for our estimator that lead to consistent estimation.
â€¢ For estimation, we provide an efficient algorithm (details can be found in Section 3.1), which is based
on the ADMM algorithm (Boyd et al. (2011)).
â€¢ Section 6 provides thorough examples on financial data that illustrate the flexibility and interpretability
of our approach.
Although our theoretical analysis builds on the work by Belloni and Chernozhukov (2011), there are
multiple challenges that we must face in order to prove the consistency of our estimator. First, the construction of the restricted set now involves the nuclear norm penalty. This requires us to define a new restricted
set that captures the contributions of the low-rank matrix. Second, when bounding the empirical processes
that naturally arise in our proof, we have to simultaneously deal with the sparse and dense components.
Furthermore, throughout our proofs, we have to carefully handle the weak dependence assumption that can
be found in Section 4.

3
3.1

High Dimensional Latent Panel Quantile Regression
Estimation with High Dimensional Covariates

In this subsection, we describe the main steps of our proposed ADMM algorithm, details can be found
in Section A. We start by introducing slack variables to the original problem (7). As a result, a problem
equivalent to (7) is
min
Î¸Ìƒ,Î Ìƒ,V

ZÎ¸ ,ZÎ  ,W

p
n T
X
1 XX
ÏÏ„ (Vi,t ) + Î½1
wj |ZÎ¸j | + Î½2 kÎ Ìƒkâˆ—
nT
i=1 t=1

j=1

(10)

subject to V = W, W = Y âˆ’ X Î¸Ìƒ âˆ’ ZÎ  ,
ZÎ  âˆ’ Î Ìƒ = 0, ZÎ¸ âˆ’ Î¸Ìƒ = 0.
To solve (10), we propose a scaled version of the ADMM algorithm which relies on the following

8

Augmented Lagrangian
L(Î¸Ìƒ, Î Ìƒ, V ZÎ¸ , ZÎ  , W, UV , UW , UÎ  , UÎ¸ ) =

p
n T
X
1 XX
ÏÏ„ (Vi,t ) + Î½1
wj |ZÎ¸j | + Î½2 kÎ Ìƒkâˆ—
nT
i=1 t=1

j=1

Î·
Î·
+ kV âˆ’ W + UV k2F + kW âˆ’ Y + XÎ¸ + ZÎ  + UW k2F
2
2
Î·
Î·
+ kZÎ  âˆ’ Î Ìƒ + UÎ  k2F + kZÎ¸ âˆ’ Î¸Ìƒ + UÎ¸ k2F ,
2
2
(11)
where Î· > 0 is a penalty parameter.
Notice that in (11), we have followed the usual construction of ADMM via introducing the scaled dual
variables corresponding to the constraints in (10) â€“ those are UV , UW , UÎ  , and UÎ¸ . Next, recall that
ADMM proceeds by iteratively minimizing the Augmented Lagrangian in blocks with respected to the
original variables, in our case (V, Î¸Ìƒ, Î Ìƒ) and (W, ZÎ¸ , ZÎ  ), and then updating the scaled dual variables (see
Equations 3.5â€“3.7 in Boyd et al. (2011)). The explicit updates can be found in the Appendix. Here, we
highlight the updates for ZÎ¸ , Î Ìƒ, and V . For updating ZÎ¸ at iteration k + 1, we solve the problem
ï£¼
ï£±
p
ï£½
ï£²1
X
Î½
(k)
(k+1)
1
kZÎ¸ âˆ’ Î¸Ìƒ(k+1) + UÎ¸ k2F +
wj |(ZÎ¸ )j | .
ZÎ¸
â† arg min
ï£¾
Î·
ZÎ¸ âˆˆRp ï£³ 2
j=1

This can be solved in closed form exploiting the well known thresholding operator, see the details in Section
B.2. As for updating Î Ìƒ, we solve

Î Ìƒ(k+1) â† arg min
Î ÌƒâˆˆRnÃ—T



1 (k)
Î½2
(k)
kÎ Ìƒkâˆ— + kZÎ  âˆ’ Î Ìƒ + UÎ  k2F
Î·
2


,

(12)

via the singular value shrinkage operator, see Theorem 2.1 in Cai et al. (2010).
Furthermore, we update V , at iteration k + 1, via
)
(
n X
T
X
1
Î·
(k)
ÏÏ„ (Vi,t ) + kV âˆ’ W (k) + UV k2F ,
V (k+1) â† arg min
nT
2
V âˆˆRnÃ—T

(13)

i=1 t=1

which can be found in closed formula by Lemma 5.1 from Ali et al. (2016).
Remark 1. After estimating Î (Ï„ ), we can estimate Î»i (Ï„ ) and gt (Ï„ ) via the singular value decomposition
of Î Ì‚(Ï„ ) and following equation
Î Ì‚(Ï„ )i,t = Î»Ì‚i (Ï„ )0 gÌ‚t (Ï„ ),
(14)
where Î»Ì‚i (Ï„ ) and gÌ‚t (Ï„ ) are of dimension rÌ‚Ï„ . This immediately leads to factors and loadings estimated that
can be used to obtain insights about the structure of the data. A formal identification statement is given in
Corollary 2.

9

3.2

Estimation without Covariates

Note, when there are no covariates, our proposed ADMM can be simplified. In this case, we face the
following problem
ï£±
ï£¼
n X
T
ï£² 1 X
ï£½
min
ÏÏ„ (Yi,t âˆ’ Î Ìƒi,t ) + Î½2 kÎ Ìƒkâˆ— .
(15)
ï£¾
Î ÌƒâˆˆRnÃ—T ï£³ nT
i=1 j=1

This can be thought as a convex relaxation of the estimator studied in Chen et al. (2018). Problem (15) is
also related to the setting of robust estimation of a latent low-rank matrix, e.g. Elsener and van de Geer
(2018). However, our approach can also be used to estimate different quantile levels. As for solving (15),
we can proceed by doing the iterative updates
(
)
n T
1 XX
Î·
(k)
(k) 2
(k+1)
Î Ìƒ
â† arg min
ÏÏ„ (Yi,t âˆ’ Î Ìƒi,t ) + kÎ Ìƒ âˆ’ ZÎ  + UÎ  kF ,
(16)
nT
2
Î Ìƒ
i=1 t=1

(k+1)

ZÎ 

â† arg min
ZÎ 

nÎ·
2

o
(k)
kÎ Ìƒ(k+1) âˆ’ ZÎ  + UÎ  k2F + Î½2 kZÎ  kâˆ— ,

(17)

and
(k+1)

UÎ 

(k+1)

â† Î (k+1) âˆ’ ZÎ 

(k)

+ UÎ  ,

(18)

where Î· > 0 is the penalty parameter (Boyd et al. (2011)). The minimization in (16) is similar to (13),
whereas (17) can be done similarly as in (12).
Although our proposed estimation procedure can be applied to settings (i) with low dimensional covariates, or (ii) without covariates, in what follows, we focus on the high dimensional covariates setting.

4

Theory

The purpose of this section is to provide statistical guarantees for the estimator developed in the previous
section. We focus on estimating the quantile function, allowing for the high dimensional scenario where
p and T can grow as n grows. Our analysis combines tools from high dimensional quantile regression
theory (e.g. Belloni and Chernozhukov (2011)), spectral theory (e.g. Vu (2007) and Chatterjee (2015)), and
empirical process theory (e.g. Yu (1994) and van der Vaart and Wellner (1996)).

4.1

Main Result

We show that our proposed estimator is consistent in a broad range of models, and in some cases attains
minimax rates, as in CandeÌ€s and Plan (2011).

10

Before arriving at our main theorem, we start by stating some modeling assumptions. For a fixed Ï„ > 0,
we assume that (4) holds. We also let TÏ„ be the support of Î¸(Ï„ ), thus
TÏ„ = {j âˆˆ [p] : Î¸j (Ï„ ) 6= 0} ,
and we write sÏ„ = |TÏ„ |, and rÏ„ = rank(Î (Ï„ )).
Throughout, we treat Î (Ï„ ) as parameters. As for the data generation process, our next condition requires
that the observations are independent across i, and weakly dependent across time.
Assumption 1. The following holds:
(i) Conditional on Î , {(Yi,t , Xi,t }t=1,...,T is independent across i. Also, for each i âˆˆ [n], the sequence
{(Yi,t , Xi,t )}t=1,...,T is stationary and Î²-mixing with mixing coefficients satisfying supi Î³i (k) =
O(k âˆ’Âµ ) for some Âµ > 2. Moreover, there exists Âµ0 âˆˆ (0, Âµ), such that
kâˆ’Âµ
j
0
npT T 1/(1+Âµ )
â†’ 0.
(19)
Here, Î³i (k) =

1
2

sup

PL

lâ‰¥1

j=1

PL0

j 0 =1 |P(Aj

âˆ© Bj 0 ) âˆ’ P(Aj )P(Bj 0 )| with {Aj }L
j=1 is a partition of
0

Ïƒ({Xi,1 , Yi,1 }, . . . , {Xi,l , Yi,l }), and {Bj 0 }L
j 0 =1 partition of Ïƒ({Xi,l+k , Yi,l+k }, {Xi,l+k+1 , Yi,l+k+1 } . . .).
(ii) There exists f > 0 satisfying
inf

1â‰¤iâ‰¤n, 1â‰¤tâ‰¤T, xâˆˆX ,

fYi,t |Xi,t ;Î¸(Ï„ ),Î i,t (Ï„ ) (x0 Î¸(Ï„ ) + Î i,t (Ï„ )|x; Î¸(Ï„ ), Î i,t (Ï„ )) > f ,

where fYi,t |Xi,t ;Î¸,Î i,t is the probability density function associated with Yi,t when conditioning on
Xi,t , and with parameters Î¸(Ï„ ) and Î i,t (Ï„ ). Furthermore, fYi,t |Xi,t ;Î¸(Ï„ ),Î i,t (Ï„ ) (y|x; Î¸(Ï„ ), Î i,t (Ï„ )) and
âˆ‚
Â¯
Â¯0
âˆ‚y fYi,t |Xi,t ;Î¸(Ï„ ),Î i,t (Ï„ ) (y|x; Î¸(Ï„ ), Î i,t (Ï„ )) are both bounded by f and f , respectively, uniformly in y
and x in the support of Xi,t .
Note that Assumption 1 is a generalization of the sampling and smoothness assumption of Belloni and
Chernozhukov (2011). Furthermore, we highlight that similar to Belloni and Chernozhukov (2011), our
framework is rich enough that avoids imposing Gaussian or homoscedastic modeling constraints. However,
unlike Belloni and Chernozhukov (2011), we consider panel data with weak correlation across time. In
particular, we refer readers to Yu (1994) for thorough discussions on Î²-mixing.
It is worth mentioning that the parameter Âµ in Assumption 1 controls the strength of the time dependence
in the data. In the case that {(Yi,t , Xi,t )}iâˆˆ[n],tâˆˆ[T ] are independent our theoretical results will hold without
imposing (19).
Next, we require that along each dimension the second moment of the covariates is one. We also assume
that the second moments can be reasonably well estimated by their empirical counterparts.
11

2 ) = 1 for all i âˆˆ [n], t âˆˆ [T ], j âˆˆ [p]. Then
Assumption 2. We assume E(Xi,t,j

ÏƒÌ‚j2 =

n T
1 XX 2
Xi,t,j , âˆ€j âˆˆ [p],
nT

(20)

i=1 t=1

and we require that

P

max

1â‰¤jâ‰¤p

|ÏƒÌ‚j2

1
âˆ’ 1| â‰¤
4


â‰¥ 1âˆ’Î³

â†’ 1,

as

n â†’ âˆ.

Assumption 2 appeared as Condition D.3 in Belloni and Chernozhukov (2011). It is met by general
models on the covariates, see for instance Design 2 in Belloni and Chernozhukov (2011).
Using the empirical second order moments {ÏƒÌ‚j2 }pj=1 , we analyze the performance of the estimator
n
o
(21)
(Î¸Ì‚(Ï„ ), Î Ì‚(Ï„ )) = arg min QÌ‚Ï„ (Î¸Ìƒ, Î Ìƒ) + Î½1 kÎ¸Ìƒk1,n,T + Î½2 kÎ Ìƒkâˆ— ,
(Î¸Ìƒ,Î Ìƒ)

where Î½2 > 0 is a tuning parameter, kÎ¸Ìƒk1,n,T :=

Pp

j=1 ÏƒÌ‚j |Î¸Ìƒj |,

and

n T
1 XX
0
QÌ‚Ï„ (Î¸Ìƒ, Î Ìƒ) =
ÏÏ„ (Yi,t âˆ’ Xi,t
Î¸Ìƒ âˆ’ Î Ìƒi,t ),
nT
i=1 t=1

with ÏÏ„ as defined in Section 2.2.
As it can been seen in Lemma 7 from Appendix B.1, (Î¸Ì‚(Ï„ ) âˆ’ Î¸(Ï„ ), Î Ì‚(Ï„ ) âˆ’ Î (Ï„ )) belongs to a restricted
set, which in our framework is defined as
(

)
âˆš
r
kâˆ†k
kâˆ†k
F
âˆ—
AÏ„ = (Î´, âˆ†) âˆˆ Rp Ã— RnÃ—T : kÎ´TÏ„c k1 + âˆš âˆš
â‰¤ C0 kÎ´TÏ„ k1 + âˆš âˆš Ï„
,
nT

log(max{n,pcT })

nT

log(max{n,pcT })

(22)
for an appropriate positive constant C0 .
Similar in spirit to other high dimensional settings such as those in CandeÌ€s and Tao (2007), Bickel et al.
(2009), Belloni and Chernozhukov (2011) and Dalalyan et al. (2017), we impose an identifiability condition
involving the restricted set which is expressed next and will be used in order to attain our main results.
Before that, we introduce some notation.
For m â‰¥ 0, we denote by T Ï„ (Î´, m) âŠ‚ {1, . . . , p}\TÏ„ the support ot the m largest components, excluding
entries in TÏ„ , of the vector (|Î´1 |, . . . , |Î´p |)T . We also use the convention T Ï„ (Î´, 0) = âˆ….
Assumption 3. For (Î´, âˆ†) âˆˆ AÏ„ , let
v
u

n X
T
2 
u f X
1/2
t
0
JÏ„ (Î´, âˆ†) :=
.
E Xi,t Î´ + âˆ†i,t
nT
i=1 t=1

12

Then there exists m â‰¥ 0 such that
1/2

0 < Îºm :=

JÏ„ (Î´, âˆ†)

inf
(Î´,âˆ†)âˆˆAÏ„ ,Î´6=0

kÎ´TÏ„ âˆªT Ï„ (Î´,m) k +

kâˆ†kF
âˆš
nT

,

(23)

0

where cT = dT 1/(1+Âµ ) e for Âµ0 as defined in Assumption 2. Moreover, we assume that the following holds
  P P
3/2
n
T
1
0 Î´ + âˆ† )2
3/2
E
(X
i,t
i,t
i=1
t=1
nT
3f
 P P
 ,
inf
0 < q :=
(24)
0
n
T
1
8 f (Î´,âˆ†)âˆˆAÏ„ ,Î´6=0
E
|X 0 Î´ + âˆ† |3
nT

i=1

t=1

i,t

i,t

0

with f and f as in Assumption 1.
Few comments are in order. First, if âˆ† = 0 then (23) and (24) become the restricted identifiability and
nonlinearity conditions as of Belloni and Chernozhukov (2011). Second, the denominator of (23) contains
âˆš
the term kâˆ†kF /( nT ). To see why this is reasonable, consider the case where E(Xi,t ) = 0, and Xi,t are
i.i.d.. Then
f
JÏ„ (Î´, âˆ†) = f E((Î´ 0 Xi,t )2 ) +
kâˆ†k2F .
nT
âˆš
Hence, kâˆ†kF /( nT ) appears also in the numerator of (23) and its presence in the denominator of (23) is
not restrictive.
We now state our result for estimating Î¸(Ï„ ) and Î (Ï„ ).
Theorem 1. Suppose that Assumptions 1-3 hold and that
p
âˆš
âˆš
Ï†n cT (1 + sÏ„ ) log p( n + dT )
,
(25)
q â‰¥C
âˆš
ndT Îº0 f 1/2
p
for a large enough constant C, and {Ï†n } is a sequence with Ï†n /( f log(cT + 1)) â†’ âˆ. Then
p p
!

Ï†n 1 + smÏ„
cT (1 + sÏ„ ) max{log(pcT âˆ¨ n), rÏ„ }
1
1
âˆš +âˆš
kÎ¸Ì‚(Ï„ ) âˆ’ Î¸(Ï„ )k = OP
, (26)
Îºm
n
dT
Îº0 f 1/2
and
1
kÎ Ì‚(Ï„ ) âˆ’ Î (Ï„ )k2F = OP
nT

Ï†2n cT (1 + sÏ„ ) max{log(pcT âˆ¨ n), rÏ„ }
Îº40 f

for choices of the tuning parameters satisfying
s
p
cT log(max{n, pcT }) âˆš
Î½1 
( n + dT ),
ndT
and
Î½2 

p 
cT âˆš
n + dT ,
nT

0

where cT = dT 1/(1+Âµ ) e, dT = bT /(2cT )c.
13



1
1
+
n dT

!
,

(27)

Theorem 1 gives an upper bound on the performance of (Î¸Ì‚(Ï„ ), Î Ì‚(Ï„ )) for estimating the vector of coefficients Î¸(Ï„ ) and the latent matrix Î (Ï„ ). For simplicity, consider the case of i.i.d data. Then the convergence
âˆš âˆš
âˆš
rate of our estimation of Î¸(Ï„ ), under the Euclidean norm, is in the order of sÏ„ rÏ„ / min{ n, T }, if
we ignore all the other factors. Hence, we can consistently estimate Î¸(Ï„ ) provided that max{sÏ„ , rÏ„ } <<
min{n, T }. This is similar to the low-rank condition in Negahban and Wainwright (2011). In the low diâˆš âˆš
âˆš
mensional case sÏ„ = O(1), the rate rÏ„ / min{ n, T } matches that of Theorem 1 in Moon and Weidner
(2018). However, we mainly focus on a loss function that is robust to outliers, and our assumptions also
allow for weak dependence across time. Furthermore, the same applies to our rate on the mean squared error
for estimating Î (Ï„ ), which also matches that in Theorem 1 of Moon and Weidner (2018).
Interestingly, it is expected that the rate in Theorem 1 is optimal. To elaborate on this point, consider the
simple case where n = T , Î¸ = 0, Ï„ = 0.5, and ei,t := Yi,t âˆ’ Î i,t (Ï„ ) are mean zero i.i.d. sub-Gaussian(Ïƒ 2 ).
The latter implies that


z2
P(|e1,1 | > z) â‰¤ C1 exp âˆ’ 2 ,
2Ïƒ
for a positive constant C1 , and for all z > 0. Then by Theorem 2.3 in CandeÌ€s and Plan (2011), we have the
following lower bound for estimating Î (Ï„ ):
!
kÎ Ì‚(Ï„ ) âˆ’ Î (Ï„ )k2F
rÏ„ Ïƒ 2
E
â‰¥
.
(28)
inf
sup
nT
n
Î Ì‚ Î (Ï„ ) : rank(Î (Ï„ ))â‰¤rÏ„
Notably, the lower bound in (28) matches the rate implied by Theorem 1, ignoring other factors depending
on sÏ„ , Îº0 , Îºm , p and Ï†n . However, we highlight that the upper bound (27) in Theorem 1 holds without the
perhaps restrictive condition that the errors are sub-Gaussian.
We conclude this section with a result regarding the estimation of the factors and loadings of the latent
matrix Î (Ï„ ). This is expressed in Corollary 2 below and is immediate consequence of Theorem 1 and
Theorem 3 in Yu et al. (2014).
Corollary 2. Suppose that the all the conditions of Theorem 1 hold. Let Ïƒ1 (Ï„ ) â‰¥ Ïƒ2 (Ï„ ) â‰¥ . . . â‰¥ ÏƒrÏ„ (Ï„ ) >
0 be the singular values of Î (Ï„ ), and ÏƒÌ‚1 (Ï„ ) â‰¥ . . . â‰¥ ÏƒÌ‚min{n,T } (Ï„ ) the singular values of Î Ì‚(Ï„ ). Let
Ëœ
g(Ï„ ), gÌ‚(Ï„ ) âˆˆ RT Ã—rÏ„ and Î»(Ï„ ), Î»Ì‚(Ï„ ), Î»Ìƒ(Ï„ ), Î»Ì‚(Ï„ ) âˆˆ RnÃ—rÏ„ be matrices with orthonormal columns satisfying
Î (Ï„ ) =

rÏ„
X

Ïƒj Î»ÌƒÂ·,j (Ï„ )gÂ·,j (Ï„ )0 =

j=1

rÏ„
X

Î»Â·,j (Ï„ )gÂ·,j (Ï„ )0 ,

j=1

Ëœ
and Î Ì‚(Ï„ )gÌ‚Â·,j (Ï„ ) = ÏƒÌ‚j (Ï„ )Î»Ì‚Â·,j (Ï„ ) = Î»Ì‚Â·,j (Ï„ ) for j = 1, . . . , rÏ„ . Then


âˆš
(Ïƒ1 (Ï„ ) + rÏ„ Err)Err
v1 := min kgÌ‚(Ï„ )O âˆ’ g(Ï„ )kF = OP
,
OâˆˆOrÏ„
(ÏƒrÏ„ âˆ’1 (Ï„ ))2 âˆ’ (ÏƒrÏ„ (Ï„ ))2

14

(29)

and
kÎ»Ì‚(Ï„ ) âˆ’ Î»(Ï„ )k2F
nT

v2 =:

= OP

rÏ„ Ï†2n cT (1+sÏ„ ) max{log(pcT âˆ¨n),rÏ„ }
Îº40 f
âˆš
Ïƒ12 (Ïƒ1 (Ï„ )+ rÏ„ Err)2 Err2
nT ((ÏƒrÏ„ âˆ’1 (Ï„ ))2 âˆ’(ÏƒrÏ„ (Ï„ ))2 )2



1
n

+

1
dT



+
(30)

!
.

Here, OrÏ„ is the group of rÏ„ Ã— rÏ„ orthonormal matrices, and
p
p 
Ï†n cT (1 + sÏ„ ) max{log(pcT âˆ¨ n), rÏ„ } âˆš
Err :=
n
+
dT .
Îº20 f 1/2
A particularly interesting instance of Corollary 2 is when
(Ïƒ1 (Ï„ ))2 , (ÏƒrÏ„ âˆ’1 (Ï„ ))2 âˆ’ (ÏƒrÏ„ (Ï„ ))2  nT,
a natural setting if the entries of Î (Ï„ ) are O(1). Then the upper bound (29) becomes
p

!
Ï†n cT (1 + sÏ„ ) max{log(pcT âˆ¨ n), rÏ„ }
1
1
âˆš +âˆš
v1 = OP
,
n
dT
Îº20 f 1/2
whereas (30) is now
v2 = OP

rÏ„ Ï†2n cT (1 + sÏ„ ) max{log(pcT âˆ¨ n), rÏ„ }
Îº40 f



1
1
+
n dT

!
.

The conclusion of Corollary 2 allows us to provide an upper on the estimation of factors (g(Ï„ )) and
loadings (Î»(Ï„ )) of the latent matrix Î (Ï„ ). In particular this justifies the heuristic discussed in (14).

4.2

Correlated predictors

We conclude our theory section by studying the case where the vector of coefficients Î¸(Ï„ ) can be dense, in
the sense that the number of non-zero entries can be large, perhaps even larger than nT . To make estimation
feasible, we impose the condition that XÎ¸(Ï„ ) can be perturbed into a low-rank matrix, a scenario that can
happen when the covariates are highly correlated to each other.
We view our setting below as an extension of the linear model in Chernozhukov et al. (2018) to the
quantile framework and reduced rank regression. The specific condition is stated next.
Assumption 4. With probability approaching one, it holds that rank(XÎ¸(Ï„ ) + Î¾) = O(rÏ„ ), and
!
âˆš
âˆš âˆš
cT Ï†n rÏ„ ( n + dT )
kÎ¾kâˆ—
âˆš
âˆš
= OP
,
nT
nT f
with cT as defined in Theorem 1. Furthermore, kXÎ¸(Ï„ ) + Î (Ï„ )k = OP (1).
15

Notice that in Assumption 4, Î¾ is an approximation error. In the case Î¾ = 0, the condition implies that
rank(XÎ¸(Ï„ )) = O(rÏ„ ) with probability close to one.
Next, exploiting Assumption 4, we show that (21) provides consistent estimation of the quantile function, namely, of XÎ¸(Ï„ ) + Î (Ï„ ).
Theorem 3. Suppose that Assumptions 1â€“2 and 4 hold. Let (Î¸Ì‚(Ï„ ), Î Ì‚(Ï„ )) be the solution to (21) with the
additional constraint that kÎ Ìƒkâˆ â‰¤ C, for a large enough positive constant C.
Then


!
0 2 2
1
)
Ï†
c
r
(f
1
1
n T Ï„
,
kÎ Ì‚(Ï„ ) âˆ’ Î (Ï„ )k2F = OP
+
nT
n dT
f4
p
where {Ï†n } is a sequence with Ï†n /( f log(1 + cT )) â†’ âˆ, and for choices

Î½1 

n T
1 XX
kXi,t kâˆ ,
nT
i=1 t=1

and
Î½2 

p 
cT âˆš
n + dT .
nT

Interestingly, unlike Theorem 1, Theorem 3 does not show that we can estimate Î¸(Ï„ ) and Î (Ï„ ) separately. Instead, we show that Î Ì‚(Ï„ ), the estimated matrix of latent factors, captures the overall contribution of
both Î¸(Ï„ ) and Î (Ï„ ). This is expected since Assumption 4 states that, with high probability, XÎ¸(Ï„ ) has rank
of the same order as of Î (Ï„ ). Notably, Î Ì‚(Ï„ ) is able to estimate XÎ¸(Ï„ ) + Î (Ï„ ) via requiring that the value of
âˆš âˆš
Î½1 increases significantly with respect to the choice in Theorem 1, while keeping Î½2  cT ( n+ dT )/(nT ).
As for the convergence rate in Theorem 3 for estimating Î (Ï„ ), this is of the order rÏ„ cT (nâˆ’1 + dâˆ’1
T ), if
0
âˆ’1
âˆ’1
we ignore f , f , and Ï†n . When the data are independent, the rate becomes of the order rÏ„ (n + T ). In
such framework, our result matches the minimax rate of estimation in CandeÌ€s and Plan (2010) for estimating
an n Ã— T matrix of rank rÏ„ , provided that n  T , see our discussion in Section 4.1.
Finally, notice that we have added an additional tuning parameter C that controls the magnitude of the
possible estimate Î Ìƒ. This is done for technical reasons. We expect that the same upper bound holds without
this additional constraint.

5

Simulation

In this section, we evaluate the performance of our proposed approach (`1 -NN-QR) with extensive numerical
simulations focusing on the median case, namely the case when Ï„ = 0.5. As benchmarks, we consider the
`1 -penalized quantile regression studied in Belloni and Chernozhukov (2009), and similarly we refer to
16

this procedure as `1 -QR. We also compare with the mean case, which we denote it as `1 -NN-LS as it
combines the `2 -loss function with `1 and nuclear norm regularization. We consider different generative
scenarios. For each scenario we randomly generate 100 different data sets and compute the estimates of
the methods for a grid of values of Î½1 and Î½2 . Specifically, these tuning parameters are taken to satisfy
Î½1 âˆˆ {10âˆ’4 , 10âˆ’4.5 , . . . , 10âˆ’8 } and Î½2 âˆˆ {10âˆ’3 , 10âˆ’4 , . . . , 10âˆ’9 }. Given any choice of tuning parameters,
we evaluate the performance of each competing method, averaging over the 100 data sets, and report values
that correspond to the best performance. These are referred as optimal tuning parameters and can be thought
of as oracle choices.
We also propose a modified Bayesian Information Criterion (BIC) to select the best pair of tuning parameters. Given a pair (Î½1 , Î½2 ), our method produces a score (Î¸Ì‚(Ï„ ), Î Ì‚(Ï„ )). Specifically, denote sÌ‚Ï„ = |{j :
Î¸Ì‚j (Ï„ ) 6= 0}| and rÌ‚Ï„ = rank(Î Ì‚(Ï„ )),
BIC(Î½1 , Î½2 ) =

n X
T
X

0
ÏÏ„ (Yi,t âˆ’ Xi,t
Î¸Ì‚(Î½1 , Î½2 ) âˆ’ Î Ì‚(Î½1 , Î½2 ))+

(31)
log(nT )
(c1 Â· sÌ‚Ï„ (Î½1 , Î½2 ) + (1 + n + T ) Â· rÌ‚Ï„ (Î½1 , Î½2 )) ,
2
where c1 > 0 is a constant. The intuition here is that the first term in the right hand side of (31) corresponds
to the fit to the data. The second term includes the factor log(nT )/2 to emulate the usual penalization in
BIC. The number of parameters in the model with choices Î½1 and Î½2 is estimated by sÌ‚Ï„ for the vector of
coefficients, and (1 + n + T ) Â· rÌ‚Ï„ for the latent matrix. The latter is reasonable since Î Ì‚(Ï„ ) is potentially a low
rank matrix and we simply count the number of parameters in its singular value decomposition. As for the
extra quantity c1 , we have included this term to balance the dominating contribution of the (1 + n + T ) Â· rÌ‚Ï„ .
We find that in practice c1 = log2 (nT ) gives reasonable performance in both simulated and real data. This
is the choice that we use in our experiments. Then for each of data set under each design, we calculate the
minimum value of BIC(Î½1 , Î½2 ), over the different choices of Î½1 and Î½2 , and report the average over the 100
Monte Carlo simulations. We refer to this as BIC-`1 -NN-QR.
i=1 t=1

As performance measure we use a scaled version (see Tables 1-2) of the squared distance between
the true vector of coefficients Î¸ and the corresponding estimate. We also consider a different metric, the
â€œQuantile errorâ€ (Koenker and Machado (1999)):
n T
1 X X âˆ’1
(FYi,t |Xi,t ;Î¸(Ï„ ),Î (Ï„ ) (0.5) âˆ’ FÌ‚Yâˆ’1
(0.5))2 ,
i,t |Xi,t ;Î¸(Ï„ ),Î (Ï„ )
nT

(32)

i=1 t=1

which measures the average squared error between the quantile functions at the samples and their respective estimates. Since our simulations consider models with symmetric mean zero error, the above metric
corresponds to the mean squared error for estimating the conditional expectation.
Next, we provide a detailed description of each of the generative models that we consider in our experiments. In each model design the dimensions of the problem are given by n âˆˆ {100, 500}, p âˆˆ {5, 30} and
T âˆˆ {100, 500}. The covariates {Xi,t } are i.i.d N (0, Ip ).
17

Design 1. (Location shift model) The data is generated from the model
0
Yi,t = Xi,t
Î¸ + Î i,t + i,t ,

(33)

âˆš
i.i.d.
where 3i,t âˆ¼ t(3), i = 1, . . . , n and t = 1, . . . , T , with t(3) the Studentâ€™s t-distribution with 3 degrees
âˆš
of freedom. The scaling factor 3 simply ensures that the errors have variance 1. In (33), we take the vector
Î¸ âˆˆ Rp to satisfy
ï£±
ï£²1 if j âˆˆ {1, . . . , min{10, p}}
Î¸j =
ï£³0 otherwise.
We also construct Î  âˆˆ RnÃ—T to be rank one, defined as Î i,t = 5i (cos(4Ï€t/T ))/n.
Design 2. (Location-scale shift model) We consider the model
0
0
Yi,t = Xi,t
Î¸ + Î i,t + (Xi,t
Î¸)i,t ,

(34)

i.i.d.

where i,t âˆ¼ N (0, 1), i = 1, . . . , n and t = 1, . . . , T . The parameters in Î¸ and Î  in (34) are taken to be
the same as in (33). The only difference now is that we have the extra parameter Î¸ âˆˆ Rp , which we define
as Î¸j = j/(2p) for j âˆˆ {1, . . . , p}.
Design 3. (Location shift model with random factors) This is the same as Design 1 with the difference
that we now generate Î  as
5
X
Î i,t =
ck uk vkT ,
(35)
k=1

where
ck âˆ¼ U [0, 1/4], uk =

vÌƒk
uÌƒk
, uÌƒk âˆ¼ N (0, In ), vk =
, vÌƒk âˆ¼ N (0, In ), k = 1, . . . , 5.
kuÌƒk k
kvÌƒk k

(36)

Design 4. (Location-scale shift model with random factors) This is a combination of Designs 2 and
3. Specifically, we generate data as in (34) but with Î  satisfying (35) and (36).
The results in Tables 1-2 show a clear advantage of our proposed method against the benchmarks across
the four designs we consider. This is true for estimating the vector of coefficients, and under the measure of
quantile error. Importantly, our approach is not only the best under the optimal choice of tuning parameters
but it remains competitive with the BIC type of criteria defined with the score (31). In particular, under
Designs 1 and 2, the data driven version of our estimator, BIC-`1 -NN-QR, performs very closely to the
ideally tuned one `1 -NN-QR. In the more challenging settings of Designs 3 and 4, we noticed that BIC-`1 NN-QR performs reasonably well compared to `1 -NN-QR.

18

Table 1: For Designs 1-2 described in the main text, under different values of (n, p, T ), we compare the
performance of different methods. The metrics use are the scaled `2 distance for estimating Î¸(Ï„ ), and the
Quantile error defined in (32). For each method we report the average, over 100 Monte Carlo simulations,
of the two performance measures.

Method
`1 -NN-QR
BIC-`1 -NN-QR
`1 -NN-LS
`1 -QR
`1 -NN-QR
BIC-`1 -NN-QR
`1 -NN-LS
`1 -QR
`1 -NN-QR
BIC-`1 -NN-QR
`1 -NN-LS
`1 -QR
`1 -NN-QR
BIC-`1 -NN-QR
`1 -NN-LS
`1 -QR
`1 -NN-QR
BIC-`1 -NN-QR
`1 -NN-LS
`1 -QR
`1 -NN-QR
BIC-`1 -NN-QR
`1 -NN-LS
`1 -QR
`1 -NN-QR
BIC-`1 -NN-QR
`1 -NN-LS
`1 -QR
`1 -NN-QR
BIC-`1 -NN-QR
`1 -NN-LS
`1 -QR

n
300
300
300
300
300
300
300
300
300
300
300
300
300
300
300
300
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100

p
30
30
30
30
30
30
30
30
5
5
5
5
5
5
5
5
30
30
30
30
30
30
30
30
5
5
5
5
5
5
5
5

T
300
300
300
300
100
100
100
100
300
300
300
300
100
100
100
100
300
300
300
300
100
100
100
100
300
300
300
300
100
100
100
100

Design 1

Design 2

kÎ¸Ì‚(Ï„ )âˆ’Î¸(Ï„ )k2
10âˆ’4

kÎ¸Ì‚(Ï„ )âˆ’Î¸(Ï„ )k2
10âˆ’4

0.86
0.86
2.58
8.18
2.99
2.99
8.06
41.0
0.22
0.22
0.37
2.6
0.50
0.53
1.12
7.87
2.97
2.97
9.77
40.0
2.3
2.3
8.4
229.0
0.64
0.65
1.25
8.79
1.82
1.85
4.06
32.0

19

Quantile error
0.03
0.03
0.05
4.19
0.04
0.04
0.12
4.19
0.003
0.003
0.01
4.19
0.008
0.009
0.02
4.19
0.04
0.04
0.12
4.23
0.04
0.06
0.79
4.23
0.008
0.008
0.02
4.23
0.009
0.01
0.03
4.23

0.69
0.89
1.03
6.81
2.39
2.39
3.11
26.0
0.39
0.48
0.69
3.27
0.80
0.97
1.46
8.56
2.26
2.81
3.39
24.0
10.0
11.0
13.0
177.0
0.89
1.32
1.67
8.61
3.30
3.74
4.45
27.0

Quantile error
0.03
0.03
0.04
4.18
0.04
0.04
0.08
4.19
0.03
0.03
0.03
4.19
0.03
0.03
0.04
4.19
0.04
0.04
0.06
4.23
0.03
0.04
0.16
4.23
0.03
0.03
0.05
4.23
0.03
0.04
0.14
4.23

Table 2: For Designs 3-4 described in the main text, under different values of (n, p, T ), we compare the
performance of different methods. The metrics use are the scaled `2 distance for estimating Î¸(Ï„ ), and the
Quantile error defined in (32). For each method we report the average, over 100 Monte Carlo simulations,
of the two performance measures.

Method
`1 -NN-QR
BIC-`1 -NN-QR
`1 -NN-LS
`1 -QR
`1 -NN-QR
BIC-`1 -NN-QR
`1 -NN-LS
`1 -QR
`1 -NN-QR
BIC-`1 -NN-QR
`1 -NN-LS
`1 -QR
`1 -NN-QR
BIC-`1 -NN-QR
`1 -NN-LS
`1 -QR
`1 -NN-QR
BIC-`1 -NN-QR
`1 -NN-LS
`1 -QR
`1 -NN-QR
BIC-`1 -NN-QR
`1 -NN-LS
`1 -QR
`1 -NN-QR
BIC-`1 -NN-QR
`1 -NN-LS
`1 -QR
`1 -NN-QR
BIC-`1 -NN-QR
`1 -NN-LS
`1 -QR

n
300
300
300
300
300
300
300
300
300
300
300
300
300
300
300
300
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100

p
30
30
30
30
30
30
30
30
5
5
5
5
5
5
5
5
30
30
30
30
30
30
30
30
5
5
5
5
5
5
5
5

T
300
300
300
300
100
100
100
100
300
300
300
300
100
100
100
100
300
300
300
300
100
100
100
100
300
300
300
300
100
100
100
100

Design 3

Design 4

kÎ¸Ì‚(Ï„ )âˆ’Î¸(Ï„ )k2
10âˆ’4

kÎ¸Ì‚(Ï„ )âˆ’Î¸(Ï„ )k2
10âˆ’4

2.17
2.17
3.33
6.59
10.0
11.0
11.0
27.0
1.13
1.56
1.58
2.47
3.04
4.37
4.43
7.65
11.0
12.0
12.0
26.0
6.12
6.64
8.63
16.5
2.99
4.19
5.27
8.59
12.3
13.1
15.0
24.7

20

Quantile error
0.17
0.29
0.19
1.09
0.18
0.26
0.25
11.10
0.17
0.33
0.19
1.10
0.19
0.27
0.27
1.11
0.18
0.27
0.34
1.10
0.17
0.22
1.08
1.12
0.18
0.26
0.33
1.10
0.16
0.20
1.09
1.10

1.58
2.81
3.54
12.0
4.61
4.61
9.21
47.2
0.27
0.74
0.49
5.52
0.69
1.11
1.12
13.4
7.06
7.29
11.2
51.0
32.1
35.4
82.0
267.6
0.86
1.43
1.45
21.0
2.15
2.43
3.61
45.7

Quantile error
0.11
0.26
0.12
1.01
0.13
0.16
0.17
1.10
0.03
0.13
0.05
1.09
0.05
0.15
0.05
1.10
0.15
0.24
0.18
1.12
0.10
0.15
0.84
1.13
0.05
0.14
0.05
1.09
0.04
0.07
0.07
1.09

6

Empirical Performance of the â€œCharacteristics + Latent Factorâ€ Model
in Asset Pricing

Data Description
We use data from CRSP and Compustat to constrtuct 24 firm level characteristics that are documented to
explain the cross section and time series of stock returns in the finance and accounting literature. The
characteristics we choose include well-known drivers of stock returns such as beta, size, book-to-market,
momentum, volatility, liquidity, investment and profitability. Table 7 in the Appendix lists details of the
characteristics used and the methods to construct the data. We follow the procedures of Green et al. (2017)
to construct the characteristics of interest. The characteristics used in our model are standardized to have
zero mean and unit variance. Figure 1 plots the histogram of monthly stock returns and 9 standardized
firm characteristics. Each of them have different distribution patterns, suggesting the potential nonlinear
relationship between returns and firm characteristics, which can be potentially captured by our quantile
model.
Our empirical design is closely related to the characteristics model proposed by Daniel and Titman
(1997, 1998). To avoid any data snooping issue cause by grouping, we conduct the empirical analysis
at individual stock level. Specifically, we use the sample period from January 2000 to December 2018,
and estimate our model using monthly returns (228 months) from 1306 firms that have non-missing values
during this period.

Figure 1: Histograms of monthly stock returns (left) and firm characteristics (right).

21

A â€œCharacteristic + Latent Factorâ€ Asset Pricing Model
We apply our model to fit the cross section and time series of stock returns ((Lettau and Pelger, 2018)).
There are n assets (stocks), and the return of the each asset can potentially be explained by p observed asset
characteristics (sparse part) and r latent factors (dense part). The asset characteristics are the covariates in
our model. Our model imposes a sparse structure on the p characteristics so that only the characteristics
having the strongest explanatory powers are selected by the model. The part thatâ€™s unexplained by the firm
characteristics are captured by latent factors.
Suppose we have n stock returns (R1 ,...,Rn ), and p observed firm characteristics (X1 ,...,Xp ) over T
periods. The return quantile at level Ï„ of portfolio i in time t is assumed to be the following:
FRâˆ’1
(Ï„ ) = Xi,tâˆ’1,1 Î¸1 (Ï„ ) + ... + Xi,tâˆ’1,k Î¸k (Ï„ ) + ... + Xi,tâˆ’1,p Î¸p (Ï„ )+
i,t |Xi,tâˆ’1 ;Î¸(Ï„ ),Î»i (Ï„ ),gt (Ï„ )

Î»i (Ï„ )

gt (Ï„ )

(1 Ã— rÏ„ ) (rÏ„ Ã— 1)
where Xi,tâˆ’1,k is the k-th characteristic (for example, the book-to-market ratio) of asset i in time t âˆ’ 1.
The coefficient Î¸k captures the extent to which assets with higher/lower characteristic Xi,t,k delivers higher
average return. The term gt contains the rÏ„ latent factors in period t which captures systematic risks in the
market, and Î»i contains portfolio iâ€™s loading on these factors (i.e. exposure to risk).
There is a discussion in academic research on â€œfactor versus characteristicsâ€ in late 1990s and early
2000s. The factor/risk based view argues that an asset has higher expected returns because of its exposure
to risk factors (e.g. Fama-French 3 factors) which represent some unobserved systematic risk. An assetâ€™s
exposure to risk factors are measured by factor loadings. The characteristics view claims that stocks have
higher expected returns simply because they have certain characteristics (e.g. higher book-to-market ratios,
smaller market capitalization), which might be independent of systematic risk (Daniel and Titman (1997,
1998)). The formulation of our model accommodates both the factor view and the characteristics view.
The sparse part is similar to Daniel and Titman (1997, 1998), in which stock returns are explained by
firm characteristics. The dense part assumes a low-dimensional latent factor structure where the common
variations in stock returns are driven by several â€œrisk factorsâ€.

Empirical Results
We first get the estimates Î¸Ì‚(Ï„ ) and Î Ì‚(Ï„ ) at three different quantiles, Ï„ = {0.1, 0.5, 0.9} using our proposed
ADMM algorithm. We then decompose Î Ì‚(Ï„ ) into the products of its rÌ‚Ï„ principal components gÌ‚(Ï„ ) and
their loadings Î»Ì‚(Ï„ ) via eq(14). The (i, k)-th element of Î»Ì‚(Ï„ ), denoted as Î»Ì‚i,k (Ï„ ), can be interpreted as the
exposure of asset i to the k-th latent factor (or in finance terminology, â€œquantity of riskâ€). And the (t, k)-th
elements of gÌ‚(Ï„ ), denoted as gÌ‚t,k (Ï„ ), can be interpreted as the compensation of the risk exposure to the
k-th latent factor in time period t (or in finance terminology, â€œprice of riskâ€). The model are estimated with

22

different tuning parameters Î½1 and Î½2 , and we use our proposed BIC to select the optimal tuning parameters.
The details of the information criteria can be found in equation (31).
The tuning parameter Î½1 governs the sparsity of the coefficient vector Î¸. The larger Î½1 is, the larger
the shrinkage effect on Î¸. Figure 2 illustrate the effect of this shrinkage. With Î½2 fixed, as the value of Î½1
increases, more coefficients in the estimated Î¸ vector shrink to zero. From a statistical point of view, the
â€œeffective characteristicsâ€ that can explain stock returns are those with non-zero coefficient Î¸ at relatively
large values of Î½1 .

Figure 2: Estimated Coefficients as a Function of Î½1
The figure plots the estimated coefficient Î¸ when the tuning parameter Î½1 changes, for Ï„ = {0.1, 0.5, 0.9}.
The parameter Î½2 is fixed at log10 (Î½2 ) = âˆ’4.
Table 3 reports the relationship between tuning parameter Î½2 and rank of estimated Î  at different quantiles. It shows that the tuning parameter Î½2 governs the rank of matrix Î , and that as Î½2 increases, we
penalize more on the rank of matrix Î  through its nuclear norm.
The left panel of Table 4 reports the estimated coefficients in the sparse part when we fix the tuning
parameters at log10 (Î½1 ) = âˆ’3.5 and log10 (Î½2 ) = âˆ’4. The signs of some characteristics are the same across
the quantiles, e.g. size (mve), book-to-market (bm), momentum (mom1m, mom12m), accurals (acc), book
equity growth (egr), leverage (lev), and standardized unexpected earnings (sue). However, some characteristics have heterogenous effects on future returns at different quantiles. For example, at the 10% quantile,
23

Table 3: The estimated rank of Î .
log10 (Î½2 )

Ï„ = 0.1

Ï„ = 0.5

Ï„ = 0.9

-6.0
-5.5
-5.0
-4.5
-4.0
-3.5
-3.0

228
228
228
164
1
1
0

228
228
228
228
7
1
0

228
228
228
168
2
1
0

Note: Estimated under different values of turning parameter Î½2 , when Î½1 = 10âˆ’5 is fixed. The results are
reported for quantiles 10%, 50% and 90%.

high beta stocks have high future returns, which is consistent with results found via the CAPM; while at
50% and 90% quantile, high beta stocks have low future returns, which conforms the â€œlow beta anomalyâ€
phenomenon. Volatility (measured by both range and idiosyncratic volatility) is positively correlated with
future returns at 90% quantile, but negatively correlated with future returns at 10% and 50% percentile. The
result suggests that quantile models can capture a wider picture of the heterogenous relationship between
asset returns and firm characteristics at different parts of the distribution (Koenker (2000)).
Table 5 reports the selected optimal tuning parameters Î½1 and Î½2 for different quantiles. The tuning
parameters are selected via BIC based on (31) as discussed in Section 5. For every Î½1 and Î½2 , we get the
e 1 , Î½2 ) and Î (Î½
e 1 , Î½2 ) and the number of factors r = rank(Î (Î½
e 1 , Î½2 )). The Î¸ vector is sparse
estimates Î¸(Î½
with non-zero coefficients on selected characteristics. The 10% quantile of returns has only 1 latent factor,
and 3 selected characteristics. The median of returns has 7 latent factors and 2 selected characteristics. The
90% quantile of returns has 2 latent factors and 7 selected characteristics. Range is the only characteristic
selected across all 3 quantiles. Idiosyncratic volatility is selected at 10% and 90% quantiles, with opposite
signs. 1-month momentum is selected at 50% and 90% percentiles, with negative sign suggesting reversal
in returns.
Overall, the empirical evidence suggests that both firm characteristics and latent risk factors have valuable information in explaining stock returns. In addition, we find that the selected characteristics and number
of latent factors differ across the quantiles.

24

Table 4: Sparse Part Coefficients at Different Quantiles.

Fixed Î½1 and Î½2
Ï„ = 0.1
Ï„ = 0.5
Ï„ = 0.9
acc
range
beta
bm
chinv
dy
egr
ep
gma
idiovol
ill
invest
lev
lgr
mom12m
mom1m
mve
operprof
roaq
roeq
std dolvol
std turn
sue
turn

-0.089
-2.574
0.174
0.371
0
-0.086
-0.106
0.199
0
-1.229
-0.334
-0.097
0.183
-0.106
-0.166
-0.150
-0.038
0
0.221
0.073
0
0.310
0.105
-0.796

-0.074
-0.481
-0.116
0.175
0
0
-0.053
0.057
0.091
-0.071
-0.218
0
0.063
-0.037
-0.077
-0.384
-0.093
0.025
0.242
0.041
0
0
0.061
-0.083

-0.041
2.526
-0.406
0.263
-0.152
0.119
-0.091
-0.479
0.201
1.438
0
0.146
0.129
0
-0.117
-0.571
-0.811
0.088
-0.147
0
-0.039
-0.247
0.045
0.386

Optimal Î½1 and Î½2 (BIC)
Ï„ = 0.1
Ï„ = 0.5
Ï„ = 0.9
0
-2.372
0
0
0
0
0
0
0
-1.055
0
0
0
0
0
0
0
0
0
0
0
0
0
-0.330

0
-0.356
0
0
0
0
0
0
0
0
0
0
0
0
0
-0.286
0
0
0
0
0
0
0
0

0
2.429
-0.115
0.168
0
0
0
-0.391
0
1.286
0
0
0
0
0
-0.477
-0.667
0
0
0
0
0
0
0

Note: The left panel reports the estimated coefficient vector Î¸ in the sparse part for quantiles
10%, 50% and 90%, when the tuning parameters are fixed at log10 (Î½1 ) = âˆ’3.5, log10 (Î½2 ) =
âˆ’4. The right panel reports the estimated coefficient vector Î¸ under the when the turning
parameters are optimal, as selected by BIC (indicated in Table 5).

25

Table 5: Selected Optimal Tuning Parameters and
Number of Factors

optimal r
optimal Î½1
optimal Î½2

Ï„ = 0.1
1
âˆ’2.5
10
10âˆ’4

Ï„ = 0.5
7
âˆ’2.5
10
10âˆ’4

Ï„ = 0.9
2
âˆ’2.75
10
10âˆ’4

Note: This table reports the selected optimal tuning
parameter Î½1 and Î½2 that minimize the objective function in equation (31) for different quantiles.

Interpretation of Latent Factors
Table 6 below reports the variance in the matrix Î  explained by each Principal Component (PC) or latent
factor. At upper and lower quantiles, the first PC dominates. At the median there are more latent factors
accounting for the variations in Î , with second PC explaining 13.8% and third PC explaining 6.8%.
Table 6: Percentage of Î  explained by PC

PC1
PC2
PC3
PC4
PC5
PC6
PC7
Total

Ï„ = 0.1
100.00%

100.00%

Ï„ = 0.5
73.82%
13.71%
6.78%
4.12%
1.11%
0.45%
0.01%
100.00%

Ï„ = 0.9
99.68%
0.32%

100.00%

Note: Variance of matrix Î  explained by each principal component for different quantiles.

We also found the first PC captures the market returns in all three quantiles: Figure 3 plots the first
principal component against the monthly returns of S&P500 index, showing that they have strong positive
correlations.

26

Figure 3: The S&P 500 Index Return and the First PC at Different Quantiles.
This figure plots the first PC of matrix Î  against S&P500 index monthly return for quantiles 10% (left),
50% (middle), and 90% (right).

References
Jason Abrevaya and Christian M Dahl. The effects of birth inputs on birthweight: evidence from quantile
estimation on panel data. Journal of Business & Economic Statistics, 26(4):379â€“397, 2008.
Alnur Ali, Zico Kolter, and Ryan Tibshirani. The multiple quantile graphical model. In Advances in Neural
Information Processing Systems, pages 3747â€“3755, 2016.
Tomohiro Ando and Jushan Bai. Quantile co-movement in financial markets: A panel quantile model with
unobserved heterogeneity. Journal of the American Statistical Association, pages 1â€“31, 2019.
Manuel Arellano and SteÌphane Bonhomme. Quantile selection models with an application to understanding
changes in wage inequality. Econometrica, 85(1):1â€“28, 2017.
Susan Athey, Mohsen Bayati, Nikolay Doudchenko, Guido Imbens, and Khashayar Khosravi. Matrix completion methods for causal panel data models. 2018.
Jushan Bai. Panel data models with interactive fixed effects. Econometrica, 77(4):1229â€“1279, 2009.
Jushan Bai and Junlong Feng. Robust principal components analysis with non-sparse errors. arXiv preprint
arXiv:1902.08735, 2019.
Jushan Bai and Kunpeng Li. Statistical analysis of factor models of high dimension. The Annals of Statistics,
40(1):436â€“465, 2012.
Jushan Bai and Serena Ng. Principal components estimation and identification of static factors. Journal of
Econometrics, 176(1):18â€“29, 2013.
Jushan Bai and Serena Ng. Principal components and regularized estimation of factor models. arXiv preprint
arXiv:1708.08137, 2017.
Jushan Bai and Serena Ng. Matrix completion, counterfactuals, and factor analysis of missing data. arXiv
preprint arXiv:1910.06677, 2019.

27

Alexandre Belloni and Victor Chernozhukov. On the computational complexity of mcmc-based estimators
in large samples. The Annals of Statistics, 37(4):2011â€“2055, 2009.
Alexandre Belloni and Victor Chernozhukov. `1 -penalized quantile regression in high-dimensional sparse
models. The Annals of Statistics, 39(1):82â€“130, 2011.
Peter Bickel, Yaâ€™acov Ritov, and Alexandre Tsybakov. Simultaneous analysis of lasso and dantzig selector.
The Annals of Statistics, 37(4):1705â€“1732, 2009.
Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein. Distributed optimization
and statistical learning via the alternating direction method of multipliers. Foundations and Trends R in
Machine learning, 3(1):1â€“122, 2011.
Pratik Prabhanjan Brahma, Yiyuan She, Shijie Li, Jiade Li, and Dapeng Wu. Reinforced robust principal
component pursuit. IEEE transactions on neural networks and learning systems, 29(5):1525â€“1538, 2017.
Jian-Feng Cai, Emmanuel CandeÌ€s, and Zuowei Shen. A singular value thresholding algorithm for matrix
completion. SIAM Journal on Optimization, 20(4):1956â€“1982, 2010.
John Campbell. Financial decisions and markets: a course in asset pricing. Princeton University Press,
2017.
Emmanuel CandeÌ€s and Yaniv Plan. Matrix completion with noise. Proceedings of the IEEE, 98(6):925â€“936,
2010.
Emmanuel CandeÌ€s and Yaniv Plan. Tight oracle inequalities for low-rank matrix recovery from a minimal
number of noisy random measurements. IEEE Transactions on Information Theory, 57(4):2342â€“2359,
2011.
Emmanuel CandeÌ€s and Benjamin Recht. Exact matrix completion via convex optimization. Foundations of
Computational mathematics, 9(6):717, 2009.
Emmanuel CandeÌ€s and Terence Tao. The dantzig selector: Statistical estimation when p is much larger than
n. The annals of Statistics, 35(6):2313â€“2351, 2007.
Gary Chamberlain and Michael Rothschild. Arbitrage, factor structure, and mean-variance analysis on large
asset markets. Econometrica (pre-1986), 51(5):1281, 1983.
Sourav Chatterjee. Matrix estimation by universal singular value thresholding. The Annals of Statistics, 43
(1):177â€“214, 2015.
Liang Chen, Juan Dolado, and JesuÌs Gonzalo. Quantile factor models. 2018.
Mingli Chen. Estimation of nonlinear panel models with multiple unobserved effects. Warwick Economics
Research Paper Series No. 1120, 2014.
Mingli Chen, IvaÌn FernaÌndez-Val, and Martin Weidner. Nonlinear panel models with interactive effects.
arXiv preprint arXiv:1412.5647, 2014.
Victor Chernozhukov, Christian Hansen, and Yuan Liao. A lava attack on the recovery of sums of dense and
sparse signals. The Annals of Statistics, 45(1):39â€“76, 2017.

28

Victor Chernozhukov, Christian Hansen, Yuan Liao, and Yinchu Zhu. Inference for heterogeneous effects
using low-rank estimations. arXiv preprint arXiv:1812.08089, 2018.
John H Cochrane. Asset pricing: Revised edition. Princeton university press, 2009.
John H Cochrane. Presidential address: Discount rates. The Journal of finance, 66(4):1047â€“1108, 2011.
Gregory Connor and Robert A Korajczyk. Risk and return in an equilibrium apt: Application of a new test
methodology. Journal of financial economics, 21(2):255â€“289, 1988.
Arnak Dalalyan, Mohamed Hebiri, and Johannes Lederer. On the prediction performance of the lasso.
Bernoulli, 23(1):552â€“581, 2017.
Kent Daniel and Sheridan Titman. Evidence on the characteristics of cross sectional variation in stock
returns. the Journal of Finance, 52(1):1â€“33, 1997.
Kent Daniel and Sheridan Titman. Characteristics or covariances. Journal of Portfolio Management, 24(4):
24â€“33, 1998.
Andreas Elsener and Sara van de Geer. Robust low-rank matrix estimation. The Annals of Statistics, 46
(6B):3481â€“3509, 2018.
Eugene F Fama and Kenneth R French. Common risk factors in the returns on stocks and bonds. Journal of
financial economics, 33(1):3â€“56, 1993.
Maryam Fazel. Matrix rank minimization with applications. 2002.
Guanhao Feng, Stefano Giglio, and Dacheng Xiu. Taming the factor zoo: A test of new factors. Technical
report, National Bureau of Economic Research, 2019.
Junlong Feng.
Regularized quantile regression with interactive fixed effects.
arXiv:1911.00166, 2019.

arXiv preprint

Antonio Galvao. Quantile regression for dynamic panel data with fixed effects. Journal of Econometrics,
164(1):142â€“157, 2011.
Antonio Galvao and Kengo Kato. Smoothed quantile regression for panel data. Journal of econometrics,
193(1):92â€“112, 2016.
Antonio Galvao and Gabriel V Montes-Rojas. Penalized quantile regression for dynamic panel data. Journal
of Statistical Planning and Inference, 140(11):3476â€“3497, 2010.
Domenico Giannone, Michele Lenza, and Giorgio Primiceri. Economic predictions with big data: The
illusion of sparsity. 2017.
Stefano Giglio and Dacheng Xiu. Asset pricing with omitted factors. Chicago Booth Research Paper,
(16-21), 2018.
Bryan S Graham, Jinyong Hahn, Alexandre Poirier, and James L Powell. A quantile correlated random
coefficients panel data model. Journal of Econometrics, 206(2):305â€“335, 2018.
Jeremiah Green, John Hand, and Frank Zhang. The characteristics that provide independent information
about average us monthly stock returns. The Review of Financial Studies, 30(12):4389â€“4436, 2017.

29

Yufeng Han, Ai He, David Rapach, and Guofu Zhou. What firm characteristics drive us stock returns?
Available at SSRN 3185335, 2018.
Xuming He, Lan Wang, and Hyokyoung Grace Hong. Quantile-adaptive model-free variable screening for
high-dimensional heterogeneous data. The Annals of Statistics, 41(1):342â€“369, 2013.
Kengo Kato, Antonio F Galvao Jr, and Gabriel V Montes-Rojas. Asymptotics for panel quantile regression
models with individual effects. Journal of Econometrics, 170(1):76â€“91, 2012.
Roger Koenker. Galton, edgeworth, frisch, and prospects for quantile regression in econometrics. Journal
of Econometrics, 95(2):347â€“374, 2000.
Roger Koenker. Quantile regression for longitudinal data. Journal of Multivariate Analysis, 91(1):74â€“89,
2004.
Roger Koenker. Quantile regression. Cambridge University Press, New York, 2005.
Roger Koenker and Jose AF Machado. Goodness of fit and related inference processes for quantile regression. Journal of the american statistical association, 94(448):1296â€“1310, 1999.
Roger Koenker, Victor Chernozhukov, Xuming He, and Limin Peng. Handbook of quantile regression. CRC
press, 2017.
Vladimir Koltchinskii, Karim Lounici, and Alexandre Tsybakov. Nuclear-norm penalization and optimal
rates for noisy low-rank matrix completion. The Annals of Statistics, 39(5):2302â€“2329, 2011.
Serhiy Kozak, Stefan Nagel, and Shrihari Santosh. Shrinking the cross-section. Journal of Financial
Economics, 2019.
Carlos Lamarche. Robust penalized quantile regression estimation for panel data. Journal of Econometrics,
157(2):396â€“408, 2010.
Martin Lettau and Markus Pelger. Estimating latent asset-pricing factors. Technical report, National Bureau
of Economic Research, 2018.
Shujie Ma, Oliver Linton, and Jiti Gao. Estimation and inference in semiparametric quantile factor models.
2019.
Hyungsik Roger Moon and Martin Weidner. Nuclear norm regularized estimation of panel regression models. arXiv preprint arXiv:1810.10987, 2018.
Sahand Negahban and Martin Wainwright. Estimation of (near) low-rank matrices with noise and highdimensional scaling. The Annals of Statistics, 39(2):1069â€“1097, 2011.
Benjamin Recht, Maryam Fazel, and Pablo Parrilo. Guaranteed minimum-rank solutions of linear matrix
equations via nuclear norm minimization. SIAM review, 52(3):471â€“501, 2010.
Angelika Rohde and Alexandre Tsybakov. Estimation of high-dimensional low-rank matrices. The Annals
of Statistics, 39(2):887â€“930, 2011.
Stephen Ross. The arbitrage theory of capital asset pricing. Journal of Economic Theory, 13(3):341â€“360,
1976.

30

AndreÌs G Sagner. Three essays on quantile factor analysis. PhD thesis, Boston University, 2019.
Yiyuan She and Kun Chen. Robust reduced-rank regression. Biometrika, 104(3):633â€“647, 2017.
Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society:
Series B (Methodological), 58(1):267â€“288, 1996.
Aad W. van der Vaart and Jon A Wellner. Weak convergence and empirical processes: with applications to
statistics. Springer, 1996.
Van Vu. Spectral norm of random matrices. Combinatorica, 27(6):721â€“736, 2007.
Lan Wang, Yichao Wu, and Runze Li. Quantile regression for analyzing heterogeneity in ultra-high dimension. Journal of the American Statistical Association, 107(497):214â€“222, 2012.
Yuanshan Wu and Guosheng Yin. Conditional quantile screening in ultrahigh-dimensional heterogeneous
data. Biometrika, 102(1):65â€“76, 2015.
Bin Yu. Rates of convergence for empirical processes of stationary mixing sequences. The Annals of
Probability, pages 94â€“116, 1994.
Yi Yu, Tengyao Wang, and Richard J Samworth. A useful variant of the davisâ€“kahan theorem for statisticians. Biometrika, 102(2):315â€“323, 2014.
Qi Zheng, Limin Peng, and Xuming He. Globally adaptive quantile regression with ultra-high dimensional
data. Annals of statistics, 43(5):2225, 2015.

31

A

Implementation Details of the Proposed ADMM Algorithm

Denoting by P+ (Â·) and Pâˆ’ (Â·) the element-wise positive and negative part operators, the ADMM proceeds
doing the iterative updates

V

(k+1)

Î¸Ìƒ(k+1)





Ï„
Ï„
(k)
(k)
(k)
(k)
0
0
â† P+ W âˆ’ UV âˆ’
11 + Pâˆ’ W âˆ’ UV âˆ’
11
(37)
nT Î·
nT Î·
( n T
)
2 Î·
Î· X X  (k)
(k)
(k)
(k)
(k)
0
â† arg min
Wi,t âˆ’ Yi,t + Xi,t
Î¸ + (ZÎ  )i,t + (UW )i,t + kZÎ¸ âˆ’ Î¸ + UÎ¸ k2
2
2
Î¸
i=1 t=1

(38)
Î Ìƒ(k+1) â† arg min



Î Ìƒ

Î½2
1 (k)
(k)
kZÎ  âˆ’ Î Ìƒ + UÎ  k2F + kÎ Ìƒkâˆ—
2
Î·


(39)

(40)
ï£¼
p
ï£½
Î½1 X
(k+1)
(k)
ZÎ¸
â† arg min
kÎ¸Ìƒ(k+1) âˆ’ UÎ¸ âˆ’ ZÎ¸ k2 +
wj |(ZÎ¸ )j |
(41)
ï£³2
ï£¾
Î·
ZÎ¸
j=1
(
Î·
Î·
(k)
(k)
(k+1)
kW âˆ’ Y + X Î¸Ìƒ(k+1) + ZÎ  + UW k2F + kV (k+1) âˆ’ W + UV k2F
(ZÎ 
, W (k+1) ) â† arg min
2
2
ZÎ  ,W
ï£±
ï£²1

(42)
Î·
(k)
+ kZÎ  âˆ’ Î Ìƒ(k+1) + UÎ  k2F
2

(k+1)

UV

(k+1)

(k+1)

(43)

(k)

UW

(k+1)

âˆ’ Î Ìƒ(k+1) + UÎ  , UÎ¸

â† V (k+1) âˆ’ W (k+1) + UV ,
UÎ 

)

â† ZÎ 

(k+1)

â† W (k+1) âˆ’ Y + X Î¸Ìƒ(k+1) + ZÎ 
(k)

(k+1)

(k+1)

â† ZÎ¸

(k)

âˆ’ Î¸Ìƒ(k+1) + UÎ¸ ,

where Î· > 0 is the penalty, see Boyd et al. (2011).
The update for Î¸Ìƒ is
"
Î¸Ìƒ

(k+1)

â†

n X
T
X

#âˆ’1 "
0
Xi,t Xi,t

âˆ’

+ Ip

i=1 t=1

n X
T
X

#
Xi,t Ai,t +

(k)
ZÎ¸

i=1 t=1

where
(k)

(k)

A := W (k) + ZÎ  + UW âˆ’ Y.
The update for Î Ìƒ is
Î Ìƒ

(k+1)



Î½2
â† P diag max 0, vj âˆ’
Î·
32

!



Q0 ,
1â‰¤jâ‰¤l

+

(k)

+ UW ,

(k)
UÎ¸

,

where
(k)

(k)

Z Î  + UÎ 

= P diag({vj }1â‰¤jâ‰¤l )Q0 .

Furthermore, for ZÎ¸ ,
(k+1)

ZÎ¸,j

(k+1)

â† sign(Î¸Ìƒj



Î½1 wj
(k)
(k+1)
(k)
âˆ’ UÎ¸,j ) |Î¸Ìƒj
âˆ’ UÎ¸,j | âˆ’
.
Î·

Finally, defining
(k)

(k)

(k)

AÌƒ = âˆ’Y + X Î¸Ìƒ(k+1) + UW , BÌƒ = âˆ’V (k+1) âˆ’ UV , CÌƒ = âˆ’Î Ìƒ(k+1) + UÎ  ,
the remaining updates are
(k+1)

ZÎ 

â†

âˆ’AÌƒ âˆ’ 2CÌƒ + BÌƒ
,
3

and
(k+1)

W (k+1) â† âˆ’AÌƒ âˆ’ CÌƒ âˆ’ 2ZÎ 

B

.

Proofs of the Main Results in the Paper

B.1

Auxiliary lemmas for proof of Theorem 1

Throughout, we use the notation
QÏ„ (Î¸Ìƒ, Î Ìƒ) = E(QÌ‚Ï„ (Î¸Ìƒ, Î Ìƒ)).
Moreover, as in Yu (1994), we define the sequence {(YÌƒi,t , XÌƒi,t )}iâˆˆ[n],tâˆˆ[T ] such that
ei,t )}iâˆˆ[n],tâˆˆ[T ] is independent of {(Yi,t , Xi,t )}iâˆˆ[n],tâˆˆ[T ] ;
â€¢ {(Yei,t , X
ei,t )}iâˆˆ[n] are independent;
â€¢ for a fixed t the random vectors {(Yei,t , X
â€¢ for a fixed i:
ei,t )}tâˆˆH ) = L({(Yi,t , Xi,t )}tâˆˆH ) = L({(Yi,t , Xi,t )}tâˆˆH ) âˆ€l âˆˆ [dT ],
L({(Yei,t , X
1
l
l
ei,t )}tâˆˆH , . . . , {(Yei,t , X
ei,t )}tâˆˆH are independent.
and the blocks {(Yei,t , X
1
dT
Here, we define Î› := {H1 , H10 , . . . , HdT , Hd0 T , R} with
Hj
Hj0

= {t : 1 + 2(j âˆ’ 1)cT â‰¤ t â‰¤ (2j âˆ’ 1)cT } ,
= {t : 1 + (2j âˆ’ 1)cT â‰¤ t â‰¤ 2jcT } , j = 1, . . . , dT ,
and R = {t : 2cT dT + 1 â‰¤ t â‰¤ T }.
33

(44)

We also use the symbol L(Â·) to denote the distribution of a sequence of random variables.
0 Î¸(Ï„ ) + Î  (Ï„ )}, and aÌƒ
e
e0
Next, define the scores ai,t = Ï„ âˆ’ 1{Yi,t â‰¤ Xi,t
i,t
i,t = Ï„ âˆ’ 1{Yi,t â‰¤ Xi,t Î¸(Ï„ ) +
Î i,t (Ï„ )}.

Lemma 4. Under Assumptions 1â€“3, we have
ï£«
ï£¶
s
 Âµ
n X
T
X
Xi,t,j ai,t
1
16
1
cT log(max{n, pcT }) ï£¸
ï£­
P
max
â‰¥ 9
â‰¤
+ 8npT
.
j=1,...,p nT
ÏƒÌ‚j
ndT
n
cT
i=1 t=1

Proof. Notice that
!
n T
1 X X Xi,t,j ai,t
P max
â‰¥ Î·|X
j=1,...,p nT
ÏƒÌ‚j
ï£¶
ï£¶
ï£« i=1 t=1
ï£«
2(lâˆ’1)+cT
dT
n X
X
X
Xi,t,j ait ï£¸
1
Î·
ï£­1
â‰¤ 2p max P ï£­
â‰¥ |X ï£¸ +
j=1,...,p
ndT
cT
ÏƒÌ‚j
9
i=1 l=1
t=2(lâˆ’1)+1
!
!
n
1 X 1 X Xi,t,j
Î·
p max P
â‰¥ |X
j=1,...,p
ndT
c
ÏƒÌ‚j
9
i=1 T
tâˆˆR
!
d
âˆ’1
n
T
1 X X Xi, (2lcT +m), j aÌƒi,t
Î·
â‰¤ 4p max P
max
â‰¥ |X +
m=1,...,cT ndT
j=1,...,p
ÏƒÌ‚j
9
i=1 l=0
!
 Âµ
n
1 X Xi, (2dT cT +m) j aÌƒi,t
1
Î·
max
2p max P
â‰¥ |X + 8npT
j=1,...,p
ÏƒÌ‚j
9
cT
m=1,...,|R| ndT
i=1
!
d
âˆ’1
n
T
Xi, (2lcT +m), j aÌƒi,t
1 X X
Î·
â‰¤ 4pcT
max P
â‰¥ |X +
ndT
ÏƒÌ‚j
9
jâˆˆ[p],mâˆˆ[cT ]
i=1 l=0
!
 Âµ
n
X
Xi, (2dT cT +m), j aÌƒi,t
1
1
Î·
2pcT
max
P
â‰¥ |X + 8npT
ndT
ÏƒÌ‚j
9
cT
jâˆˆ[p],mâˆˆ[|R|]

(45)

i=1

where the first inequality follows from union bound, and the second by Lemmas 4.1 and 4.2 from Yu (1994).
Therefore, since
n dT âˆ’1
1 X X
Xi,2 (2lcT +m) ,j â‰¤ 3cT ÏƒÌ‚j2 ,
ndT
i=1 l=0

and with a similar argument for the second term in the last inequality of (45), we obtain the result by
Hoeffdingâ€™s inequality and integrating over X.
Lemma 5. Supposes that Assumptions 1â€“3 hold, and let
(
G =

âˆ†âˆˆ

RnÃ—T

34

: kâˆ†kâˆ— â‰¤ 1

)
.

(46)

Then there exists positive constants c1 and c2 such that
n T
1 XX
sup
âˆ†i,t ai,t
âˆ†âˆˆG nT

â‰¤

i=1 t=1

p 
100cT âˆš
n + dT ,
nT

with probability at least

1 âˆ’ 2nT

1
cT

Âµ
âˆ’ 2c1 exp(âˆ’c2 max{n, dT } + log cT ),

for some positive constants c1 and c2 .
Proof. Notice that by Lemma 4.3 from Yu (1994),

ï£«

ï£®
1
nT

dT X
n X
X

dT X
n X
X

n X
X

ï£¹

ï£¶

1
1
âˆ†i,t ai,t ï£» â‰¥ Î· ï£¸
âˆ†i,t ai,t +
nT
nT
âˆ†âˆˆG
0
i=1 l=1 tâˆˆHl
i=1 tâˆˆR
ï£¶i=1 l=1
ï£«tâˆˆHl
ï£¶
ï£«
d
d
n
n
T X
T X
X
X
X
X
1
Î·
Î·
1
âˆ†i,t aÌƒi,t â‰¥ ï£¸ + P ï£­ sup
âˆ†i,t0 aÌƒi,t â‰¥ ï£¸ +
â‰¤ P ï£­ sup
3
3
âˆ†âˆˆG nT i=1 l=1
âˆ†âˆˆG nT i=1 l=1 tâˆˆH
tâˆˆHl0
l
!


n
1 XX
Î·
1 Âµ
P sup
+ 2nT
âˆ†i,t aÌƒi,t â‰¥
3
cT
âˆ†âˆˆG nT i=1 tâˆˆR
!
d
âˆ’1
n
T
1 X X
Î·
â‰¤ 2cT max P sup
âˆ†i, (2cT l+m) aÌƒi, (2cT l+m) â‰¥
+
9
mâˆˆ[cT ]
âˆ†âˆˆG ndT i=1 l=0
!
 Âµ
n
1 X
1
Î·
+ 2nT
2cT max P sup
âˆ†i, (2cT dT +m) aÌƒi, (2cT dT +m) â‰¥
9
cT
mâˆˆ[|R|]
âˆ†âˆˆG ndT i=1
(47)
P ï£­ sup ï£°

âˆ†i,t ai,t +

We now proceed to bound each of the terms in the upper bound of (47). For the first term, notice that for
a fixed m
n dT âˆ’1
1 X X
sup
âˆ†i, (2cT l+m) aÌƒi, (2cT l+m)
âˆ†âˆˆG ndT
i=1 l=0

1 
aÌƒi, (2cT l+m)
âˆ†âˆˆG ndT
p 
3 âˆš
â‰¤
n + dT ,
ndT
â‰¤ sup

iâˆˆ[n],lâˆˆ[dT ] 2

kâˆ†kâˆ—

(48)

where the first inequality holds by the duality between the nuclear norm and spectral norm, and the second
inequality happens with probability at least 1 âˆ’ c1 exp (âˆ’c2 max{n, dT }) by Theorem 3.4 from Chatterjee
(2015).
On the other hand,
n
1 X
sup
âˆ†i, (2cT dT +m) aÌƒi, (2cT dT +m)
âˆ†âˆˆG ndT
i=1

35

âˆš

nk{âˆ†i, (2cT dT +m) }iâˆˆ[n] k
â‰¤ sup
ndT
âˆ†âˆˆG
âˆš
nkâˆ†kâˆ—
â‰¤
,
ndT

(49)

with probability at least 1 âˆ’ c1 exp (âˆ’c2 max{n, dT }), also by Theorem 3.4 from Chatterjee (2015).
âˆš
âˆš
âˆš
The claim follows by combining (47), (48), and (49), taking Î· = 30( n + dT )/ ndT , and the fact
that cT /T â‰¤ 1/3.

Lemma 6. For every Î Ìƒ, Î ÌŒ âˆˆ RnÃ—T , we have that
q
kÎ ÌŒ âˆ’ Î Ìƒkâˆ— + kÎ ÌŒkâˆ— âˆ’ kÎ Ìƒkâˆ— â‰¤ 6 rank(Î ÌŒ)kÎ ÌŒ âˆ’ Î ÌƒkF
Proof. This follows directly from Lemma 2.3 in Elsener and van de Geer (2018).
Lemma 7. Assume that 1â€“3 hold. Then, with probability approaching one,
3
5
kÎ¸k1 â‰¤ kÎ¸k1,n,T â‰¤ kÎ¸k1 ,
4
4

(50)

for all Î¸ âˆˆ Rp .
Moreover, for c0 âˆˆ (0, 1) letting
9
Î½1 =
1 âˆ’ c0

s

p
cT log(max{n, pcT }) âˆš
( n + dT ),
ndT

and
Î½2 =

p 
200cT âˆš
n + dT ,
nT

we have that
(Î¸Ì‚(Ï„ ) âˆ’ Î¸(Ï„ ), Î Ì‚(Ï„ ) âˆ’ Î (Ï„ )) âˆˆ AÏ„ ,
with probability approaching one, where
(
AÏ„ =

(Î´, âˆ†) : kÎ´TÏ„c k1 +

kâˆ†kâˆ—
âˆš âˆš
nT log(max{n,pcT })


â‰¤ C0 kÎ´TÏ„ k1 +

âˆš
âˆš

and C0 is a positive constant that depends on Ï„ and c0 .
Proof. By Lemma 4, Lemma 5, Lemma 6, and Assumption 2, we have that

36

nT

âˆš

rÏ„ kâˆ†kF

log(max{n,pcT })

)
,



0 â‰¤ QÌ‚(Î¸(Ï„ ), Î (Ï„ )) âˆ’ QÌ‚(Î¸Ì‚(Ï„ ), Î Ì‚(Ï„ )) + Î½1 kÎ¸(Ï„ )k1,n âˆ’ kÎ¸Ì‚(Ï„ )k1,n,T + Î½2 (kÎ (Ï„ )kâˆ— âˆ’ kÎ Ì‚(Ï„ )kâˆ— )
n T
1 X X Xi,t,j ai,t
â‰¤ max
1â‰¤jâ‰¤p nT
ÏƒÌ‚j

" p
X

i=1 t=1

#



ÏƒÌ‚k |Î¸k (Ï„ ) âˆ’ Î¸Ì‚k (Ï„ )| + Î½1 kÎ¸(Ï„ )k1,n,T âˆ’ kÎ¸Ì‚(Ï„ )k1,n,T +

k=1

n X
T
X

1
ai,t (Î i,t (Ï„ ) âˆ’ Î Ì‚i,t (Ï„ )) + Î½2 (kÎ (Ï„ )kâˆ— âˆ’ kÎ Ì‚(Ï„ )kâˆ— )
nT
i=1 t=1
s
" p
#


cT log(max{n, pcT }) X
ÏƒÌ‚k |Î¸k (Ï„ ) âˆ’ Î¸Ì‚k (Ï„ )| + Î½1 kÎ¸(Ï„ )k1,n,T âˆ’ kÎ¸Ì‚(Ï„ )k1,n,T
â‰¤ 9
ndT
k=1
!
n T
1 XX
Ëœ i,t + Î½2 (kÎ (Ï„ )kâˆ— âˆ’ kÎ Ì‚(Ï„ )kâˆ— )
+ kÎ (Ï„ ) âˆ’ Î Ì‚(Ï„ )kâˆ—
sup
ai,t âˆ†
nT
Ëœ
kâˆ†kâˆ— â‰¤1
i=1 t=1
s
#
" p


cT log(max{n, pcT }) X
ÏƒÌ‚k |Î¸k (Ï„ ) âˆ’ Î¸Ì‚k (Ï„ )| + Î½1 kÎ¸(Ï„ )k1,n,T âˆ’ kÎ¸Ì‚(Ï„ )k1,n,T
â‰¤ 9
ndT
k=1

+


p 
p 
200cT âˆš
200cT âˆš
n + dT kÎ (Ï„ ) âˆ’ Î Ì‚(Ï„ )kâˆ— +
n + dT
kÎ (Ï„ )kâˆ— âˆ’ kÎ Ì‚(Ï„ )kâˆ— ,
nT
nT

p 
100cT âˆš
n + dT kÎ (Ï„ ) âˆ’ Î Ì‚(Ï„ )kâˆ—
nT
s
" p
#


cT log(max{n, pcT }) X
ÏƒÌ‚k |Î¸k (Ï„ ) âˆ’ Î¸Ì‚k (Ï„ )| + Î½1 kÎ¸(Ï„ )k1,n,T âˆ’ kÎ¸Ì‚(Ï„ )k1,n,T
â‰¤ 9
ndT
âˆ’

k=1

âˆš

+

p 
1200cT rÏ„ âˆš
n + dT kÎ (Ï„ ) âˆ’ Î Ì‚(Ï„ )kF
nT

âˆ’

p 
100cT âˆš
n + dT kÎ (Ï„ ) âˆ’ Î Ì‚(Ï„ )kâˆ—
nT

with probability at least
16
âˆ’ 8npT
1âˆ’Î³âˆ’
n



1
cT

Âµ


âˆ’ 2nT

1
cT

Âµ
âˆ’ 2c1 exp(âˆ’c2 max{n, dT } + log cT ).

Therefore, with probability approaching one, for positive constants C1 and C2 , we have
ï£®
ï£¹
p 

X
0 â‰¤ ï£°
(1 âˆ’ c0 )ÏƒÌ‚j |Î¸Ì‚j (Ï„ ) âˆ’ Î¸j (Ï„ )| + ÏƒÌ‚j |Î¸j (Ï„ )| âˆ’ ÏƒÌ‚j |Î¸Ì‚j (Ï„ )| ï£»
j=1


+

âˆš
3C1 rÏ„ kÎ (Ï„ )âˆ’Î Ì‚(Ï„ )kF
âˆš âˆš
nT log(max{n,pcT })

âˆ’
37

C2 kÎ (Ï„ )âˆ’Î Ì‚(Ï„ )kâˆ—
âˆš âˆš
nT log(max{n,pcT })


,

and the claim follows.

Lemma 8. Under Assumption 3, for all (Î´, âˆ†) âˆˆ AÏ„ , we have that
ï£±
ï£¼
2
ï£´
ï£´
ï£² JÏ„1/2 (Î´, âˆ†)
ï£½
1/2
QÏ„ (Î¸(Ï„ ) + Î´, Î (Ï„ ) + âˆ†) âˆ’ QÏ„ (Î¸(Ï„ ), Î (Ï„ )) â‰¥ min
, qJÏ„ (Î´, âˆ†) .
ï£´
ï£´
4
ï£³
ï£¾
Proof. Let
(
vAÏ„



Ëœ âˆ’ QÏ„ (Î¸(Ï„ ), Î (Ï„ )) â‰¥
= sup v : QÏ„ (Î¸(Ï„ ) + Î´Ìƒ, Î (Ï„ ) + âˆ†)
v
)
1/2
Ëœ â‰¤v .
JÏ„ (Î´Ìƒ, âˆ†)

1/2

JÏ„

Ëœ
(Î´Ìƒ,âˆ†)
4

2

Ëœ âˆˆ AÏ„ ,
, âˆ€(Î´Ìƒ, âˆ†)

Then by the convexity of QÏ„ (Â·) and the definition of vAu , we have that
Ëœ âˆ’ QÏ„ (Î¸(Ï„ ), Î (Ï„ ))
QÏ„ (Î¸(Ï„ ) + Î´Ìƒ, Î (Ï„(
) + âˆ†)


â‰¥
â‰¥

(Î´,âˆ†)

2

4

2
1/2
JÏ„ (Î´,âˆ†)
4


â‰¥

1/2

JÏ„

1/2

JÏ„

(Î´,âˆ†)

1/2

âˆ§

JÏ„

vAÏ„


âˆ§

(Î´,âˆ†)

Â·

inf
1/2

Ëœ
(Î´Ìƒ,âˆ†)âˆˆA
Ï„ , JÏ„
2

1/2

JÏ„

)

(Î´,âˆ†) vAÏ„
vAÏ„
4

Ëœ
(Î´Ìƒ,âˆ†)â‰¥v
AÏ„

Ëœ âˆ’ QÏ„ (Î¸(Ï„ ), Î (Ï„ ))
QÏ„ (Î¸(Ï„ ) + Î´Ìƒ, Î (Ï„ ) + âˆ†)



2

4

1/2

âˆ§ qJÏ„ (Î´, âˆ†),

where in last inequality we have used the fact that vAÏ„ â‰¥ 4q. To see why this is true, notice that there exists
zXit ,z âˆˆ [0, z] such that
QÏ„ (Î¸(Ï„ ) + Î´, Î (Ï„ ) + âˆ†) âˆ’ QÏ„ (Î¸(Ï„ ), Î (Ï„ ))
!
Z X 0 Î´+âˆ†it 
n T

i,t
1 XX
0
0
=
FYi,t |Xi,t ,Î i,t (Xi,t Î¸(Ï„ ) + Î i,t + z) âˆ’ FYi,t |Xi,t ,Î i,t (Xi,t Î¸(Ï„ ) + Î i,t (Ï„ )) dz
E
nT
0
i=1 t=1
Z X 0 Î´+âˆ†i,t 
n T

i,t
1 XX
0
=
E
zfYi,t |Xi,t ,Î i,t (Ï„ ) (Xi,t
Î¸(Ï„ ) + Î i,t (Ï„ )) +
nT
0
i=1 t=1
!
z2 0
0
2 fYi,t |Xi,t ,Î i,t (Xi,t Î¸(Ï„ )

+ Î i,t (Ï„ ) + zXi,t ,z )dz

n X
T
n T


2 
f X
1 fÂ¯0 X X  0
3
0
E Xi,t Î´ + âˆ†i,t
âˆ’
E Xi,t Î´ + âˆ†i,t
.
â‰¥
nT
6 nT
i=1 t=1

i=1 t=1

(51)
1/2

Hence, if (Î´, âˆ†) âˆˆ AÏ„ with JÏ„ (Î´, âˆ†) â‰¤ 4q then
  P P
3/2
v
u
n
T
1


0 Î´ + âˆ† )2
n
T
3/2


E
(X
u f XX
i,t
2
i,t
i=1
t=1
nT
3f
t
0 Î´+âˆ†
 P P

â‰¤
E Xi,t
Â·
inf
i,t
0
n
T
nT
2 fÂ¯ (Î´,âˆ†)âˆˆAÏ„ ,Î´6=0
E 1
|X 0 Î´ + âˆ† |3
i=1 t=1

nT

38

i=1

t=1

i,t

i,t

combined with (51) implies


2
1/2
JÏ„ (Î´, âˆ†)

QÏ„ (Î¸(Ï„ ) + Î´, Î (Ï„ ) + âˆ†) âˆ’ QÏ„ (Î¸(Ï„ ), Î (Ï„ )) â‰¥

4

.

Lemma 9. Under Assumption 3, for all (Î´, âˆ†) âˆˆ AÏ„ , we have
(
)
âˆš
âˆš
rÏ„
2(C0 + 1) sÏ„ + 1
kÎ´k1,n,T â‰¤
max p
, 1 JÏ„1/2 (Î´, âˆ†),
Îº0
log(n âˆ¨ pcT )
and
âˆš
rÏ„

(

âˆš

p
kâˆ†kâˆ— â‰¤ (C0 + 1) sÏ„ + 1 nT log(max{pcT , n})Îºâˆ’1
0 max

)

p
, 1 JÏ„1/2 (Î´, âˆ†),
log(n âˆ¨ pcT )

with C0 as in Lemma 7.
1/2

Proof. By Cauchy-Schwartzâ€™s inequality, and the definition of AÏ„ and JÏ„ (Î´, âˆ†) we have

kÎ´k1,n,T â‰¤ 45 kÎ´TÏ„ k1 + kÎ´TÏ„c k1

âˆš
rÏ„ kâˆ†kF
5C0
5
â‰¤ 4 kÎ´TÏ„ k1 + 4 kÎ´TÏ„ k1 + âˆš
T)

nT log(nâˆ¨pc
âˆš
âˆš
rÏ„ kâˆ†kF
â‰¤ 2(C0 + 1) sÏ„ kÎ´TÏ„ k2 + 2C0 âˆš
nT log(nâˆ¨pcT )


âˆš
âˆš
rÏ„
â‰¤ 2(C0 + 1) sÏ„ + 1 kÎ´TÏ„ k2 + âˆš
kâˆ†kF
nT log(nâˆ¨pcT )


âˆš
1/2
âˆš
rÏ„
â‰¤ 2(C0 + 1) sÏ„ + 1 max âˆš
, 1 JÏ„ Îº(Î´,âˆ†)
.
0
log(nâˆ¨pcT )

On the other hand, by the triangle inequality, the construction of the set AÏ„ , and Cauchy-Schwartzâ€™s inequality


âˆš
p
r kâˆ†kF
kâˆ†kâˆ— â‰¤ C0 nT log(n âˆ¨ pcT ) kÎ´TÏ„ k1 + âˆš Ï„
nT log(nâˆ¨pcT )


âˆš
p
âˆš
rÏ„ kâˆ†kF
â‰¤
sÏ„ + 1(C0 + 1) nT log(n âˆ¨ pcT ) kÎ´TÏ„ k2 + âˆš
nT log(nâˆ¨pcT )


âˆš
1/2
p
âˆš
rÏ„
âˆš
, 1 JÏ„ Îº(Î´,âˆ†)
â‰¤
sÏ„ + 1(C0 + 1) nT log(n âˆ¨ pcT ) max
.
0
log(nâˆ¨pcT )

Lemma 10. Let
(Î·) =

QÌ‚Ï„ (Î¸(Ï„ ) + Î´, Î (Ï„ ) + âˆ†) âˆ’ QÌ‚Ï„ (Î¸, Î )âˆ’

sup
1/2

(Î´,âˆ†)âˆˆAÏ„ : JÏ„

(Î´,âˆ†)â‰¤Î·

QÏ„ (Î¸(Ï„ ) + Î´, Î (Ï„ ) + âˆ†) + QÏ„ (Î¸(Ï„ ), Î (Ï„ )) ,

39

p
f log(cT + 1)) â†’ âˆ. Then for all Î· > 0
p
âˆš
âˆš
CÌƒ0 Î·cT Ï†n (1 + sÏ„ ) max{log(pcT âˆ¨ n), rÏ„ }( n + dT )
âˆš
(Î·) â‰¤
,
nT Îº0 f 1/2

and {Ï†n } a sequence with Ï†n /(

for some constant CÌƒ0 > 0, with probability at least 1 âˆ’ Î±n . Here, the sequence {Î±n } is independent of Î·,
and Î±n â†’ 0.
Proof. Let â„¦1 be the event maxjâ‰¤p |ÏƒÌ‚j âˆ’ 1| â‰¤ 1/4. Then, by Assumption , P (â„¦1 ) â‰¥ 1 âˆ’ Î³ . Next let
Îº > 0, and f = (Î´, âˆ†) âˆˆ AÏ„ and write
JÏ„1/2 (Î´, âˆ†) â‰¤ Î·}.

F = {(Î´, âˆ†) âˆˆ AÏ„ :

Then notice that by Lemmas 4.1 and 4.2 from Yu (1994),
ï£«
ï£¶
dT X
n X


X
âˆš
Z
(f
)
Îº
1
i,t
â‰¤ 2 P ï£­ sup âˆš
â‰¥ ï£¸+
P (Î·) nT â‰¥ Îº
âˆš
cT
3
nd
f âˆˆF
T i=1 l=1 tâˆˆH
l
!
 Âµ
n X
X
Zi,t (f )
1
1
Îº
P sup âˆš
+ 2nT
â‰¥
,
âˆš
cT
3
cT
ndT i=1 tâˆˆR
f âˆˆF
 Âµ
,
=: A1 + A2 + 2nT c1T

(52)

where
e 0 (Î¸(Ï„ ) + Î´) âˆ’ (Î i,t (Ï„ ) + âˆ†i,t )) âˆ’ ÏÏ„ (Yei,t âˆ’ X
e 0 Î¸(Ï„ ) âˆ’ Î i,t (Ï„ )))
Zi,t (f ) = ÏÏ„ (Yei,t âˆ’ X
i,t
i,t

e 0 (Î¸(Ï„ ) + Î´) âˆ’ (Î i,t (Ï„ ) + âˆ†i,t )) âˆ’ ÏÏ„ (Yei,t âˆ’ X
e 0 Î¸(Ï„ ) âˆ’ Î i,t (Ï„ ))) .
âˆ’E ÏÏ„ (Yei,t âˆ’ X
i,t

i,t

Next we proceed to bound each term in (52). To that end, notice that
ï£«
ï£«ï£«
ï£¶2 ï£¶
ï£¶
dT X
dT
n X
n X
X
X
X
Z
(f
)
1
i,t
ï£¸ â‰¤
Var ï£­
E ï£­ï£­ âˆš
Zi,t (f )ï£¸ ï£¸
âˆš
cT
cT
i=1 l=1 tâˆˆHl
i=1 l=1
tâˆˆHl

dT X
n X
2 
X
0
â‰¤
E
XÌƒi,t Î´ + âˆ†i,t
i=1 l=1 tâˆˆHl

â‰¤

2
nT  1/2
JÏ„ (Î´, âˆ†) .
f

Let {Îµi,l }iâˆˆ[n], lâˆˆ[dT ] be i.i.d Rademacher variables independent of the data.
Therefore, by Lemma 2.3.7 in van der Vaart and Wellner (1996)
ï£«
P ï£­ sup âˆš
f âˆˆF

1
ndT

ï£¶
dT X
n X
X
Zi,t (f )
â‰¥ Îºï£¸ â‰¤
âˆš
cT
i=1 l=1 tâˆˆHl

â‰¤

P

sup
f âˆˆF

1âˆ’

âˆš1
ndT

Pn PdT

12
nT Îº2

sup Var(

f âˆˆF
Îº
P(A0 (Î·) â‰¥ 12
|â„¦1 ) + P(â„¦c1 )
1âˆ’

72cT Î· 2
f Îº2

l=1 Îµi,l

i=1

P

tâˆˆHl

Pn PdT P
i=1

l=1

Zi,t (f )
âˆš
cT

tâˆˆHl



!
â‰¥

Îº
4

Zi,t (f )
âˆš
cT )

,
(53)

40

where
A0 (Î·) :=
sup âˆš
f âˆˆF

1
ndT

dT
n X
X
i=1 l=1

ï£«

ï£¶
e 0 (Î¸(Ï„ ) + Î´) âˆ’ (Î i,t (Ï„ ) + âˆ†i,t )) âˆ’ ÏÏ„ (Yei,t âˆ’ X
e 0 Î¸(Ï„ ) âˆ’ Î i,t (Ï„ ))
X ÏÏ„ (Yei,t âˆ’ X
i,t
i,t
ï£¸ .
Îµi,l ï£­
âˆš
cT
tâˆˆHl

Next, note that


0
0
0
e
e
e
e
e
ÏÏ„ (Yi,t âˆ’ Xi,t (Î¸(Ï„ ) + Î´) âˆ’ (Î i,t (Ï„ ) + âˆ†i,t )) âˆ’ ÏÏ„ (Yi,t âˆ’ Xi,t Î¸(Ï„ ) âˆ’ Î i,t (Ï„ )) = Ï„ Xi,t Î´ + âˆ†i,t
+vi,t (Î´, âˆ†) + wi,t (Î´, âˆ†),
where
|vi,t (Î´, âˆ†)| =

e 0 (Î¸(Ï„ ) + Î´) âˆ’ (Î i,t (Ï„ ) + âˆ†i,t ))âˆ’ âˆ’ (Yi,t âˆ’ X
e 0 (Î¸(Ï„ ) + Î´) âˆ’ Î i,t (Ï„ ))âˆ’
(Yei,t âˆ’ X
i,t
i,t

â‰¤ |âˆ†i,t |.
(54)
and
e 0 Î¸(Ï„ ) âˆ’ Î i,t (Ï„ ))âˆ’
e 0 (Î¸(Ï„ ) + Î´) âˆ’ Î i,t (Ï„ ))âˆ’ âˆ’ (Yei,t âˆ’ X
(Yei,t âˆ’ X
i,t
i,t
0
e
â‰¤ |Xi,t Î´|.

|wi,t (Î´, âˆ†)| =

Moreover, notice that by Lemma 9,
{(Î´, âˆ†) âˆˆ AÏ„ : JÏ„1/2 (Î´, âˆ†) â‰¤ Î·} âŠ‚ {(Î´, âˆ†) âˆˆ AÏ„ : kÎ´k1,n,T â‰¤ Î·Ï…},
where

(
)
âˆš
âˆš
rÏ„
2(C0 + 1) 1 + sÏ„
max p
,1 .
Ï… :=
Îº0
log(n âˆ¨ pcT )

Also by Lemma 9, for (Î´, âˆ†) âˆˆ AÏ„
âˆš
kâˆ†kâˆ— â‰¤

p
1/2
1 + sÏ„ (C0 + 1)JÏ„ (Î´, âˆ†) nT max{log(pcT âˆ¨ n), rÏ„ }
,
Îº0

and so,
1/2

{(Î´,
â‰¤ Î·} âŠ‚
n âˆ†) âˆˆ AÏ„ : JÏ„ (Î´, âˆ†)
o
p
âˆš
(Î´, âˆ†) âˆˆ AÏ„ : kâˆ†kâˆ— â‰¤ 1 + sÏ„ (C0 + 1)Î· nT max{log(pcT âˆ¨ n), rÏ„ }/Îº0 .

41

(55)

Hence, defining
B10 (Î·) =

âˆš

cT

âˆš

sup
Î´ : kÎ´k1,n,T â‰¤Î·Ï…

1
ndT

dT
n X
X
i=1 l=1

ï£¶
e0 Î´
X X
i,t ï£¸
,
Îµi,l ï£­
cT
ï£«

tâˆˆHl

ï£¶
ï£«
dT
n X
X
X âˆ†i,t
1
ï£¸
âˆš
B20 (Î·) = cT
sup
Îµi,l ï£­
âˆš
âˆš
cT
ndT i=1 l=1
âˆ† : kâˆ†kâˆ— â‰¤ 1+sÏ„ (C0 +1)Î· nT max{log(pcT âˆ¨n),rÏ„ }/Îº0
ï£«tâˆˆHl
ï£¶
dT
n X
X
X vi,t (Î´, âˆ†)
âˆš
1
ï£¸ ,
âˆš
B30 (Î·) = cT
sup
Îµi,l ï£­
âˆš
âˆš
c
nd
T
T
âˆ† : kâˆ†kâˆ— â‰¤ 1+sÏ„ (C0 +1)Î· nT max{log(pcT âˆ¨n),rÏ„ }/Îº0
i=1 l=1
tâˆˆHl
ï£¶
ï£«
d
n
T
X wi,t (Î´, âˆ†)
âˆš
1 XX
ï£¸ .
âˆš
sup
B40 (Î·) = cT
Îµi,l ï£­
cT
ndT
Î´ : kÎ´k1,n,T â‰¤Î·Ï…
âˆš

i=1 l=1

tâˆˆHl

By union bound we obtain that
P(A0 (Î·) â‰¥ Îº|â„¦1 ) â‰¤

4
X

P(Bj0 (Î·) â‰¥ Îº|â„¦1 ),

(56)

j=1

so we proceed to bound each term in the right hand side of the inequality above.
First, notice that
B10 (Î·)

â‰¤ 2cT max

sup

mâˆˆ[cT ] Î´ : kÎ´k1,n,T â‰¤Î·Ï…

n dT âˆ’1
1 X X
e0
âˆš
Îµi,l X
i, (2lcT +m) Î´ ,
nT i=1 l=0

and hence by a union bound and the same argument on the proof of Lemma 5 in Belloni and Chernozhukov
(2011), we have that
Îº2
âˆš
P(B10 (Î·) â‰¥ Îº|â„¦1 ) â‰¤ 2pcT exp âˆ’ 2
4cT (16 2Î·Ï…)2

!
.

(57)

Next we proceed to bound B30 (Î·), by noticing that
B30 (Î·) â‰¤

max
mâˆˆ[cT ]

sup
âˆš

âˆš
âˆ† : kâˆ†kâˆ— â‰¤ 1+sÏ„ (C0 +1)Î·

nT max{log(pcT âˆ¨n),rÏ„ }/Îº0

âˆš
n dT âˆ’1
cT X X
âˆš
Îµi,l vi, (2lcT +m) (Î´, âˆ†) .
ndT i=1 l=0

Towards that end we proceed to bound the moment generating function of B30 (Î·) and the use that to obtain

42

an upper bound on B30 (Î·). Now fix m âˆˆ [dT ] and notice that
ï£«
ï£«

ï£¶ï£¶
âˆš
n dX
T âˆ’1
X
cT
âˆš
E ï£­exp ï£­Î»
Îµi,l vi, (2lcT +m) (Î´, âˆ†) ï£¸ï£¸
sup
âˆš
âˆš
nd
T
âˆ† : kâˆ†kâˆ— â‰¤ 1+sÏ„ (C0 +1)Î· nT max{log(pcT âˆ¨n),rÏ„ }/Îº0
i=1 l=0
ï£«
ï£«
ï£¶ï£¶
âˆš
dT
n X
X
c
T
âˆš
â‰¤ E ï£­exp ï£­Î»
sup
Îµi,l âˆ†i, (2lcT +m) ï£¸ï£¸
âˆš
âˆš
nd
T i=1 l=1
âˆ† : kâˆ†kâˆ— â‰¤ 1+sÏ„ (C0 +1)Î· nT max{log(pcT âˆ¨n),rÏ„ }/Îº0
 âˆš

Î» cT kâˆ†kâˆ— E(k{Îµil }k2 )
âˆš
exp
â‰¤ E
sup
âˆš
âˆš
ndT
âˆ† : kâˆ†kâˆ— â‰¤ 1+sÏ„ (C0 +1)Î· nT max{log(pcT âˆ¨n),rÏ„ }/Îº0
 !!
 âˆš
cT kâˆ†kâˆ— (k{Îµi,l }k2 âˆ’ E(k{Îµi,l }k2 ))
âˆš
exp Î»
ndT
p
âˆš !
âˆš
âˆš
1 + sÏ„ (C0 + 1)c4 Î·cT 3 max{log(pcT âˆ¨ n), rÏ„ } n + dT
Â·
â‰¤ exp Î»
Îº0


(sÏ„ + 1)(C0 + 1)2 c4 Î»2 c2T Î· 2 max{log(pcT âˆ¨ n), rÏ„ }
exp
,
Îº20
(58)
for a positive constant c4 > 0, and where the first inequality holds by Ledoux-Talagrandâ€™s contraction
inequality, the second by the the duality of the spectral and nuclear norms and the triangle inequality, the
third by Theorem 1.2 in Vu (2007) and by basic properties of sub-Gaussian random variables.
Therefore, by Markovâ€™s inequality and (58),
P B30 (Î·) â‰¥ï£«Îº|â„¦1



ï£¶
âˆš
n dX
T âˆ’1
X
c
T
âˆš
â‰¤ cT max P ï£­
sup
Îµi,l vi, (2lcT +m) (Î´, âˆ†) â‰¥ Îºï£¸
âˆš
âˆš
mâˆˆ[cT ]
nd
T i=1 l=0
âˆ† : kâˆ†kâˆ— â‰¤ 1+sÏ„ (C0 +1)Î· nT max{log(pcT âˆ¨n),rÏ„ }/Îº0
"
 âˆš

âˆš
âˆš âˆš
1+sÏ„ (C0 +1)c4 Î·cT 3 max{log(pcT âˆ¨n),rÏ„ }( n+ dT )
â‰¤ inf exp (âˆ’Î»Îº) exp Î»
Â·
Îº0
Î»>0

#
(1 + sÏ„ )(C0 + 1)2 c4 Î»2 c2T Î· 2 max{log(pcT âˆ¨ n), rÏ„ }
exp
+ log cT
Îº20


ÎºÎº0
âˆš
â‰¤ c5 exp âˆ’ âˆš
+ log cT ,
âˆš âˆš
1+sÏ„ (C0 +1)Î·cT 3 max{log(pcT âˆ¨n),rÏ„ }( n+ dT )
(59)
for a positive constant c5 > 0.
Furthermore, we observe that
B20 (Î·)

â‰¤ max
mâˆˆ[dT ]

sup

âˆš
âˆš
âˆ† : kâˆ†kâˆ— â‰¤ 1+sÏ„ (C0 +1)Î· nT max{log(pcT âˆ¨n),rÏ„ }/Îº0

43

âˆš
n dT âˆ’1
cT X X
âˆš
Îµi,l âˆ†i, (2lcT +m) .
ndT i=1 l=0

Hence, with the same argument for bounding B30 (Î·), we have
P B20 (Î·) â‰¥ Îº|â„¦1 â‰¤ c5 exp âˆ’ âˆš

!

ÎºÎº0



1 + sÏ„ (C0 + 1)Î·cT

p
âˆš  + log cT
âˆš
3 max{log(pcT âˆ¨ n), rÏ„ } n + dT
(60)

Finally, we proceed to bound B40 (Î·). To that end, notice that
B40 (Î·)

â‰¤ max

sup

mâˆˆ[dT ] Î´ : kÎ´k1,n,T â‰¤Î·Ï…

âˆš
n dT âˆ’1
cT X X
âˆš
Îµi,l wi, (2lcT +m) (Î´, âˆ†) ,
ndT i=1 l=0

and by (55) and Ledoux-Talagrandâ€™s inequality, as in (57), we obtain

P(B40 (Î·)

Îº2
âˆš
â‰¥ Îº|â„¦1 ) â‰¤ 2pcT exp âˆ’ 2
4cT (16 2Î·Ï…)2

!
.

(61)

Therefore, letting
Îº =

Î·cT Ï†n (1 + C0 )2

p
âˆš
âˆš
(1 + sÏ„ ) max{log(pcT âˆ¨ n), rÏ„ }( n + dT )
Îº0 f 1/2

,

and repeating the argument above for bounding A2 in (52), combining (52), (53), (56), (57), (59), (60) and
(61), we obtain that
P((Î·) â‰¥

âˆšÎº )
nT

â‰¤ 5




âˆš
Ï†2 max{log(pcT âˆ¨n),rÏ„ } âˆš
2 +2c exp âˆ’C
Î³+4 exp max{log(pcT âˆ¨n),rÏ„ }âˆ’C1 n
(
n+
d
)
5
2
T
f
1âˆ’

+ nT



1
cT

Âµ

3Îº2
âˆš0
âˆš
cT Ï†n (sÏ„ +1) (1+C0 )2 ( n+ dT )2 max{log(pcT âˆ¨n),rÏ„ }

,

for some positive constants C1 and C2 .

B.2

Proof of Theorem 1

Proof. Recall from Lemma 7, our choices of Î½1 and Î½2 are
s
p 
cT log(max{n, pcT }) âˆš
n + dT ,
Î½1 = C00
ndT
and
Î½2 =

p 
200cT âˆš
n + dT ,
nT

for C00 = 9/(1 âˆ’ c0 ), and c0 as in Lemma 7.

44

2Ï†n
f 1/2



.

Let
p
âˆš
âˆš
8Ï†n (C00 (1 + C0 ) + CÌƒ0 + 200(1 + C0 )) cT (1 + sÏ„ ) max{log(pcT âˆ¨ n), rÏ„ }( n + dT )
Î· =
, (62)
âˆš
ndT Îº0 f 1/2
for C0 as in Lemma 7, and CÌƒ0 as in Lemma 10.
Throughout we assume that the following events happen:
â€¢ â„¦1 := the event that (Î¸Ì‚(Ï„ ) âˆ’ Î¸(Ï„ ), Î Ì‚(Ï„ ) âˆ’ Î (Ï„ )) âˆˆ AÏ„ .
â€¢ â„¦2 := the event for which the upper bound on (Î·) in Lemma 10 holds.
Suppose that
|JÏ„1/2 (Î¸Ì‚(Ï„ ) âˆ’ Î¸(Ï„ ), Î Ì‚(Ï„ ) âˆ’ Î (Ï„ ))| > Î·.

(63)

Then, by the convexity of AÏ„ , and of the objective QÌ‚ with its constraint, we obtain that
0 >

QÌ‚Ï„ (Î¸(Ï„ ) + Î´, Î (Ï„ ) + âˆ†) âˆ’ QÌ‚(Î¸(Ï„ ), Î (Ï„ )) + Î½1 [kÎ¸(Ï„ ) + Î´k1,n,T âˆ’ kÎ¸(Ï„ )k1,n,T ]

min
1/2

(Î´,âˆ†)âˆˆAÏ„ : |JÏ„

(Î´,âˆ†)|=Î·

+Î½2 [kÎ (Ï„ ) + âˆ†kâˆ— âˆ’ kÎ (Ï„ )kâˆ— ]
Moreover, by Lemma 9 and the triangle inequality,
kÎ¸(Ï„ )k1,n,T âˆ’ kÎ¸(Ï„ ) + Î´k1,n,T

â‰¤ kÎ´TÏ„ k1,n,T
1/2
âˆš
â‰¤ 2(1 + C0 ) 1 + sÏ„ JÏ„ Îº(Î´,âˆ†)
max
0



âˆš

âˆš

rÏ„

log(nâˆ¨pcT )


,1 ,

and
kÎ (Ï„ )kâˆ— âˆ’ kÎ (Ï„ ) + âˆ†kâˆ— â‰¤ kâˆ†kâˆ—

1/2
p
âˆš
â‰¤ (1 + C0 ) 1 + sÏ„ nT max{log(pcT âˆ¨ n), rÏ„ } JÏ„ Îº(Î´,âˆ†)
.
0

Therefore,
0 >

min
1/2

(Î´,âˆ†)âˆˆAÏ„ : |JÏ„

=

QÌ‚(Î¸(Ï„ ) + Î´, âˆ† + Î (Ï„ ))âˆ’
(Î´,âˆ†)|=Î·



âˆš
1/2
âˆš
rÏ„
JÏ„ (Î´,âˆ†)
max âˆš
,1 ,
QÌ‚(Î¸(Ï„ ), Î (Ï„ )) âˆ’ 2Î½1 (1 + C0 ) 1 + sÏ„
Îº0
log(nâˆ¨pcT )
1/2
p
âˆš
JÏ„ (Î´,âˆ†)
âˆ’Î½
" 2 (1 + C0 ) 1 + sÏ„ nT max{log(pcT âˆ¨ n), rÏ„ } Îº0
QÌ‚Ï„ (Î¸(Ï„ ) + Î´, âˆ† + Î (Ï„ )) âˆ’ QÌ‚(Î¸(Ï„ ), Î (Ï„ ))

min
1/2

(Î´,âˆ†)âˆˆAÏ„ : |JÏ„

(Î´,âˆ†)|=Î·

âˆ’Q(Î¸(Ï„ ) + Î´, âˆ† + Î (Ï„ )) + Q(Î¸(Ï„ ), Î (Ï„ ))
+Q(Î¸(Ï„ ) + Î´, âˆ† + Î (Ï„ )) âˆ’ Q(Î¸(Ï„ ), Î (Ï„ )) âˆ’
q
1/2
âˆš
âˆš
T âˆ¨n),rÏ„ }
2C00 (1 + C0 ) cT (1+sÏ„ ) max{log(pc
( n + dT ) JÏ„ Îº(Î´,âˆ†)
âˆ’
ndT
0
#
1/2
p
âˆš âˆš
JÏ„ (Î´,âˆ†)
200cT âˆš
n + dT
1 + sÏ„ (1 + C0 ) nT max{log(pcT âˆ¨ n), rÏ„ } Îº0
nT
45

â‰¥

min
1/2

(Î´,âˆ†)âˆˆAÏ„ : |JÏ„

â‰¥

Î·2
âˆ§ (Î·q) âˆ’ [2C00 (1 + C0 ) + 200(C0 + 1)]
4
âˆš
âˆš âˆš

CÌƒ0 Î·Ï†n

â‰¥

Q(Î¸(Ï„ ) + Î´, âˆ† + Î (Ï„ )) âˆ’ Q(Î¸(Ï„ ), Î (Ï„ ))
q
1/2
âˆš
âˆš
JÏ„ (Î´,âˆ†)
T âˆ¨n),rÏ„ }
âˆ’[2C00 (1 + C0 ) + 200(C0 + 1)] cT (1+sÏ„ ) max{log(pc
(
n
+
d
)
T
nd
Îº0
T
p
âˆš
âˆš
CÌƒ0 Î·Ï†n cT (1 + sÏ„ ) max{log(pcT âˆ¨ n), rÏ„ }( n + dT )
âˆ’
âˆš
ndT Îº0 f 1/2

(Î´,âˆ†)|=Î·

Î·2
âˆ’
4

cT sÏ„ max{log(pcT âˆ¨n),rÏ„ }( n+ dT )
âˆš
ndT Îº0 f 1/2
0
2Î·Ï†n (C0 (1 + C0 ) + CÌƒ0 + 200(C0

s

p
cT (1 + sÏ„ ) max{log(pcT âˆ¨ n), rÏ„ } âˆš
Î·
( n + dT ) âˆ’
ndT
Îº0

p
âˆš
âˆš
+ 1)) cT (1 + sÏ„ ) max{log(pcT âˆ¨ n), rÏ„ }( n + dT )
âˆš
ndT Îº0 f 1/2

= 0,
(64)
where the the second inequality follows from Lemma 10, the third from Lemma 8, the fourth from our
choice of Î· and (25), and the equality also from our choice of Î·. Hence, (64) leads to a contradiction which
shows that (63) cannot happen in the first place. As a result, by Assumption 3,
kÎ Ì‚(Ï„ ) âˆ’ Î (Ï„ )kF
1 1/2
Î·
âˆš
â‰¤
|JÏ„ (Î¸Ì‚(Ï„ ) âˆ’ Î¸(Ï„ ), Î Ì‚(Ï„ ) âˆ’ Î (Ï„ ))| â‰¤
,
Îº0
Îº0
nT
which holds with probability approaching one.
To conclude the proof, let Î´Ì‚ = Î¸Ì‚ âˆ’ Î¸ and notice that
kÎ´Ì‚(TÏ„ âˆªT Ï„ (Î´Ì‚,m))c k2

â‰¤
â‰¤
â‰¤
â‰¤

which implies
kÎ´Ì‚k â‰¤
â‰¤

X kÎ´Ì‚T c k21
Ï„
k2

kâ‰¥m+1
kÎ´Ì‚TÏ„c k21

m"
#
rÏ„ kÎ (Ï„ ) âˆ’ Î Ì‚(Ï„ )k2F
4C0
2
kÎ´Ì‚TÏ„ k1 +
m
nT log(pcT âˆ¨ n)
"
#
2
r
kÎ (Ï„
)
âˆ’
Î Ì‚(Ï„
)k
4C0
Ï„
F
sÏ„ kÎ´Ì‚TÏ„ âˆªT Ï„ (Î´Ì‚,m) k2 +
,
m
nT log(pcT âˆ¨ n)



âˆš
rÏ„ kÎ (Ï„ )âˆ’Î Ì‚(Ï„ )kF
âˆš
1 + 2C0 m
kÎ´Ì‚TÏ„ âˆªT Ï„ (Î´Ì‚,m) k +
nT log(cT pâˆ¨n)
1/2
p sÏ„ 
JÏ„ (Î´Ì‚,Î Ì‚(Ï„ ) âˆ’ Î (Ï„ ))
1 + 2C0 m ,
Îºm
p sÏ„ 

and the result follows.

46

B.3

Proof of Theorem 3

Lemma 11. Suppose that Assumptions 1â€“2 and 4 hold. Let
n T
2 XX
Î½1 =
kXi,t kâˆ .
nT
i=1 t=1

and
Î½2 =

p 
200cT âˆš
n + dT .
nT

We have that
(Î Ì‚(Ï„ ) âˆ’ Î (Ï„ ) âˆ’ XÎ¸(Ï„ )) âˆˆ A0Ï„ ,
with probability approaching one, where
(
A0Ï„ =

)

âˆš

âˆ† âˆˆ RnÃ—T : kâˆ†kâˆ— â‰¤ c0 rÏ„ (kâˆ†kF + kÎ¾kâˆ— ) , kâˆ†kâˆ â‰¤ c1 ,

and c0 and c1 are positive constants that depend on Ï„ . Furthermore, Î¸Ì‚(Ï„ ) = 0.
Proof. First, we observe that C in the statement of Theorem 3 can be take as C = kX 0 Î¸(Ï„ ) + Î (Ï„ )kâˆ . And
so,
XÎ¸(Ï„ ) + Î (Ï„ ) âˆ’ Î Ì‚(Ï„ )
â‰¤ 2C =: c1 .
âˆ

Next, notice that for any Î ÌŒ âˆˆ RnÃ—T and Î¸ÌŒ âˆˆ Rp ,
QÌ‚Ï„ (0, Î ÌŒ) âˆ’ QÌ‚Ï„ (Î¸ÌŒ, Î ÌŒ) âˆ’ Î½1 kÎ¸ÌŒk1,n,T

â‰¤

n T
1 XX 0
|Xi,t Î¸ÌŒ| âˆ’ Î½1 kÎ¸ÌŒk1
nT

â‰¤

n T
1 XX 0
|Xi,t Î¸ÌŒ| âˆ’ Î½1 kÎ¸ÌŒk1
nT

â‰¤

n T
1 XX
0
kâˆ kÎ¸ÌŒk1 âˆ’ Î½1 kÎ¸ÌŒk1
kXi,t
nT

i=1 t=1

i=1 t=1

i=1 t=1

< 0,
where the first inequality follows since ÏÏ„ is a contraction map. Therefore, Î¸Ì‚(Ï„ ) = 0. Furthermore, by

47

Lemma 5, we have


0 â‰¤ QÌ‚Ï„ (0, XÎ¸(Ï„ ) + Î (Ï„ )) âˆ’ QÌ‚Ï„ (0, Î Ì‚(Ï„ )) + Î½2 kXÎ¸(Ï„ ) + Î (Ï„ )kâˆ— âˆ’ kÎ Ì‚(Ï„ )kâˆ—
n T


1 XX
ai,t (XÎ¸(Ï„ ) + Î (Ï„ )) âˆ’ Î Ì‚(Ï„ )
â‰¤
nT
i=1
 t=1

+ Î½2 kXÎ¸(Ï„ ) + Î (Ï„ )kâˆ— âˆ’ kÎ Ì‚(Ï„ )kâˆ—
!
n T
1 XX
Ëœ i,t
sup
â‰¤ kXÎ¸(Ï„ ) + Î (Ï„ ) âˆ’ Î Ì‚(Ï„ )kâˆ—
ai,t âˆ†
nT
Ëœ
kâˆ†kâˆ— â‰¤1 
i=1 t=1

+ Î½2 kXÎ¸(Ï„ ) + Î (Ï„ )kâˆ— âˆ’ kÎ Ì‚(Ï„ )kâˆ—

p 
200cT âˆš
â‰¤
n + dT
kXÎ¸(Ï„ ) + Î (Ï„ ) âˆ’ Î Ì‚(Ï„ )kâˆ— + kXÎ¸(Ï„ ) + Î (Ï„ )kâˆ— âˆ’ kÎ Ì‚(Ï„ )kâˆ—
nT
p 
100cT âˆš
âˆ’
n + dT kXÎ¸(Ï„ ) + Î (Ï„ ) âˆ’ Î Ì‚(Ï„ )kâˆ—
nT

p 
200cT âˆš
n + dT
â‰¤
kXÎ¸(Ï„ ) + Î (Ï„ ) + Î¾ âˆ’ Î Ì‚(Ï„ )kâˆ— + kXÎ¸(Ï„ ) + Î (Ï„ ) + Î¾kâˆ— âˆ’ kÎ Ì‚(Ï„ )kâˆ—
nT
p 
p 
100cT âˆš
400cT âˆš
n + dT kÎ¾kâˆ— âˆ’
n + dT kXÎ¸(Ï„ ) + Î (Ï„ ) âˆ’ Î Ì‚(Ï„ )kâˆ—
+
nT
âˆš
p âˆš
c1 cT nT
â‰¤
n + dT
rÏ„ kXÎ¸(Ï„ ) + Î (Ï„ ) + Î¾ âˆ’ Î Ì‚(Ï„ )kF
nT
p 
p 
400cT âˆš
100cT âˆš
+
n + dT kÎ¾kâˆ— âˆ’
n + dT kXÎ¸(Ï„ ) + Î (Ï„ ) âˆ’ Î Ì‚(Ï„ )kâˆ—
nT
nT
for some positive constant c1 ,

Lemma 12. Let
0 (Î·) =

QÌ‚Ï„ (0, XÎ¸(Ï„ ) + Î (Ï„ ) + âˆ†) âˆ’ QÌ‚Ï„ (0, XÎ¸(Ï„ ) + Î (Ï„ ))âˆ’

sup
1/2

(Î´,âˆ†)âˆˆAÏ„ : JÏ„

(Î´,âˆ†)â‰¤Î·

QÏ„ (0, XÎ¸(Ï„ ) + Î (Ï„ ) + âˆ†) + QÏ„ (0, XÎ¸(Ï„ ) + Î (Ï„ )) ,
and {Ï†n } a sequence with Ï†n /(

p
f log(cT + 1)) â†’ âˆ. Then for all Î· > 0
âˆš
âˆš âˆš
CÌƒ0 Î·cT Ï†n rÏ„ ( n + dT )
âˆš
 (Î·) â‰¤
,
nT f
0

for some constant CÌƒ0 > 0, with probability at least 1 âˆ’ Î±n . Here, the sequence {Î±n } is independent of Î·,
and Î±n â†’ 0.
Proof. This follows similarly to the proof of Lemma 10.

48

Lemma 13. Let
A00Ï„ =
with



âˆ† âˆˆ A0Ï„ : q(âˆ†) â‰¥ 2Î·0 , âˆ† 6= 0 ,

âˆš
âˆš âˆš
CÌƒ1 cT Ï†n rÏ„ ( n + dT )
âˆš
Î·0 =
,
nT f

for an appropriate constant CÌƒ1 > 0, and
3/2
3f
q(âˆ†) =
2 fÂ¯0



1
nT

3/2
2
(âˆ†
)
i,t
t=1
i=1
.
Pn PT
3
i=1
t=1 |âˆ†i,t |

Pn PT

1
nT

Under Assumptions 1-2 and 4, for any âˆ† âˆˆ A00Ï„ we have that
(
QÏ„ (0, XÎ¸(Ï„ ) + Î (Ï„ ) + âˆ†) âˆ’ QÏ„ (0, XÎ¸(Ï„ ) + Î (Ï„ )) â‰¥ min

f kâˆ†k2 2Î·f 1/2 kâˆ†k
âˆš
,
4nT
nT

)
.

Proof. This follows as the proof of Lemma 8.
The proof of Theorem 3 proceeds by exploiting Lemmas 11 and 13. By Lemma 11, we have that
Ë† := Î Ì‚(Ï„ ) âˆ’ XÎ¸(Ï„ ) âˆ’ Î (Ï„ ) âˆˆ A0 ,
âˆ†
Ï„
Ë† âˆˆ
with high probability. Therefore, we assume that (65) holds. Hence, if âˆ†
/ A00Ï„ , then

P P
T
n
Ë† i,t |3
0
0 Ë†
|
âˆ†
4
Î·
t=1
i=1
1
f
4f kâˆ†k
âˆÎ·
Ë† F <

P P
âˆš kâˆ†k
â‰¤
.
T
n
Ë†2
nT
f 3/2
3f 3/2
âˆ†
3
i=1

t=1

(65)

(66)

i,t

Ë† âˆˆ A00Ï„ , then we proceed as in the proof of Theorem 1 by exploiting Lemma 12, and treating
If âˆ†
XÎ¸(Ï„ ) + Î (Ï„ ) as the latent factors matrix, the design matrix as the matrix zero, A00Ï„ as AÏ„ , and
Îº0 = f 1/2 ,
in Assumption 3. This leads to
1
Ë† F â‰¤ Î·,
âˆš kâˆ†k
nT
and the claim in Theorem 3 follows combining (66) and (67).

49

(67)

B.4

Proof of Corollary 2

First notice that by Theorem 1 and Theorem 3 in Yu et al. (2014),




âˆš
(Ïƒ1 (Ï„ ) + rÏ„ Err)Err
Ëœ
.
v := max min kgÌ‚(Ï„ )O âˆ’ g(Ï„ )kF , min kÎ»Ì‚(Ï„ )O âˆ’ Î»Ìƒ(Ï„ )kF = OP
OâˆˆOrÏ„
OâˆˆOrÏ„
(ÏƒrÏ„ âˆ’1 (Ï„ ))2 âˆ’ (ÏƒrÏ„ (Ï„ ))2
(68)
Furthermore,
kÎ»Ì‚(Ï„ ) âˆ’ Î»(Ï„ )k2F
nT

=
â‰¤
â‰¤

rÏ„
1 X
kÎ»Â·,j (Ï„ ) âˆ’ Î»Ì‚Â·,j (Ï„ )k2
nT
j=1
r
rÏ„
X
2 X
2 Ï„
Ëœ
2
(Ïƒj âˆ’ ÏƒÌ‚j ) +
Ïƒj2 kÎ»Ì‚j (Ï„ ) âˆ’ Î»Ìƒj (Ï„ )k2
nT
nT
j=1
j=1
r
rÏ„
Ï„
2 X
2 X
2
Ïƒ
Ëœ
1
(Ïƒj âˆ’ ÏƒÌ‚j )2 +
kÎ»Ì‚j (Ï„ ) âˆ’ Î»Ìƒj (Ï„ )k2
nT
nT
j=1

j=1

2 rÏ„
2 Ïƒ12 Ëœ
â‰¤
(Ïƒ1 âˆ’ ÏƒÌ‚1 )2 +
kÎ»Ì‚(Ï„ ) âˆ’ Î»Ìƒ(Ï„ )k2F
nT
nT
2 Ïƒ12 Ëœ
2 rÏ„
kÎ (Ï„ ) âˆ’ Î Ì‚(Ï„ )k2F +
kÎ»Ì‚(Ï„ ) âˆ’ Î»Ìƒ(Ï„ )k2F
â‰¤
nT
nT

!
1
rÏ„ Ï†2n cT (1 + sÏ„ ) max{log(pcT âˆ¨ n), rÏ„ } 1
+
+
= OP
n dT
Îº40 f
 2

âˆš
Ïƒ1 (Ïƒ1 (Ï„ ) + rÏ„ Err)2 Err2
OP
,
nT ((ÏƒrÏ„ âˆ’1 (Ï„ ))2 âˆ’ (ÏƒrÏ„ (Ï„ ))2 )2
where the third inequality follows from Weylâ€™s inequality, and the last one from (68).

50

Table 7: Firm Characteristics Construction.
Characteristics

Name

Construction

acc

Working capital accruals

agr
beta

Asset growth
Beta

bm

Book-to-market

chinv

Change in inventory

chmom

Change in 6-month momentum

dolvol

Dollar trading volume

dy

Dividend to price

egr

Earnings announcement return

ep

Earnings to price

gma

Gross profitability

idiovol

Idiosyncratic return volatility

ill

Illiquidity (Amihud)

invest

Capital expenditures and inventory

lev

Leverage

lgr
mom1m

Growth in long-term debt
1-month momentum

Annual income before extraordinary items (ib)
minus operating cash flows (oancf) divided by
average total assets (at)
Annual percent change in total assets (at)
Estimated market beta from weekly returns and
equal weighted market returns for 3 years
Book value of equity (ceq) divided by end of
fiscal year-end market capitalization
Change in inventory (inv) scaled by average total assets (at)
Cumulative returns from months t-6 to t-1 minus months t-12 to t-7
Natural log of trading volume times price per
share from month t-2
Total dividends (dvt) divided by market capitalization at fiscal year-end
Annual percent change in book value of equity
(ceq)
Annual income before extraordinary items (ib)
divided by end of fiscal year market cap
Revenues (revt) minus cost of goods sold (cogs)
divided by lagged total assets (at)
Standard deviation of residuals of weekly returns on weekly equal weighted market returns
for 3 years prior to month end
Average of daily (absolute return / dollar volume).
Annual change in gross property, plant, and
equipment (ppegt) + annual change in inventories (invt) all scaled by lagged total assets (at)
Annual change in gross property, plant, and
equipment (ppegt) + annual change in inventories (invt) all scaled by lagged total assets (at)
Annual percent change in total liabilities (lt)
1-month cumulative return

51

Continuation of Table 7
mom6m

6-month momentum

5-month cumulative returns ending one month
before month end
mve
Size
Natural log of market capitalization at end of
month t-1
operprof
Operating profitability
Revenue minus cost of goods sold - SG&A expense - interest expense divided by lagged common shareholdersâ€™ equity
range
Range of stock price
Monthly average of daily price range: (highlow)/((high+low)/2) (alternative measure of
volatility)
retvol
Return volatility
Standard deviation of daily returns from month
t-1
roaq
Return on assets
Income before extraordinary items (ibq) divided by one quarter lagged total assets (atq)
roeq
Return on equity
Earnings before extraordinary items divided by
lagged common shareholdersâ€™ equity
sue
Unexpected quarterly earnings
Unexpected quarterly earnings divided by
fiscal-quarter-end market cap. Unexpected
earnings is I/B/E/S actual earnings minus median forecasted earnings if available, else it
is the seasonally differenced quarterly earnings before extraordinary items from Compustat quarterly file
turn
Share turnover
Average monthly trading volume for most recent 3 months scaled by number of shares outstanding in current month
Note: Estimated under different values of turning parameter Î½2 , when Î½1 = 10âˆ’5 is fixed. The results are
reported for quantiles 10%, 50% and 90%.

52

