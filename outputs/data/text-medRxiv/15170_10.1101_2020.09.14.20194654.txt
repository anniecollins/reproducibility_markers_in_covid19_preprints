medRxiv preprint doi: https://doi.org/10.1101/2020.09.14.20194654; this version posted September 18, 2020. The copyright holder for this
preprint (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
All rights reserved. No reuse allowed without permission.

Dual Attention Multiple Instance Learning with Unsupervised Complementary
Loss for COVID-19 Screening
Philip Chikontwea , Miguel Lunaa , Myeongkyun Kanga , Kyung Soo Hongb , June Hong Ahnb,âˆ—, Sang Hyun Parka,âˆ—
a Department
b Division

of Robotics Engineering, Daegu Gyeongbuk Institute of Science and Technology (DGIST), Daegu, South Korea
of Pulmonology and Allergy, Department of Internal Medicine, Regional Center for Respiratory Diseases, Yeungnam University
Medical Center, College of Medicine, Yeungnam University, Daegu, Korea.

Abstract
Chest computed tomography (CT) based analysis and diagnosis of the Coronavirus Disease 2019 (COVID-19) plays a
key role in combating the outbreak of the pandemic that has rapidly spread worldwide. To date, the disease has infected
more than 18 million people with over 690k deaths reported. Reverse transcription polymerase chain reaction (RTPCR) is the current gold standard for clinical diagnosis but may produce false positives; thus, chest CT based diagnosis
is considered more viable. However, accurate screening is challenging due to difficulty in annotation efforts of infected
areas, curation of large datasets, and the slight discrepancies between COVID-19 and other viral pneumonia. In this
study, we propose an attention-based end-to-end weakly supervised framework for the rapid diagnosis of COVID-19
and bacterial pneumonia based on multiple instance learning (MIL). We further incorporate unsupervised contrastive
learning for improved accuracy with attention applied both in spatial and latent contexts, herein we propose Dual
Attention Contrastive based MIL (DA-CMIL). DA-CMIL takes as input a several patient CT slices (considered as a
bag of instances) and outputs a single label. Attention based pooling is applied to implicitly select key slices in latent
space, and spatial attention learns slice spatial context for interpretable diagnosis. A contrastive loss is applied at the
instance level to encode similarity in features from the same patient against pooled patient features. Empirical results
show our algorithm achieves an overall accuracy of 98.6% and an AUC of 98.4%. Moreover, ablation studies show
the benefit of contrastive learning with MIL.
Keywords: COVID-19, CT Images, Deep Learning, Multiple Instance Learning, Unsupervised Complementary Loss

1. Introduction

increasing infections each day. Despite having a relatively
lower fatality rate Mahase (2020) than SARS and Middle
East Respiratory Syndrome (MERS), COVID-19 has already caused more deaths. Consequently, there is an urgent need for rapid diagnosis to improve prevention while
an effective vaccine is being developed.

The coronavirus disease 2019 (COVID-19), first recognized in Wuhan, China has spread to a global scale infecting millions and causing death to hundreds of thousands. As of August, 2020 infections surpassed 18 million, with reported deaths reaching over 690,000 globally.
Reverse transcription polymerase chain reaction (RTCaused by severe acute respiratory syndrome coronavirus PCR) is the current gold standard for COVID-19 diagno2 (SARS-CoV-2), COVID-19 is highly contagious with sis based on viral nucleic acid (VNA) Zu et al. (2020).
However, low sensitivity, high number of false positives
and lengthy test to diagnosis times pose a challenge for
âˆ— Corresponding authors.
early identification and treatment of patients Ai et al.
Email addresses: fireajh@gmail.com (June Hong Ahn),
shpark13135@dgist.ac.kr (Sang Hyun Park)
(2020). Moreover, potential patients left unattended inPreprint submitted to Medical Image Analysis

September 14, 2020

NOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.

medRxiv preprint doi: https://doi.org/10.1101/2020.09.14.20194654; this version posted September 18, 2020. The copyright holder for this
preprint (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
All rights reserved. No reuse allowed without permission.

crease the risk of spreading the infection. As an easy
non invasive imaging alternative, chest computed tomography (CT) is viable for fast diagnosis Ai et al. (2020). It
can detect key imaging characteristics manifested in infected areas such as ground glass opacity (GGO), multifocal patchy consolidation and/or bilateral patchy shadows Wang et al. (2020a). However, image characteristics between COVID-19 and other pneumonia types may
possess similarities, making accurate diagnosis challenging. Also, automated screening sensitivity is limited and
not on par with radiologist level performance Wang et al.
(2020b). Therefore, there is an urgent need to improve
and/or develop robust screening methods based on chest
CT.
On the other hand, deep learning LeCun et al. (2015)
based solutions have shown success in medical image
analysis Litjens et al. (2017) due to the ability to extract rich features from clinical datasets, and include a
wide range of application areas such as organ segmentation Ronneberger et al. (2015) and disease diagnosis, etc. Deep learning has been employed for the diagnosis of COVID-19 in chest CT Song et al. (2020);
Gozes et al. (2020a,b) and community acquired pneumonia (CAP) Kermany et al. (2018). For example, Ouyang
et al. (2020) proposed a 3D convolutional neural network (CNN) with online attention refinement to diagnose
COVID-19 from CAP and introduced a sampling strategy
to mitigate the imbalanced distribution of infected regions
between COVID-19 and CAP. Song et al. (2020) proposed DeepPneumonia for localization and detection of
COVID-19 pneumonia; attention was also applied to detect key regions with impressive results on a large cohort.
Despite showing promising performance, most methods
are supervised and require considerable labeling efforts.
Notably, even without annotated examples of infection areas, some works use existing deep learning models Shan
et al. (2020) to extract infection regions and/or manually
select slices in CT that show key characteristics. However, taking into consideration that during the pandemic
experts have had limited time to perform labeling of CT
volumes for supervised methods, unsupervised or weakly
supervised learning methods that do not heavily rely on
extensive data pre-processing and/or strong prior knowledge are a preferred option for accurate diagnosis.
Recently, several works focused on accurate diagnosis under weak supervision have been proposed. Notably,

we consider approaches that use (a) patch-based Jin et al.
(2020); Shi et al. (2020b) (b) slice-based Gozes et al.
(2020b,a); Hu et al. (2020), and (c) 3D CT-based Han
et al. (2020); Zheng et al. (2020) methods for diagnostic decisions. The first often uses prior segmented infection regions as input to train classifiers in a two-stage
setup. The second performs slice-wise inference directly,
whereas 3D based methods use the entire 3D CT scans
as input with 3D convolutional neural networks (CNN).
For the patch and slice-based approaches to be effective,
infected regions must be well selected for training. Also,
3D CNN models are inherently slow during inference due
to bigger model size and may lack interpretability.
In this work, we propose a novel end-to-end attentionbased weakly supervised framework using multiple instance learning (MIL) Carbonneau et al. (2018) and selfsupervised contrastive learning Chen et al. (2020a) of features towards accurate diagnosis of COVID-19 from bacterial pneumonia. We refer to this framework as DACMIL. The goal of DA-CMIL is to assign patients a single category label i.e. (COVID-19 or bacterial pneunomia) given as input a CT volume of multiple 2D slices.
In general, each patient CT scan is considered as a bag
of instances that may be positive or negative. Moreover,
it would be beneficial to identify which slices/instances
contribute to the final patient diagnosis with the potential to localize infected regions. Herein, we propose
a permutation-invariant attention based MIL pooling of
slices to obtain a single representative feature for patients.
In addition, spatial attention is jointly applied to learn spatial features key for infection area discovery. We incorporate contrastive learning at the instance level to encourage
instance features from the same patient to be semantically
similar to the patient level aggregated features in an unsupervised manner. To achieve this, an unsupervised contrastive loss is employed alongside patient category labels
for the supervised loss during training.
Existing works using MIL applied in different domains
often decouple instance and bag level learning into a twostep procedure i.e. first learn instance level encoders, then
learn aggregation models for inference using trained encoders with MIL pooling Hashimoto et al. (2020); Hou
et al. (2016). However, due to ambiguity of the instance
labels and noise, learning a robust encoder can be challenging. Herein, the proposed framework aims to address the aforementioned challenges via end-to-end learn2

medRxiv preprint doi: https://doi.org/10.1101/2020.09.14.20194654; this version posted September 18, 2020. The copyright holder for this
preprint (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
All rights reserved. No reuse allowed without permission.

ing; instance selection is implicity achieved via attention based pooling of CT slices with model optimization
focused only on accurate patient labels. Moreover, by
jointly using supervised and constrastive loss, our model
can avoid overfitting when trained on smaller datasets
and improve feature robustness at the instance level without sacrificing accuracy. We empirically show the benefit of DA-CMIL on a recently collected dataset, with interpretable results and competitive performance against
state-of-the-art methods.
The main contributions of this study include:

2.1. Deep Learning for COVID-19 diagnosis
The success of deep learning based techniques applied
to medical image analysis has shown promising results
for several application areas such as segmentation and
disease detection. Several pioneering methods Shi et al.
(2020a); Gozes et al. (2020a); Xie et al. (2020); Jin et al.
(2020) have been proposed for the analysis of COVID19 in both X-ray and CT images. COVID-19 lesion segmentation Gozes et al. (2020a); Xie et al. (2020), automated screening Jin et al. (2020); Song et al. (2020); Han
et al. (2020) and severity assessment Huang et al. (2020)
have been key areas of research. Notably, a recent review Shi et al. (2020a) shows that screening is predominant and continues to receive much interest. Moreover,
given that chest CT best shows key image characteristics
for COVID-19 diagnosis, CT is preferred over X-ray despite it being a low cost solution. Ng et al. (2020) recently
claimed that consolidative and/or ground glass opacities
(GGO) on CT are often undetectable in chest radiography
and highlighted the pros and cons of each imaging modality.
Accordingly, Oh et al. (2020) recently proposed a
patch-based CNN for COVID-19 diagnosis applied to
chest radiography with limited datasets. They show that
statistically significant differences in patch-wise intensity distributions can serve as biomarkers for diagnosis
of COVID-19; with existing correlations to current radiological findings of chest X-ray. Alom et al. (2020) introduced a multi-task deep model that jointly considers chest
CT and X-ray for diagnosis. They showed impressive results in both modalities for both detection and localization
of infected regions. Song et al. (2020) developed DeepPneumonia, a deep learning system with rapid diagnosis to
aide clinicians. Mei et al. (2020) used deep learning to integrate chest CT findings with clinical information such as
laboratory tests and exposure history to rapidly diagnose
COVID-19 patients. From a technical standpoint, most
methods require pre-segmented lesions prior to training,
and/or include multi-stages in the frameworks. Moreover,
patch-based methods may suffer from noisy samples in
scans, often requiring careful manual selection of slices
for efficiency.

â€¢ We propose a novel end-to-end model for weakly supervised classification of COVID-19 from bacterial
pneumonia.
â€¢ We show that joint contrastive learning of instance
features and patient level features in the MIL setting
is viable. A novel setting of learning instance level
features without inferring labels.
â€¢ Towards interpretability, we show that dual attention,
in particular spatial attention can be used to assess
and visualize model decisions.
â€¢ We empirically show that DA-CMIL is robust to different CT sizes when instance (i.e. slice/patch) count
varies via ablation studies.
The rest of the article is arranged as follows. In Section
2, we review recent works related to computer aided diagnosis with artificial intelligence for COVID-19 and relevant methodologies under weak supervision. We introduce the relavant background and details regarding DACMIL in Section 3. In Section 4, we provide descriptions
on experimental settings and datasets employed. Experimental results are discussed in Sections 5 & 6. We conclude this study in Section 7.

2. Related Works

This section presents related works in terms of COVID- 2.2. Weak Supervision and Multiple Instance Learning
19 screening, methods for weak supervision and selfMIL is a form of weakly supervised learning where lasupervised learning.
bels/categories are provided only for a bag of instances
3

medRxiv preprint doi: https://doi.org/10.1101/2020.09.14.20194654; this version posted September 18, 2020. The copyright holder for this
preprint (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
All rights reserved. No reuse allowed without permission.

i.e. training instances arranged in sets and the labels of
instances contained in the bags are unknown Carbonneau
et al. (2018). In this study, we consider a patient CT scan
as a bag with unlabeled slices (instances), having only
the diagnostic label for training. In general, existing algorithms can be categorized as instance-level Hou et al.
(2016), bag-level Hashimoto et al. (2020), embeddingbased, and joint methods that combine several approaches
such as attention mechanisms Hashimoto et al. (2020);
Ilse et al. (2018); Han et al. (2020). In literature, MIL has
been applied to several domains including object detection Zhang et al. (2016a), image classification Yao et al.
(2019); Hou et al. (2016); Zhang et al. (2016a), and object
tracking Hu et al. (2017).
Also, several works have been applied in the medical
imaging domain Zheng et al. (2020); Hu et al. (2020);
Han et al. (2020); Wang et al. (2020c); Campanella et al.
(2019). Hashimoto et al. (2020) recently introduced a
novel CNN for the classification of malignant lymphoma
in histopathology slides. Notably, they combined domain
adaptation and multi-scale approaches with MIL for improved performance. Ilse et al. (2018) proposed attentionbased pooling for MIL in an end-to-end framework; impressive results are shown across different domain problems including cancer region detection in histopathology. Weakly supervised detection of COVID-19 infection regions in chest CT is presented by Hu et al. (2020)
with multi-scale learning applied for localization. More
recently, Wang et al. (2020c) proposed (DeCoVNet), a
method applied to 3D CT volumes using 3D CNNs with
weak labels. DeCoVNet takes as input a CT volume and
its lung mask for COVID-19 classification. Han et al.
(2020) proposed AD3DMIL, a 3D MIL method with attention for COVID-19 screening with a deep instance generation module based on 3D latent features inspired by the
pioneering work of Feng and Zhou (2017).

et al. (2016). Most recently, Chen et al. (2020a) introduced a simple framework for contrastive learning (SimCLR) that uses extensive data-augmentation for defining
predictive tasks, which achieves comparable performance
to state-of-the-art supervised methods. For medical imaging tasks, He et al. (2020) recently proposed (Self-Trans)
a method that combines contrastive self-training with
transfer learning for COVID-19 diagnosis. Notably, the
authors focus on establishing robust strategies for transfer learning with limited data, and/or when using external
datasets for COVID-19 Chest CT analysis.
Inspired by recent works both for MIL and SSL,
we propose to synergistically integrate contrastive selfsupervision with MIL in an end-to-end framework. Notably, though previous works such as AD3DMIL Han
et al. (2020) have shown impressive results; the model is
based on 3D CNN and considerably has a larger model
size. Also, Self-Trans He et al. (2020) follows a twostep approach by first pre-training the network via selfsupervision using Chen et al. (2020b); then performs
fine-tuning or transfer learning, joint self supervised training with transfer learning is underexplored. Thus, we
aim to extend the current scope of the literature regarding COVID-19 via a novel formulation of MIL and selfsupervised contrastive learning.
3. Methods
This sections presents the necessary notations and overall objectives of the task of COVID-19 diagnosis, including details of the relative modules of the proposed
method.
3.1. Preliminaries
We consider a chest CT dataset D = {S 1 , ..., S n } where
the model receives a set of m labeled example scans
{(S i , Yi )}m
i=1 drawn from the joint distribution defined by
S Ã— Y. S i is a patient CT scan with instances (i.e. 2D
CT slices or patches) and Y is the label set of patientlevel labels, wherein Y is {0, 1} for binary classification
of COVID-19 and other. Also, S i is considered as a bag
of instances with S i = {s1 , s2 , ..., sN } where N denotes the
total number of instances in the bag. It can be assumed
that each instance sn has a label yn âˆˆ {0, 1}, however not
all instances may be negative or positive. Moreover, not

2.3. Self Supervised Learning
Self Supervised Learning (SSL) is a form of unsupervised learning where the data provides the supervision,
and the network is trained to solve auxiliary tasks with a
proxy loss. This is highly beneficial, especially in medical imaging where supervision is limited and the existing
difficulty of curating annotations. Auxiliary tasks include
context prediction Oord et al. (2018), automatic colorization Zhang et al. (2016b), and image inpainting Pathak
4

medRxiv preprint doi: https://doi.org/10.1101/2020.09.14.20194654; this version posted September 18, 2020. The copyright holder for this
preprint (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
All rights reserved. No reuse allowed without permission.

Figure 1: Overview of the proposed framework. For a given patient CT scan, we sample k instances during training to create a bag as input and
feed them through the backbone FÎ¸ to obtain feature maps. Modules AÎ¸,S and AÎ¸,I learn spatial and instance attention, then perform permutation
invariant pooling via AÎ¸,I on feature maps to obtain a single bag representation. Prior to pooling, attention-weighted instance features from AÎ¸,I are
employed for unsupervised contrastive learning as well as patient level learning to obtain final predictions and update the model.

Figure 2: Illustration of contrastive learning applied to the MIL setting during model training.

all slices in a scan may show infection regions vital for the case of positive bags, at least one instance is assumed
diagnosis, as others may be noisy artifacts not useful for to be positive. Formally, it follows that
learning.
ï£±
P
ï£´
ï£´
ï£²0, iff
n yn = 0,
Y
=
(1)
Accordingly, MIL must satisfy the following conï£´
ï£´
ï£³1, otherwise.
straints: if a bag S i is negative, then it can be assumed
that all corresponding instances should be negative. In
In this work, this assumption may not hold given that
5

medRxiv preprint doi: https://doi.org/10.1101/2020.09.14.20194654; this version posted September 18, 2020. The copyright holder for this
preprint (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
All rights reserved. No reuse allowed without permission.

both sets of bags (COVID-19 and other pneumonia) considered contain both negative and positive instances (lesions). Thus, we consider a relaxed version of this constraint wherein an attention mechanism is applied to implicitly weight instances and learn their labels.

3.3. Dual Attention based Learning
In recent works Hashimoto et al. (2020); Ilse et al.
(2018); Han et al. (2020) attention has shown to be vital for learning robust features, especially under the MIL
setting. In particular, attention-based poolingIlse et al.
(2018) is preferred over existing pooling methods such as
max or mean, since they are not differentiable/applicable
for end-to-end model updates. In this work, we implement both spatial (AÎ¸,S ) and latent embedding (AÎ¸,I ) based
attention pooling via the respective modules. In the spatial module, given the input gi j âˆˆ RCÃ—HÃ—W , we employ two
convolutional layers each followed by hyperbolic tangent
(tanh) and sigmoid (sigm) non-linearities, respectively.
Feature maps are passed to each module successively,
then to the final convolutional layer with single channel
output representing the presence of infection. Notably, we
perform element-wise multiplication between the output
of each branch of the convolutional layers before passing
to the final layer to obtain spatial scores Ï†i j âˆˆ R1Ã—HÃ—W .
It is worth noting that we simply implement gated spatial
attentionDauphin et al. (2017) instead of the commonly
applied global average pooling (GAP) on the final backbone features gn .
In order to aggregate the features Ï†n , we employ attention based pooling proposed by Ilse et alIlse et al.
(2018) in the instance attention module AÎ¸,I . Formally,
we consider the same architecture applied for gated spatial attention, except all convolutional layers are replaced
with fully connected layers since attention is applied to instance embeddings. We denote H = {Ï†1 , Ï†2 , Ï†3 , . . . , Ï†N },
with hi âˆˆ H N as a bag with N instance features. Then,
attention based pooling MIL with gating mechanism is
defined as

3.2. Proposed Approach

We developed a CNN model for patient CT scan level
diagnosis between COVID-19 and other pneumonia in a
single end-to-end framework. Herein, a dual-attention
multi-instance learning deep model with unsupervised
contrastive learning (DA-CMIL) is proposed. As presented in Figure 1, our method takes a CT scan with unlabeled instances as input and learns key semantic representations. It further uses an attention-based pooling to
transform patient instances into a single bag representation for final prediction (see Section 3.3). Unsupervised
contrastive learning is employed to encourage instances in
a bag to be semantically similar to the bag representation
during training (see Section 3.4).
In the proposed framework, a backbone CNN model FÎ¸
is implemented as a feature extractor to transform i-th instance from a CT bag into a low dimension embedding
gi j = FÎ¸ (si j ) with spatial dimensions of shape C Ã— H Ã— W,
where C, H and W are channel size, height and width,
respectively. Then, a spatial attention module AÎ¸,S is employed to learn spatial representative features in the instances and output spatial attention maps 1 Ã— H âˆ— Ã— W âˆ—
with C = 1; used to weight all instances to obtain a single spatial pooled feature Ï†i j = AÎ¸,S (gi j ), with Ï† âˆˆ RD ,
where D is the feature dimension size. To aggregate the
instance features Ï†n for each CT scan, we implement a
N
X
second module AÎ¸,I that performs attention-based permuz=
an hn ,
(3)
tation invariant pooling to obtain a single bag representan=1
tion zi j = AÎ¸,I (Ï†i j ), with z âˆˆ RD having the same dimension for consistency. Following, zn is passed to the patient with,
level classifier HB to obtain predictions for the entire bag
yÌ‚ = HB (zi ), where yÌ‚ is the probability of a CT scan beexp{wT (tanh(VhTn ) sigm(UhTn ))}
, (4)
an = PN
ing labeled as COVID-19 or other pneumonia. Formally,
T
T
sigm(UhTj ))}
j=1 exp{w (tanh(Vh j )
we employ the bag loss LB (yÌ‚, yi ) using cross-entropy. It
follows that
where w âˆˆ RNÃ—1 , V âˆˆ RNÃ—D , and U âˆˆ RNÃ—D are trainable
parameters. tanh(Â·) and sigm(Â·) are element wise nonX
LB = âˆ’
yi log yÌ‚.
(2) linearities, with representing element-wise multiplica6

medRxiv preprint doi: https://doi.org/10.1101/2020.09.14.20194654; this version posted September 18, 2020. The copyright holder for this
preprint (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
All rights reserved. No reuse allowed without permission.

tion. From a technical standpoint, attention based pool- Algorithm 1 DA-CMIL Algorithm
ing allows different weights to be assigned to instances
1: input: parameters Î¸F , Î¸A,S , Î¸A,I , Î¸H , weight Î», epoch
alleviating the need for explicit instance selection. MoreT , temperature Ï„
over, the final bag representation will be more informa2: Initialize parameters Î¸F , Î¸A,S , Î¸A,I , Î¸H
tive. The synergistic combination of spatial and attention
3: for t = 1, 2, . . . , T do
based pooling allows for improved training towards learn4:
preprocess CT scans Sn and create bags with j
ing robust and interpretable features.
slices
5:
obtain features: gi j = FÎ¸ (si j )
6:
spatial pooling: Ï†i j = AÎ¸,S (gi j )
3.4. Contrastive MIL
7:
obtain attention weights an with Eq. (4) using
Inspired by recently proposed self-supervised learning
AÎ¸,I (Ï†i j )
methodsChen et al. (2020a,b), we integrate unsupervised
8:
combine instance features to get z with Eq. (3)
contrastive loss with the proposed MIL method for im9:
obtain bag predictions: yÌ‚ = HB (zi )
proved learning of instance level features. Formally, our 10: collect z and z0 : bag and instance features
model learns representations that maximize the agreement 11: normalize z and z0 with l2 norm.
between instance features and aggregated bag features of 12: compute cost in Eq. (6): Î»LB (yÌ‚, yi ) + (1 âˆ’
the same patient via a contrastive lossChen et al. (2020a)
Î»)LF (z0 , z, Ï„)
in the latent space. Figure 2 shows the overall concept of 13: update parameters Î¸F , Î¸A,S , Î¸A,I , Î¸H
the applied technique.
14: endfor
According to the previously proposed self-supervised 15: output: Î¸F , Î¸A,S , Î¸A,I , Î¸H
framework that uses contrastive loss, stochastic data augmentation is applied on 2D data samples to create two correlated views of the same example Chen et al. (2020b,a). loss is computed across all patient slice features and reAugmentations include random cropping, color distor- spective global features, herein considered as augmentations and random Gaussian bluring. Moreover, the con- tions per mini-batch. The final loss function of the entire
trastive loss is proposed to define contrastive predictive framework is defined as:
tasks on unlabeled samples, wherein positive and negative pairs are identified for given samples. To incorporate
L = Î»LB + (1 âˆ’ Î»)LF ,
(6)
this idea, stochastic data augmentation is omitted in our
study since contrastive loss is applied in the latent space. where Î» is a parameter to weight the contribution of bag
In addition, for any given patient CT scan; we infer that and constrastive losses, respectively. The detailed algoeach slice can be considered as pseudo augmentation of rithm is presented in Algorithm 1.
the overall patient characteristics. Thus, we consider that
the stochastic augmentation is implicitly applied (i.e. dif4. Experiments
ferent views of the same patient).
Let z0 be the latent instance level feature of patient, and
We evaluate the proposed method on a recently
z the global patient-level aggregated feature obtained via collected dataset and compare diagnostic performance
the proposed modules. Then, following l2 normalization against existing methods similar to ours. We present deof z0 and z features, a contrastive loss can be defined as
tails on evaluation settings and any pre-processing apLF = âˆ’log P2N
k=1

exp(sim(z0i , z j )/Ï„)
Q[k,i] exp(sim(z0i , zk )/Ï„)

plied.
,

(5)
4.1. Datasets

where Q[k,i] âˆˆ {0, 1} is an indicator function that evaluIn this study, we collected a chest CT dataset comprised
ates to 1 iff k , i and Ï„ denotes a temperature parameter. of 173 samples at Yeungnam University Medical Center
sim(Â·, Â·) is a similarity function i.e. cosine similarity. The (YUMC), in Daegu, South Korea. The dataset includes
7

medRxiv preprint doi: https://doi.org/10.1101/2020.09.14.20194654; this version posted September 18, 2020. The copyright holder for this
preprint (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
All rights reserved. No reuse allowed without permission.

4.2. Experimental Settings
Accordingly, all the datasets were split into training,
validation and testing by patient IDs with ratios 0.5, 0.1,
and 0.4, respectively. The same split was used across all
the dataset variants with all versions using only cropped
lung regions. CT examples were 512 Ã— 512, 128 Ã— 128 and
256 Ã— 256 Ã— 256 in size for the slices, patches and 3D CT
volume sets, respectively. Each CT slice was resized from
512Ã—512 to 256Ã—256 and patch slices were resized to 256
from 128. In particular, the slices set consisted of approximately 14,000 slices, whereas the patch version yielded
64,000 patches that mainly showed â‰¥ 30% of lung tissue. In the case of 3D CT volumes, all slices belonging
to a patient were used to construct a volume with nearest neighbor sampling applied to obtain the desired input
sizes.
The proposed model was implemented in Pytorch. A
ResNet-34He et al. (2016) finetuned from imageNet pretrained weights was used as the feature extraction module
FÎ¸ (Â·), with a single fully connected (FC) layer employed
as the bag classifier HB (Â·). The dimension of the features
was fixed to 512; this includes the feature maps obtained
from FÎ¸ (Â·) which had 512Ã—8Ã—8, with C = 512. Following
spatial pooling, features were reshaped back to 512.
During training, data augmentation consisting of random transformations such as flipping were applied for
both 2D and 3D based methods. All models were trained
for 30 epochs except 3D based methods with an initial
learning rate of 1 âˆ’ 4 for Î¸F , and 1 âˆ’ 3 for the other
modules with Adam optimization and a batch size of 8.
On the other hand, 3D CNN based methods used a batch
size of 4, learning rate of 1 âˆ’ 4 and were trained for 60
epochs.
For the proposed method, a bag was constructed with
k instances during training following step 4 of our algorithm, though for inference all available instances per patient were used to obtain the final prediction. We also
evaluated the efficacy of our method based on varying k
during training via ablation studies. For stable training,
the learning rate was annealed by a factor of 0.1 at epochs
10, 15 and 25, respectively. We empirically set the loss
hyper-parameters Î» and Ï„ to 0.5 and 1.0, respectively.

Figure 3: Pre-processed CT examples. (Red-section) COVID-19 CT
slice and patch samples. (Green-section) bacterial pneumonia samples.

75 CT examples for patients with COVID-19, and 98 examples from patients with bacterial pneumonia collected
between February and April, 2020. The study was approved by the Institutional Review Board (IRB) of Yeungnam University Hospital. COVID-19 patients were confirmed by RT-PCR assay of nasal and pharyngeal swab
samples.
Further, we designed variants of YUMC CT dataset to
fairly assess the performance of our method and others
such as 3D based approaches. Namely, based on the original YUMC CT data using CT slices per patient, we processed a patch-based version of the dataset. In the MIL
framework, 2D CT slice or patches can be used as instances, thus we evaluate our method on both cases. In
addition, a 3D CT volume dataset is also processed for
training/testing 3D based methods under fully-supervised
settings.
For pre-processing, lung regions were segmented for
all CT examples. To achieve this, we employed a
ResNeSt Zhang et al. (2020a) model for segmentation training and inference. The model was trained
on two public datasets i.e. non-small cell lung cancer
(NSCLC) Aerts et al. (2014) and COVID-19 lung infection dataset Jun et al. (2020). Herein, a total of 50,756
lung slices were used for training and evaluated on 1,222 4.3. Comparison Methods
independent slices. Figure 3 shows examples of CT slices
To evaluate the efficacy of the proposed method, we
and patches employed.
compared against recent MIL based methods i.e. Deep8

medRxiv preprint doi: https://doi.org/10.1101/2020.09.14.20194654; this version posted September 18, 2020. The copyright holder for this
preprint (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
All rights reserved. No reuse allowed without permission.

Table 1: Evaluation of the proposed methods on YUMC dataset including results of using DA-CMIL with/without contrastive loss LF .

Method
DeCoVNet Wang et al. (2020c)
MIL Campanella et al. (2019)
DeepAttentionMIL Ilse et al. (2018)
JointMIL Chikontwe et al. (2020)
Zhang3DCNN Zhang et al. (2020b)
DA-CMIL (w/o LF )
DA-CMIL (w/ LF )

Accuracy
0.831
0.803
0.859
0.901
0.93
0.93
0.986

AUC
0.825
0.796
0.875
0.909
0.938
0.934
0.984

F1
0.8
0.767
0.861
0.896
0.925
0.923
0.984

Specificity
0.875
0.85
0.75
0.85
0.875
0.9
0.975

Sensitivity
0.774
0.742
1
0.968
1
0.968
1

Table 2: Evaluation of the proposed methods on YUMC patch dataset.

Method
MIL Campanella et al. (2019)
DeepAttentionMIL Ilse et al. (2018)
JointMIL Chikontwe et al. (2020)
DA-CMIL (w/o LF )
DA-CMIL (w/ LF )

Accuracy
0.845
0.845
0.845
0.93
0.958

AUC
0.852
0.859
0.837
0.934
0.955

F1
0.836
0.845
0.814
0.923
0.951

Specificity
0.8
0.75
0.9
0.9
0.975

Sensitivity
0.903
0.968
0.774
0.968
0.935

of bag size and the weighting parameter Î» are presented.

AttentionMIL Ilse et al. (2018), ClassicMIL Campanella
et al. (2019) and JointMILChikontwe et al. (2020). Also,
recent 3D based methods DeCovNetWang et al. (2020c)
and Zhang3DCNNZhang et al. (2020b) were included for
comparison. For a fair evaluation, the same backbone feature extractor is used in all methods except for the 3D
methods as we used the publicly available implementations.
In particular, ClassicMIL follows the traditional assumption of the MIL setting and focuses on instance level
learning wherein only the top instance per bag is considered for the final patient level prediction. DeepAttentionMIL uses attention-based pooling for bag level learning.
In constrast, JointMIL combines both instance and bag
level learning with bag feature clustering during training.
Lastly, DeCoVNet and Zhang3DCNN both use all available CT slices in a constructed volume under the fully
supervised setting. The later methods serve as an upperbound over the weakly supervised methods evaluated in
this study.

5.1. Quantitative Results

Diagnostic performance was evaluated on YUMC CT
slice, patch and CT volume based datasets using accuracy,
area under the curve (AUC), f1-score, specificity and sensitivity, respectively. Tables 1 and 2 show the performance
of the evaluated methods on the datasets.
In Table 1, DA-CMIL with contrastive loss LF achieves
the best overall performance of 98.6% accuracy and an
AUC of 98.4%. Notably, even when LF was not applied during training, our method still reports 93%(+2.9)
and 93.4%(+2.5) in terms of accuracy and AUC over the
best weakly supervised method JointMIL. MIL reports
the lowest performance among all methods, which is expected since it only considers the top instance among multiple 2D slices in bag for inference. Interestingly, our
method outperformed both Zhang3DCNN and DeCoVNet which are fully supervised methods even without LF
used in the training stage. Though both DA-CMIL and
DeepAttentionMIL employ attention based pooling, the
proposed method shows improved performance via dual5. Results
attention pooling, validating the architectural design.
To further validate the proposed method, we applied
We present both quantitative and qualitative results of
the proposed methods. Also, ablation studies on the effect DA-CMIL to randomly cropped patches of the CT sam9

medRxiv preprint doi: https://doi.org/10.1101/2020.09.14.20194654; this version posted September 18, 2020. The copyright holder for this
preprint (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
All rights reserved. No reuse allowed without permission.

Figure 4: The Receiver operating characteristic(ROC) curves of compared methods on the YUMC CT slices and patch datasets.

Figure 5: Confusion matrices of compared methods on YUMC CT Slices dataset. CP represent Pneumonia and NCP implies COVID-19, respectively.

ples. As shown in Table 2, performance was consistently
better than the compared methods. All weakly supervised method showed similar accuracy with considerable
margins observed for sensitivity. DeepAttentionMIL reported the best sensitivity at 96.8% with accuracy consistent with other methods. However, DA-CMIL showed an
improvement of +11.3% in accuracy over the best compared method with an equally larger margin without LF
employed. The effect of using attention and contrastive
loss was more pronounced in the case of patches as not
applying LF showed reduced performance (-2.8%).
Figure 4 shows the receiver operating characteristic(ROC) curves of the compared methods on different
datasets. Overall, the proposed method shows a high TPR
and lower FPR across all settings. This is further evi-

denced in the summaries of the confusion matrices of the
comparison methods as presented in Figures 5 and 6. This
indicates DA-CMIL can be viable option for accurate and
robust screening of COVID-19.
5.2. Effect of the bag size
To assess the effect of bag size during training on the
proposed method, we performed an ablation study where
the bag was constructed by varying k i.e. each bag consisted of k max instances (slices/patches). As shown in
Table 3 and Figure 7, as the bag size increases DA-CMIL
performance improves. The best result was achieved
when k = 32 with a considerable margin across all metrics. We limited evaluation to k = 32 due to computational limitations. Moreover, it worth noting that con-

10

medRxiv preprint doi: https://doi.org/10.1101/2020.09.14.20194654; this version posted September 18, 2020. The copyright holder for this
preprint (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
All rights reserved. No reuse allowed without permission.

Table 3: Evaluation of varying bag sizes with the proposed method on YUMC CT slices dataset.

Method
DA-CMIL (w/ k = 8 )
DA-CMIL (w/ k = 16)
DA-CMIL (w/ k = 24 )
DA-CMIL (w/ k = 32)

Accuracy
0.93
0.944
0.944
0.986

AUC
0.934
0.939
0.943
0.988

Figure 6: Confusion matrices of compared methods on YUMC CT patch
dataset.

F1
0.923
0.933
0.935
0.984

Specificity
0.9
0.975
0.95
0.975

Sensitivity
0.968
0.903
0.935
1

Figure 7: ROC curves of DA-CMIL on YUMC CT Slide dataset when k
is varied during training.

trastive methods benefit from large batch sizes; as evidenced from the reported results, our findings are consistent with existing observations based on self-supervised
methods applied to general vision datasets. However, as
k is increased the relative batch size based on bags should
be reduced to compensate for training time and memory
requirements. In general, results show that our method is
not limited/affected by the number of instances available
per CT scan and can benefit from using more instances for
training, though during the inference stage all instances
are used.

Table 4: Evaluation on varying Î» in the cost function L on the YUMC
Dataset.

Accuracy

Î»=0.1
0.972

Î»=0.5
0.986

Î»=0.9
0.986

Î»=1.0
0.932

learning and showed a lower performance of 93% compared to not using LF (Â·) i.e. when Î» < 1.0. Though similar performance was noted across different values of Î», the
most significant is when contrastive loss was not applied
entirely as presented in Table 4.

5.3. Effect of the weight parameter Î»
DA-CMIL uses contrastive feature learning of multiple 5.4. Qualitative Results
instances with a weighting parameter Î» to balance the efIn Figure 8, qualitative results are presented based on
fect of the losses. When Î» = 1.0, LF (Â·) has no effect on spatial attention maps and attention scores, respectively.
11

medRxiv preprint doi: https://doi.org/10.1101/2020.09.14.20194654; this version posted September 18, 2020. The copyright holder for this
preprint (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
All rights reserved. No reuse allowed without permission.

Figure 8: Qualitative examples of DA-CMIL spatial attention maps with attention scores on CT samples from a single patient with COVID-19. The
top row shows the attention value of each slice with the spatial maps normalized to focus on the lung regions only.

This demonstrates that DA-CMIL is able to find key slices
related to infected areas with coarse maps. Interestingly,
low attention scores were observed for slices such as noisy
slices/artifacts with no infected areas further indicating
the utility of our method. Moreover, attention maps focus on key areas such as ground-glass opacities and consolidations, both consistent with clinical findings. This
can be highly beneficial in clinical evaluation since our
method avoids post-hoc analysis based on class activation
maps (CAM).
6. Discussion
Though RT-PCR is the gold standard for COVID-19
diagnosis, it is still hindered by lengthy test times, as
it can take days to obtain the results. Accordingly, CT
has been considered as a reasonable alternative for current testing methods as it can produce results within minutes. We showed a novel approach to the application deep
CNNs for COVID-19 diagnosis under weak supervision

with clinical implications. It is important to have a fully
automated and interpretable method in actual settings for
rapid evaluation. Moreover, given the subtleties that exist between COVID-19 and other pneumonia in terms of
imaging characteristics that field experts find hard to differentiate, accurate diagnosis is highly relevant.
Our method was evaluated on recently curated dataset
wherein only patient diagnostic labels are available without lesion infected regions of interest as is common in
existing methods. To further validate our approach, we
qualitatively showed the regions that are focused on by
our model via coarse attention maps alongside attention
scores. Our method achieved an AUC of 98.4%, accuracy of 98.6% and a true positive rate (TPR) of 96.8%.
In addition, attention maps obtained highlight key infection areas in the majority of samples with attention scores
corresponding to key slices.
We also empirically showed the benefit of using an unsupervised contrastive loss to complement the supervised
learning of patient labels and may serve as a base for more

12

medRxiv preprint doi: https://doi.org/10.1101/2020.09.14.20194654; this version posted September 18, 2020. The copyright holder for this
preprint (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
All rights reserved. No reuse allowed without permission.

complex methods. Moreover, the proposed method surpassed 3D based methods by large margins. We infer
this may be due to the limited size of the dataset employed as most recent methods applied to 3D CT volumes report using large cohorts in literature. In addition,
since DeCoVNet was trained from scratch and has an custom deep architecture, performance was subpar. Though
ZhangCNNâ€™s performance was considerably better than
the later, it still did not achieve comparable performance
even when the model was trained for more epochs. It is
also worth noting that models trained with extensive augmentation did not achieve any considerable improvements
across the evaluation metrics, since COVID-19 and bacterial pneumonia present similar characteristics.
There exist a few limitations with regard to the proposed method. Though attention maps could show interpretability and explainability for COVID-19 diagnosis,
there exist some failure cases where the attention map do
not correctly indicate an infected region as shown in Figure 5. Second, we found that extensive data augmentation
such as color jittering lead to reduced performance and
was largely negligible compared to the benefit of using
a contrastive loss which showed consistent improvements
across all evaluation settings. This motivates us to consider using more complex attention modes for better diagnostic interpretability as well as explore unsupervised
pre-training using the proposed method both in 2D or 3D
as future directions.

7. Conclusion

CRediT authorship contribution statement
Philip Chikontwe: Conceptualization, Formal analysis, Investigation, Methodology, Validation, Visualization, Software, Writing - original draft, Writing - review
& editing. Miguel Luna: Investigation, Methodology,
Writing - review & editing. Myeong Kyun Kang: Investigation and Methodology. Kyung Soo Hong: Resources
& Data curation. June Hong Ahn: Resources, Data curation, Supervision, Writing - review & editing. Sang Hyun
Park: Funding acquisition, Methodology, Project administration, Supervision, Writing - review & editing.

Acknowledgments
This work was supported by the National Research
Foundation of Korea (NRF) grant funded by the Korean
Government (MSIT)(No.2019R1C1C1008727).

References
Aerts, H.J., Velazquez, E.R., Leijenaar, R.T., Parmar, C.,
Grossmann, P., Carvalho, S., Bussink, J., Monshouwer,
R., Haibe-Kains, B., Rietveld, D., et al., 2014. Decoding tumour phenotype by noninvasive imaging using a
quantitative radiomics approach. Nature communications 5, 1â€“9.
Ai, T., Yang, Z., Hou, H., Zhan, C., Chen, C., Lv, W., Tao,
Q., Sun, Z., Xia, L., 2020. Correlation of chest ct and
rt-pcr testing in coronavirus disease 2019 (covid-19) in
china: a report of 1014 cases. Radiology , 200642.

In this study, we developed a 2D CNN framework with
dual attention modules and contrastive feature learning Alom, M.Z., Rahman, M., Nasrin, M.S., Taha, T.M.,
under the multiple instance learning (MIL) framework to
Asari, V.K., 2020. Covid mtnet: Covid-19 detecdistinguish COVID-19 and a bacterial sub-type of pneution with multi-task deep learning approaches. arXiv
monia in chest CTs. We verified performance on both CT
preprint arXiv:2004.03747 .
patch and slice based versions of the datasets and report
results comparable to state-of-the-art methods. In addi- Campanella, G., Hanna, M.G., Geneslaw, L., Miraflor,
A., Silva, V.W.K., Busam, K.J., Brogi, E., Reuter,
tion, ablation experiments show the benefit of using large
V.E., Klimstra, D.S., Fuchs, T.J., 2019. Clinical-grade
bag sizes during training and the effect of weighting losses
computational pathology using weakly supervised deep
correctly for stable learning. Through this study, we hope
learning on whole slide images. Nature medicine 25,
to add valuable contribution to the current literature on
1301â€“1309.
weakly supervised methods for COVID-19 screening.
13

medRxiv preprint doi: https://doi.org/10.1101/2020.09.14.20194654; this version posted September 18, 2020. The copyright holder for this
preprint (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
All rights reserved. No reuse allowed without permission.

Carbonneau, M.A., Cheplygina, V., Granger, E., Gagnon,
G., 2018. Multiple instance learning: A survey of problem characteristics and applications. Pattern Recognition 77, 329â€“353.
Chen, T., Kornblith, S., Norouzi, M., Hinton, G., 2020a.
A simple framework for contrastive learning of visual
representations. arXiv preprint arXiv:2002.05709 .

with unannotated histopathological images, in: Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pp. 3852â€“3861.
He, K., Zhang, X., Ren, S., Sun, J., 2016. Deep residual
learning for image recognition, in: Proceedings of the
IEEE conference on computer vision and pattern recognition, pp. 770â€“778.

Chen, X., Fan, H., Girshick, R., He, K., 2020b. Improved He, X., Yang, X., Zhang, S., Zhao, J., Zhang, Y., Xing,
baselines with momentum contrastive learning. arXiv
E., Xie, P., 2020. Sample-efficient deep learning for
preprint arXiv:2003.04297 .
covid-19 diagnosis based on ct scans. medRxiv .
Chikontwe, P., Kim, M., Nam, S.J., Go, H., Park, S.H., Hou, L., Samaras, D., Kurc, T.M., Gao, Y., Davis, J.E.,
2020. Multiple instance learning with center embedSaltz, J.H., 2016. Patch-based convolutional neural
dings for histopathology classification, in: Internanetwork for whole slide tissue image classification, in:
tional Conference on Medical Image Computing and
Proceedings of the ieee conference on computer vision
Computer-Assisted Intervention, Springer (In Press).
and pattern recognition, pp. 2424â€“2433.
pp. 1â€“1.
Hu, M., Liu, Z., Zhang, J., Zhang, G., 2017. Robust object
Dauphin, Y.N., Fan, A., Auli, M., Grangier, D., 2017.
tracking via multi-cue fusion. Signal Processing 139,
Language modeling with gated convolutional net86â€“95.
works, in: International conference on machine learning, pp. 933â€“941.
Hu, S., Gao, Y., Niu, Z., Jiang, Y., Li, L., Xiao, X., Wang,
M., Fang, E.F., Menpes-Smith, W., Xia, J., et al., 2020.
Feng, J., Zhou, Z.H., 2017. Deep miml network, in:
Weakly supervised deep learning for covid-19 infection
Thirty-First AAAI conference on artificial intelligence.
detection and classification from ct images. IEEE Access 8, 118869â€“118883.
Gozes, O., Frid-Adar, M., Greenspan, H., Browning, P.D.,
Zhang, H., Ji, W., Bernheim, A., Siegel, E., 2020a.
Rapid ai development cycle for the coronavirus (covid- Huang, L., Han, R., Ai, T., Yu, P., Kang, H., Tao, Q.,
Xia, L., 2020. Serial quantitative chest ct assessment
19) pandemic: Initial results for automated detection &
of covid-19: Deep-learning approach. Radiology: Carpatient monitoring using deep learning ct image analydiothoracic Imaging 2, e200075.
sis. arXiv preprint arXiv:2003.05037 .
Gozes, O., Frid-Adar, M., Sagie, N., Zhang, H., Ji, W., Ilse, M., Tomczak, J.M., Welling, M., 2018. Attentionbased deep multiple instance learning. arXiv preprint
Greenspan, H., 2020b. Coronavirus detection and analarXiv:1802.04712 .
ysis on chest ct with deep learning. arXiv preprint
arXiv:2004.02640 .
Jin, S., Wang, B., Xu, H., Luo, C., Wei, L., Zhao, W.,
Hou, X., Ma, W., Xu, Z., Zheng, Z., et al., 2020. AiHan, Z., Wei, B., Hong, Y., Li, T., Cong, J., Zhu, X., Wei,
assisted ct imaging analysis for covid-19 screening:
H., Zhang, W., 2020. Accurate screening of covid-19
Building and deploying a medical ai system in four
using attention based deep 3d multiple instance learnweeks. medRxiv .
ing. IEEE Transactions on Medical Imaging .
Hashimoto, N., Fukushima, D., Koga, R., Takagi, Y., Ko, Jun, M., Cheng, G., Yixin, W., Xingle, A., Jiantao, G.,
K., Kohno, K., Nakaguro, M., Nakamura, S., Hontani,
Ziqi, Y., Minqing, Z., Xin, L., Xueyuan, D., Shucheng,
H., Takeuchi, I., 2020. Multi-scale domain-adversarial
C., Hao, W., Sen, M., Xiaoyu, Y., Ziwei, N., Chen, L.,
multiple-instance cnn for cancer subtype classification
Lu, T., Yuntao, Z., Qiongjie, Z., Guoqiang, D., Jian,
14

medRxiv preprint doi: https://doi.org/10.1101/2020.09.14.20194654; this version posted September 18, 2020. The copyright holder for this
preprint (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
All rights reserved. No reuse allowed without permission.

H., 2020. COVID-19 CT Lung and Infection Segmen- Pathak, D., Krahenbuhl, P., Donahue, J., Darrell, T.,
tation Dataset. URL: https://doi.org/10.5281/
Efros, A.A., 2016. Context encoders: Feature learning
zenodo.3757476, doi:10.5281/zenodo.3757476.
by inpainting, in: Proceedings of the IEEE conference
on computer vision and pattern recognition, pp. 2536â€“
Kermany, D.S., Goldbaum, M., Cai, W., Valentim, C.C.,
2544.
Liang, H., Baxter, S.L., McKeown, A., Yang, G., Wu,
X., Yan, F., et al., 2018. Identifying medical diagnoses Ronneberger, O., Fischer, P., Brox, T., 2015. U-net:
Convolutional networks for biomedical image segand treatable diseases by image-based deep learning.
mentation, in: International Conference on Medical
Cell 172, 1122â€“1131.
image computing and computer-assisted intervention,
Springer. pp. 234â€“241.
LeCun, Y., Bengio, Y., Hinton, G., 2015. Deep learning.
nature 521, 436â€“444.
Shan, F., Gao, Y., Wang, J., Shi, W., Shi, N., Han, M.,
Xue, Z., Shi, Y., 2020. Lung infection quantification
Litjens, G., Kooi, T., Bejnordi, B.E., Setio, A.A.A.,
of covid-19 in ct images with deep learning. arXiv
Ciompi, F., Ghafoorian, M., Van Der Laak, J.A.,
preprint arXiv:2003.04655 .
Van Ginneken, B., SaÌnchez, C.I., 2017. A survey on
deep learning in medical image analysis. Medical imShi, F., Wang, J., Shi, J., Wu, Z., Wang, Q., Tang, Z., He,
age analysis 42, 60â€“88.
K., Shi, Y., Shen, D., 2020a. Review of artificial intelligence techniques in imaging data acquisition, segMahase, E., 2020. Coronavirus: covid-19 has killed more
mentation and diagnosis for covid-19. IEEE reviews in
people than sars and mers combined, despite lower case
biomedical engineering .
fatality rate.
Mei, X., Lee, H.C., Diao, K.y., Huang, M., Lin, B., Liu,
C., Xie, Z., Ma, Y., Robson, P.M., Chung, M., et al.,
2020. Artificial intelligenceâ€“enabled rapid diagnosis
of patients with covid-19. Nature Medicine , 1â€“5.

Shi, F., Xia, L., Shan, F., Wu, D., Wei, Y., Yuan, H., Jiang,
H., Gao, Y., Sui, H., Shen, D., 2020b. Large-scale
screening of covid-19 from community acquired pneumonia using infection size-aware classification. arXiv
preprint arXiv:2003.09860 .

Ng, M.Y., Lee, E.Y., Yang, J., Yang, F., Li, X., Wang,
Song, Y., Zheng, S., Li, L., Zhang, X., Zhang, X., Huang,
H., Lui, M.M.s., Lo, C.S.Y., Leung, B., Khong, P.L.,
Z., Chen, J., Zhao, H., Jie, Y., Wang, R., et al.,
et al., 2020. Imaging profile of the covid-19 infection:
2020. Deep learning enables accurate diagnosis of
radiologic findings and literature review. Radiology:
novel coronavirus (covid-19) with ct images. medRxiv
Cardiothoracic Imaging 2, e200034.
.
Oh, Y., Park, S., Ye, J.C., 2020. Deep learning covid-19 Wang, D., Hu, B., Hu, C., Zhu, F., Liu, X., Zhang, J.,
features on cxr using limited training data sets. IEEE
Wang, B., Xiang, H., Cheng, Z., Xiong, Y., et al.,
Transactions on Medical Imaging .
2020a. Clinical characteristics of 138 hospitalized patients with 2019 novel coronavirusâ€“infected pneumoOord, A.v.d., Li, Y., Vinyals, O., 2018. Representania in wuhan, china. Jama 323, 1061â€“1069.
tion learning with contrastive predictive coding. arXiv
preprint arXiv:1807.03748 .
Wang, S., Kang, B., Ma, J., Zeng, X., Xiao, M., Guo, J.,
Cai, M., Yang, J., Li, Y., Meng, X., et al., 2020b. A
Ouyang, X., Huo, J., Xia, L., Shan, F., Liu, J., Mo, Z.,
deep learning algorithm using ct images to screen for
Yan, F., Ding, Z., Yang, Q., Song, B., et al., 2020. Dualcorona virus disease (covid-19). MedRxiv .
sampling attention network for diagnosis of covid-19
from community acquired pneumonia. IEEE Transac- Wang, X., Deng, X., Fu, Q., Zhou, Q., Feng, J., Ma,
tions on Medical Imaging .
H., Liu, W., Zheng, C., 2020c. A weakly-supervised
15

medRxiv preprint doi: https://doi.org/10.1101/2020.09.14.20194654; this version posted September 18, 2020. The copyright holder for this
preprint (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.
All rights reserved. No reuse allowed without permission.

framework for covid-19 classification and lesion localization from chest ct. IEEE Transactions on Medical
Imaging .
Xie, W., Jacobs, C., Charbonnier, J.P., van Ginneken, B.,
2020. Relational modeling for robust and efficient pulmonary lobe segmentation in ct scans. IEEE Transactions on Medical Imaging .
Yao, J., Zhu, X., Huang, J., 2019. Deep multi-instance
learning for survival prediction from whole slide images, in: International Conference on Medical Image Computing and Computer-Assisted Intervention,
Springer. pp. 496â€“504.
Zhang, D., Meng, D., Han, J., 2016a. Co-saliency detection via a self-paced multiple-instance learning framework. IEEE transactions on pattern analysis and machine intelligence 39, 865â€“878.
Zhang, H., Wu, C., Zhang, Z., Zhu, Y., Zhang, Z.,
Lin, H., Sun, Y., He, T., Mueller, J., Manmatha, R.,
et al., 2020a. Resnest: Split-attention networks. arXiv
preprint arXiv:2004.08955 .
Zhang, K., Liu, X., Shen, J., Li, Z., Sang, Y., Wu, X.,
Zha, Y., Liang, W., Wang, C., Wang, K., et al., 2020b.
Clinically applicable ai system for accurate diagnosis,
quantitative measurements, and prognosis of covid-19
pneumonia using computed tomography. Cell .
Zhang, R., Isola, P., Efros, A.A., 2016b. Colorful image
colorization, in: European conference on computer vision, Springer. pp. 649â€“666.
Zheng, C., Deng, X., Fu, Q., Zhou, Q., Feng, J., Ma,
H., Liu, W., Wang, X., 2020. Deep learning-based
detection for covid-19 from chest ct using weak label.
medRxiv .
Zu, Z.Y., Jiang, M.D., Xu, P.P., Chen, W., Ni, Q.Q., Lu,
G.M., Zhang, L.J., 2020. Coronavirus disease 2019
(covid-19): a perspective from china. Radiology ,
200490.

16

