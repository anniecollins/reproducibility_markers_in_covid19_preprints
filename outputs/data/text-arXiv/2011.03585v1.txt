Noname manuscript No.
(will be inserted by the editor)

Chest X-ray Image Phase Features for Improved
Diagnosis of COVID-19 Using Convolutional Neural
Network
Xiao Qi ¬∑ Lloyd Brown ¬∑ David
J. Foran ¬∑ John Nosher ¬∑ Ilker
Hacihaliloglu

arXiv:2011.03585v1 [eess.IV] 6 Nov 2020

Received: date / Accepted: date

Abstract Purpose: Recently, the outbreak of the novel Coronavirus disease
2019 (COVID-19) pandemic has seriously endangered human health and life.
In fighting against COVID-19, effective diagnosis of infected patient is critical for preventing the spread of diseases. Due to limited availability of test
kits, the need for auxiliary diagnostic approach has increased. Recent research
has shown radiography of COVID-19 patient, such as CT and X-ray, contains
salient information about the COVID-19 virus and could be used as an alternative diagnosis method. Chest X-ray (CXR) due to its faster imaging time,
wide availability, low cost and portability gains much attention and becomes
very promising. In order to reduce intra- and inter-observer variability, during radiological assessment, computer aided diagnostic tools have been used
in order supplement medical decision making and subsequent management.
Computational methods with high accuracy and robustness are required for
rapid triaging of patients and aiding radiologist in the interpretation of the
collected data.
Xiao Qi
Department of Electrical and Computer Engineering, Rutgers University E-mail:
xq53@scarletmail.rutgers.edu
Lloyd Brown
Department
of
Surgery,
brownl8@njms.rutgers.edu

Rutgers

New

Jersey

Medical

School

E-mail:

David J. Foran
Rutgers Cancer Institute of New Jersey E-mail: foran@cinj.rutgers.edu
John Nosher
Department of Radiology, Rutgers Robert Wood Johnson Medical School E-mail:
nosher@rwjms.rutgers.edu
Ilker Hacihaliloglu
Department of Biomedical Engineering, Rutgers University
Department of Radiology, Rutgers Robert Wood Johnson Medical School E-mail:
ilker.hac@soe.rutgers.edu

2

Xiao Qi et al.

Method: In this study, we design a novel multi-feature convolutional neural
network (CNN) architecture for multi-class improved classification of COVID19 from CXR images. CXR images are enhanced using a local phase-based
image enhancement method. The enhanced images, together with the original
CXR data, are used as an input to our proposed CNN architecture. Using ablation studies, we show the effectiveness of the enhanced images in improving
the diagnostic accuracy. We provide quantitative evaluation on two datasets
and qualitative results for visual inspection. Quantitative evaluation is performed on data consisting of 8,851 normal (healthy), 6,045 pneumonia, and
3,323 Covid-19 CXR scans.
Results: In Dataset-1, our model achieves 95.57% average accuracy for a three
classes classification, 99% precision, recall, and F1-scores for COVID-19 cases.
For Dataset-2, we have obtained 94.44% average accuracy, and 95% precision,
recall, and F1-scores for detection of COVID-19.
Conclusions: Our proposed multi-feature guided CNN achieves improved results compared to single-feature CNN proving the importance of the local
phase-based CXR image enhancement. Future work will involve further evaluation of the proposed method on a larger size COVID-19 dataset as they
become available.
Keywords Chest X-ray ¬∑ COVID-19 Diagnosis ¬∑ Image Enhancement ¬∑ Image
Phase ¬∑ Multi-feature CNN
1 Introduction
Coronavirus disease 2019 (COVID-19) is an infectious disease caused by severe
acute respiratory syndrome coronavirus 2 (SARS-CoV-2), a newly discovered
coronavirus [1, 2]. In March 2020, the World Health Organization (WHO) declared the COVID-19 outbreak a pandemic. Up to now, more than 9.23 million
cases have been reported across 188 countries and territories, resulting in more
than 476,000 deaths [3]. Early and accurate screening of infected population
and isolation from public is an effective way to prevent and halt spreading of
virus. Currently, the gold standard method used for diagnosing COVID-19 is
real-time reverse transcription polymerase chain reaction (RT-PCR) [4]. The
disadvantages of RT-PCR include its complexity and problems associated with
its sensitivity, reproducibility, and specificity [5]. Moreover, the limited availability of test kits makes it challenging to provide the sufficient diagnosis for
every suspected patients in the hyper-endemic regions or countries. Therefore,
a faster, reliable and automatic screening technique is urgently required.
In clinical practice, easily accessible imaging, such as chest X-ray (CXR),
provides important assistance to clinicians in decision making. Compared to
computed tomography (CT) the main advantages of CXR are: Enabling fast
screening of patients, being portable, and easy to setup (can be setup in isolation rooms). However, the sensitivity and specificity (radiographic assessment
accuracy) of CXR for diagnosing COVID-19 is low compared to CT. This is
especially problematic for identifying early stage COVID-19 patients with mild

Title Suppressed Due to Excessive Length

3

symptoms. This causes larger intra- and inter-observer variability in reading
the collected data by radiologists since qualitative indicators can be subtle.
Therefore, there is increased demand for computer aided diagnostic method
to aid the radiologist during decision making for improved management of
COVID-19 disease.
In view of these advantages and motivated by the need for accurate and automatic interpretation of CXR images, a number of studies based on deep convolutional neural networks (CNNs) have shown quite promising results. Ozturk
et al.[6] proposed a CNN architecture, termed DarkCovidNet, and achieved
87.02% three class classification accuracy. The method was evaluated on 127
COVID-19, 500 healthy and 500 pneumonia CXR scans. COVID-19 data
was obtained from 125 patients. Wang et al.[7] built a public dataset named
COVIDx, which is comprised of a total of 13975 CXR images from 13870 patient case and developed COVID-Net, a deep learning model. Their dataset
had 358 Covid-19 images obtained from 266 patients. Their model achieved
93.3% overall accuracy in classifying normal, pneumonia, and COVID-19 scans.
In [8] a ResNet-50 architecture was utilized to achieve a 96.23% overall accuracy in classifying four classes, where pneumonia was split into bacterial
pneumonia and viral pneumonia. However, there were only eight COVID-19
CXR images used for testing. In [9], 76.37% overall accuracy was reported on
a dataset including 1583 normal, 4290 pneumonia and 76 COVID-19 scans.
COVID-19 data was collected from 45 patients. In order to improve the performance of the proposed method, data augmentation was performed on the
COVID-19 dataset bringing the total COVID-19 datasize to 1,536. With data
augmentation they have improved the overall accuracy 97.2%. In [10], Contrast
Limited Adaptive Histogram Equalization (CLAHE) was used to enhance the
CXR data. The authors proposed a depth-wise separable convolutional neural network (DSCNN) architecture. Evaluation was performed on 668 normal,
619 pneumonia, and 536 COVID-19 CXR scans. Average reported multi-class
accuracy was 96.43%. Number of patients for the COVID-19 dataset was not
available. In [11], a stacked CNN architecture achieved an average accuracy of
92.74%. The evaluation dataset had 270 COVID-19 scans from 170 patients,
1139 normal scans from 1015 patients, and 1355 pneumonia scans from 583
patients. In [12], the reported multi-class average classification accuracy was
s 94.2%. The evaluation dataset included 5000 normal, 4600 pneumonia, and
738 COVID-19 CXR scans. The data was collected from various sources and
patient information was not specified. In [13] transfer learning was investigated for training the CNN architecture. The evaluation dataset included 224
COVID-19, 504 normal, and 700 pneumonia images. 93.48% average accuracy
was reported for three-class classification. The average accuracy increased to
94.72% if viral pneumonia was included in the evaluation. In [14], performance of three different, previously proposed, CNN architectures was evaluated for multi-class classification. With 2,265 COVID-19 images, the study
used the largest COVID-19 dataset reported so far. Average area under the
curve (AUC), for classification of COVID-19 from regular pneumonia, was 0.73
[14].

4

Xiao Qi et al.

Fig. 1: Block diagram of the proposed framework for improved COVID-19
diagnosis from CXR.

Although numerous studies have shown the capability of CNNs in effective
identification of COVID-19 from CXR images, none of these studies investigated local phase CXR image features as multi-feature input to a CNN architecture for improved diagnosis of COVID-19 disease. Furthermore, except [14,
7], most of the previous work was evaluated on a limited number of COVID19 CXR scans. In this work we show how local phase CXR features based
image enhancement improves the accuracy of CNN architectures for COVID19 diagnosis. Specifically, we extract three different CXR local phase image
features which are combined as a multi-feature image. We design a new CNN
architecture for processing multi-feature CXR data. We evaluate our proposed
methods on large scale CXR images obtained from healthy subjects as well as
subjects who are diagnosed with community acquired pneumonia and COVID19. Quantitative results show the usefulness of local phase image features for
improved diagnosis of COVID-19 disease from CXR scans.

2 Material and methods
Our proposed method is designed for processing CXR images and consists
of two main stages as illustrated in Figure 1: 1- We enhance the CXR images (CXR(x, y)) using local phase-based image processing method in order to obtain a multi-feature CXR image (M F (x, y)), and 2- we classify
CXR(x, y) by designing a deep learning approach where multi feature CXR
images (M F (x, y)), together with original CXR data (CXR(x, y)), is used
for improving the classification performance. Next, we describe how these two
major processes are achieved.

2.1 Image Enhancement
In order to enhance the collected CXR images, denoted as CXR(x, y), we use
local phase-based image analysis [15]. Three different CXR(x, y) image phase
features are extracted: 1- Local weighted mean phase angle (LwP A(x, y)), 2LwP A(x, y) weighted local phase energy (LP E(x, y)), and 3- Enhanced local
energy attenuation image (ELEA(x, y)). LP E(x, y) and LwP A(x, y) image
features are extracted using monogenic signal theory where the monogenic

Title Suppressed Due to Excessive Length

5

signal image (CXRM (x,y)) is obtained by combining the bandpass filtered
CXR(x, y) image, denoted as CXRB (x, y), with the Riesz filtered components
as:
CXRM (x, y) = [CXRM 1 , CXRM 2 , CXRM 3 ]
= [CXRB (x, y), CXRB √ó h1 (x, y), CXRB (x, y) √ó h2 (x, y)]
Here h1 and h2 represent the vector valued odd filter (Riesz filter) [16]. Œ±-scale
space derivative quadrature filters (ASSD) are used for band-pass filtering
due to their superior edge detection [17]. The LwP A(x, y) image is calculated
using:
P

sc CXRM 1 (x, y)
).
P
2
2
sc CXRM 1 (x, y) +
sc CXRM 2 (x, y)

LwP A(x, y) = arctan( pP

We do not employ noise compensation during the calculation of the LwP A(x, y)
image in order to preserve the important structural details of CXR(x, y). The
LP E(x, y) image is obtained by averaging the phase sum of the response vectors over many scales using:
LP E(x, y) = {

P

sc

|CXRM 1 (x, y)| ‚àí

p

2 (x, y) + CXR2 (x, y)} √ó LwP A(x, y).
CXRM
M3
2

In the above equation sc represents the number of scales. LP E(x, y) image
extracts the underlying tissue characteristics by accumulating the local energy
of the image along several filter responses. The LP E(x, y) image is used in
order to extract the third local phase image ELEA(x, y). This is achieved by
using LP E(x, y) image feature as an input to an L1 norm based contextual
regularization method. The image model, denoted as CXR image transmission
map (CXRA (x, y)), enhances the visibility of lung tissue features inside a local
region and assures that the mean intensity of the local region is less than
the echogenicity of the lung tissue. The scattering and attenuation effects in
the tissue are combined as: LP E(x, y) = CXRA (x, y) √ó ELEA(x, y) + (1 ‚àí
CXRA (x, y))œÅ. Here œÅ is a constant value representative of echogenicity in the
tissue. In order to calculate ELEA(x, y), CXRA (x, y) is estimated first by
minimizing the following objective function [15]:
X
Œª
k CXRA (x, y) ‚àí LP E(x, y) k22 +
k Wj ‚ó¶ (Dj ‚àó CXRA (x, y)) k1 .
2
j‚ààœá
In the above equation ‚ó¶ represents element-wise multiplication, œá is an index
set, and ‚àó is convolution operator. Dj is calculated using a bank of high order differential filters [18]. The filter bank enhances the CXR tissue features
inside a local region while attenuating the image noise. Wj is a weighting matrix calculated using: Wj (x, y) = exp(‚àí | Dj (x, y) ‚àó LP E(x, y) |2 ). In above

6

Xiao Qi et al.

Fig. 2: Local phase enhancement of CXR(x, y) images.

equation the first part measures the dependence of CXRA (x, y) on LP E(x, y)
and the second part models the contextual constraints of CXRA (x, y) [15].
These two terms are balanced using a regularization parameter Œª [15]. After
estimating CXRA (x, y), ELEA(x, y) image is obtained using: ELEA(x, y) =
[(LP E(x, y)‚àíœÅ)/[max(CXRA (x, y), )]Œ¥ ]+œÅ. Œ¥ is related to tissue attenuation
coefficient (Œ∑) and  is a small constant used to avoid division by zero [15].
Combination of these three types of local phase images as three-channel input
creates a new multi-feature image, denoted as M F (x, y). Qualitative results
corresponding to the enhanced local phase images are displayed in Figure
2. Investigating Figure 2 we can observe that the enhanced local phase images extract new lung features that are not visible in the original CXR(x, y)
images. Since local phase image processing is intensity invariant, the enhancement results will not be affected from the intensity variations due to patient
characteristics or X-ray machine acquisition settings. The multi-feature image M F (x, y) and the original CXR(x, y) image are used as an input to our
proposed deep learning architecture which is explained in the next section.

2.2 Network Architecture
Our proposed multi-feature CNN architecture consists of two same convolutional network streams for processing CXR(x, y) images and the corresponding M F (x, y) respectively. Strategies for the optimal fusion of features
from multi-modal images is an active area of research. Generally, data is
fused earlier when the image features are correlated, and later when they

Title Suppressed Due to Excessive Length

7

Fig. 3: Our proposed multi-feature mid-level (left) and late-level (right) fusion
architectures.

are less correlated [19]. Depending on the dataset, different types of fusion
strategies outperform the other [20]. In [21], our group has also investigated
early, mid, and late-level fusion operations in the context of bone segmentation from ultrasound data. Late-fusion operation has outperformed the other
fusion operations. In [22], authors have also used late-fusion network, for segmenting brain tumors from MRI data, has outperformed other fusion operations. During this work we design mid-fusion and late-fusion architectures
(Fig.3). As part of this work we have also investigate several fusion operations: sum fusion, max fusion, averaging fusion, concatenation fusion, convolution fusion. Based on the performance of the fusion operations and fusion
architectures, on a preliminary experiment, we use concatenation fusion operation for both of our architectures. We use the following network architectures
as the encoder network: Pretrained AlexNet [23], ResNet50 [24], SonoNet64
[25], XNet(Xception)[26], InceptionV4(Inception-Resnet-V2)[27] and EfficientNetB4 [28]. Pretrained AlexNet [23] and ResNet50 [24] have been incorporated into various medical image analysis tasks [29]. SonoNet64 achieved excellent performance in implementation of both classification and localization
tasks [25]. XNet(Xception)[26], InceptionV4 (Inception-Resnet-V2)[27] and EfficientNetB4 [28] were chosen due to their outstanding performance on recent
medical data classification tasks as well as classification of COVID-19 from
chest CT data [30, 31].

2.3 Dataset
We use the following datasets to evaluate the performance of proposed fusion
network models: BIMCV [32], COVIDx [7], and COVID-CXNet [12]. COVID19 CXR scans from BIMCV[32] and COVIDx [7] datasets were combined

8

Xiao Qi et al.

Table 1: Data distribution of the evaluation dataset

# images
# subjects

Normal

Pneumonia

8851
8851

6045
6031

COVID-19[7]
(COVIDx)
400
301

COVID-19[32]
(BIMCV)
2167
1183

COVID-19
(Merged)
2567
1484

Table 2: Distribution of 5-fold cross validation dataset split for training, validation, and testing for COVID-19 data only. Same split was also performed
for Normal and Pneumonia datasets.
Training data
Validation data
Test data

# images
# subjects
# images
# subjects
# images
# subjects

k1
1555
890
494
297
518
297

k2
1560
890
504
297
503
297

k3
1541
890
512
297
514
297

k4
1547
890
511
297
509
297

k5
1529
891
515
297
523
296

Table 3: Data distribution of Test Dataset-2.
# images
# subjects

Normal
6284
6284

Pneumonia
3478
3464

COVID-19 (COVID-CXNet) [12]
756
Unknown

to generate the ‚ÄôEvaluation Dataset‚Äô (Table 1). For Normal and Pneumonia
datasets we have randomly selected a subset of 2567 images (from 2567 subjects) from the evaluation dataset (Table 1). In total 2567 images from each
class (Normal, Pneumonia, COVID-19) were used during 5-fold cross validation. Table 2 shows the data split for COVID-19 data only. Similar split was
also performed for Normal and Pneumonia datasets. In order to provide additional testing for our proposed networks, we have designed a new test dataset
which we call ‚ÄôTest Dataset-2‚Äô (Table 3). The images from Normal and Pneumonia cases which were not included in the ‚ÄôEvaluation Dataset‚Äô were part of
the ‚ÄôTest Dataset-2‚Äô. Furthermore, we have included all the COVID-19 scans
from COVID-CXNet [12].
In order to show the improvements achieved using our proposed multifeature CNN architecture we also trained the same CNN architectures using
only M F (x, y) or CXR(x, y) images. We refer to these architectures as monofeature CNNs. Quantitative performance was evaluated by calculating average
accuracy, precision, recall, and F1-scores for each class [9, 7].

3 Results
The experiments were implemented in Python using Pytorch framework. All
models were trained using stochastic gradient descent (SGD) optimizer, crossentropy loss function, learning rate 0.001 for the first epoch and a learning rate

Title Suppressed Due to Excessive Length

9

Fig. 4: Grad-CAM images [33] obtained by late fusion ResNet50 architecture.
decay of 0.1 every 15 epochs with a mini-batches of size 16. For local phase
image enhancement, we have used sc = 2 and the rest of the ASSD filter
parameters were kept same as reported in [15]. For calculating ELEA(x, y)
images we used Œª = 2,  = 0.0001, Œ∑ = 0.85, and œÅ, the constant related
to tissue echogenicity, was chosen as the mean intensity value of LP E(x, y).
These values were determined empirically and kept constant during qualitative
and quantitative analysis.
Qualitative analysis: Gradient-weighted Class Activation Mapping (Grad-CAM)
[33] visualization of normal, pneumonia, and COVID-19 are presented as qualitative results in Figure 4. Investigating Figure 4 we can see the discriminative
regions of interest localized in the normal, pneumonia, and COVID-19 data.
Quantitative analysis of Evaluation Dataset: Table 4 shows average accuracy
of the 5-fold cross validation on the ‚ÄôEvaluation Dataset‚Äô for mono-feature
CNN architectures as well as the proposed multi-feature CNN architectures.
A Box and Whisker plot is presented in Figure 5. In most of the investigated network designs M F (x, y)-based mono-feature CNN architectures outperform CXR(x, y)-based mono-feature CNN architectures. The best average accuracy is obtained when using our proposed multi-feature ResNet50
[24] architecture. All multi-feature CNNs with mid- and late-fusion operation compared with mono-feature CNNs, with original CXR(x, y) images
as input, achieved statistically significant difference in terms of classification accuracy (p<0.05 using a paired t-test at %5 significance level). Except SonoNet64 [25], XNet(Xception)[26], and InceptionV4(Inception-ResnetV2)[27], all multi-feature CNNs with mid-fusion operation compared with
mono-feature CNNs with M F (x, y) images as input show statistically significant difference in terms of classification accuracy (p<0.05 using a paired t-test
at %5 significance level). We did not find any statistical significant difference

10

Xiao Qi et al.

Table 4: Mean overall accuracy after 5-fold cross validation on ‚ÄôEvaluation
Data‚Äô using mono-feature CNNs and multi-feature CNNs. Bold denotes the
best results obtained.
CXR(x,y)
MF(x,y)
Middle Fusion
Late Fusion
CXR(x,y)
MF(x,y)
Middle Fusion
Late Fusion

AlexNet
91.9¬± 0.55
93.51¬± 0.39
94.27¬±0.64
94.32¬±0.27
Xception
93.38¬±0.38
93.83¬±0.47
94.47¬±0.76
94.95¬±0.52

ResNet50
94.58¬± 0.43
94.82¬±0.58
95.44¬±0.28
95.57¬±0.3
InceptionV4
93.43¬±0.31
94.17¬±0.59
94.89¬±0.36
94.90¬±0.46

SonoNet64
93.59¬±0.7
94.70¬±0.4
95.30¬±0.42
95.35¬±0.4
EfficientNetB4
93.47¬±0.62
94.19¬±0.45
95.26¬±0.61
95.26¬±0.43

Fig. 5: A quantitative results showing classification accuracy of different models for Evaluation Dataset
in the average accuracy results between the middle-level and late-fusion networks (p>0.05 using a paired t-test at %5 significance level). Figure 6 presents
confusion matrix results together with average precision, recall, and F1-scores
for all multi-feature late-fusion CNN architectures. One important aspect observed from the presented results we can see that almost all the investigated
multi-feature networks achieved very high precision, recall, and F1-scores for
COVID-19 data indicating very few cases were misclassified as COVID-19 from
other infected types.
Quantitative analysis of Test Dataset-2: Multi-feature ResNet50 provides the
highest overall accuracy shown in Table 5, which is consistent with the quantitative result achieved with the ‚ÄôEvaluation Dataset‚Äô. Figure 7 shows a Box
and Whisker plot for each network. All multi-feature CNNs with late-fusion
operation compared with mono-feature CNNs, with original CXR(x, y) im-

Title Suppressed Due to Excessive Length

11

Fig. 6: Confusion matrix, and average precision, recall and F1-scores obtained
from 5-fold cross validation on ‚ÄôEvaluation Data‚Äô using all multi-feature network models.

ages as input, achieved statistically significant difference in terms of classification accuracy (p<0.05 using a paired t-test at %5 significance level). Except
XNet(Xception)[26], all the multi-feature CNNs with mid fusion operation
compared with mono-feature CNNs with original CXR(x, y) images as input
achived statistically significant difference in terms of classification accuracy
(p<0.05 using a paired t-test at %5 significance level). Except XNet(Xception)[26],
all multi-feature CNNs with mid-fusion operation compared with mono-feature
CNNs with M F (x, y) images as input show statistically significant difference
in terms of classification accuracy (p<0.05 using a paired t-test at %5 significance level). Similar to ‚ÄôEvaluation Dataset‚Äô results, there was no statistically
significant difference in the average accuracy results between the middle-level
and late-fusion networks (p>0.05 using a paired t-test at %5 significance level)
except ResNet50[24], and XNet(Xception)[26] architectures. Confusion matrix results, together with average precision recall and F1-score values, for all
multi-feature late-fusion CNN architectures evaluated are presented in Figure8. Similar to the results presented for ‚ÄôEvaluation Dataset‚Äô, high precision,
recall, and F1-score values are obtained for the COVID-19 data.

12

Xiao Qi et al.

Fig. 7: A quantitative results showing classification accuracy of different models for Test Dataset-2

Fig. 8: Confusion matrix, and average precision, recall and F1-scores obtained
from 5-fold cross validation on ‚ÄôTest Dataset-2‚Äô using all multi-feature network
models.

Title Suppressed Due to Excessive Length

13

Table 5: Mean overall accuracy after 5-fold cross validation on ‚ÄôTest Dataset2‚Äô using mono-feature CNNs and multi-feature CNNs. Bold denotes the best
results obtained.
CXR(x,y)
MF(x,y)
Middle Fusion
Late Fusion
CXR(x,y)
MF(x,y)
Middle Fusion
Late Fusion

AlexNet
90.59¬±0.21
91.97¬± 0.24
92.52¬±0.32
92.72¬±0.17
Xception
92.28¬±0.46
92.61¬±0.19
92.89¬±0.12
93.77¬±0.15

ResNet50
93.4¬±0.17
93.17¬±0.3
94.26¬±0.19
94.44¬±0.2
InceptionV4
92.99¬±0.2
92.89¬±0.27
93.8¬±0.27
94.01¬±0.09

SonoNet64
91.1¬±0.8
93.46¬±0.15
93.94¬±0.13
94.02¬±0.14
EfficientNetB4
92.16¬±0.49
93.1¬±0.17
93.54¬±0.29
93.91¬±0.07

4 Discussion and Conclusion
Development of a new computer aided diagnostic methods for robust and accurate diagnosis of COVID-19 disease from CXR scans is important for improved
management of this pandemic. In order to provide a solution to this need, in
this work, we present a multi-feature deep learning model for classification of
CXR images into three classes including COVID-19, pneumonia,and normal
healthy subjects. Our work was motivated by the need for enhanced representation of CXR images for achieving improved diagnostic accuracy. To this
end we proposed a local phase-based CXR image enhancement method. We
have shown that by using the enhanced CXR data, denoted as M F (x, y), in
conjunction with the original CXR data, diagnostic accuracy of CNN architectures can be improved. Our proposed multi-feature CNN architectures were
trained on a large dataset in terms of the number of COVID-19 CXR scans
and have achieved improved classification accuracy across all classes. One of
the very encouraging result is the proposed models show high precision, recall,
and F1-scores on the COVID-19 class for both testing datasets. In addition,
except for AlexNet[23], all multi-feature CNNs with late fusion operation has
less number of parameters compared with corresponding multi-feature CNNs
with middle fusion operation (Figure 9). Since the image classifier of AlexNet
[23] is consist of three fully connected layers (fc), which store majority of parameters, AlexNet [23] with late fusion operation almost double the number of
parameters compared with middle fusion operation. The rest of networks have
only one or no fc layer in the image classifiers. Finally, compared to previously
reported results, our work achieves the highest three class classification accuracy on a significantly larger COVID-19 dataset (Table 6). This will ensure
few false positive cases for the COVID-19 detected from CXR images and will
help alleviate burden on the healthcare system by reducing the amount of CT
scans performed. While the obtained results are very promising, more evaluation studies are required specifically for diagnosing early stage COVID-19
from CXR images. Our future work will involve the collection of CXR scans

14

Xiao Qi et al.

Fig. 9: Model Size vs. Overall Accuracy

from early stage or asymptotic COVID-19 patients. We will also investigate
the design of a CXR-based patient triaging system.
Acknowledgements The authors are thankful to all the research groups, and national
agencies worldwide who provided the open source X-ray images.

Compliance with ethical standards
Funding: Nothing to declare.
Ethical approval The article uses open source datasets.
Conflict of interest The authors declare that they have no conflict of interest.

References
1. Singhal, T.: A review of coronavirus disease-2019 (covid-19). The Indian Journal of
Pediatrics pp. 1‚Äì6 (2020)
2. Zu, Z.Y., Jiang, M.D., Xu, P.P., Chen, W., Ni, Q.Q., Lu, G.M., Zhang, L.J.: Coronavirus
disease 2019 (covid-19): a perspective from china. Radiology p. 200490 (2020)
3. Dong, E., Du, H., Gardner, L.: An interactive web-based dashboard to track covid-19
in real time. The Lancet infectious diseases 20(5), 533‚Äì534 (2020)
4. Wang, W., Xu, Y., Gao, R., Lu, R., Han, K., Wu, G., Tan, W.: Detection of sars-cov-2
in different types of clinical specimens. Jama 323(18), 1843‚Äì1844 (2020)
5. Bleve, G., Rizzotti, L., Dellaglio, F., Torriani, S.: Development of reverse transcription
(rt)-pcr and real-time rt-pcr assays for rapid detection and quantification of viable
yeasts and molds contaminating yogurts and pasteurized food products. Applied and
Environmental Microbiology 69(7), 4116‚Äì4122 (2003)
6. Ozturk, T., Talo, M., Yildirim, E.A., Baloglu, U.B., Yildirim, O., Acharya, U.R.: Automated detection of covid-19 cases using deep neural networks with x-ray images.
Computers in Biology and Medicine p. 103792 (2020)
7. Wang, L., Wong, A.: Covid-net: A tailored deep convolutional neural network design for
detection of covid-19 cases from chest x-ray images. arXiv preprint arXiv:2003.09871
(2020)

Title Suppressed Due to Excessive Length

15

Table 6: Comparison of proposed method with recent state-of-the-art methods
for COVID-19 detection using CXR images
Study

Method

Wang et al. [4]

COVID-Net

Ozturk et al. [6]

DarkCovidNet

Haghanifar et al. [12]

UNet+DenseNet

Siddhartha and
Santra [10]

COVIDLite

Apostolopoulos and
Mpesiana [13]

VGG19

Proposed Method

Fus-ResNet50

Dataset
Training data:
Testing data:
7966 Normal
100 Normal
5438 Pneumonia
100 Pneumonia
258 COVID-19
100 COVID-19
500 Normal
500 Pneumonia
127 COVID-19
Training data:
Testing data:
3000 Normal
724 Normal
3400 Pneumonia
672 Pneumonia
400 COVID-19
144 COVID-19
668 Normal
619 Viral Pneumonia
536 COVID-19
Testing data 1:
Testing data 2:
504 Normal
504 Normal
700 Bacterial
714 Viral &
Pneumonia
Bacterial Pneumonia
224 COVID-19
224 COVID-19
Testing data 1:
Testing data 2:
2567 Normal
6284 Normal
2567 Pneumonia
3478 Pneumonia
2567 COVID-19
756 COVID-19

Acc (%)

8. Farooq, M., Hafeez, A.: Covid-resnet: A deep learning framework for screening of covid19
from radiographs. arXiv preprint arXiv:2003.14395 (2020)
9. Ucar, F., Korkmaz, D.: Covidiagnosis-net: Deep bayes-squeezenet based diagnostic of
the coronavirus disease 2019 (covid-19) from x-ray images. Medical Hypotheses p.
109761 (2020)
10. Siddhartha, M., Santra, A.: Covidlite: A depth-wise separable deep neural network with
white balance and clahe for detection of covid-19. arXiv preprint arXiv:2006.13873
(2020)
11. Gour, M., Jain, S.: Stacked convolutional neural network for diagnosis of covid-19 disease
from x-ray images. arXiv preprint arXiv:2006.13817 (2020)
12. Haghanifar, A., Majdabadi, M.M., Ko, S.: Covid-cxnet: Detecting covid-19 in frontal
chest x-ray images using deep learning (2020). URL https://github.com/armiro/
COVID-CXNet
13. Apostolopoulos, I.D., Mpesiana, T.A.: Covid-19: automatic detection from x-ray images
utilizing transfer learning with convolutional neural networks. Physical and Engineering
Sciences in Medicine p. 1 (2020)
14. GonzaÃÅlez, G., Bustos, A., Salinas, J.M., de la Iglesia-Vaya, M., Galant, J., CanoEspinosa, C., Barber, X., Orozco-BeltraÃÅn, D., Cazorla, M., Pertusa, A.: Umls-chestnet:
A deep convolutional neural network for radiological findings, differential diagnoses and
localizations of covid-19 in chest x-rays. arXiv preprint arXiv:2006.05274 (2020)
15. Hacihaliloglu, I.: Localization of bone surfaces from ultrasound data using local phase
information and signal transmission maps. In: International Workshop and Challenge

93.3

87.02

87.21

96.43

93.48 &
94.72

95.57 &
94.44

16

16.
17.
18.
19.
20.
21.

22.
23.

24.

25.

26.

27.
28.
29.

30.

31.

32.

33.

Xiao Qi et al.
on Computational Methods and Clinical Applications in Musculoskeletal Imaging, pp.
1‚Äì11. Springer (2017)
Felsberg, M., Sommer, G.: The monogenic signal. IEEE Transactions on Signal Processing 49(12), 3136‚Äì3144 (2001)
Belaid, A., Boukerroui, D.: Œ± scale spaces filters for phase based edge detection in
ultrasound images. In: 2014 IEEE ISBI, pp. 1247‚Äì1250. IEEE (2014)
Meng, G., Wang, Y., Duan, J., Xiang, S., Pan, C.: Efficient image dehazing with boundary constraint and contextual regularization. In: IEEE ICCV, pp. 617‚Äì624 (2013)
Ngiam, J., Khosla, A., Kim, M., Nam, J., Lee, H., Ng, A.Y.: Multimodal deep learning.
In: ICML (2011)
Zhou, T., Ruan, S., Canu, S.: A review: Deep learning for medical image segmentation
using multi-modality fusion. Array 3, 100004 (2019)
Alsinan, A.Z., Patel, V.M., Hacihaliloglu, I.: Automatic segmentation of bone surfaces
from ultrasound using a filter-layer-guided cnn. International journal of computer assisted radiology and surgery 14(5), 775‚Äì783 (2019)
AyguÃàn, M., SÃßahin, Y.H., UÃànal, G.: Multi modal convolutional neural networks for brain
tumor segmentation. arXiv preprint arXiv:1809.06191 (2018)
Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep convolutional neural networks. In: Advances in neural information processing systems, pp.
1097‚Äì1105 (2012)
He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In:
Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770‚Äì778 (2016)
Baumgartner, C.F., Kamnitsas, K., Matthew, J., Fletcher, T.P., Smith, S., Koch, L.M.,
Kainz, B., Rueckert, D.: Sononet: real-time detection and localisation of fetal standard
scan planes in freehand ultrasound. IEEE transactions on medical imaging 36(11),
2204‚Äì2215 (2017)
Chollet, F.: Xception: Deep learning with depthwise separable convolutions. In: Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1251‚Äì1258
(2017)
Szegedy, C., Ioffe, S., Vanhoucke, V., Alemi, A.: Inception-v4, inception-resnet and the
impact of residual connections on learning (2016)
Tan, M., Le, Q.V.: Efficientnet: Rethinking model scaling for convolutional neural networks. arXiv preprint arXiv:1905.11946 (2019)
Litjens, G., Kooi, T., Bejnordi, B.E., Setio, A.A.A., Ciompi, F., Ghafoorian, M., Van
Der Laak, J.A., Van Ginneken, B., SaÃÅnchez, C.I.: A survey on deep learning in medical
image analysis. Medical image analysis 42, 60‚Äì88 (2017)
Ha, Q., Liu, B., Liu, F.: Identifying melanoma images using efficientnet ensemble:
Winning solution to the siim-isic melanoma classification challenge. arXiv preprint
arXiv:2010.05351 (2020)
Kassani, S.H., Kassasni, P.H., Wesolowski, M.J., Schneider, K.A., Deters, R.: Automatic
detection of coronavirus disease (covid-19) in x-ray and ct images: A machine learningbased approach. arXiv preprint arXiv:2004.10641 (2020)
de la Iglesia VayaÃÅ, M., Saborit, J.M., Montell, J.A., Pertusa, A., Bustos, A., Cazorla, M.,
Galant, J., Barber, X., Orozco-BeltraÃÅn, D., Garcƒ±ÃÅa-Garcƒ±ÃÅa, F., CaparroÃÅs, M., GonzaÃÅlez,
G., Salinas, J.M.: Bimcv covid-19+: a large annotated dataset of rx and ct images from
covid-19 patients. arXiv preprint arXiv:2006.01174 (2020)
Selvaraju, R.R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., Batra, D.: Grad-cam:
Visual explanations from deep networks via gradient-based localization. International
Journal of Computer Vision 128(2), 336‚Äì359 (2019). DOI 10.1007/s11263-019-01228-7.
URL http://dx.doi.org/10.1007/s11263-019-01228-7

