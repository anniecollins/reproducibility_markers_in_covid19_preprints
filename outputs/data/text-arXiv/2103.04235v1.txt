GRAPH-BASED PYRAMID GLOBAL CONTEXT REASONING WITH A SALIENCYAWARE PROJECTION FOR COVID-19 LUNG INFECTIONS SEGMENTATION
Huimin Huang 1, *Ming Cai1, *Lanfen Lin1, Jing Zheng2, Xiongwei Mao2, Xiaohan Qian2, Zhiyi Peng2,
*Jianying Zhou2, Yutaro Iwamoto3, Xian-Hua Han3, *Yen-Wei Chen3,4,1, Ruofeng Tong1
1

College of Computer Science and Technology, Zhejiang University, China
Collegue of Medicine, The First Affiliated Hospital, Zhejiang University, China
3
College of Information Science and Engineering, Ritsumeikan University, Japan
4
Research Center for Healthcare Data Science, Zhejiang Lab, Hangzhou, China
2

ABSTRACT
Coronavirus Disease 2019 (COVID-19) has rapidly spread in
2020, emerging a mass of studies for lung infection segmentation from CT images. Though many methods have been
proposed for this issue, it is a challenging task because of infections of various size appearing in different lobe zones. To
tackle these issues, we propose a Graph-based Pyramid
Global Context Reasoning (Graph-PGCR) module, which is
capable of modeling long-range dependencies among disjoint
infections as well as adapt size variation. We first incorporate
graph convolution to exploit long-term contextual information from multiple lobe zones. Different from previous average pooling or maximum object probability, we propose a
saliency-aware projection mechanism to pick up infection-related pixels as a set of graph nodes. After graph reasoning,
the relation-aware features are reversed back to the original
coordinate space for the down-stream tasks. We further construct multiple graphs with different sampling rates to handle
the size variation problem. To this end, distinct multi-scale
long-range contextual patterns can be captured. Our GraphPGCR module is plug-and-play, which can be integrated into
any architecture to improve its performance. Experiments
demonstrated that the proposed method consistently boost the
performance of state-of-the-art backbone architectures on
both of public and our private COVID-19 datasets.
Index Termsâ€”COVID-19, Lung infections segmentation, Graph convolution, Multi-scale.
1. INTRODUCTION
The break of Coronavirus Disease 2019 (COVID-19) has rapidly spread over the world, which has been declared as a
global pandemic [1]. Accurate lung infections segmentation
is one of the most important pre-processing steps for assessment and quantification of COVID-19 [2-5]. The classic
UNet [6] and UNet++ [7] were widely performed as segmentation architectures for COVID-19. Recently, UNet-Inf [8]
with a parallel partial decoder was proposed to segment lung
infections. Despite achieving good results, these approaches
are still incapable of exploring sufficient information from
multifocal infections, appearing in different lobe zones [9,10].
It may hinder the infections segmentation performance, especially considering each pixel in isolation, as local information
is noisy and ambiguous. It is also noteworthy that infections
with various size occur in different scales. To tackle these two
challenges, it is imperative to perform multi-scale long-term

interactions on COVID-19 CT images, which contributes to
model long-term dependencies among multiple lesions.
Recently, graph convolution [11] has been incorporated
into computer vision tasks for globally reasoning, which can
be generally summarized as two kinds of approaches: feature
space graph convolution and coordinate space graph convolution. The feature space graph convolution captures interdependencies along the channel dimensions of the feature map,
which projects the feature into a non-coordinate space [1215]; whistle coordinate space graph convolution explicitly
models the spatial relationships between pixels [16-20],
which projects the feature into a new coordinate space, to produce coherent prediction between the disjoint infections.
In this paper, we propose a saliency-aware projectionbased Graph-based Pyramid Global Context Reasoning
(Graph-PGCR) module for COVID-19 lung infections segmentation. Different from the existing work that the infection-related pixels were highlighted via average pooling [16]
or maximum object probability [17], we propose a saliencyaware projection (SAP) to keep eye on â€˜whereâ€™ is an informative part, and thus selects discriminative pixels to form a
fully-connected graph. In addition, we further take the multiscale cues into consideration to address the challenge that different infections appear in various scales. Inspired by the Pyramid Pooling Module [21], we build a pyramid global context
reasoning architecture to harvest multi-scale representations
via SAP with various sampling rates. Hence, a coarser graph
is constructed with lower sampling rate, providing more
global dependencies for the larger receptive scale; while a
finer graph is modeled with higher sampling rate, embedding
more explicit long-range context for the smaller receptive
field. In this way, we can perform graph reasoning on each
scale and aggregate local and global clues to make the final
prediction more reliable.
Our Graph-PGCR module is plug-and-play and thus can be
integrated into a wide variety of existing network architectures to further enhance their performance. In summary, the
main contributions of this research are four-fold: (i) We propose a Graph-PGCR module to model long-range dependencies among disjoint infections as well as adapt size variation;
(ii) We propose a SAP mechanism to select the infection-related pixels as a set of graph nodes, where global contextual

Fig. 1. Overview of the Graph-based Pyramid Global Context Reasoning (Graph-PGCR) module in UNet architecture.
information can be propagated via graph convolution; (iii)
We construct multiple graphs to harvest multi-scale contextual patterns from infections with various size; (iv) We conduct extensive experiments on public and private COVID-19
dataset, where our method yields consistent improvements
over a number of baselines.
2. METHODS
Fig.1 illustrates an overview of the proposed Graph-PGCR
module in the segmentation architecture (e.g. UNet). Given
an input image, we first extract features via the UNet-Encoder,
and then our Graph-PGCR module is integrated to capture
multi-scale long-range representations. Benefitting from the
saliency-aware projection, the input feature map ğ‘‹ is firstly
sampled into ğ¾ (e.g. ğ¾ = 3) parallel pyramid levels with various scales. After individual graph convolution, the reprojection via upsampling and the multi-scale fusion with concatenation layers are performed to generate the feature representation ğ‘‹, which is finally fed into the UNet-decoder for prediction. It is worth noting that the input feature map ğ‘‹ can be
extracted from any layer of deep convolutional model. In the
following subsections, we introduce the detail of each component in the Graph-PGCR module.
2.1. Saliency-aware Projection in Coordinate Space
In order to project infection-related pixels in coordinate space
into a set of graph nodes in a new coordinate space, we proposed a saliency-aware projection mechanism, which integrates attention mechanism with pooling operation. Specifically, the attention mechanism aims at learning where to emphasize or suppress; while the pooling operation desires of
picking out discriminative pixels.
Before projection, we need to reduce the dimension of feature map ğ‘‹, which enhances the capacity of the projection.
Inspired by the dual attention network [22], we implement
the channel attention module to capture the channel dependencies between any two channel maps via self-attention
mechanism. After enhancing the feature representation, we
adopt a 1Ã—1 convolution layer to reduce the feature dimension from ğ¶ to ğ‘†. Similarly, the feature map also enhanced
by spatial attention module to model the spatial dependencies

between any two positions. Benefiting from the channel and
spatial attention modules, we could emphasize interdependent feature maps and improve the feature representation of
specific semantics.
Considering that pooling along the channel dimension can
effectively highlight informative regions [23], we further perform the max-pooling and average-pooling operations on the
channel axis and then concatenate them to generate an attention map ğ‘ âˆˆ â„,Ã—- which focus on the salient pixels related
to infections and surpass unnecessary ones. As vividly shown
in Fig.2, the attention map ğ‘ with spatial size of ğ»Ã—ğ‘Š is divided into several non-overlapping sub-regions with the
stride of ğ›¿ pixels. Within each region, the pixel ğ‘ with maximum localization probability: ğ‘ = arg max89 ğ‘(ğ‘; ) is selected as a node. This process results in a set of nodes ğ’© =
ğ’©
{ğ‘›@ }@BC
for the feature map ğ‘‹. ğ’© equals to ğ»/ğ›¿ Ã— ğ‘Š/ğ›¿
and represents the number of nodes. âˆ™ is the ceiling operation, which gives the smallest integer equal or larger than its
input. Note that the process can be considered as a sampling
process and 1/ğ›¿ can be considered as a sampling rate. In
view of this, each node ğ‘›@ is represented by its corresponding
image coordinates. It is worth noting that the spatial interval
between nodes can be controlled by adjusting ğ›¿. The coarser
graph is constructed with lager ğ›¿ values, which perhaps captures longer-range interactions among nodes. In contrast, all
pixels are assigned as individual nodes in the extreme case,
where ğ›¿ = 1. The initial feature representation of each ğ‘›@ is
extracted from the feature map ğ‘‹ enhanced in both channel
and spatial dimensions. This results in a set of node features,
ğ’µ âˆˆ â„ GÃ— ğ’© , where ğ‘† equals to the feature dimension of ğ‘‹.
2.2. Multi-Scale Reasoning with Graph Convolution
In spite of graph reasoning exploring the global context, the
long-term context pattern differs in multiple scales of the
same image. Specifically speaking, the finer representation
with smaller receptive field (smaller ğ›¿) embeds more explicit
context; while the coarser representation with larger receptive
scale (larger ğ›¿ ) explores global dependencies. Taking the
multi-scale schema into consideration, we incorporate it with
graph reasoning to extend the long-range contextual patterns,
and thus devise the Graph-PGCR module.

Fig. 2. An illustration of the proposed saliency-aware projection (SAP) mechanism. In this example, the attention map
ğ‘ âˆˆ â„HÃ—H is sampled with the stride ğ›¿ = 2. We then find the
pixel ğ‘ with maximum localization probability in each region
(shown in different colors), which is selected as the node.
As seen in Fig.1, the graph convolution begins with subsampling the convolved features into ğ¾ parallel pyramid levels with various scales via saliency-aware projection. Higher
sampling rate (smaller ğ›¿ ) generates finer representations;
while the coarser features are extracted from lower sampling
rate (larger ğ›¿). After selecting infection-related pixels as a set
of nodes, a lightweight fully-connected graph with adjacency
matrix ğ’œK âˆˆ â„ ğ’©L Ã— ğ’©L is generated from ğ‘˜ -th separate
branch for propagating information across nodes, as depicted
in Fig.3. The adjacency matrix ğ’œK is defined as the similarity
between nodes, where the more similar feature representations of two nodes, the stronger connectivity between them.
It can be formulated as:
ğ’œK = ğœŒK ğ’µK O â¨‚ ğœ‘K ğ’µK
(1)
where ğœŒK (âˆ™) and ğœ‘K (âˆ™) are two learnable 1-dimensional linear transformations along node-wise dimension, â¨‚ is the matrix multiplication. We further apply a softmax layer to yield
a normalized adjacency matrix ğ’œK . Then we conduct the
graph convolution [11] in our model as:
ğ’µK = ğœ ğ’œK (ğœ‡K ğ’µK )O ğ’²K O
(2)
where ğœ‡K âˆ™ is a learnable linear transformation, ğ’²K âˆˆ â„ GÃ—G
is a trainable weight matrix, ğœ âˆ™ is the ReLU activation function, and ğ’µK is the output feature map after graph convolution
in ğ‘˜-th separate branch.
2.3. Reprojection and Multi-scale Fusion
To provide complementary feature for the down-stream task,
the last step is to map the relation-aware features (â„ GÃ— ğ’©L )
generated from ğ‘˜-th separate branch back to the coordinate
space (â„UÃ—,Ã—- ), which is compatible with the regular CNN.
To achieve the dimension transformation, we reshape the relation-aware ğ’µK âˆˆ â„ GÃ— ğ’©L into ğ’µK âˆˆ â„ GÃ— ,/VL Ã— -/VL .
Then, a simple but effective upsampling operation is adopted
as the reprojection function. In practice, the bilinear interpolation is performed to resize ğ»/ğ›¿K Ã— ğ‘Š/ğ›¿K to the original
spatial input size ğ»Ã—ğ‘Š. To maintain the original information,
we further utilize a multi-scale fusion to fuse the reshaped
relation-aware features from each scale with the original feature map ğ‘‹ in a learnable way, which carries both local and
global context information. The multi-scale fusion process
can be formulated as:

Fig. 3. An illustration of reasoning with Graph convolution

ğ‘‹=ğ¹

ğ’° ğ’µK

Y
KBC

,ğ‘‹

3

where ğ¹(âˆ™) realizes the feature aggregation mechanism with
a 1Ã—1 convolution followed by a batch normalization and a
ReLU activation function. ğ’°(âˆ™) indicate up-sampling operation, and âˆ™ represents the concatenation. As a result, we
have the feature ğ‘‹ with channel dimension of ğ¶.
3. EXPERIMENTS AND RESULTS
3.1. Datasets and Implementation
The method was evaluated on two datasets: the public and our
private COVID-19 datasets. (i) The public COVID-19 dataset [24]: It contains 20 COVID-19 CT scans from the Coronacases Initiative and Radiopaedia, which were manually
annotated for the left lung, right lung and COVID-19 infection. In the experiment, we trained our models using the 16
volumes with 2-fold cross-validation and average the experiment results as the final performance. (ii) The private
COVID-19 dataset: we collected 102 COVID-19 CT scans
(from the Department of Radiology, The First Affiliated Hospital, College of Medicine, Zhejiang University), which has
passed the ethic approvals. The left lung, right lung, and infection were annotated by two radiologists with 5-year experience in chest radiology. For our study, 82 scans were randomly selected for training and the other 20 scans for testing.
After 2-fold cross-validation, we averaged the experiment results as the final performance.
The input image consisted of three slices: the slice to be
segmented and the upper and lower slices, which was cropped
to 224Ã—224Ã—3. The networks were updated utilizing the stochastic gradient descent, where the learning rate was 1e-3 and
weight decay was 5e-4. To effectively model the global contextual information, our Graph-PGCR module was appended
at the end of the encoder as seen in Fig.1. After feature extraction, the input feature map ğ‘‹ had the size of 1024Ã—14Ã—
14. We simply set node feature dimension ğ‘† = 64 in our implementation. The Dice coefficient was employed as our principal performance metric for each case.
3.1. Ablation Study
This section experiments the effect of key components of the
Graph-PGCR module on the public COVID-19 dataset for
lung infection segmentation. It includes the architecture of
the original UNet as a baseline, Dual Attention (DA) Module
[22], the proposed Graph-PGCR with different projection

Table 2. Comparison with state-of-art methods on both public dataset (ğ·ğ‘–ğ‘ğ‘’ 8b ) and private dataset (ğ·ğ‘–ğ‘ğ‘’ 8c ).

UNet [6]

UNet_Inf [8]

Method
Baseline
DA [22]
GloRe [13]
Graph-PGCR (ğœ¹ = ğŸ)
Graph-PGCR (ğœ¹ = ğŸ, ğŸ’, ğŸ•)
Baseline
DA [22]
GloRe [13]
Graph-PGCR (ğœ¹ = ğŸ)
Graph-PGCR (ğœ¹ = ğŸ, ğŸ’, ğŸ•)

ğ·ğ‘–ğ‘ğ‘’ 8b (%)
77.50
78.11
78.23
80.84
81.38
78.63
79.65
79.16
81.67
82.03

ğ·ğ‘–ğ‘ğ‘’ 8c (%)
76.02
77.06
76.95
77.47
78.92
78.46
79.55
78.96
79.98
80.95

Method

UNet ++ [7]

UNet 3+ [25]

ğ·ğ‘–ğ‘ğ‘’ 8b (%)
79.84
80.57
80.29
81.03
81.95
82.42
83.09
82.22
83.56
85.01

ğ·ğ‘–ğ‘ğ‘’ 8c (%)
77.14
77.98
77.53
78.60
79.88
80.28
81.32
79.98
81.63
82.21

Table 1. Lung infections segmentation performances on public
COVID-19 dataset when gradually adding the proposed com-

ponents to the UNet.
Architecture
Baseline UNet
UNet + DA [22]
UNet + Graph-PGCR (AvgPooling)
UNet + Graph-PGCR (MaxPooling)
UNet + Graph-PGCR (SAP)
UNet + Graph-PGCR (SAP)
UNet + Graph-PGCR (SAP)

ğ¾
1
1
1
2
3

ğ›¿
2
2
2
2,4
2,4,7

Dice(%)
77.50
78.11
79.96
80.02
80.84
81.16
81.38

mechanisms and multi-scales with different ğ›¿. Table 1 shows
the segmentation performances when gradually adding components to the UNet. As seen, each component of the GraphPGCR module contributes to the performance. Generally, a
total improvement of 3.88% was gained by our proposed
Graph-PGCR module compared to baseline UNet.
3.3. Comparison with the State of the Art
A series of UNet based variations are adopted to further exam
the effectiveness of the proposed Graph-PGCR module, including UNet++ [7], UNet-Inf [8] and UNet 3+ [25]. Except
for Dual Attention (DA) module, we further compare our
method with the state-of-the-art graph context reasoning
module (i.e., GloRe) [13]. The hyper-parameters of the graph,
e.g., the number of the nodes and its feature dimensions, are
set based on [13]. It is worth noting that they are appended at
the same place as our proposed module.
(i) Quantitative comparison: Table 2 shows the comparison results on public and private dataset, where we have the
following observations. First, the proposed Graph-PGCR
module (ğ›¿ = 2) improves the performance from the baselines
under different segmentation networks. Moreover, our proposed Graph-PGCR module ( ğ›¿ = 2 ) has superior performance over GloRe module. Additionally, the Graph-PGCR
module (ğ›¿ = 2,4,7) with multiple GCR achieves the best performance in four architectures, obtaining average improvement of 3.0 and 2.5 point between four backbones performed
on two datasets.
(ii) Qualitative comparison: Fig.4 visualizes the segmentation results of different plugin based on UNet 3+ network

Fig. 4. Qualitative comparisons of different units incorporated with
UNet 3+. Purple areas: true positive (TP); Yellow areas: false negative (FN); Green areas: the false positive (FP).

in our private datasets, including DA module, Glore unit, our
proposed Graph-PGCR module (ğ›¿ = 2,4,7). The results illustrated how efficient our proposed Graph-PGCR module is
on segmenting the irregular and even small infections.
Specifically, it generates segmentation results that are close
to the ground truth with much less missegmented infections.
The success of Graph-PGCR module is owed to the ability of
capturing multi-scale long-range dependencies.
4. CONCLUSIONS
In this paper, we develop an effective GCN-based approach,
termed as Graph-based Pyramid Global Context Reasoning
(Graph-PGCR) module, to model the multi-scale long-range
contextual relationships, which is critical for COVID-19 lung
infections segmentation. Benefiting from the saliency-aware
projection that selects infection-related pixels as graph nodes,
a fully-connected graph is constructed where global contextual information is propagated across all nodes via graph convolution. The multi-scale schema is also adopted to explore
distinct contextual patterns from multiple graphs. Experiments show that the proposed Graph-PGCR module can effectively capture global contextual dependencies in COVID19 CT images and consistently improve over four strong
baselines on lung infections segmentation task.

5. REFERENCES

[17] S.Y Shin, et al. "Deep vessel segmentation by learning graphical connectivity." Medical image analysis 58: 101556, 2019.

[1] World Health Organization. "Coronavirus disease 2019
(COVID-19): situation report, 88.", 2020.

[18] G. Te, et al. "Edge-aware Graph Representation Learning and
Reasoning
for
Face
Parsing." arXiv
preprint
arXiv:2007.11240 (2020)

[2] F. Shi, et al. "Review of artificial intelligence techniques in imaging data acquisition, segmentation and diagnosis for covid19." IEEE reviews in biomedical engineering (2020).
[3] G. Wang, et al. "A noise-robust framework for automatic segmentation of covid-19 pneumonia lesions from ct images." IEEE Transactions on Medical Imaging 39.8 (2020):
2653-2663.
[4] L. Zhou, et al. "A rapid, accurate and machine-agnostic segmentation and quantification method for CT-based covid-19 diagnosis." IEEE Transactions on Medical Imaging 39.8 (2020):
2638-2652.
[5] X. Chen, Y. Lina, and Z. Yu. "Residual Attention U-Net for
Automated Multi-Class Segmentation of COVID-19 Chest CT
Images." arXiv preprint arXiv:2004.05645 (2020).
[6] O. Ronneberger, P. Fischer and T. Brox, â€œU-Net: Convolutional
Networks for Biomedical Image Segmentation,â€ Medical Image Computing and Computer-Assisted Intervention, pp.234241, 2015.
[7] Z.W. Zhou, M.M.R. Siddiquee, N. Tajbakhsh and J.M. Liang,
â€œUNet++: A Nested U-Net Architecture for Medical Image
Segmentation,â€ Deep Learning in Medical Image Anylysis and
Multimodal Learning for Clinical Decision Support, pp: 3-11,
2018.
[8] D.P. Fan, et al. â€œInf-Net: Automatic COVID-19 Lung Infection
Segmentation from CT Images.â€ IEEE Transactions on Medical Imaging, 2020.
[9] W. Xie, et al. "Relational modeling for robust and efficient pulmonary lobe segmentation in ct scans." IEEE transactions on
medical imaging , 2020.
[10] National Health Commission. "Diagnosis and treatment protocol for novel coronavirus pneumonia (Trial Version 6)." Chin
Med J (Engl) 133.9: 1087-1095, 2020.
[11] T. N. Kipf and M. Welling. â€œSemi-supervised classiï¬cation
with graph convolutional networks,â€ ICLR, 2017.
[12] Y. Li, and G. Abhinav. "Beyond grids: Learning graph representations for visual recognition." Advances in Neural Information Processing Systems. 2018.
[13] Y. Chen, et al. "Graph-based global reasoning networks." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.
[14] QZ, Li, and KQ, Huang. "Pedestrian attribute recognition by
joint visual-semantic reasoning and knowledge distillation."
(2019).
[15] S. Huang, et al. "Referring Image Segmentation via CrossModal Progressive Comprehension." Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.
[16] L. Zhang, et al. "Dual graph convolutional network for semantic segmentation." arXiv preprint arXiv:1909.06121, 2019.

[19] T. Wu, et al. "GINet: Graph Interaction Network for Scene
Parsing." arXiv preprint arXiv:2009.06160 (2020).
[20] J. Li, et al. "Multiple-human parsing in the wild." arXiv preprint
arXiv:1705.07206 (2017).
[21] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia. â€œPyramid scene
parsing network.â€ In Computer Vision and Pattern Recognition
(CVPR), 2017.
[22] J. Fu, et al. "Dual attention network for scene segmentation." Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition. 2019.
[23] S. Woo, et al. "CBAM: Convolutional block attention module."
Proceedings of the European conference on computer vision
(ECCV). 2018.
[24] J. Ma, Y. Wang, et al. â€œTowards Efficient COVID-19 CT Annotation: A Benchmark for Lung and Infection Segmentation.â€
arXiv preprint arXiv:2004.12537, 2020.
[25] H. Huang, et al. "UNet 3+: A Full-Scale Connected UNet for
Medical Image Segmentation." ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020.

