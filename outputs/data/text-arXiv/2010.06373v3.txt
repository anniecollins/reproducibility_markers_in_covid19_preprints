Generalized Rescaled PoÌlya urn and its statistical applications
Giacomo Aletti

âˆ—

and

Irene Crimaldi

â€ 

March 8, 2021

arXiv:2010.06373v3 [math.ST] 5 Mar 2021

Abstract
We introduce the Generalized Rescaled PoÌlya (GRP) urn, that provides a generative model for a chisquared test of goodness of fit for the long-term probabilities of clustered data, with independence between
clusters and correlation, due to a reinforcement mechanism, inside each cluster. We apply the proposed test
to a data set of Twitter posts about COVID-19 pandemic: in a few words, for a classical chi-squared test
the data result strongly significant for the rejection of the null hypothesis (the daily long-run sentiment rate
remains constant), but, taking into account the correlation among data, the introduced test leads to a different conclusion. Beside the statistical application, we point out that the GRP urn is a simple variant of the
standard Eggenberger-PoÌlya urn, that, with suitable choices of the parameters, shows â€œlocalâ€ reinforcement,
almost sure convergence of the empirical mean to a deterministic limit and different asymptotic behaviours
of the predictive mean. Moreover, the study of this model provides the opportunity to analyze stochastic
approximation dynamics, that are unusual in the related literature.
Keywords: central limit theorem, chi-squared test, PoÌlya urn, preferential attachment, reinforcement learning, reinforced stochastic process, stochastic approximation, urn model.
MSC2010 Classification: 60F05, 60F15, 62F03, Secondary 62F05, 62L20.

Contents
Page
1 Introduction

2

2 The Generalized Rescaled PoÌlya (GRP) urn

3

3 Related literature

5

4 Main theorem: goodness of fit result

6

5 Statistical applications
5.1 Estimation of the parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

7
8

6 COVID-19 epidemic Twitter analysis

10

7 Asymptotic results for the empirical means

13

8 Proof of Theorem 7.2

13

References

17

âˆ— ADAMSS
â€  IMT

Center, UniversitaÌ€ degli Studi di Milano, Milan, Italy, giacomo.aletti@unimi.it
School for Advanced Studies, Lucca, Italy, irene.crimaldi@imtlucca.it

1

Page
SM Supplemental Materials

S1

S1 Proofs and intermediate results
S1
S1.1 Proof of Theorem 4.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . S 1
S1.2 A preliminary central limit theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . S 2
S1.3 Proof of Proposition 7.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . S 2
S2 Case

P

n n

< +âˆ

S3

S3 Computations regarding the local
S3.1 Case Î± = Î² âˆˆ (0, 1) . . . . . . . .
S3.2 Case Î± = Î² = 1 . . . . . . . . . .
S3.3 Case 0 < Î± < Î² < (1 + Î±)/2 . . .

reinforcement
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

S4 Technical results

S6
S7
S7
S8
S9

S5 Some stochastic approximation results

S 10

S6 Stable convergence

S 12

SR References

S 13

1

Introduction

The standard Eggenberger-PoÌlya urn (see EggPol23,mah) has been widely studied and generalized (for
instance, some recent variants can be found in [7, 8, 11, 15, 18, 33, 34, 35]). In its simplest form, this model
with k-colors works as follows. An urn contains N0 i balls of color i, for i = 1, . . . , k, and, at each discrete
time, a ball is extracted from the urn and then it is returned inside the urn together with Î± > 0 additional
balls of the same color. Therefore, if we denote by Nn i the number of balls of color i in the urn at time n,
we have
Nn i = Nnâˆ’1 i + Î±Î¾n i
for n â‰¥ 1,
where Î¾n i = 1 if the extracted ball at time n is of color i, and Î¾n i = 0 otherwise. TheP
parameter Î± regulates
the reinforcement mechanism: the greater Î±, the greater the dependence of Nn i on n
h=1 Î¾h i .
The Generalized Rescaled PoÌlya (GRP) urn model is characterized by the introduction of the sequence
(Î²n )n of parameters, together with the replacement of the parameter Î± of the original model by a sequence
(Î±n )n , so that
Nn i = b0 i + Bn i
with
Bn+1 i = Î²n Bn i + Î±n+1 Î¾n+1 i

n â‰¥ 0.

Therefore, the urn initially contains b0 i +B0 i balls of color i and the parameters Î²n â‰¥ 0, together with Î±n > 0,
regulate the reinforcement mechanism. More precisely, the term Î²n Bn i links Nn+1 i to the â€œconfigurationâ€
at time n through the â€œscalingâ€ parameter Î²n , and the term Î±n+1 Î¾n+1 i links Nn+1 i to the outcome of the
extraction at time n + 1 through the parameter Î±n+1 .
We are going to show that, with a suitable
choice of the model parameters, we have a long-term
almost
P
Pn
sure convergence of the empirical mean N
n=1 Î¾n i /N to the deterministic limit p0 i = b0 i /
i=1 b0 i , and a
chi-squared goodness of fit result for the long-term probabilities {p0 1 , . . . , p0 k }. In particular, regarding the
last point, we have that the chi-squared statistics
Ï‡2 = N

k
k
X
X
(Oi âˆ’ N p0 i )2
(b
p i âˆ’ p 0 i )2
=
,
p
N p0 i
0i
i=1
i=1

(1.1)

PN
where N is the size of the sample, pbi = Oi /N , with Oi =
n=1 Î¾n i the number of observations equal
2
to i in the sample, is asymptotically distributed as Ï‡ (k âˆ’ 1)Î», with Î» > 1, or Ï‡2 (k âˆ’ 1)N 1âˆ’2e Î», where
Î» > 0 may be smaller than 1, but e is always strictly smaller than 1/2. In both cases, the presence of

2

correlation among units mitigates the effect in (1.1) of the sample size N , that multiplies the chi-squared
distance between the observed frequencies and the expected probabilities. This aspect is important for the
statistical applications in the context of a â€œbig sampleâ€, when a small value of the chi-squared distance
might be significant, and hence a correction related to the correlation between observations is desirable (see,
for instance, [9, 12, 14, 26, 27, 31, 38, 42, 43, 45]). More precisely, in the first case, the observed value of
the chi-squared distance has to be compared with the â€œcriticalâ€ value Ï‡21âˆ’Î¸ (k âˆ’ 1)Î»/N , where Ï‡21âˆ’Î¸ (k âˆ’ 1)
denotes the quantile of order 1 âˆ’ Î¸ of the chi-squared distribution Ï‡2 (k âˆ’ 1). In the second case, the critical
value for the chi-squared distance becomes Ï‡21âˆ’Î¸ (k âˆ’1)Î»/N 2e , where, although the constant Î» may be smaller
than 1, the effect of the sample size N is mitigated by the exponent 2e < 1. In other words, for this second
case, the Fisher information given by the sample does not scale with the sample size N , but with rate N 2e .
Hence, since the long-term correlation, collecting more and more data does not provide a linear increment
of the information.
Summing up, the GRP urn provides a theoretical framework for a chi-squared test of goodness of fit for the
long-term probabilities of correlated data, generated according to a reinforcement mechanism. Specifically,
we describe a possible application in the context of clustered data, with independence between clusters and
correlation, due to a reinforcement mechanism, inside each cluster. In particular, we develop a suitable
estimation technique for the fundamental model parameters. We then apply the proposed test to a data set
of Twitter posts about COVID-19 pandemic. Given the null hypothesis that the daily long-run sentiment
rate of the posts is the same for all the considered days (suitably spaced days in the period February 20th April 20th 2020), performing a classical Ï‡2 test, the data result strongly significant for the rejection of the
null hypothesis, while, taking into account the correlation among posts sent in the same day, the proposed
test leads to a different conclusion.
The sequel of the paper is so structured. In Section 2 we set up the notation and we define the GRP
urn. In Section 3 we illustrate its relationships with previous models and we discuss the connections with
related literature. In particular, the object of the present work gives us the opportunity to study Stochastic
Approximation (SA) dynamics, which are infrequent in SA literature and so fill in some theoretical gaps.
In Section 4 we provide the main result of this work, that is the almost sure convergence of the empirical
means to the deterministic limits p0 i and the goodness of fit result for the long-term probabilities p0 i ,
together with comments and examples. In Section 5 we describe a possible statistical application of the
GRP urn and the related results: a chi-squared test of goodness of fit for the long-term probabilities of
clustered data, with independence between clusters and correlation, due to a reinforcement mechanism,
inside each cluster. We apply the proposed test to a data set of Twitter posts about COVID-19 pandemic.
In Section 7 we state two convergence results for the empirical means, which are the basis for the proof of the
main theorem. All the shown theoretical results are analytically proven. The proofs are left to Section S1
in the Supplementary Material [2], except for the proof of Theorem 7.2, which is methodologically new
and emphasizes new techniques of martingale limit theory and so it is illustrated in Section 8. Finally, in
the Supplementary Material we also provide some complements, some technical lemmas and some recalls
about stochastic approximation theory and about stable convergence. When necessary, the references to the
Supplementary Material are preceded by an â€œSâ€, so that (S1.2) will refer to the equation (S1.2) in [2].

2

The Generalized Rescaled PoÌlya (GRP) urn

In all the sequel, we suppose given two sequences of parametersP
(Î±n )nâ‰¥1 , with Î±n > 0 and (Î²nP
)nâ‰¥0 with
Î²n â‰¥ 0. Given a vector x = (x1 , . . . , xk )> âˆˆ Rk , we set |x| = ki=1 |xi | and kxk2 = x> x = ki=1 |xi |2 .
Moreover we denote by 1 and 0 the vectors with all the components equal to 1 and equal to 0, respectively.
The urn initially contains b0 i + B0 i > 0 distinct balls of color i, with i = 1, . . . , k. We set b0 =
(b0 1 , . . . , b0 k )> and B0 = (B0 1 , . . . , B0 k )> . We assume |b0 | > 0 and we set p0 = |bb00 | . At each discrete time
(n+1) â‰¥ 1, a ball is drawn at random from the urn, obtaining the random vector Î¾n+1 = (Î¾n+1 1 , . . . , Î¾n+1 k )>
defined as
(
1 when the extracted ball at time n + 1 is of color i
Î¾n+1 i =
0 otherwise,
and the number of balls in the urn is so updated:
Nn+1 = b0 + Bn+1

with

Bn+1 = Î²n Bn + Î±n+1 Î¾n+1 ,

3

(2.1)

which gives (since |Î¾n+1 | = 1)
|Bn+1 | = Î²n |Bn | + Î±n+1 .
Therefore, setting

rnâˆ—

= |Nn | = |b0 | + |Bn |, we get
âˆ—
rn+1
= rnâˆ— + (Î²n âˆ’ 1)|Bn | + Î±n+1 ,

(2.2)

âˆ—
rn+1
âˆ’ rnâˆ— = |b0 |(1 âˆ’ Î²n ) âˆ’ rnâˆ— (1 âˆ’ Î²n ) + Î±n+1 .

(2.3)

that is
Moreover, setting F0 equal to the trivial Ïƒ-field and Fn = Ïƒ(Î¾1 , . . . , Î¾n ) for n â‰¥ 1, the conditional probabilities Ïˆn = (Ïˆn 1 , . . . , Ïˆn k )> of the extraction process, also called predictive means, are
Ïˆn = E[Î¾n+1 |Fn ] =

Nn
b0 + Bn
=
|Nn |
rnâˆ—

n â‰¥ 0.

(2.4)

It is obvious that we have |Ïˆn | = 1. Moreover, when Î²n > 0 for all n, the probability Ïˆn i results increasing
with the number of times we observed the value i, that is the random variables Î¾n i are generated according to
a reinforcement mechanism: the probability that the extraction of color i occurs has an increasing dependence
on the number of extractions of color i occurred in the past (see, e.g. [41]). More precisely, we have
Pn  Qnâˆ’1 
Q
b0 + B0 nâˆ’1
j=h Î²j Î¾h
h=1 Î±h
j=0 Î²j +
(2.5)
Ïˆn =
Pn  Qnâˆ’1  .
Qnâˆ’1
|b0 | + |B0 | j=0 Î²j + h=1 Î±h j=h Î²j
Q
The dependence of Ïˆn on Î¾h depends on the factor f (h, n) = Î±h nâˆ’1
j=h Î²j , with 1 â‰¤ h â‰¤ n, n â‰¥ 0. In
the case of the standard Eggenberger-PoÌlya urn, that corresponds to Î±n = Î± > 0 and Î²n = 1 for all n,
each observation Î¾h has the same â€œweightâ€ f (h, n) = Î±. Instead, if the factor f (h, n) increases with h,
then the main contribution is given by the most recent extractions. We refer to this phenomenon as â€œlocalâ€
reinforcement. For instance, this is the case when (Î±n ) is increasing and Î²n = 1 for all n. Another case is
when Î±n = Î± > 0 and Î²n < 1 for all n. The case Î²n = 0 for
Q all n is an extreme case, for which Ïˆn depends
only on the last extraction Î¾n (recall that conventionally nâˆ’1
j=n = 1). For the next examples, we will show
that they exhibit a broader sense local reinforcement, in the sense that the â€œweightâ€ of the observations is
eventually increasing with time.
By means of (2.4), together with (2.1) and (2.2), we have
Ïˆn+1 âˆ’ Ïˆn = âˆ’

 Î±n+1

(1 âˆ’ Î²n )
|b0 | Ïˆn âˆ’ p0 + âˆ—
Î¾n+1 âˆ’ Ïˆn .
âˆ—
rn+1
rn+1

(2.6)

n)
and
Setting Î¸n = Ïˆn âˆ’ p0 and âˆ†Mn+1 = Î¾n+1 âˆ’ Ïˆn = Î¾n+1 âˆ’ p0 âˆ’ Î¸n and letting n = |b0 | (1âˆ’Î²
râˆ—
n+1

âˆ—
Î´n = Î±n+1 /rn+1
, from (2.6) we obtain

Ïˆn+1 âˆ’ Ïˆn = âˆ’n (Ïˆn âˆ’ p0 ) + Î´n âˆ†Mn+1

(2.7)

Î¸n+1 âˆ’ Î¸n = âˆ’n Î¸n + Î´n âˆ†Mn+1 .

(2.8)

and so
Therefore, the asymptotic behaviour of (Î¸nP
) depends on the two sequences (n )n and (Î´n )n .
Finally, we observe that, setting Î¾N = N
n=1 Î¾n /N and Âµn = Î¾ n âˆ’ p0 , we have the equality
1
1
Âµn+1 âˆ’ Âµn = âˆ’ (Âµn âˆ’ Î¸n ) + âˆ†Mn+1 ,
n
n

(2.9)

that links the asymptotic behaviour of (Âµn ) and the one of (Î¸n ).
Different kinds of sequences (n )n and (Î´n )n provide different kinds of asymptotic behaviour of Î¸n , i.e. of
the empirical mean Î¾N . In Section 4, we P
provide two cases in which we have a long-term almost sure
convergence of the empirical mean Oi /N = N
n=1 Î¾n i /N toward the constant p0i = b0 i /|b0 |, together with
a chi-squared goodness of fit result. In particular, the quantities p0 1 , . . . , p0 k can be seen as a long-run
probability distribution on the possible values (colors) {1, . . . , k}.

4

3

Related literature

The particular case when in the GRP urn model we have Î²n = Î² = 0 for all n corresponds to a version of
the so-called â€œmemory-1 senile reinforced random walkâ€ on a star-shaped graph introduced in [29]. The case
Î±n = Î± > 0 and Î²n = Î² = 1 for all n corresponds to the standard Eggenberger-PoÌlya urn with an initial
number N0 i = b0 i + B0 i of balls of color i. When (Î±n ) is a not-constant sequence, while Î²n = Î² = 1 for
all n, the GRP urn coincides with the variant of the Eggenberger-PoÌlya urn introduced in [40] (see also [41,
Sec. 3.2]). Instead, when Î² 6= 1, the GRP urn does not fall in any variants of the Eggenberger-PoÌlya urn
discussed in [41, Sec. 3.2].
The case when Î±n = Î± > 0 and Î²n = Î² â‰¥ 0 for all n corresponds to the Rescaled PoÌlya (RP) urn
introduced and studied in [1] and applied in [6]. It is worthwhile to point out that the two cases studied in
the present work do not include (and are not included in) the case studied in [1]. Moreover, the techniques
employed here and in [1] are completely different: when Î²n = Î² âˆˆ [0, 1) as in [1], the jumps âˆ†Ïˆn do not
vanish and the process Ïˆ = (Ïˆn )n converges to a stationary Markov chain and so the appropriate Markov
ergodic theory is employed; in this work, we have |âˆ†Ïˆn | = o(1), so that the martingale limit theory is here
exploited to achieve the asymptotic results. Obviously, the two techniques are not exchangeable or adaptable
from one contest to the other one.
When (Î²n ) is not identically equal to 1, since the first term in the right hand of the above relation, the
GRP urn does not belong to the class of Reinforced Stochastic Processes (RSPs) studied in [3, 5, 4, 20, 21, 23].
Indeed, the RSPs are characterized by a â€œstrictâ€ reinforcement mechanism such that Î¾n i = 1 implies Ïˆn i >
Ïˆnâˆ’1 i and so, as a consequence, Ïˆn i has an increasing dependence on the number of times we have Î¾h i = 1
for h = 1, . . . , n. When (Î²n ) is not identically equal to 1, the GRP urn does not satisfy the â€œstrictâ€
reinforcement mechanism, because the first term is positive or negative according to the sign of (1 âˆ’ Î²n ) and
of (Ïˆn âˆ’ p0 ). Furthermore, we observe that equation (2.6) recalls the dynamics of a RSP with a â€œforcing
inputâ€ (see [3, 20, 44]), but the main difference relies on the fact that such a process is driven by a classical
stochastic
P approximationPdynamics, that is a dynamics of the kind (2.7) with n = Î´n (up to a constant)
with n n = +âˆ and n 2n < +âˆ, while the GRP urn model also allows for n and Î´n with different
rates and also for
P
P 2
â€¢
n n = +âˆ and
n Î´n = +âˆ or
P
â€¢
n n < +âˆ.
Since (2.7) is the fundamental equation of the Stochastic Approximation (SA) theory, we deem it appropriate
to say a few more words on the relationship of the present work with the SA literature. The case when
Î´n = cn in (2.7) is essentially covered by the Stochastic Approximation
S5, where
P (SA) theory (see
P Section
2
we refer to [25,
32,
37,
39,
48]).
The
most
known
case
is
when


<
+âˆ.
The
n = +âˆ and
n
n
n
P 2
P

=
+âˆ
is
less
usual
in
literature,
but
it
is
well
characterized
in

=
+âˆ
and
case n â†’ 0,
n
n n
n
[32]. The case when (n )n and (Î´n )n in (2.7) go to zero with different rates is typically neglected in SA
literature. To our best knowledge, it is taken into consideration only in [39], where the weak convergence
rate of the sequence (ÏˆN ) toward a certain point Ïˆ âˆ— is established under suitable assumptions, given the
event {ÏˆN â†’ Ïˆ âˆ— }. No result is given for the empirical mean Î¾N , which instead is the focus of the present
paper (see Theorem 4.1 below, whose proof is based on Theorem 7.2). More precisely, the assumptions on
n and Î´n in the following Theorem 7.2 imply assumption (A1.3) in [39] and so Theorem 1 in that paper
provides the weak convergence rate of the sequence (ÏˆN âˆ’ Ïˆ âˆ— ) given the event {ÏˆN â†’ Ïˆ âˆ— }. However, this
result is not useful for our scope because of two reasons: first, we need convergence results for the empirical
mean Î¾N , not for the predictive mean ÏˆN ; second, in one case included in Theorem 7.2 (see Section 7 for
more details), it seems to us not immediate to check the convergence of the predictive means and so we
develop another technique that does not ask for this convergence (see Section 8). Hence, the contribution
of Theorem 7.2 to the SA literature is that, for a dynamics of the type (2.7) with (n )n and (Î´n )n going to
zero with
P different rates,Pit provides the asymptotic behaviour of the empirical mean Î¾N , covering a case
when n n = +âˆ and n Î´n2 = +âˆ and without requiring the convergence ofPthe empirical means ÏˆN .
Finally, it is worthwhile to point out that we also analyze the case when n n < +âˆ, which is also
excluded in SA literature and so it could be relevant in that field. Specifically, we prove almost sure
convergence of the predictive means and of the empirical means toward a random variable and we give a
central limit theorem in the sense of stable convergence. However, even if interesting from a theoretical
point of view, we collect these results in Section S2, because they are not related to the chi-squared test of
goodness of fit.
The following statistical application of the GRP urn was inspired by [1, 12, 36]. However, those papers

5

only deal with the case when the statistics (1.1) is asymptotically distributed as Ï‡2 (k âˆ’ 1)Î», with Î» > 1,
while we also face the case when the statistics (1.1) is asymptotically distributed as Ï‡2 (k âˆ’ 1)N 1âˆ’2e Î»,
illustrating a suitable estimation procedure for the fundamental parameters Î· = 1 âˆ’ 2e and Î». To the best
of our knowledge, this is the first work presenting a model that provides a theoretical framework for a such
chi-squared test of goodness of fit.

4

Main theorem: goodness of fit result

Given a sample (Î¾1 , . . . , Î¾N ) generated by a GRP urn, the statistics
N
X

Oi = #{n = 1, . . . , N : Î¾n i = 1} =

Î¾n i ,

i = 1, . . . , k,

n=1

counts the number of times we observed the value i. The theorem below
states, under suitable assumptions,
P
the almost sure convergence of the empirical mean pbi = Oi /N = N
n=1 Î¾n i /N toward the probability p0 i ,
together with a chi-squared goodness of fit test for the long-term probabilities p0 1 , . . . , p0 k . More precisely,
we prove the following result:
Theorem 4.1. Assume p0 i > 0 for all i = 1, . . . , k and suppose to be in one of the following cases:
a) n = (n + 1)âˆ’ and Î´n = cn , with  âˆˆ (0, 1] and c > 0, or
b) n = (n + 1)âˆ’ , Î´n âˆ¼ c(n + 1)âˆ’Î´ , with  âˆˆ (0, 1), Î´ âˆˆ (/2, ) and c > 0.
Define the constants e and Î» as
(
1/2
e=
1/2 âˆ’ ( âˆ’ Î´) < 1/2

in case a)
in case b)

and
ï£±
2
ï£´
ï£²(c + 1)
Î» = (c + 1)2 + c2 = [2c(c + 1) + 1]
ï£´
ï£³ c2
1+2(âˆ’Î´)

in case a) with  âˆˆ (0, 1) ,
in case a) with  = 1 ,
in case b) .

(4.1)

a.s.

Then pbi = Oi /N âˆ’â†’ p0 i and
1
N 1âˆ’2e

k
k
X
X
(Oi âˆ’ N p0 i )2
(b
p i âˆ’ p 0 i )2 d
âˆ’â†’ Wâˆ— = Î»W0
= N 2e
N â†’âˆ
N p0 i
p0 i
i=1
i=1

where W0 has distribution Ï‡2 (k âˆ’ 1) = Î“

kâˆ’1 1
, 2)
2

and, consequently, Wâˆ— has distribution Î“

kâˆ’1 1
, 2Î»
2


.

We note that Î» is a constant greater than 1 in case a); while, in case b), it is a strictly positive quantity.
Moreover, in case b), we have 0 < ( âˆ’ Î´) < /2 < 1/2 and so (1 âˆ’ 2e) = 2( âˆ’ Î´) âˆˆ (0, 1). As a consequence,
we have N 1âˆ’2e Î» > 1 for N large enough.
In the next two examples we show that it is possible to construct suitable sequences (Î±n )n and (Î²n )n
of the model such that the corresponding sequences (n )n and (Î´n ) converge to zero with the same rate or
with different rates and satisfy the assumptions a) or b) of the above theorem, respectively.
Example 4.2. (Case n = (n + 1)âˆ’ and Î´n = cn , with  > 0 and c > 0 )
Take Î±n+1 = c|b0 |(1 âˆ’ Î²n ), with Î²n âˆˆ [0, 1) and c > 0, that implies Î´n =

Î±n+1
âˆ—
rn+1

n)
= c |b0r|(1âˆ’Î²
= cn . Set
âˆ—

rnâˆ— = (1 + c)|b0 |(1 âˆ’ tn ) so that from (2.3) we obtain tn+1 = Î²n tn . Hence, we have
tn+1 = t0

n
Y
k=0

Î²k =

n
c|b0 | âˆ’ |B0 | Y
Î²k
(1 + c)|b0 |
k=0

and so
âˆ—
rn+1
= (1 + c)|b0 | + |B0 | âˆ’ c|b0 |

n
Y
k=0

6

Î²k .

n+1

Q
âˆ—
âˆ—
âˆ—
Therefore, setting Î² âˆ— = âˆ
k=0 Î²k âˆˆ [0, 1), we get rn âˆ’â†’ r = (1 + c)|b0 | + (|B0 | âˆ’ c|b0 |)Î² > 0. If we choose
âˆ—
|B0 | = c|b0 |, then rn = râˆ— = (1 + c)|b0 | for each n and so, setting Î²n = 1 âˆ’ (1 + c)(1 + n)âˆ’ with  > 0,
we obtain n = (1 + n)âˆ’ and Î´n = cn . Taking  âˆˆ (0, 1], we have that n and Î´n satisfy assumption a)
of Theorem 4.1. Moreover, we have Q
Î±n = c|b0 |(1 + c)nâˆ’ and 1 âˆ’ Î²n = (1 + c)(1 + n)âˆ’ and so, for the
nâˆ’1
Î²j in (2.5), we refer to Section S3.
behaviour of the factor f (h, n) = Î±h j=h
Example 4.3. (Case n = (n + 1)âˆ’ and Î´n âˆ¼ c(n + 1)âˆ’Î´ , with 0 < Î´ <  < 1 and c > 0)
Take 0 < Î´ <  < 1 and set Î³ =  âˆ’ Î´ > 0, rnâˆ— = nÎ³ and (1 âˆ’ Î²n ) = |b0 |âˆ’1 (1 + n)âˆ’Î´ . We immediately have
n = |b0 |

(1 âˆ’ Î²n )
= (1 + n)âˆ’Î´âˆ’Î³ = (n + 1)âˆ’
âˆ—
rn+1

and (2.3) yields Î±n+1 = (n + 1)Î³ âˆ’ nÎ³ [1 âˆ’ |b0 |âˆ’1 (1 + n)âˆ’Î´ ] âˆ’ (1 + n)âˆ’Î´ , so that


Î±n+1
Î±n+1
1 Î³ 
Î´n = âˆ—
=
=
1
âˆ’
1
âˆ’
1 âˆ’ |b0 |âˆ’1 (1 + n)âˆ’Î´ âˆ’ (1 + n)âˆ’Î´âˆ’Î³
rn+1
(n + 1)Î³
n+1



= 1 âˆ’ 1 âˆ’ Î³(n + 1)âˆ’1 + O(nâˆ’2 ) 1 âˆ’ |b0 |âˆ’1 (1 + n)âˆ’Î´ âˆ’ (1 + n)âˆ’


= |b0 |âˆ’1 (1 + n)âˆ’Î´ 1 + Î³|b0 |(n + 1)âˆ’1+Î´ âˆ’ |b0 |(1 + n)âˆ’+Î´ âˆ’ Î³(n + 1)âˆ’1 + O(nâˆ’2+Î´ ) .
Setting c = |b0 |âˆ’1 > 0, we obtain n = (n + 1)âˆ’ and Î´n âˆ¼ c(n + 1)âˆ’Î´ . Taking Î´ âˆˆ (/2, ), we have
that n and Î´n satisfy assumption b) of Theorem 4.1. Moreover, we have Î±n = cnâˆ’(2Î´âˆ’) (1 + Î³câˆ’1 nâˆ’1+Î´ âˆ’
câˆ’1 nâˆ’+Î´ âˆ’ Î³nâˆ’1 + O(nâˆ’2+Î´ )) and (1 âˆ’ Î²
) = c(1 + n)âˆ’Î´ , with 0 < 2Î´ âˆ’  < Î´ < (1 + 2Î´ âˆ’ )/2, and so, for
Qnnâˆ’1
the behaviour of the factor f (h, n) = Î±h j=h Î²j in (2.5), we refer to Section S3.

5

Statistical applications

In a big sample the units typically can not be assumed independent and identically distributed, but they
exhibit a structure in clusters, with independence between clusters and with correlation inside each cluster
[12, 17, 30, 36, 46, 47]. The model and the related results presented in [1] and in the present paper may
be useful in the situation when inside each cluster the probability that a certain unit chooses the value i is
affected by the number of units in the same cluster that have already chosen the value i, hence according to
a reinforcement rule. Formally, given a â€œbigâ€ sample {Î¾n : n = 1, . . . , N }, we suppose that the N units are
ordered so that we have the following L clusters of units:
( `âˆ’1
)
`
X
X
C` =
Nl + 1, . . . ,
Nl ,
` = 1, . . . , L.
l=1

l=1

Therefore, the cardinality of each cluster C` is N` . We assume that the units in different clusters are
independent, that is
[Î¾1 , . . . , Î¾N1 ], . . . , [Î¾P`âˆ’1 N
l=1

l +1

, . . . , Î¾P`

l=1

Nl ],

. . . , [Î¾PLâˆ’1 N
l=1

l +1

, . . . , Î¾N ]

are L independent multidimensional random variables. Moreover, we assume that the observations inside
each cluster can be modelled as a GRP satisfying case a) or case b) of Theorem 4.1. Given certain (strictly
positive) intrinsic probabilities pâˆ—0 1 (`), . . . , pâˆ—0 k (`) for each cluster C` , we firstly want to estimate the model
parameters and then perform a test with null hypothesis
H0 :

p0 i (`) = pâˆ—0 i (`)

âˆ€i = 1, . . . , k

based on the the statistics
Q` =

1
2(âˆ’Î´)

N`

k
X
Oi (`) âˆ’ N` pâˆ—0 i (`)
N` pâˆ—0 i (`)
i=1

2
,

with Oi (`) = #{n âˆˆ C` : Î¾n i = 1},

(5.1)


1
and its corresponding asymptotic distribution Î“ kâˆ’1
, 2Î»
, where Î» is given in (4.1). Note that we can
2
perform the
above
test
for
a
certain
cluster
`,
or
we
can
consider
all the clusters together using the aggregate
P
L(kâˆ’1) 1
statistics L
, 2Î» ).
`=1 Q` and its corresponding distribution Î“(
2
Regarding the probabilities pâˆ—0 i (`), some possibilities are:

7

â€¢ we can take pâˆ—0 i (`) = 1/k for all i = 1, . . . , k if we want to test possible differences in the probabilities
for the k different values;
(1)

â€¢ we can suppose to have two different periods of times, and so two samples, say {Î¾n : n = 1, . . . , N }
P
(2)
(1)
and {Î¾n : n = 1, . . . , N }, take pâˆ—0 i (`) = nâˆˆC` Î¾n i /N` for all i = 1, . . . , k, and perform the test on
the second sample in order to check possible changes in the intrinsic long-run probabilities;
P
â€¢ we can take one of the clusters as benchmark, say `âˆ— , set pâˆ—0 i (`) = nâˆˆC`âˆ— Î¾n i /N`âˆ— for all i = 1, . . . , k
and ` 6= `âˆ— , and perform the test for the other L âˆ’ 1 clusters in order to check differences with the
benchmark cluster `âˆ— .
P
Finally, if we want to test possible differences in the clusters, then we can take pâˆ—0 i (`) = pâˆ—0 i = N
n=1 Î¾n i /N
PL
for all ` = 1, . . . , L and perform the test using the aggregate statistics `=1 Q` with asymptotic distribution
1
, 2Î»
).
Î“( (Lâˆ’1)(kâˆ’1)
2

5.1

Estimation of the parameters

The model parameters are , Î´ and c. However, as we have seen, the fundamental quantities are Î· = 2( âˆ’ Î´)
and Î» given in (4.1). Moreover, recall that in case a), we have Î· = 0 and Î» > 1 and, in case b), we
have Î· âˆˆ (0, 1) and Î» > 0. Therefore, according the considered model, the pair (Î·, Î») belongs to S =
{0} Ã— (1, +âˆ) âˆª (0, 1) Ã— (0, +âˆ). In order to estimate the pair (Î·, Î») âˆˆ S, we define
T` = N`Î· Q` =

k
X
Oi (`) âˆ’ N` pâˆ—0 i (`)
N` pâˆ—0 i (`)
i=1

2
.

Given the observed values t1 , . . . , tL , the log-likelihood function of Q` reads
L

ln(L(Î·, Î»)) = ln L(Î·, Î»; t1 , . . . , tL ) = âˆ’

kâˆ’1 X
kâˆ’1
L ln(Î») âˆ’
Î·
ln(N` ) âˆ’
2
2
`=1

1
2Î»

L
X
`=1

t`
Î·
N`

+ R1 ,

where R1 is a remainder term that does not depend on (Î·, Î»). Now, we look for the maximum likelihood
estimator of the two parameters (Î·, Î»).
We immediately observe that, when all the clusters have the same cardinality, that is all the N` are equal
to a certain N0 , then we cannot hope to estimate Î· and Î», separately. Indeed, the log-likelihood function
becomes
ln(L(Î·, Î»)) = ln L(Î·, Î»; t1 , . . . , tL ) = âˆ’

i
kâˆ’1 h
L ln(Î») + Î· ln(N0 ) âˆ’
2

1
Î·
2Î»N0

L
X

t` + R1 = f (Î»N0Î· ) .

`=1

dÎ· = PL t` /(k âˆ’ 1)L.
This fact implies that it possible to estimate only the parameter (Î»N0Î· ) as Î»N
0
`=1
From now on, we assume that at least two clusters have different cardinality, that is at least a pair
of cardinalities N` are different. We have to find (if they exist!) the maximum points of the function
(Î·, Î») 7â†’ ln(L(Î·, Î»)) on the set S, which is not closed nor limited. First of all, we note that ln(L(Î·, Î»)) â†’ âˆ’âˆ
for Î» â†’ +âˆ and Î» â†’ 0. Thus, the log-likelihood function has maximum value on the closure S of S and
its maximum points are stationary points belonging to (0, 1) Ã— (0, +âˆ) or they belong to {0, 1} Ã— (0, +âˆ).
For detecting the points of the first type, we compute the gradient of the log-likelihood function, obtaining
PL
PL t` ln(N` ) !
1
âˆ’ kâˆ’1
Î·
`=1 ln(N` ) + 2Î»
`=1
2
N`
P
âˆ‡(Î·, Î») ln L =
.
L
t`
1
L
+
âˆ’ kâˆ’1
Î·
2
`=1 N
2Î»
2Î»
`

Hence, the stationary points (Î·, Î») of the log-likelihood function are solutions of the system
ï£± PL t`
PL
ï£´
ï£´ `=1 N`Î· ln(N` )
`=1 ln(N` )
ï£´
ï£´
=
P
ï£´
L
t`
ï£²
L
Î·
`=1 N
`

PL t`
ï£´
ï£´
ï£´
`=1 N Î·
ï£´
ï£´
`
ï£³Î» =
.
L(k âˆ’ 1)

8

In particular, we get that the stationary points are of the form (Î·, Î»(Î·)), with
PL t`
Î»(Î·) =

`=1 N Î·
`

L(k âˆ’ 1)

.

(5.2)

In order to find the maximum points on the border, that is belonging to {0, 1} Ã— (0, +âˆ), we observe
that, fixed any Î·, the function
Î» 7â†’ âˆ’

kâˆ’1
L ln(Î») âˆ’
2

1
2Î»

L
X
`=1

t`
Î·
N`

+ R2 ,

where R2 is a remainder term not depending on Î», takes its maximum value at the point Î»(Î·) defined in
(5.2).
Summing up, the problem of detecting the maximum points of the log-likelihood function on S reduces
to the study of the maximum points on [0, 1] of the function
Î· 7â†’ ln(L(Î·, Î»(Î·))) = âˆ’

L

L

`=1

`=1

X t  k âˆ’ 1 X
kâˆ’1
`
L ln
âˆ’
Î·
ln(N` ) + R3 ,
2
N`Î·
2

(5.3)

where R3 is a remainder term not depending on Î·. To this purpose, we note that we have
ï£® PL t
ï£¹
PL
`
ln(L(Î·, Î»(Î·))
(k âˆ’ 1)L
k âˆ’ 1 ï£° `=1 N`Î· ln(N` )
`=1 ln(N` ) ï£»
d
âˆ’
=
L
g(Î·) ,
=
PL t`
dÎ·
2
L
2
Î·
`=1
N`

where

PL
g(x) =

t`
`=1 N`x ln(N` )
PL t`
`=1 N`x

PL

ln(N` )
.
L

`=1

âˆ’

Setting
t`
N`x

p(x, `) = PL

tl
l=1 Nlx

and denoting by Ex [Â·] and by Eu [Â·] the mean value with respect to the discrete probability distribution {p(x, `) : ` = 1, . . . , L} on {N1 , . . . , NL } and with respect to the uniform discrete distribution on
{N1 , . . . , NL } respectively, the above function g can be written as
PL
L
X
ln(N` )
g(x) =
p(x, `) ln(N` ) âˆ’ `=1
= Ex [ln(N )] âˆ’ Eu [ln(N )] .
L
`=1

Moreover, we have

0

âˆ’

PL

g (x) =

t`
`=1 N`x

ln2 (N` )

 P

L
t`
`=1 N`x

P

L
t`
`=1 N`x

=âˆ’

L
X
`=1

p(x, `) ln2 (N` ) +

L
X



+

P

L
t`
`=1 N`x

ln(N` )

2

2

p(x, `) ln(N` )

2

= âˆ’ V arx [ln(N )] ,

`=1

where V arx [Â·] denotes the variance with respect to the discrete probability distribution {p(x, `) : ` =
1, . . . , L} on {N1 , . . . , NL }. Since, we are assuming that at least two N` are different, we have V arx [ln(N )] >
0 and so the function g is strictly decreasing. Finally, we observe that we have
PL
PL
P
PL
t` L
t`
`=1 t` ln(N` )
`=1 ln(N` )
Covu (ln(N ), T ) =
âˆ’ `=1
= g(0) `=1
L
L
L
L
and
PL t`
PL t` PL
PL t`
`=1 N` ln(N` )
`=1 N`
`=1 N`
T
`=1 ln(N` )
Covu (ln(N ), N
)=
âˆ’
= g(1)
,
L
L
L
L
where Covu (Â·, Â·) denotes the covariance with respect to the discrete joint distribution concentrated on the
diagonal and such that P {N = N` , T = t` } = 1/L with ` = 1, . . . , L. Hence, we distinguish the following
cases.

9

First case: Covu (ln(N ), T ) â‰¤ 0
We are in the case when g(0) â‰¤ 0 and so the function (5.3) is strictly decreasing for Î· > 0. Thus, its
PL

t

b = Î»(0) = `=1 ` . Recall that
maximum value on [0, 1] is assumed at Î·b = 0. Consequently, we have Î»
L(kâˆ’1)
b âˆˆ S and so Î»
b > 1. If the model fits well the data, this is a consequence. Indeed, Î»
b is an
we need (0, Î»)
d
b âˆ¼ Î“(L(k âˆ’ 1)/2, 1/(2Î»)) and so E[Î»]
b = Î» > 1. A value Î»
b â‰¤ 1 means a bad fit of the
unbiased estimator: Î»
consider model to the data (the smaller the value of Î», the worse the fitting). Note that in the threshold
b = 1), the corresponding test statistics (5.1) and its distribution coincide with the classical
case (b
Î· = 0, Î»
ones used for independent observations.
T
Second case: Covu (ln(N ), T ) > 0 and Covu (ln(N ), N
)<0

We are in the case when g(0) > 0 and g(1) < 0. Hence, the function (5.3) has a unique stationary point
PL

`=1

b = Î»(b
Î·b âˆˆ (0, 1), which is the maximum point. Consequently, we have Î»
Î·) =
belongs to S.

t`
Î·
b

N`
L(kâˆ’1)

b
> 0. The point (b
Î· , Î»)

T
Third case: Covu (ln(N ), N
)â‰¥0

We are in the case when g(1) â‰¥ 0 and so the function (5.3) is strictly increasing on [0, 1]. Hence, its
PL

t`

b = Î»(1) = `=1 N` . However, the point (1, Î»)
b does
maximum point is at Î·b = 1, and, accordingly, we have Î»
L(kâˆ’1)
not belong to S and so, in this case, we conclude that we have a bad fit of the model to the data. Note
d
that, if the considered model fits well the data, then we have T /N âˆ¼ Î»e(Î·âˆ’1) ln(N ) Ï‡2 (k âˆ’ 1) with Î· < 1
T
and, consequently, we expect Covu (ln(N ), N ) < 0. Moreover, a value Î· â‰¥ 1 in the statistics (5.1) means a
d

central limit theorem of the type N (1âˆ’Î·)/2 (Î¾N âˆ’ p0 ) âˆ¼ N (0, CÎ“) with (1 âˆ’ Î·)/2 â‰¤ 0. This is impossible
since (Î¾N âˆ’ p0 ) is bounded.

6

COVID-19 epidemic Twitter analysis

We illustrate the application of the above statistical methodology to a data set containing posts on the
on-line social network Twitter about the COVID-19 epidemic. More precisely, the data set covers the period
from February 20th (h. 11pm) to April to 20th (h. 10pm) 2020, including tweets in Italian language. More
details on the keywords used for the query can be found in [13]. For every message, the relative sentiment
has been calculated using the polyglot python module developed in [16]. This module provides a numerical
value v for the sentiment and we have fixed a threshold T = 0.35 so that we have classified as a tweet with
positive sentiment those with v > T and as a tweet with negative sentiment those with v < âˆ’T . We have
discarded tweets with a value v âˆˆ [âˆ’T, T ].
We are in the case k = 2 and the random variables Î¾n = Î¾n 1 take the value 1 when the sentiment of the
post n is positive and the value 0 when the sentiment of the post n is negative. We have partitioned the data
so that each set Pd collect the messages of the single day d, for d = 1(February 20st), . . . , 61(April 20th)
and then, in order to obtain independent clusters, we have set C` = P1+3(`âˆ’1) , for ` = 1, . . . , 21 = L. (We
have tested the independence of the timed sequence {Q` : ` = 1, . . . , 21} with a Ljungâ€“Box test and we give
the results
in Table 2.) Therefore N` is the total number of tweets posted during the day 1 + 3(` âˆ’ 1) and
P
N= L
`=1 N` = 699 450 is the sample size.
It is plausible that inside each cluster the sentiment associated to each message is driven by a reinforcement mechanism, that can be modelled by means of a GRP: the probability to have a tweet with positive
sentiment is increasing with the number of past tweets with positive sentiment and the reinforcement is
mostly driven by the most recent tweets, in the sense explained in Section 2. Note that the main effect of
the GRP urn model is the presence of â€œlocal fashionsâ€, resulting in unexpected excursions of Ïˆn around the
long-run probabilities p0 . In order to point out that the considered data set exhibits this characteristics,
for each `, we have computed the daily sentiment rate pb0 (`), then, according to this probability, we have
generated an independent sequence (Î¾n0 ) of bernoulli variables, finally we have used the same smoothing
procedure (i.e. classical cubic spline given in R package) to get an estimate of Ïˆn = Ïˆn 1 , for both the real
and the simulated independent data. In Fig. 1 the daily curves clearly show different behaviors in the two
cases, highlighting a local reinforcement among tweets.

10

Figure 1: Smoothed daily estimate of Ïˆn 1 for the Twitter dataset (left) and for the simulated independent data
(right). The daily mean rate pb0 (`) is the same for both the left and the right panel. x-axis: daily time. y-axis:
cubic spline smoothing of the observed data Î¾n and of the simulated independent data Î¾n0 .

Figure 2: Plot of the function (5.3). Its maximum point gives the estimated value of the model parameter Î·.
âˆ—
âˆ—
PN Our purpose is to test the null hypothesis H0 : p0 (`) = p0 for any `. Therefore, taking p0 1 (`) = p0 =
Î¾
/N
for
each
`,
we
have
firstly
estimated
the
model
parameters
and
then
we
have
performed
the
n=1 n
PL
chi-squared test based on the aggregate statistics
Q
and
its
corresponding
asymptotic
distribution
`
`=1
1
b = 2.728098 (in Fig. 2 we plot the function
Î“( (Lâˆ’1)(kâˆ’1)
, 2Î»
). The estimated values are Î·b = 0.4363572 and Î»
2
(5.3)).
The contingency table and the associated statistics for testing H0 is given in Table 1. The obtained
Ï‡2 -statistics for a usual Ï‡2 -test is 5507.803, which is significant atPany level of confidence. Under the
proposed GRP model and the null hypothesis, the aggregate statistics L
`=1 Q` has (asymptotic) distribution
, 21Î»b ) and the corresponding p-value associated to the data is equal to 0.4579297. The null hypothesis
Î“( Lâˆ’1
2
that the daily long-run sentiment rate of the posts is the same for all the considered days is therefore
strongly rejected with a classical Ï‡2 test, while the same hypothesis is accepted if we take into account the
reinforcement mechanism of correlation given in GRP model.
In Fig. 3 there are the values of the single statistics Q` compared to the 95th-quantile of the distribution
Î“( 12 , 21Î»b ).

11

Date
2020-02-20
2020-02-23
2020-02-26
2020-02-29
2020-03-03
2020-03-06
2020-03-09
2020-03-12
2020-03-15
2020-03-18
2020-03-21
2020-03-24
2020-03-27
2020-03-30
2020-04-02
2020-04-05
2020-04-08
2020-04-11
2020-04-14
2020-04-17
2020-04-20

Obs+
25
53564
29831
18220
16801
27906
41650
255
14193
12064
11571
13339
14798
12689
12714
13373
14889
12153
13406
13977
13753

Obsâˆ’
43
60476
37175
22184
14834
27030
34769
156
13562
10089
10026
9172
10039
10651
9300
10815
11987
10777
11430
11371
12393

Exp+
35.11
58886.18
34599.51
20863.18
16335.18
28366.99
39460.04
212.23
14331.69
11439.02
11151.92
11623.88
12824.94
12051.94
11367.24
12489.82
13877.81
11840.23
12824.42
13088.80
13500.86

Expâˆ’
32.89
55153.82
32406.49
19540.82
15299.82
26569.01
36958.96
198.77
13423.31
10713.98
10445.08
10887.12
12012.06
11288.06
10646.76
11698.18
12998.19
11089.77
12011.58
12259.20
12645.14

Ï‡2+
2.91
481.02
657.20
334.87
13.28
7.49
121.54
8.62
1.34
34.15
15.75
253.07
303.55
33.67
159.56
62.45
73.68
8.26
26.37
60.27
4.71

Ï‡2âˆ’
3.11
513.58
701.67
357.53
14.18
8.00
129.76
9.20
1.43
36.46
16.81
270.20
324.09
35.95
170.36
66.68
78.67
8.82
28.16
64.35
5.03

(c)

Ï‡2+
0.46
2.99
5.15
3.27
0.14
0.06
0.90
0.62
0.02
0.43
0.20
3.19
3.67
0.42
2.03
0.76
0.86
0.10
0.32
0.72
0.06

(c)

Ï‡2âˆ’
0.49
3.19
5.50
3.49
0.15
0.07
0.96
0.67
0.02
0.46
0.22
3.41
3.92
0.45
2.17
0.82
0.92
0.11
0.34
0.77
0.06

Table 1: Contingency table associated to COVID-Twitter data: Obs+ (Obsâˆ’ ) are the number of posts with positive (negative) sentiment posted in the day ` reported in the first column (DataTime); Exp+ (Expâˆ’ ) corresponds
to N` pâˆ—0 (resp. N` (1 âˆ’ pâˆ—0 )), where N` = Obs+ + Obsâˆ’ ; Ï‡2+ (Ï‡2âˆ’ ) is the quantity (Obs+ âˆ’ Exp+ )2 /Exp+ (resp.
(Obsâˆ’ âˆ’ Expâˆ’ )2 /Expâˆ’ ); Ï‡2+
Ï‡2+

(c)

+ Ï‡2âˆ’

(c)

(c)

(Ï‡2âˆ’

(c)

) is the quantity Ï‡2+ /N`Î·b (resp. Ï‡2âˆ’ /N`Î·b). The statistics Q` corresponds to

.

Figure 3: Plot of the Q` -series. The black line corresponds to the value of 95th-quantile of the distribution
Î“( 12 , 1b ), that is 10.48.
2Î»

12

Df
Ï‡2
pâˆ’value

1
3.454
0.063

2
3.624
0.163

3
4.209
0.240

4
4.640
0.326

5
5.065
0.408

6
7.103
0.311

7
8.660
0.278

8
8.812
0.358

9
10.360
0.322

10
12.852
0.232

Table 2: Summary of Ljungâ€“Box test for autocorrelation of {Q` : ` = 1, . . . , 21}, with different numbers of autocorrelation lags being tested. Df: number of lags under investigation; Ï‡2 : Ljungâ€“Box test statistics, which is
distributed as a Ï‡2 distribution with Df degrees of freedom under the null hypothesis of independence; pâˆ’value:
pâˆ’value of the Ljungâ€“Box test.
The strong emotional involvement of the considered period had a â€œmixing effectâ€ that cancelled possible significant autocorrelation during different 3-delayed days.

7

Asymptotic results for the empirical means

Theorem 4.1 is a consequence of the following Proposition7.1 and Theorem 7.2 for the empirical means Î¾N .
s
In the sequel, we will use the symbol âˆ’â†’ in order to denote the stable convergence (for a brief review on
stable convergence, see Section S6).
Leveraging the Stochastic Approximation results collected in Section S5, we prove in Section S1.3 the
following result:
Proposition 7.1. Take n = (n+1)âˆ’ and Î´n = cn , with  âˆˆ (0, 1] and c > 0, and set Î“ = diag(p0 )âˆ’p0 p0 > .
a.s.
Then Î¾N âˆ’â†’ p0 and
âˆš
 s
N Î¾N âˆ’ p0 âˆ’â†’ N (0, Î»Î“) ,
with Î» = (c + 1)2 when 0 <  < 1 and Î» = (c + 1)2 + c2 = 2c(c + 1) + 1 when  = 1.
For the case when (n )n and (Î´n )n in (2.7) go to zero with different rates, we prove the following theorem
(the proof is illustrated in Section 8):
Theorem 7.2. Take n = (n + 1)âˆ’ and Î´n âˆ¼ c(n + 1)âˆ’Î´ , with  âˆˆ (0, 1), Î´ âˆˆ (/2, ) and c > 0. Then
a.s.
Î¾N âˆ’â†’ p0 and


 s
c2
Î“ ,
N 1/2âˆ’(âˆ’Î´) Î¾N âˆ’ p0 âˆ’â†’ N 0,
1 + 2( âˆ’ Î´)
with Î“ = diag(p0 ) âˆ’ p0 p0 > .
In the framework of the above theorem, we can distinguish the following two cases:
1)  âˆˆ (1/2, 1) and Î´ âˆˆ (1/2, ) or
2)  âˆˆ (0, 1) and Î´ âˆˆ (/2, min{, 1/2}] \ {}.
P
P
P
In case 1), we have n n = +âˆ, n 2n < +âˆ and n Î´n2 < +âˆ and so the typical asymptoticP
behaviour of
the predictive
mean
of
an
urn
process,
that
is
its
almost
sure
convergence.
In
case
2),
we
have
n n = +âˆ
P
P
and n Î´n2 = +âˆ (while the series n 2n may be convergent or divergent) and it seems to us not immediate
to check the convergence of the predict means. Therefore, for the proof of Theorem 7.2 in this last case, we
will employ a different technique, which is based on the L2 -estimate of Lemma 8.1 for the predictive mean
ÏˆN and the almost sure convergence of the corresponding empirical mean Ïˆ N âˆ’1 .

8

Proof of Theorem 7.2

P
PN
For all the sequel, we set Ïˆ N âˆ’1 = N
n=1 Ïˆnâˆ’1 /N and Î¸ N âˆ’1 =
n=1 Î¸nâˆ’1 /N . To the proof of Theorem
7.2, we premise some intermediate results.
Lemma 8.1. Under the same assumptions of Theorem 7.2, we have E[kÎ¸n k2 ] = O(nâˆ’2Î´ ) â†’ 0.
Proof. We observe that, starting from (2.7), we get
kÎ¸n+1 k2 = Î¸n+1 > Î¸n+1 = (1 âˆ’ n )2 kÎ¸n k2 + Î´n2 kâˆ†Mn+1 k2 + 2(1 âˆ’ n )Î´n Î¸n > âˆ†Mn+1
and so
E[kÎ¸n+1 k2 |Fn ] = (1 âˆ’ n )2 kÎ¸n k2 + Î´n2 E[kâˆ†Mn+1 k2 |Fn ] .

13

(8.1)

Hence, setting xn = E[kÎ¸n k2 ], we get
xn+1 = (1 âˆ’ 2n )xn + 2n xn + Î´n2 E[kâˆ†Mn+1 k2 ]


Î´2
= (1 âˆ’ 2n )xn + n n xn + n E[kâˆ†Mn+1 k2 ]
n

with 0 â‰¤ Î¶n =



= (1 âˆ’ 2n )xn + 2n Î¶n ,

Î´2
n xn + nn E[kâˆ†Mn+1 k2 ] /2. Applying Lemma S4.4 (with Î³n = 2n ), we find that

lim supn xn â‰¤ lim supn Î¶n . On the other hand, since (âˆ†Mn+1 )n is uniformly bounded and 2n /Î´n2 âˆ¼
2
2
câˆ’2 nâˆ’2(âˆ’Î´) â†’ 0, we have Î¶n = O(n + Î´n2 âˆ’1
n ) = O(Î´n /n ) and so xn = O(Î´n /n ). We can conclude
2
2 âˆ’2Î´
recalling that Î´n /n âˆ¼ c n
.
Lemma 8.2. Under the same assumptions of Theorem 7.2, we have
Î¸ N âˆ’1 =

N âˆ’1
N
1 X
1 X Î´n
âˆ†Mn+1 + RN ,
Î¸nâˆ’1 =
N n=1
N n=0 n

(8.2)



a.s.
where RN âˆ’â†’ 0 and N e E |RN | âˆ’â†’ 0 with e = 1/2 âˆ’ ( âˆ’ Î´) âˆˆ (0, 1/2).
Proof. By (2.8), we have
Î¸n = âˆ’

1
Î´n
(Î¸n+1 âˆ’ Î¸n ) +
âˆ†Mn+1 .
n
n

Therefore, we can write
N
âˆ’1
X
n=0

N
âˆ’1
X
Î´n
1
(Î¸n+1 âˆ’ Î¸n ) +
âˆ†Mn+1

n
n
n=0
n=0

 N

âˆ’1 
N
âˆ’1
X
X
Î¸N
Î¸0
1
1
Î´n
=âˆ’
âˆ’
âˆ’
âˆ’
Î¸n +
âˆ†Mn+1 ,
N âˆ’1
0


n
nâˆ’1
n
n=1
n=0

Î¸n = âˆ’

N
âˆ’1
X

where the second equality is due to the Abel transformation for a series. It follows the decomposition (8.2)
with



N âˆ’1 
1
Î¸N
Î¸0
1 X
1
1
RN = âˆ’
âˆ’
âˆ’
âˆ’
Î¸n .
(8.3)
N N âˆ’1
0
N n=1 nâˆ’1
n
Since |Î¸n | = O(1), we have
|RN | =

O(N âˆ’1 âˆ’1
N âˆ’1 )

+O

N

âˆ’1

N
âˆ’1
X

!
|âˆ’1
nâˆ’1

âˆ’

âˆ’1
n |

n=1

PN âˆ’1 âˆ’1
âˆ’1
âˆ’1
Note that
âˆ’ âˆ’1
N âˆ’1 when (n ) is decreasing and so the last term in the above
n=1 |nâˆ’1 âˆ’ n | = 0
âˆ’1 âˆ’1
expression is O(N N âˆ’1 ). Therefore, since  < 1 by assumption, we have |RN | = O(N âˆ’(1âˆ’) ) â†’ 0.
Regarding

 the last statement of the lemma, we observe that, from what we have proven before, we obtain
N e E |RN | = O(N eâˆ’(1âˆ’) ) = O(N Î´âˆ’1/2 ) â†’ 0 when Î´ < 1/2. However, in the considered cases 1) and 2),
we might have Î´ â‰¥ 1/2. Therefore, we need other arguments in order to prove the last statement. To this
purpose, we observe that, by Lemma 8.1, we have E[ |Î¸n | ] = O(n/2âˆ’Î´ ) and so, by (8.3), we have
!
N âˆ’1

1 X âˆ’1
e 
âˆ’(1âˆ’e) 3/2âˆ’Î´
âˆ’1 /2âˆ’Î´
N E |RN | = O(N
N
)+O
|
âˆ’ n |n
N 1âˆ’e n=1 nâˆ’1
!
N âˆ’1
1 X âˆ’1
âˆ’(1âˆ’)/2
âˆ’1 /2âˆ’Î´
= O(N
)+O
|
âˆ’ n |n
.
N 1âˆ’e n=1 nâˆ’1
Moreover, we have
N
âˆ’1
X
n=1

âˆ’1 /2âˆ’Î´
|âˆ’1
=
nâˆ’1 âˆ’ n |n

N
âˆ’1
X

[(n âˆ’ 1) âˆ’ n ] n/2âˆ’Î´ =

n=1

N
âˆ’1
X

nâˆ’1+/2âˆ’Î´ âˆ¼ N 3/2âˆ’Î´ = o(N 1âˆ’e ) ,

n=1

because e = 1/2 âˆ’ ( âˆ’ Î´) and  < 1. Summing up, we have N e E[|RN |] = O(N âˆ’(1âˆ’)/2 ) + o(1) â†’ 0.

14

a.s.

a.s.

Lemma 8.3. Under the same assumptions of Theorem 7.2, we have Î¸ N âˆ’1 âˆ’â†’ 0, that is Ïˆ N âˆ’1 âˆ’â†’ p0 .
a.s.
a.s.
In particular, when  âˆˆ (1/2, 1) and Î´ âˆˆ (1/2, ), we have Î¸N âˆ’â†’ 0, that is ÏˆN âˆ’â†’ p0 .
Proof. Let us distinguish the following two cases:
1)  âˆˆ (1/2, 1) and Î´ âˆˆ (1/2, ) or
2)  âˆˆ (0, 1) and Î´ âˆˆ (/2, min{, 1/2}] \ {}.
For the case 1), we observe that, by (8.1), we have
E[kÎ¸n+1 k2 |Fn ] â‰¤ (1 + 2n )E[kÎ¸n k2 |Fn ] + Î´n2 E[kâˆ†Mn+1 k2 |Fn ].
P
P
Therefore, since (âˆ†Mn+1 )n is uniformly bounded and, in case 1), we have n 2n < +âˆ and n Î´n2 < +âˆ,
2
the sequence (kÎ¸n k )n is a bounded non-negative almost supermartingale. As a consequence, it converges
almost surely to a certain random variable. This limit random variable is necessarily equal to 0 because,
by Lemma 8.1, we have E[kÎ¸n k2 ] = O(nâˆ’2Î´ ) â†’ 0. Hence, we have the almost sure convergence of Î¸N to
0 and, consequently, the almost sure convergence of Î¸ N âˆ’1 to 0 follows by Lemma S4.2 and Remark S4.3
(with cn = n and vN,n = n/N ), because E[Î¸nâˆ’1 |Fnâˆ’2 ] = (1 âˆ’ nâˆ’2 )Î¸nâˆ’2 â†’ 0 almost surely.
a.s.
For the case 2), we use Lemma 8.2, that gives the
(8.2), with RN âˆ’â†’ 0. Indeed, by this
Pdecomposition
N âˆ’1 Î´n
decomposition, it is enough to prove that the term n=0 n âˆ†Mn+1 /N converges almost surely to 0. To
this purpose, we observe that, if we set
Ln =

n
X
1 Î´jâˆ’1
âˆ†Mj ,
j jâˆ’1
j=1

then (Ln ) is a square integrable martingale. Indeed, we have
+âˆ
2
X
1 Î´nâˆ’1
E[kâˆ†Mn k2 ] = O
2
2
n

nâˆ’1
n=1

Therefore, (Ln ) converges almost surely, that is we have
Lemma S4.1 (with vN,n = n/N ), we find

+âˆ
X
n=1

P

!

1
n1+2e

< +âˆ .

1 Î´nâˆ’1
n n nâˆ’1 âˆ†Mn

< +âˆ almost surely. Applying

N âˆ’1
N
X
1 Î´nâˆ’1
1 X Î´n
a.s.
âˆ†Mn+1 =
vN,n
âˆ†Mn âˆ’â†’ 0
N n=0 n
n

nâˆ’1
n=1
a.s.

and so Î¸ N âˆ’1 âˆ’â†’ 0.
Proof of Theorem 7.2. Set e = 1/2 âˆ’ ( âˆ’ Î´) âˆˆ (0, 1/2) and Î» = c2 /[2(1 âˆ’ e)] = c2 /[1 + 2( âˆ’ Î´)]. Moreover,
let us distinguish the following two cases:
1)  âˆˆ (1/2, 1) and Î´ âˆˆ (1/2, ) or
2)  âˆˆ (0, 1) and Î´ âˆˆ (/2, min{, 1/2}] \ {}.
Almost sure convergence: In case 1), by Lemma 8.3, ÏˆN converges almost surely to p0 . Therefore, the almost
sure convergence of Î¾N to p0 follows by Lemma S4.2
P(with cn = n and vN,n = n/N ),
P and Remark S4.3
because E[Î¾n+1 |Fn ] = Ïˆn â†’ p0 almost surely and n E[kÎ¾n k2 ]nâˆ’2 â‰¤ n nâˆ’2 < +âˆ.
In case 2), we use a different argument. Take Î³ âˆˆ [0, e) and set
Ln =

n
X
j=1

1 Î´jâˆ’1
âˆ†Mj .
j 1âˆ’Î³ jâˆ’1

Then (Ln ) is a square integrable martingale, because we have
+âˆ
X
n=1

1
n2âˆ’2Î³

2
Î´nâˆ’1
E[kâˆ†Mn k2 ] = O
2nâˆ’1

15

+âˆ
X
n=1

1
n1+2eâˆ’2Î³

!
< +âˆ .

Therefore, (Ln ) converges almost surely, that is we have

Lemma S4.1 (with vN,n = (n/N )1âˆ’Î³ nâˆ’1 /Î´nâˆ’1 âˆ¼ n1âˆ’Î³âˆ’+Î´ /N
N
âˆ’1
X

1
N 1âˆ’Î³

âˆ†Mn+1 =

n=0

N
X

vN,n

n=1

Î´nâˆ’1
1
n n1âˆ’Î³ nâˆ’1 âˆ†Mn
1âˆ’Î³

P

< +âˆ almost surely. By

), we get

1 Î´nâˆ’1
a.s.
âˆ†Mn âˆ’â†’ 0.
n1âˆ’Î³ nâˆ’1

Therefore, we have

N Î³ Î¾N âˆ’ Ïˆ N âˆ’1 =

N
âˆ’1
X

1
N 1âˆ’Î³

a.s.

âˆ†Mn+1 âˆ’â†’ 0,

n=0


that is Î¾N âˆ’ Ïˆ N âˆ’1 = o(N âˆ’Î³ ) for each Î³ âˆˆ [0, e). Recalling Lemma 8.3, we obtain in particular that Î¾N
converges almost surely to p0 .
Second order asymptotic behaviour: We have
âˆš


N e Î¾N âˆ’ p0 = N e ÂµN = N eâˆ’1/2 N ÂµN âˆ’ Î¸ N âˆ’1 + N e Î¸ N âˆ’1 .
(8.4)
Moreover, by Lemma 8.1, we have
N âˆ’1
N
X
1 X
E[|Î¸n |] = O(N âˆ’1
n/2âˆ’Î´ ) = O(N âˆ’1âˆ’Î´+/2+1 ) = O(N /2âˆ’Î´ ) â†’ 0 ,
N n=0
n=1
N âˆ’1
N
X
1 X
E[kÎ¸n k2 ] = O(N âˆ’1
nâˆ’2Î´ ) = O(N âˆ’1âˆ’2Î´++1 ) = O(N âˆ’2Î´ ) â†’ 0 ,
N n=0
n=1

and so Theorem S1.1 holds true with V = Î“ (see Remark S1.2). Therefore, the first term in the right side
of (8.4) converges in probability to 0 because e < 1/2. Hence, if we prove that
s

N e Î¸ N âˆ’1 âˆ’â†’ N (0, Î»Î“) ,

(8.5)

then the proof is concluded.
In order to prove (8.5), we observe that, by decomposition (8.2) in Lemma 8.2, we have
N e Î¸ N âˆ’1 =

N
X

YN,n + N e RN ,

n=1
Î´nâˆ’1
âˆ†Mn
N 1âˆ’e nâˆ’1


and N RN converges in probability to 0 (because N e E |RN |] â†’ 0). TherePN
fore, it is enough to prove that the term n=1 YN,n stably converges to the Gaussian kernel N (0, Î»Î“), with
P
Î» = c2 /[2(1 âˆ’ e)] = c2 /[1 + 2( âˆ’ Î´)]. To this purpose, we observe that E[YN,n |Fnâˆ’1 ] = 0 and so N
n=1 YN,n
converges stably to N (0, Î»Î“) if the conditions (c1) and (c2) of Theorem S6.1, with V = Î»Î“, hold true.
Regarding (c1), we note that Î´nâˆ’1 /nâˆ’1 âˆ¼ cnâˆ’Î´ = cn1/2âˆ’e and so we have
where YN,n =

e

1

max |YN,n | â‰¤ N âˆ’(1âˆ’e) max

1â‰¤nâ‰¤N

1â‰¤nâ‰¤N

Î´nâˆ’1
Î´nâˆ’1
|Î¾n âˆ’ Ïˆnâˆ’1 | â‰¤ N âˆ’(1âˆ’e) max
= O(N âˆ’1/2 ) â†’ 0 .
1â‰¤nâ‰¤N nâˆ’1
nâˆ’1

Condition (c2) means
1
N 2(1âˆ’e)
We note that N âˆ’2(1âˆ’e)

PN

n=1

N
2
X
Î´nâˆ’1
P
(Î¾n âˆ’ Ïˆnâˆ’1 )(Î¾n âˆ’ Ïˆnâˆ’1 )> âˆ’â†’ Î»Î“.
2

nâˆ’1
n=1

(8.6)

2
2
Î´nâˆ’1
/2nâˆ’1 â†’ Î», because Î´nâˆ’1
/2nâˆ’1 âˆ¼ c2 n1âˆ’2e , and

E[(Î¾n âˆ’ Ïˆnâˆ’1 )(Î¾n âˆ’ Ïˆnâˆ’1 )> |Fnâˆ’1 ] = diag(Ïˆnâˆ’1 ) âˆ’ Ïˆnâˆ’1 Ïˆnâˆ’1 > .
Therefore, in case 1), condition (8.6) immediately follows by the almost sure convergence of Ïˆn to p0 .
2
It is enough to apply Lemma S4.2 and Remark S4.3 with cn = n and vN,n = nÎ´nâˆ’1
/(N 2(1âˆ’e) 2nâˆ’1 ) âˆ¼
2 1+2(âˆ’Î´)
2âˆ’2e
2
2(1âˆ’e)
c n
/N
= c (n/N )
. In case 2), we apply again Lemma S4.2 with the above cn and vN,n ,
but we note that Ïˆn = Î¸n + p0 and so condition (S4.1) in Lemma S4.2, with V = Î»Î“, is equivalent to
1
N 2âˆ’2e

N
âˆ’1
X
n=0

Î´n2
P
Î¸n âˆ’â†’ 0
2n

1

and

N 2âˆ’2e

16

N
âˆ’1
X
n=0

Î´n2
P
Î¸n Î¸n > âˆ’â†’ 0kÃ—k .
2n

These two convergences hold true because, by Lemma 8.1, we have
N
âˆ’1
X

1
N 2âˆ’2e
1
N 2âˆ’2e

n=0

N
âˆ’1
X
n=0

N
X
Î´n2
E[|Î¸n |] = O(N âˆ’2+2e
nâˆ’2Î´+2âˆ’Î´+/2 ) = O(N âˆ’2+2eâˆ’3Î´+5/2+1 ) = O(N âˆ’Î´+/2 ) â†’ 0 ,
2
n
n=1

N
X
Î´n2
E[kÎ¸n k2 ] = O(N âˆ’2+2e
nâˆ’2Î´+2âˆ’2Î´+ ) = O(N âˆ’2+2eâˆ’4Î´+3+1 ) = O(N âˆ’2Î´+ ) â†’ 0 .
2
n
n=1

Therefore, in both cases 1) and 2), conditions c1) and c2) of Theorem S6.1 are satisfied and so
stably converges to the Gaussian kernel N (0, Î»Î“).
Declaration

PN

n=1

YN,n

Both authors equally contributed to this work.
Acknowledgments
Giacomo Aletti is a member of the Italian Group â€œGruppo Nazionale per il Calcolo Scientificoâ€ of the Italian Institute â€œIstituto Nazionale di Alta Matematicaâ€ and Irene Crimaldi is a member of the Italian Group
â€œGruppo Nazionale per lâ€™Analisi Matematica, la ProbabilitaÌ€ e le loro Applicazioniâ€ of the Italian Institute
â€œIstituto Nazionale di Alta Matematicaâ€.
Funding Sources
Irene Crimaldi is partially supported by the Italian â€œProgramma di AttivitaÌ€ Integrataâ€ (PAI), project â€œTOol
for Fighting FakEsâ€ (TOFFE) funded by IMT School for Advanced Studies Lucca.

References
[1] G. Aletti and I. Crimaldi. The rescaled PoÌlya urn: local reinforcement and chi-squared goodness of fit
test. arXiv:1906.10951, 2019.
[2] G. Aletti and I. Crimaldi. Generalized rescaled PoÌlya urn and its statistical applications. Supplementary
Material of this article, 2020.
[3] G. Aletti, I. Crimaldi, and A. Ghiglietti. Synchronization of reinforced stochastic processes with a
network-based interaction. Ann. Appl. Probab., 27(6):3787â€“3844, 2017.
[4] G. Aletti, I. Crimaldi, and A. Ghiglietti. Networks of reinforced stochastic processes: asymptotics for
the empirical means. Bernoulli, 25(4B):3339â€“3378, 2019.
[5] G. Aletti, I. Crimaldi, and A. Ghiglietti. Interacting reinforced stochastic processes: Statistical inference
based on the weighted empirical means. Bernoulli, 26(2):1098â€“1138, 2020.
[6] G. Aletti, I. Crimaldi, and F. Saracco. A model for the twitter sentiment curve. arXiv:2011.05933,
2020.
[7] G. Aletti, A. Ghiglietti, and W. F. Rosenberger. Nonparametric covariate-adjusted response-adaptive
design based on a functional urn model. Ann. Statist., 46(6B):3838â€“3866, 2018.
[8] G. Aletti, A. Ghiglietti, and A. N. Vidyashankar. Dynamics of an adaptive randomly reinforced urn.
Bernoulli, 24(3):2204â€“2255, 2018.
[9] D. Bergh. Sample size and chi-squared test of fitâ€” a comparison between a random sample approach
and a chi-square value adjustment method using swedish adolescent data. In Q. Zhang and H. Yang,
editors, Pacific Rim Objective Measurement Symposium (PROMS) 2014 Conference Proceedings, pages
197â€“211, Berlin, Heidelberg, 2015. Springer Berlin Heidelberg.
[10] P. Berti, I. Crimaldi, L. Pratelli, and P. Rigo. A central limit theorem and its applications to multicolor
randomly reinforced urns. J. Appl. Probab., 48(2):527â€“546, 2011.
[11] P. Berti, I. Crimaldi, L. Pratelli, and P. Rigo. Asymptotics for randomly reinforced urns with random
barriers. J. Appl. Probab., 53(4):1206â€“1220, 2016.

17

[12] D. Bertoni, G. Aletti, G. Ferrandi, A. Micheletti, D. Cavicchioli, and R. Pretolani. Farmland use
transitions after the cap greening: a preliminary analysis using markov chains approach. Land Use
Policy, 79:789 â€“ 800, 2018.
[13] G. Caldarelli, R. de Nicola, M. Petrocchi, M. Pratelli, and F. Saracco. Analysis of online misinformation
during the peak of the covid-19 pandemics in italy. arXiv: 2010.01913, 2020.
[14] K. C. Chanda. Chi-squared tests of goodness-of-fit for dependent observations. In Asymptotics, NonParametrics and Time Series, Statist. Textbooks Monogr., volume 158, pages 743â€“756. Dekker, 1999.
[15] M.-R. Chen and M. Kuba. On generalized poÌlya urn models. J. Appl. Probab., 50(4):1169â€“1186, 12
2013.
[16] Y. Chen and S. Skiena. Building sentiment lexicons for all major languages. In Proceedings of the 52nd
Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 383â€“389, 2014.
[17] A. Chessa, I. Crimaldi, M. Riccaboni, and L. Trapin. Cluster analysis of weighted bipartite networks:
A new copula-based approach. PLOS ONE, 9(10):1â€“12, 10 2014.
[18] A. Collevecchio, C. Cotar, and M. LiCalzi. On a preferential attachment and generalized poÌlyaâ€™s urn
model. Ann. Appl. Probab., 23(3):1219â€“1253, 06 2013.
[19] I. Crimaldi. Introduzione alla nozione di convergenza stabile e sue varianti (Introduction to the notion of
stable convergence and its variants), volume 57. Unione Matematica Italiana, Monograf s.r.l., Bologna,
Italy., 2016. Book written in Italian.
[20] I. Crimaldi, P. Dai Pra, P.-Y. Louis, and I. G. Minelli. Synchronization and functional central limit theorems for interacting reinforced random walks. Stochastic Processes and their Applications, 129(1):70â€“
101, 2019.
[21] I. Crimaldi, P. Dai Pra, and I. G. Minelli. Fluctuation theorems for synchronization of interacting
PoÌlyaâ€™s urns. Stochastic Process. Appl., 126(3):930â€“947, 2016.
[22] I. Crimaldi, G. Letta, and L. Pratelli. A Strong Form of Stable Convergence, volume 1899, pages
203â€“225. Springer, 2007.
[23] P. Dai Pra, P.-Y. Louis, and I. G. Minelli. Synchronization via interacting reinforcement. J. Appl.
Probab., 51(2):556â€“568, 2014.
[24] F. Eggenberger and G. PoÌlya. UÌˆber die statistik verketteter vorgaÌˆnge. ZAMM - Journal of Applied
Mathematics and Mechanics / Zeitschrift fuÌˆr Angewandte Mathematik und Mechanik, 3(4):279â€“289,
1923.
[25] G. Fort. Central limit theorems for stochastic approximation with controlled markov chain dynamics.
ESAIM: PS, 19:60â€“80, 2015.
[26] T. Gasser. Goodness-of-fit tests for correlated data. Biometrika, 62(3):563â€“570, 1975.
[27] L. J. Gleser and D. S. Moore. The effect of dependence on chi-squared and empiric distribution tests
of fit. The Annals of Statistics, 11(4):1100â€“1108, 1983.
[28] P. Hall and C. C. Heyde. Martingale limit theory and its application. Academic Press Inc. [Harcourt
Brace Jovanovich Publishers], New York, 1980. Probability and Mathematical Statistics.
[29] M. Holmes and A. Sakai. Senile reinforced random walks. Stochastic Processes and their Applications,
117(10):1519â€“1539, 2007.
[30] F. Ieva, A. M. Paganoni, D. Pigoli, and V. Vitelli. Multivariate functional clustering for the morphological analysis of electrocardiograph curves. Journal of the Royal Statistical Society. Series C (Applied
Statistics), 62(3):401â€“418, 2013.
[31] D. Knoke, G. W. Bohrnstedt, and A. Potter Mee. Statistics for Social Data Analysis. F.E.Peacock
Publishers, 2002.
[32] H. J. Kushner and G. G. Yin. Stochastic approximation and recursive algorithms and applications,
volume 35 of Applications of Mathematics (New York). Springer-Verlag, New York, second edition,
2003. Stochastic Modelling and Applied Probability.
[33] S. Laruelle and G. PageÌs. Randomized urn models revisited using stochastic approximation. Ann. Appl.
Proba., 23(4):1409â€“1436, 2013.

18

[34] N. Lasmar, C. Mailler, and O. Selmi. Multiple drawing multi-colour urns by stochastic approximation.
J. Appl. Probab., 55(1):254â€“281, 2018.
[35] H. M. Mahmoud. PoÌlya urn models. Texts in Statistical Science Series. CRC Press, Boca Raton, FL,
2009.
[36] A. Micheletti, G. Aletti, G. Ferrandi, D. Bertoni, D. Cavicchioli, and R. Pretolani. A weighted Ï‡2 test
to detect the presence of a major change point in non-stationary Markov chains. Stat. Methods Appl.,
29(4):899â€“912, 2020.
[37] A. Mokkadem and M. Pelletier. Convergence rate and averaging of nonlinear two-time-scale stochastic
approximation algorithms. Ann. Appl. Probab., 16(3):1671â€“1702, 08 2006.
[38] W. Pan. Goodness-of-fit tests for GEE with correlated binary data. Scand. J. Statist., 29(1):101â€“110,
2002.
[39] M. Pelletier. Weak convergence rates for stochastic approximation with application to multiple targets
and simulated annealing. Ann. Appl. Probab., 8(1):10â€“44, 1998.
[40] R. Pemantle. A time-dependent version of poÌlyaâ€™s urn. J. Theor. Probab., 3:627â€“637, 1990.
[41] R. Pemantle. A survey of random processes with reinforcement. Probab. Surveys, 4:1â€“79, 2007.
[42] R. Radlow and E. F. Alf Jr. An alternate multinomial assessment of the accuracy of the Ï‡2 test of
goodness of fit. Journal of the American Statistical Association, 70(352):811â€“813, 1975.
[43] J. N. K. Rao and A. J. Scott. The analysis of categorical data from complex sample surveys: chi-squared
tests for goodness of fit and independence in two-way tables. J. Amer. Statist. Assoc., 76(374):221â€“230,
1981.
[44] N. Sahasrabudhe. Synchronization and fluctuation theorems for interacting Friedman urns. J. Appl.
Probab., 53(4):1221â€“1239, 2016.
[45] M.-L. Tang, Y.-B. Pei, W.-K. Wong, and J.-L. Li. Goodness-of-fit tests for correlated paired binary
data. Stat. Methods Med. Res., 21(4):331â€“345, 2012.
[46] A. Tharwat. Independent component analysis: An introduction. Applied Computing and Informatics,
2018.
[47] D. Xu and Y. Tian. A comprehensive survey of clustering algorithms. Annals of Data Science, 2(2):165â€“
193, 2015.
[48] L.-X. Zhang. Central limit theorems of a recursive stochastic algorithm with applications to adaptive
designs. Ann. Appl. Probab., 26(6):3630â€“3658, 2016.

19

SM Supplemental Materials
In this document we collect some proofs, complements, technical results and recalls, useful for [S2]. Therefore,
the notation and the assumptions used here are the same as those used in that paper.

S1

Proofs and intermediate results

We here collect some proofs omitted in the main text of the paper [S2].

S1.1

Proof of Theorem 4.1

The proof is based on Proposition 7.1 (for case a)) and Theorem 7.2 (for case b)). The almost sure convergence of Oi /N immediately follows since Oi /N = Î¾ N i . In order to prove the stated convergence in
distribution, we mimic the classical proof for the Pearson chi-squared test based on the Sherman Morison
formula (see [S18]), but see also [S16, Corollary 2].
We start recalling the Sherman Morison formula: if A is an invertible square matrix and we have 1 âˆ’
v > Aâˆ’1 u 6= 0, then
Aâˆ’1 uv > Aâˆ’1
(A âˆ’ uv > )âˆ’1 = Aâˆ’1 +
.
1 âˆ’ v > Aâˆ’1 u
âˆ—
Given the observation Î¾n = (Î¾n 1 , . . . , Î¾n k )> , we define the â€œtruncatedâ€ vector Î¾n
= (Î¾nâˆ— 1 , . . . , Î¾nâˆ— kâˆ’1 )> , given
by the first k âˆ’ 1 components of Î¾n . Proposition 7.1 (for case a)) and Theorem 7.2 (for case b)) give the
second order asymptotic behaviour of (Î¾n ), that immediately implies
PN
âˆ—
âˆ—
 âˆ—

n=1 (Î¾n âˆ’ p ) d
N e Î¾N âˆ’ pâˆ— =
âˆ’â†’ N (0, Î“âˆ— ),
(S1.1)
N 1âˆ’e

where pâˆ— is given by the first k âˆ’ 1 components of p0 and Î“âˆ— = Î»(diag(pâˆ— ) âˆ’ pâˆ— pâˆ—T ). By assumption p0 i > 0
1
for all i = 1, . . . , k and so diag(pâˆ— ) is invertible with inverse diag(pâˆ— )âˆ’1 = diag( p01 1 , . . . , p0 kâˆ’1
) and, since
(diag(pâˆ— )âˆ’1 )pâˆ— = 1 âˆˆ Rkâˆ’1 , we have
1 âˆ’ pâˆ—T diag(pâˆ— )âˆ’1 pâˆ— = 1 âˆ’

kâˆ’1
X

p0 i =

i=1

k
X
i=1

p0 i âˆ’

kâˆ’1
X

p0 i = p0 k > 0.

i=1

Therefore we can use the Sherman Morison formula with A = diag(pâˆ— ) and u = v = pâˆ— , and we obtain

1
1
1
1
(Î“âˆ— )âˆ’1 = (diag(pâˆ— ) âˆ’ pâˆ— pâˆ—T )âˆ’1 =
diag( p01 1 , . . . , p0 kâˆ’1
)+
11> .
(S1.2)
Î»
Î»
p0 k
P
P
Now, since ki=1 (Î¾ N i âˆ’ p0 i ) = 0, then Î¾ N k âˆ’ p0 k = kâˆ’1
i=1 (Î¾ N i âˆ’ p0 i ) and so we get
k
k
h kâˆ’1
X
X
X (Î¾ âˆ’ p0 i )2
(Oi âˆ’ N p0 i )2
(Î¾ N i âˆ’ p0 i )2
(Î¾
âˆ’ p0 k )2 i
Ni
=N
=N
+ Nk
N p0 i
p0 i
p0 i
p0 k
i=1
i=1
i=1
P
kâˆ’1
kâˆ’1
h X (Î¾ âˆ’ p )2
( i=1
(Î¾ N i âˆ’ p0 i ))2 i
0i
Ni
=N
+
p0 i
p0 k
i=1

=N

kâˆ’1
X
i1 ,i2


1
1 
(Î¾ N i1 âˆ’ p0 i1 )(Î¾ N i2 âˆ’ p0 i2 ) Ii1 ,i2
+
,
p0 i 1
p0 k
=1

where Ii1 i2 is equal to 1 if i1 = i2 and equal to zero otherwise. Finally, from the above equalities, recalling
(S1.1) and (S1.2), we obtain
1
N 1âˆ’2e

k
X
(Oi âˆ’ N p0 i )2
âˆ—
âˆ—
d
= Î»N 2e (Î¾N âˆ’ pâˆ— )> (Î“âˆ— )âˆ’1 (Î¾N âˆ’ pâˆ— ) âˆ’â†’ Î»W0 = Wâˆ— ,
N
p
0i
i=1

S1

where 1 âˆ’ 2e â‰¥ 0 and W0 is a random variable with distribution Ï‡2 (k âˆ’ 1) = Î“((k âˆ’ 1)/2, 1/2), where Î“(a, b)
denotes the Gamma distribution with density function
f (w) =

ba aâˆ’1 âˆ’bw
w
e
.
Î“(a)

As a consequence, Wâˆ— has distribution Î“((k âˆ’ 1)/2, 1/(2Î»)).

S1.2

A preliminary central limit theorem

The following preliminary central limit theorem is useful for the proofs of the other central limit theorems
stated in [S2] and in Section S2.
Theorem S1.1. If
N
1 X
P
diag(Ïˆnâˆ’1 ) âˆ’ Ïˆnâˆ’1 Ïˆnâˆ’1 > âˆ’â†’ V ,
N n=1

(S1.3)

where V is a random variable with values in the space of positive semidefinite k Ã— k-matrices, then
âˆš
 âˆš
 s
N ÂµN âˆ’ Î¸ N âˆ’1 = N Î¾N âˆ’ Ïˆ N âˆ’1 âˆ’â†’ N (0, V ) .
Proof. We can write
âˆš

N


1
1 X
N Î¾N âˆ’ Ïˆ N âˆ’1 = âˆš N Î¾N âˆ’ Ïˆ N âˆ’1 = âˆš
(Î¾n âˆ’ Ïˆnâˆ’1 )
N
N n=1
N
N
X
1 X
= âˆš
âˆ†Mn =
YN,n ,
N n=1
n=1

P
with YN,n = N âˆ’1/2 âˆ†Mn . For the convergence of N
n=1 YN,n , we observe that E[YN,k |Fkâˆ’1 ] = 0 and so,
by Theorem S6.1, it converges stably to N (0, V ) if the conditions (c1)âˆšand (c2) hold true. Regarding (c1),
we note that max1â‰¤nâ‰¤N |YN,n | â‰¤ âˆš1N max1â‰¤nâ‰¤N |Î¾n âˆ’ Ïˆnâˆ’1 | = O(1/ N ) â†’ 0. Condition (c2) means
N
X
n=1

YN,n Y >
N,n =

N
1 X
P
(Î¾n âˆ’ Ïˆnâˆ’1 )(Î¾n âˆ’ Ïˆnâˆ’1 )> âˆ’â†’ V.
N n=1

The above convergence
holds true by Assumption
P (S1.3) and Lemma S4.2 (with cn = n and vN,n = n/N ).
P
Indeed, we have nâ‰¥1 E[kÎ¾n âˆ’ Ïˆnâˆ’1 k2 ]/n2 â‰¤ nâ‰¥1 nâˆ’2 < +âˆ and
E[(Î¾n âˆ’ Ïˆnâˆ’1 )(Î¾n âˆ’ Ïˆnâˆ’1 )> |Fnâˆ’1 ] = diag(Ïˆnâˆ’1 ) âˆ’ Ïˆnâˆ’1 Ïˆnâˆ’1 > .

Remark S1.2. Recalling that Ïˆn = Î¸n + p0 , the convergence (S1.3) with V = Î“ = diag(p0 ) âˆ’ p0 p0 > , means
Î¸ N âˆ’1 =

N
1 X
P
Î¸nâˆ’1 âˆ’â†’ 0
N n=1

and

N
1 X
P
Î¸nâˆ’1 Î¸nâˆ’1 > âˆ’â†’ 0kÃ—k ,
N n=1

where 0kÃ—k is the null matrix with dimension k Ã— k.

S1.3

Proof of Proposition 7.1

By Lemma S4.2 (with cn = n and vN,n = n/N ), Remark S4.3 and Theorem S5.1,Pwe immediately get
Î¾N â†’ p0 almost surely. Indeed, we have E[Î¾n+1 |Fn ] = Ïˆn â†’ p0 almost surely and nâ‰¥1 E[kÎ¾n k2 ]nâˆ’2 â‰¤
P
âˆ’2
< +âˆ.
nâ‰¥1 n
Regarding the central limit theorem for Î¾N , we have to distinguish the two cases 1/2 <  â‰¤ 1 or
0 <  â‰¤ 1/2. In the first case, the result follows from Theorem S5.3, because (2.9) and the fact that

S2

E[âˆ†Mn+1 âˆ†Mn+1 > |Fn ] = diag(Ïˆnâˆ’1 ) âˆ’ Ïˆnâˆ’1 Ïˆnâˆ’1 > â†’ Î“ almost surely; while for the second case the
result follows from Theorem S1.1. Indeed, we have
âˆš
 âˆš

 âˆš
N Î¾N âˆ’ p0 = N Î¾N âˆ’ Ïˆ N âˆ’1 + N Ïˆ N âˆ’1 âˆ’ p0
âˆš
 âˆš
= (c + 1) N Î¾N âˆ’ Ïˆ N âˆ’1 âˆ’ N DN ,
âˆš



where D N = c Î¾N âˆ’ Ïˆ N âˆ’1 âˆ’ Ïˆ N âˆ’1 âˆ’ p0 . By Theorem S1.1, the term (c + 1) N Î¾N âˆ’ Ïˆ N âˆ’1 stably
2
converges to N (0, (c + 1) Î“) (note that assumption (S1.3) is satisfiedâˆšwith V = Î“, because Ïˆn â†’ p0 almost
surely). Therefore, in order to conclude, it is enough to show that N DN converges in probability to 0.
To this purpose, we observe that, by (2.7) with Î´n = cn , we have
Ïˆn âˆ’ Ïˆnâˆ’1 = nâˆ’1 [c(Î¾n âˆ’ Ïˆnâˆ’1 ) âˆ’ (Ïˆnâˆ’1 âˆ’ p0 )]
and so
DN =

N
1 X Ïˆn âˆ’ Ïˆnâˆ’1
.
N n=1
nâˆ’1

P
Moreover, we note that +âˆ
n=1 (Ïˆn âˆ’ Ïˆnâˆ’1 ) = limN ÏˆN âˆ’ Ïˆ0 = p0 âˆ’ Ïˆ0 < +âˆ and, by Lemma S4.1 (with
vN,n = N âˆ’1 /nâˆ’1 ), we get
N
X
Ïˆn âˆ’ Ïˆnâˆ’1 a.s.
âˆ’â†’ 0.
N âˆ’1
nâˆ’1
n=1
For  â‰¤ 1/2, this fact implies
âˆš

N DN = âˆš

N
âˆ’1
X
Ïˆn âˆ’ Ïˆnâˆ’1 a.s.
1
N âˆ’1
âˆ’â†’ 0 .
nâˆ’1
N N âˆ’1
n=1

The proof is thus concluded.

S2

Case

P

n n

< +âˆ

P
In this section we provide some results regarding the case n n < +âˆ, even if, as we will see, this case
is not interesting for the chi-squared test of goodness of fit. Indeed, as shown in the following result, the
empirical mean almost surely converges to a random variable, which does not coincide almost surely with a
deterministic vector.
P
a.s.
Theorem S2.1. If +âˆ
n=0 n < +âˆ, then Î¾ N âˆ’â†’ Ïˆâˆ , where Ïˆâˆ is a random variable, which is not almost
surely equal to a deterministic vector, that is P (Ïˆâˆ 6= q0 ) > 0 for all q0 âˆˆ Rk .
P
Proof. When +âˆ
n=0 n < +âˆ, the sequence (Ïˆn ) is a (bounded) non-negative almost supermartingale (see
[S17]) because, by (2.7), we have
E[Ïˆn+1 |Fn ] = Ïˆn (1 âˆ’ n ) + n p0 â‰¤ Ïˆn + n p0 .
As a consequence, it converges almost surely (and in Lp with p â‰¥ 1) to a certain random
P variable Ïˆâˆ . An
alternativeP
proof of this fact follows from quasi-martingale theory [S12]: indeed, since n E[ |E[Ïˆn+1 |Fn ] âˆ’
Ïˆn | ] = O( n n ) < +âˆ, the stochastic process (Ïˆn ) is a non-negative quasi-martingale and so it converges
almost surely (and in Lp with p â‰¥ 1) to a certain random variable Ïˆâˆ .
The almost sure convergence of Î¾n to Ïˆâˆ follows by Lemma S4.2
(with cn = n and
P and Remark S4.3 P
vN,n = n/N ), because E[Î¾n+1 |Fn ] = Ïˆn â†’ Ïˆâˆ almost surely and nâ‰¥1 E[kÎ¾n k2 ]nâˆ’2 â‰¤ nâ‰¥1 nâˆ’2 < +âˆ.
In order to show that Ïˆâˆ is not almost surely equal to a deterministic vector, we set
yn = E[kÏˆn âˆ’ p0 k2 ] âˆ’ kE[Ïˆn âˆ’ p0 ]k2 =

k
X

V ar[Ïˆn i âˆ’ p0 i ]

i=1

and observe that, starting from (2.7), we get
Ïˆn+1 âˆ’ p0 = (1 âˆ’ n )(Ïˆn âˆ’ p0 ) + Î´n âˆ†Mn+1

S3

and so
kE[Ïˆn âˆ’ p0 ]k2 = E[Ïˆn âˆ’ p0 ]> E[Ïˆn âˆ’ p0 ] = (1 âˆ’ n )2 kE[Ïˆn âˆ’ p0 ]k2
and
E[kÏˆn+1 âˆ’ p0 k2 ] = E[(Ïˆn+1 âˆ’ p0 )> (Ïˆn+1 âˆ’ p0 )]
= (1 âˆ’ n )2 E[kÏˆn âˆ’ p0 k2 ] + Î´n2 E[kâˆ†Mn+1 k2 ] .
Hence, we obtain
yn+1 = (1 âˆ’ n )2 yn + Î´n2 E[kâˆ†Mn+1 k2 ] = (1 âˆ’ 2n )yn + Î¶en

(S2.1)

2
with Î¶en =
yn + Î´n2 E[kâˆ†Mn+1 k2 ] â‰¥ 0. It
QNnâˆ’1
yN â‰¥ ynÌƒ n=nÌƒ (1 âˆ’ 2n ) for each N â‰¥ nÌƒ and

E[kÏˆâˆ âˆ’ p0 k2 ] âˆ’ kE[Ïˆâˆ âˆ’ p0 ]k2 = yâˆ

follows that, given nÌƒ such that n < 1/2 for n â‰¥ nÌƒ, we have
so
!
+âˆ
+âˆ
Y
X
= lim yN â‰¥ ynÌƒ
(1 âˆ’ 2n ) = ynÌƒ exp
ln(1 âˆ’ 2n ) .
N â†’+âˆ

n=nÌƒ

P+âˆ

n=nÌƒ

P+âˆ

The above exponential is strictly greater than 0 because n=nÌƒ ln(1 âˆ’ 2n ) âˆ¼ âˆ’2 n=nÌƒ n > âˆ’âˆ. Therefore,
if ynÌƒ > 0, then we have yâˆ > 0. This means that Ïˆâˆ âˆ’ p0 , and consequently Ïˆâˆ , is not almost surely
equal to a deterministic vector, that is P (Ïˆâˆ 6= q0 ) > 0 for all q0 âˆˆ Rk . If ynÌƒ = 0, that is if ÏˆnÌƒ is almost
e then, by (S2.1), we get
surely equal to a deterministic vector Ïˆ,
e 2] > 0 ,
ynÌƒ+1 = Î´n2 E[kâˆ†Mn+1 k2 ] = Î´nÌƒ2 E[kÎ¾nÌƒ+1 âˆ’ Ïˆk
e is different from a vector of the canonical base of Rk by means of the
because Î´n > 0 for each n and Ïˆ
assumption b0 i + B0 i > 0 and equality (2.4). It follows that we can repeat the above argument replacing nÌƒ
by nÌƒ + 1 and conclude that Ïˆâˆ is not almost surely equal to a deterministic vector.
As a consequence of the above theorem, if P
we aim at having the almost sure convergence of Î¾N to a
deterministic vector, we have to avoid the case +âˆ
n=0 n < +âˆ. However, for the sake of completeness, we
provide a second-order convergence result also in this case. First, we note that Theorem S1.1 still holds true
with V = diag(Ïˆâˆ ) âˆ’ Ïˆâˆ Ïˆâˆ > . Indeed, assumption (S1.3) is satisfied by Lemma S4.2 and Remark S4.3
(with cn = n and vN,n = n/N ), because of the almost sure convergence of Ïˆn to Ïˆâˆ . Moreover, we have
the following theorem:
Theorem S2.2. Suppose to be in one of the following two cases:
âˆš
âˆš
PN
PN
a)
n=1 nnâˆ’1 = o( N ) and
n=1 nÎ´nâˆ’1 = o( N );
b) n = (n + 1)âˆ’ and Î´n âˆ¼ c(n + 1)âˆ’Î´ with c > 0, Î´ âˆˆ (1/2, 1) and  > Î´ + 1/2 ( = +âˆ included, that
means n = 0 for all n).
Set e = 1/2 and Î» = 1 in case a) and e = Î´ âˆ’ 1/2 âˆˆ (0, 1/2) and Î» = c2 /[2(1 âˆ’ e)] = c2 /(3 âˆ’ 2Î´) in case b).
Then, we have
 s
N e Î¾N âˆ’ ÏˆN âˆ’â†’ N (0, Î»Î“) ,
where Î“ = diag(Ïˆâˆ ) âˆ’ Ïˆâˆ Ïˆâˆ > .
When (ÏˆN âˆ’ Ïˆâˆ ) = oP (N âˆ’e ), we also have
 s
N e Î¾N âˆ’ Ïˆâˆ âˆ’â†’ N (0, Î»Î“) .
Note that case a) covers the case n = (n + 1)âˆ’ and Î´n âˆ¼ c(n + 1)âˆ’Î´ with c > 0 and min{, Î´} > 3/2.
The case n = 0 (that is Î²n = 1) for all n corresponds to the case considered in [S15], but in that paper
the author studies only the limit Ïˆâˆ and he does not provide second-order convergence results.
Proof. We have

N e Î¾N âˆ’ ÏˆN =

1

1
N 1âˆ’e
=


N Î¾N âˆ’ N ÏˆN =

N 1âˆ’e
N
X

(Î¾n âˆ’ Ïˆnâˆ’1 ) +

n=1

1
N 1/2âˆ’e

N
X
n=1

YN,n +

[Î¾n âˆ’ Ïˆnâˆ’1 + n(Ïˆnâˆ’1 âˆ’ Ïˆn )]

n=1

N
X

1
N 1âˆ’e

N
X

N
X

1
N 1âˆ’e

nnâˆ’1 (Ïˆnâˆ’1 âˆ’ p0 ) âˆ’

n=1

ZN,n + QN ,

n=1

S4

1
N 1âˆ’e

N
X
n=1

nÎ´nâˆ’1 âˆ†Mn

where
YN,n =

Î¾n âˆ’ Ïˆnâˆ’1
âˆ†Mn
âˆš
= âˆš
,
N
N

ZN,n = âˆ’

nÎ´nâˆ’1 (Î¾n âˆ’ Ïˆnâˆ’1 )
nÎ´nâˆ’1 âˆ†Mn
=
N 1âˆ’e
N 1âˆ’e

and
QN =

1
N 1âˆ’e

N
X

nnâˆ’1 (Ïˆnâˆ’1 âˆ’ p0 ).

n=1

P
1âˆ’e
) and so QN converges almost surely to 0. Moreover,
In both cases a) and b), we have N
n=1 nnâˆ’1 = o(N
PN
by Theorem S1.1, n=1 YN,n stable converges to N (0, V ) with V = Î“ = diag(Ïˆâˆ ) âˆ’ Ïˆâˆ Ïˆâˆ > . Therefore
P
it is enough to study the convergence of N
n=1 ZN,n . To this purpose, we observe that, if we are in case a),
PN
then n=1 ZN,n converges almost surely to 0 and so
âˆš
 s
N Î¾N âˆ’ ÏˆN âˆ’â†’ N (0, Î“).
PN
Otherwise, if we are in case b), we observe that E[ZN,n |Fnâˆ’1 ] = 0 and so
n=1 ZN,n converges stably
to N (0, Î»Î“) if the conditions (c1) and (c2) of Theorem S6.1, with V = Î»Î“, hold
âˆš true. Regarding (c1), we
1
observe that max1â‰¤nâ‰¤N |ZN,n | â‰¤ N 1âˆ’e
max1â‰¤nâ‰¤N nÎ´nâˆ’1 |Î¾n âˆ’ Ïˆnâˆ’1 | = O(1/ N ). Regarding condition
(c2), that is
N
X

ZN,n ZN,n > =

n=1

N
X

1
N 2(1âˆ’e)

P

2
n2 Î´nâˆ’1
(Î¾n âˆ’ Ïˆnâˆ’1 )(Î¾n âˆ’ Ïˆnâˆ’1 )> âˆ’â†’

n=1

c2
Î“,
2(1 âˆ’ e)

PN
2 2
2
2
1
we observe that it holds true even almost surely, because N 2(1âˆ’e)
n=1 n Î´nâˆ’1 â†’ c /[2(1 âˆ’ e)] = c /(3 âˆ’ 2Î´)
and
a.s.
E[(Î¾n âˆ’ Ïˆnâˆ’1 )(Î¾n âˆ’ Ïˆnâˆ’1 )> |Fnâˆ’1 ] = diag(Ïˆnâˆ’1 ) âˆ’ Ïˆnâˆ’1 Ïˆnâˆ’1 > âˆ’â†’ Î“
2
(see Lemma S4.2 and Remark S4.3 with cn = n and vN,n = n3 Î´nâˆ’1
/N 2(1âˆ’e) âˆ¼ c2 (n/N )3âˆ’2Î´ ). Therefore, we
have
 s

N e Î¾N âˆ’ ÏˆN âˆ’â†’ N 0, c2 (3 âˆ’ 2Î´)âˆ’1 Î“ .

Finally, we observe that


N e Î¾N âˆ’ Ïˆâˆ = N e Î¾N âˆ’ ÏˆN + N e (ÏˆN âˆ’ Ïˆâˆ ) .
Therefore, when (ÏˆN âˆ’ Ïˆâˆ ) = oP (N âˆ’e ), we have
 s
N e Î¾N âˆ’ Ïˆâˆ âˆ’â†’ N (0, Î»Î“) .

An example of the case a) of Theorem S2.2 with (ÏˆN âˆ’ Ïˆâˆ ) = oP (N âˆ’e ) is the RP urn with Î±n = Î± > 0
and Î²n = Î² > 1 (see [S1]). Indeed, in this case, we have n âˆ¼ c Î² âˆ’n and Î´n âˆ¼ cÎ´ Î² âˆ’n , where c > 0
and cÎ´ > 0 are suitable constants, and (ÏˆN âˆ’ Ïˆâˆ ) = O(Î² âˆ’N ). We conclude this section with other two
examples regarding the case n = 0 (that is Î²n = 1) for all n.
Example S2.3. (Case n = 0 and Î´n âˆ¼ c(n + 1)âˆ’Î´Pwith c > 0 and Î´ > 3/2)
n
âˆ’Î´
If n = 0 for all n, then we have rnâˆ— = |b0 | + |B0 | + P
, with Î´ > 3/2,
h=1 Î±h . Therefore, if we take Î±n = n
âˆ’Î´
âˆ—
âˆ’Î´
then rnâˆ— converges to the constant râˆ— = |b0 | + |B0 | + +âˆ
h
and
Î´
=
Î±
/r
âˆ¼
cÎ±
,
n
n+1
n+1 = c(n + 1)
n+1
h=1
âˆ—
with
c
=
1/r
.
Moreover,
since
Î´
>
3/2,
assumption
a)
of
Theorem
S2.2
is
satisfied.
We
also
observe
that
P 2
n Î´n < +âˆ and so Ïˆâˆ i is not concentrated on {0, 1} and has no atoms in (0, 1) (see [S15, Th. 2 and
Th. 3]). More precisely, we have
P
b0 + B0 + +âˆ
n=1 Î±n Î¾n
Ïˆâˆ =
P
|b0 | + |B0 | + +âˆ
n=1 Î±n
and so
ÏˆN âˆ’ Ïˆâˆ =
P
P
PN
P
(b0 + B0 + N
n=1 Î±n Î¾n )
nâ‰¥N +1 Î±n âˆ’ (|b0 | + |B0 | +
n=1 Î±n )
nâ‰¥N +1 Î±n Î¾n
=
P
P+âˆ
(|b0 | + |B0 | + N
Î±
)(|b
|
+
|B
|
+
Î±
)
0
0
n=1 n
n=1 n
ï£«
ï£¶


X
Oï£­
Î±n ï£¸ = O N 1âˆ’Î´ .
nâ‰¥N +1

S5

Since Î´ > 3/2, we get (ÏˆN âˆ’ Ïˆâˆ ) = o(N âˆ’1/2 ). This fact can also be obtained as a consequence of Theorem
S2.5 below. Indeed, this theorem states that the rate of convergence of ÏˆN to Ïˆâˆ is N âˆ’(Î´âˆ’1/2) .
Note that, since Î²n = 1 for all n, the factor f (h, n) in (2.5) coincides with Î±h and so, in this case, it is
decreasing.
Example S2.4. (Case n = 0 and Î´n âˆ¼ c(n + 1)âˆ’Î´ with c > 0 and Î´ âˆˆ (1/2, 1))
P
As in the
since n = 0 for all n, we have rnâˆ— = |b0 | + |B0 | + n
h=1 Î±h . Let us set
Pn previous example,
Î±
âˆ—
An =
h=1 Î±h = exp(bn ) with b > 0 and Î± âˆˆ (0, 1/2), which brings to rn âˆ¼ An â†‘ +âˆ and Î±n =
exp(bnÎ± ) âˆ’ exp(b(n âˆ’ 1)Î± ) and
Pnâˆ’1
Î±h
Î±n
Î´nâˆ’1 =
âˆ¼ 1 âˆ’ Ph=1
n
Î±
|b0 | + |B0 | + An
h=1 h
= 1 âˆ’ exp [b ((n âˆ’ 1)Î± âˆ’ nÎ± )]



= bnÎ± 1 âˆ’ (1 âˆ’ nâˆ’1 )Î± + O n2Î± (1 âˆ’ (1 âˆ’ nâˆ’1 )Î± )2 = bnÎ± Î±nâˆ’1 + O(nâˆ’2 ) + O(nâˆ’(2âˆ’2Î±) )
= bÎ±nâˆ’(1âˆ’Î±) + O(nâˆ’(2âˆ’Î±) ) + O(nâˆ’(2âˆ’2Î±) ) = bÎ±nâˆ’(1âˆ’Î±) + O(nâˆ’2(1âˆ’Î±) ),
so that Î´ = (1 âˆ’ Î±) âˆˆ (1/2, 1) and c = bÎ± > 0. P
Hence, we have Î´n âˆ¼ c(n + 1)âˆ’Î´ and assumption b) of
2
Theorem S2.2 is satisfied. We also observe that
n Î´n < +âˆ and so Ïˆâˆ i is not concentrated on {0, 1}
and has no atoms in (0, 1) (see [S15,
 Th. 2 and Th. 3]). Moreover, by Theorem S2.5 below, we get that
N e (Ïˆ N âˆ’ Ïˆâˆ ) âˆ’â†’N 0, c2 (2e)âˆ’1 Î“ , where e = Î´ âˆ’ 1/2. Hence, applying Theorem S6.3, we obtain

 s
N e Î¾N âˆ’ Ïˆâˆ âˆ’â†’ N 0, c2 [2e(1 âˆ’ e)]âˆ’1 Î“ .
Finally, note that, as before, since Î²n = 1 for all n, the factor f (h, n) in (2.5) coincides with Î±h and so, in
this case, `(h) = ln(f (h, n)) = ln(Î±h ) âˆ¼ ln(Î´hâˆ’1 ) + bhÎ± âˆ¼ bhÎ± âˆ’ bÎ±(1 âˆ’ Î±) ln(h). Hence, there exists hâˆ— such
that h 7â†’ `(h) is increasing for h â‰¥ hâˆ— . Since maxhâ‰¤hâˆ— `(h) â‰¤ C, for a suitable constant C, the contributions
of the observations until hâˆ— are eventually smaller than those with h â‰¥ hâˆ— , that are increasing with h.
Theorem S2.5. For n = 0 for all n and Î´n âˆ¼ c(n + 1)âˆ’Î´ with c > 0 and 1/2 < Î´ â‰¤ 1, we have

1
N Î´âˆ’ 2 (Ïˆ N âˆ’ Ïˆâˆ ) âˆ’â†’N 0, c2 (2Î´ âˆ’ 1)âˆ’1 Î“
stably in the strong sense w.r.t. F,
where Î“ = diag(Ïˆâˆ ) âˆ’ Ïˆâˆ Ïˆâˆ > .
Proof. We want to apply Theorem S6.2. To this purpose, we recall that, when n = 0 for all n, the process
(Ïˆn ) is a martingale with respect to F. Moreover, it converges almost surely and in mean to Ïˆâˆ . Therefore,
in order to conclude, it is enough to check conditions (c1) and (c2) of Theorem S6.2. Regarding the first
condition, we note that
N Î´âˆ’1/2 sup |Ïˆn âˆ’ Ïˆn+1 | = N Î´âˆ’1/2 sup Î´n |âˆ†Mn+1 | = O(N Î´âˆ’1/2âˆ’Î´ ) = O(N âˆ’1/2 ) âˆ’â†’ 0.
nâ‰¥N

nâ‰¥N

Finally, regarding the second condition, we observe that
X
X
N 2Î´âˆ’1
(Ïˆn âˆ’ Ïˆn+1 )(Ïˆn âˆ’ Ïˆn+1 )> âˆ¼ N 2Î´âˆ’1 c2
(n + 1)âˆ’2Î´ (âˆ†Mn+1 )(âˆ†Mn+1 )>
nâ‰¥N

nâ‰¥N
a.s.

âˆ’â†’

2

c
Î“,
(2Î´ âˆ’ 1)

where the almost sure convergence follows from [S6, Lemma 4.1] and the fact that
a.s.

E[(âˆ†Mn+1 )(âˆ†Mn+1 )> |Fn ] = E[(Î¾n+1 âˆ’ Ïˆn )(Î¾n+1 âˆ’ Ïˆn )> |Fn ] âˆ’â†’ Î“.

S3

Computations regarding the local reinforcement

Suppose Î±n âˆ¼ anâˆ’Î± for n â‰¥ 1 and (1Qâˆ’ Î²n ) âˆ¼ b(n + 1)âˆ’Î² for n â‰¥ 0. In the following subsections we study the
behaviour of the factor f (h, n) = Î±h nâˆ’1
j=h Î²j in some particular cases that cover the cases of the two examples
Qnâˆ’1
P
in Section 4. Specifically, for all the considered cases, we set `(h, n) = ln(Î±h j=h
Î²j ) = ln(Î±h )+ nâˆ’1
j=h ln(Î²j )
for n â‰¥ h and we prove that there exists hâˆ— such that maxhâ‰¤hâˆ— `(h, n) â‰¤ `(hâˆ— , n) and h 7â†’ `(h, n) is increasing
for h â‰¥ hâˆ— . This means that the weights f (h, n) of the observations until hâˆ— are smaller than those with
h â‰¥ hâˆ— and the contribution of the observation for h â‰¥ hâˆ— is increasing with h.

S6

S3.1

Case Î± = Î² âˆˆ (0, 1)

Suppose Î±n = anâˆ’Î± and 1 âˆ’ Î²n = b(n + 1)âˆ’Î± , with a, b > 0 and Î± âˆˆ (0, 1). For n â‰¥ h, we have
`(h + 1, n) âˆ’ `(h, n) = ln(a(h + 1)âˆ’Î± ) âˆ’ ln(ahâˆ’Î± ) âˆ’ ln(1 âˆ’ b(h + 1)âˆ’Î± )



1
b
Î±
b
= âˆ’Î± ln 1 +
âˆ’ ln 1 âˆ’
=âˆ’ +
.
Î±
h
(h + 1)
h
(h + 1)Î±
Since Î± < 1, there exists h0 such that the function h 7â†’ `(h, n) is monotonically increasing for h â‰¥ h0 . Now,
âˆ’Î±
fix Î· > 0 and let j0 such that j â‰¥ j0 implies ln(Î²j ) â‰¤ âˆ’ bj1+Î· . Then take hâˆ— â‰¥ max(h0 , j0 ) + 1 and h â‰¤ h0 âˆ’ 1.
For hâˆ— large enough, we get
`(hâˆ— , n) âˆ’ `(h, n) = ln(Î±hâˆ— ) âˆ’ ln(Î±h ) âˆ’

hX
âˆ— âˆ’1

âˆ’Î±
ln(Î²j ) = ln(ahâˆ’Î±
)âˆ’
âˆ— ) âˆ’ ln(ah

j=h

â‰¥ ln(hâˆ’Î±
âˆ— )+

hX
âˆ— âˆ’1
j=max(h0 ,j0 )

â‰¥ âˆ’Î± ln(hâˆ— ) + C1 +

hX
âˆ— âˆ’1

ln(Î²j )

j=h

bj âˆ’Î±
1+Î·
Z hâˆ— âˆ’1

b
1+Î·

xâˆ’Î± dx

max(h0 ,j0 )



b
(hâˆ— âˆ’ 1)1âˆ’Î± âˆ’ max(h0 , j0 )1âˆ’Î±
(1 + Î·)(1 âˆ’ Î±)
b
= C2 âˆ’ Î± ln(hâˆ— ) +
(hâˆ— âˆ’ 1)1âˆ’Î± â‰¥ 0 .
(1 + Î·)(1 âˆ’ Î±)
= âˆ’Î± ln(hâˆ— ) + C1 +

Therefore, taking hâˆ— large enough, we have maxhâ‰¤hâˆ— `(h, n) = maxhâ‰¤h0 âˆ’1 `(h, n) âˆ¨ maxh0 â‰¤hâ‰¤hâˆ— `(h, n) â‰¤
`(hâˆ— , n).

S3.2

Case Î± = Î² = 1

Suppose Î±n = anâˆ’1 and 1 âˆ’ Î²n = b(n + 1)âˆ’1 , with a > 0 and b > 1. For n â‰¥ h, we have
`(h + 1, n) âˆ’ `(h, n) = ln(a(h + 1)âˆ’1 ) âˆ’ ln(ahâˆ’1 ) âˆ’ ln(1 âˆ’ b(h + 1)âˆ’1 )


b 
bâˆ’1
1
âˆ’ ln 1 âˆ’
=
+ o(hâˆ’1 ).
= âˆ’ ln 1 +
h
(h + 1)
h+1
Since b > 1, we can argue as in the previous subsection. Therefore, there exists h0 such that the function
h 7â†’ `(h, n) is monotonically increasing for h â‰¥ h0 . Now, fix Î· = (b âˆ’ 1)/(b + 1) > 0 and let j0 such that
âˆ’1
j â‰¥ j0 implies ln(Î²j ) â‰¤ âˆ’ bj
. Then take hâˆ— â‰¥ max(h0 , j0 ) + 1 and h â‰¤ h0 âˆ’ 1. For hâˆ— large enough, we get
1+Î·
`(hâˆ— , n) âˆ’ `(h, n) = ln(Î±hâˆ— ) âˆ’ ln(Î±h ) âˆ’

hX
âˆ— âˆ’1

âˆ’1
ln(Î²j ) = ln(ahâˆ’1
)âˆ’
âˆ— ) âˆ’ ln(ah

j=h

â‰¥ ln(hâˆ’1
âˆ— )+

hX
âˆ— âˆ’1
j=max(h0 ,j0 )

â‰¥ âˆ’ ln(hâˆ— ) + C1 +
= âˆ’ ln(hâˆ— ) + C1 +

b
1+Î·

hX
âˆ— âˆ’1

ln(Î²j )

j=h

bj âˆ’1
1+Î·
Z hâˆ— âˆ’1

xâˆ’1 dx

max(h0 ,j0 )


b 
ln(hâˆ— âˆ’ 1) âˆ’ ln(max(h0 , j0 ))
(1 + Î·)

bâˆ’1âˆ’Î·
ln(hâˆ— ) âˆ’ O(1/hâˆ— )
(1 + Î·)
b(b âˆ’ 1)
= C2 +
ln(hâˆ— ) âˆ’ O(1/hâˆ— ) â‰¥ 0 .
2b

= C2 +

Therefore, taking hâˆ— large enough, we have maxhâ‰¤hâˆ— `(h, n) = maxhâ‰¤h0 âˆ’1 `(h, n) âˆ¨ maxh0 â‰¤hâ‰¤hâˆ— `(h, n) â‰¤
`(hâˆ— , n).

S7

S3.3

Case 0 < Î± < Î² < (1 + Î±)/2

Suppose


c3
+ O(1/n2âˆ’Î² )
n
and 1 âˆ’ Î²n = b(n + 1)âˆ’Î² , with a, b > 0, 0 < Î± < Î² < (1 + Î±)/2 and c1 , c2 , c3 âˆˆ R. Set Î³ = Î² âˆ’ Î± âˆˆ (0, 1/2).
For n â‰¥ h, we have

Î±n = anâˆ’Î± 1 +

c1
n1âˆ’Î²

+

c2
nÎ²âˆ’Î±

+

`(h + 1, n) âˆ’ `(h, n) = ln(a(h + 1)âˆ’Î± ) âˆ’ ln(ahâˆ’Î± ) âˆ’ ln(1 âˆ’ b(h + 1)âˆ’Î² )
+ ln 1 + c1 /(h + 1)1âˆ’Î² + c2 /(h + 1)Î³ + c3 /(h + 1) + O(1/h2âˆ’Î² )

âˆ’ ln 1 + c1 /h1âˆ’Î² + c2 /hÎ³ + c3 /h + O(1/h2âˆ’Î² ) .



(S3.1)

Now, we aim at obtaining a series expansion with a reminder term of the type o(1/hÎ² ). Since Î² < 1, the
first three terms of the right-hand side of the above equation give



b
b
1
ln(a(h + 1)âˆ’Î± ) âˆ’ ln(ahâˆ’Î± ) âˆ’ ln(1 âˆ’ b(h + 1)âˆ’Î² ) = âˆ’Î± ln 1 +
âˆ’ ln 1 âˆ’
=
+ o(hâˆ’Î² ).
Î²
h
(h + 1)
(h + 1)Î²
We deal now with the last two terms of (S3.1). We recall that
ln(1 + x) = x âˆ’

x2
x3
xj
+
+ Â· Â· Â· + (âˆ’1)jâˆ’1
+ o(xj ) ,
2
3
j

and therefore, since 2 âˆ’ Î² = 1 + 1 âˆ’ Î² > 1 > Î² and j(1 âˆ’ Î²) > Î² and jÎ³ = j(Î² âˆ’ Î±) > Î² for j large enough,
there are only a finite number J0 of terms with an order Ï„j â‰¤ Î². In other words, we can write

ln 1 + c1 /(h + 1)1âˆ’Î² + c2 /(h + 1)Î³ + c3 /(h + 1) + O(1/n2âˆ’Î² )

âˆ’ ln 1 + c1 /h1âˆ’Î² + c2 /hÎ³ + c3 /h + O(1/n2âˆ’Î² )
=

J0
X

Cj (h + 1)âˆ’Ï„j âˆ’

=

Cj hâˆ’Ï„j + o(1/hÎ² )

j=1

j=1
J0
X

J0
X

J0
X




Cj hâˆ’Ï„j (1 + hâˆ’1 )âˆ’Ï„j âˆ’ 1 + o(1/hÎ² )
Cj (h + 1)âˆ’Ï„j âˆ’ hâˆ’Ï„j + o(1/hÎ² ) =
j=1

j=1
J0

=

X


Cj hâˆ’Ï„j (Ï„j hâˆ’1 + o(1/h) + o(1/hÎ² ) = o(1/hÎ² ) .

j=1

Summing up, we have
b
+ o(hâˆ’Î² ).
(h + 1)Î²
Then there exists h0 such that the function h 7â†’ `(h, n) is monotonically increasing for h â‰¥ h0 . Now, fix
âˆ’Î²
. Then take hâˆ— â‰¥ max(h0 , j0 ) + 1 and h â‰¤ h0 âˆ’ 1.
Î· > 0 and let j0 such that j â‰¥ j0 implies ln(Î²j ) â‰¤ âˆ’ bj
1+Î·
Since Î² < (1 + Î±)/2, we have Î±n = anâˆ’Î± (1 + O(1/nÎ³ )) and so, for hâˆ— large enough, we get
`(h + 1, n) âˆ’ `(h, n) =

`(hâˆ— , n) âˆ’ `(h, n) = ln(Î±hâˆ— ) âˆ’ ln(Î±h ) âˆ’

hX
âˆ— âˆ’1

ln(Î²j )

j=h
âˆ’Î±
= ln(ahâˆ’Î±
) + ln(1 + O(hâˆ’Î³
âˆ— ) âˆ’ ln(ah
âˆ— )) + C1 âˆ’

hX
âˆ— âˆ’1

ln(Î²j )

j=h
hX
âˆ— âˆ’1

âˆ’Î³
â‰¥ ln(hâˆ’Î±
âˆ— ) + ln(1 + O(hâˆ— )) + C1 +

j=max(h0 ,j0 )

b
â‰¥ âˆ’Î± ln(hâˆ— ) + O(hâˆ’Î³
âˆ— ) + C2 +
1+Î·

Z

hâˆ— âˆ’1

bj âˆ’Î²
1+Î·
xâˆ’Î² dx

max(h0 ,j0 )



b
(hâˆ— âˆ’ 1)1âˆ’Î² âˆ’ max(h0 , j0 )1âˆ’Î²
(1 + Î·)(1 âˆ’ Î²)
b
âˆ’Î³
= C3 + O(hâˆ— ) âˆ’ Î± ln(hâˆ— ) +
(hâˆ— âˆ’ 1)1âˆ’Î² â‰¥ 0 .
(1 + Î·)(1 âˆ’ Î²)

= âˆ’Î± ln(hâˆ— ) + O(hâˆ’Î³
âˆ— ) + C2 +

S8

Therefore, taking hâˆ— large enough, we have maxhâ‰¤hâˆ— `(h, n) = maxhâ‰¤h0 âˆ’1 `(h, n) âˆ¨ maxh0 â‰¤hâ‰¤hâˆ— `(h, n) â‰¤
`(hâˆ— , n).

S4

Technical results

We recall the generalized Kronecker lemma [S3, Corollary A.1]:
Lemma S4.1. (Generalized Kronecker Lemma)
Let {vN,n : 1 â‰¤ n â‰¤ N } and (zn )n be respectively a triangular array and a sequence of complex numbers such
that vN,n 6= 0 and
lim vN,n = 0,

lim vn,n exists finite,
n

N

and

P

n

zn is convergent. Then limN

N
X

|vN,n âˆ’ vN,nâˆ’1 | = O(1)

n=1

PN

n=1

vN,n zn = 0.

The above corollary is useful to get the following result for complex random variables, which slightly
extends the version provided in [S3, Lemma A.2]:
Lemma S4.2. Let H = (Hn )n be a filtration and (Yn )n a H-adapted sequence

 random variables.
P of complex
Moreover, let (cn )n be a sequence of strictly positive real numbers such that n E |Yn |2 /c2n < +âˆ and let
{vN,n , 1 â‰¤ n â‰¤ N } be a triangular array of complex numbers such that vN,n 6= 0 and
lim vN,n = 0,
N

lim vn,n exists finite,
n

N
X

|vN,n âˆ’ vN,nâˆ’1 | = O(1) .

n=1

Suppose that
N
X

vN,n

n=1

where V is a suitable random variable. Then

E[Yn |Hnâˆ’1 ] P
âˆ’â†’ V,
cn

PN

n=1

(S4.1)

P

vN,n Yn /cn âˆ’â†’ V .

If the convergence in (S4.1) is almost sure, then also the convergence of
almost sure.

PN

n=1

vN,n Yn /cn toward V is

Proof. Consider the martingale (Mn )n defined by
Mn =
It is bounded in L2 since

P

n

E[|Yn |2 ]
c2
n

n
X
Yj âˆ’ E[Yj |Hjâˆ’1 ]
.
cj
j=1

< +âˆ by assumption and so it is almost surely convergent, that means

X Yn (Ï‰) âˆ’ E[Yn |Hnâˆ’1 ](Ï‰)
< +âˆ
cn
n
Y (Ï‰)âˆ’E[Yn |Hnâˆ’1 ](Ï‰)
,
cn

for Ï‰ âˆˆ B with P (B) = 1. Therefore, fixing Ï‰ âˆˆ B and setting zn = n
we get
N
X
Yn (Ï‰) âˆ’ E[Yn |Hnâˆ’1 ](Ï‰)
lim
vN,n
= 0,
N
cn
n=1
that is
N
X
n=1

vN,n

Yn âˆ’ E[Yn |Hnâˆ’1 ] a.s.
âˆ’â†’ 0.
cn

In order to conclude, it is enough to observe that
N
X

vN,n

n=1

N
N
X
Yn âˆ’ E[Yn |Hnâˆ’1 ] X
E[Yn |Hnâˆ’1 ]
Yn
=
vN,n
+
vN,n
cn
c
cn
n
n=1
n=1

and use assumption (S4.1).

S9

by Lemma S4.1,

PN |vN,n |
P
a.s.
vN,n
Remark S4.3. If we have
= O(1), limN N
= Î» âˆˆ C and E[Yn |Hnâˆ’1 ] âˆ’â†’ Y , then
n=1
n=1 cn
cn
(S4.1) is satisfied with almost sure convergence and V = Î»Y . Indeed, if we denote by A an event such that
P (A) = 1 and limn E[Yn |Hnâˆ’1 ](Ï‰) = Y (Ï‰) for each Ï‰ âˆˆ A, then we can fix Ï‰ âˆˆ A, set wn = E[Yn |Hnâˆ’1 ](Ï‰)
and w = Y (Ï‰), and apply the generalized Toeplitz lemma [S3, Lemma A.1] (with zN,n
n Î») and
P = vN,n /(c
wn
s = 1 when Î» 6= 0 and with zN,n = vN,n /cn and s = 0 when Î» = 0) in order to get N
n=1 vN,n cn â†’ Î»Y
almost surely.
The proof of the following lemma can be found in [S8]. We here rewrite the proof only for the readerâ€™s
convenience.
Lemma S4.4. ([S8], Lemma 18)
Let xn , Î¶n , Î³n be non-negative sequences such that Î³n â†’ 0,

P

n

Î³n = +âˆ and

xn â‰¤ (1 âˆ’ Î³n )xnâˆ’1 + Î³n Î¶n .
Then lim supn xn â‰¤ lim supn Î¶n .
Proof. Take L > lim supn Î¶n and nâˆ— large enough so that Î¶n < L and Î³n â‰¤ 1 when n â‰¥ nâˆ— . Then, using
that (x + y)+ â‰¤ x+ + y + , we have for n â‰¥ nâˆ—
(xn âˆ’ L)+ â‰¤ ((1 âˆ’ Î³n )(xnâˆ’1 âˆ’ L) + Î³n (Î¶n âˆ’ L))+
â‰¤ (1 âˆ’ Î³n )(xnâˆ’1 âˆ’ L)+ + Î³n (Î¶n âˆ’ L)+
â‰¤ (1 âˆ’ Î³n )(xnâˆ’1 âˆ’ L)+ .
P
+
Since
= 0. This is enough to conclude,
n Î³n = +âˆ, the above inequality implies that limn (xn âˆ’ L)
because we can choose L arbitrarily close to lim supn Î¶n .

S5

Some stochastic approximation results

Consider a stochastic process (Î¸n ) taking values in Î˜ = [âˆ’1, 1]k , adapted to a filtration F = (Fn )n and
following the dynamics
Î¸n+1 = (1 âˆ’ n )Î¸n + cn âˆ†Mn+1 ,
(S5.1)
where c > 0, (âˆ†Mn+1 )n is a uniformly bounded martingale difference sequence with respect to F and
P
fn+1 = câˆ†Mn+1 , equation
n = (n + 1)âˆ’ with  âˆˆ (0, 1] so that n â†’ 0 and n n = +âˆ. Setting âˆ†M
(S5.1) becomes
fn+1 .
Î¸n+1 = (1 âˆ’ n )Î¸n + n âˆ†M
Then:
a.s.

Theorem S5.1. In the above setting, we have Î¸N âˆ’â†’ 0 .
Proof. We have the following two cases:
P
â€¢  âˆˆ (1/2, 1] so that n 2n < +âˆ or
P
â€¢  âˆˆ (0, 1/2] so that n 2n = +âˆ.
For the first case, we refer to [S11, Cap. 5, Th. 2.1]. For the second case, we refer to [S11, Cap. 5, Th. 3.1]).
fn ) are uniformly bounded, the key assumption to be verified in order to
In this case, since (Î¸n ) and (âˆ†M
apply [S11, Cap. 5, Th. 3.1] is the â€œrate of changeâ€ condition (see [S11, p. 137]), that is
lim sup sup |M 0 (N + t) âˆ’ M 0 (N )| = 0,
N

a.s.

tâˆˆ[0,1]

Pm(t)âˆ’1
fj+1 and m(t) = inf{n : t < tn+1 = Pn j } (see [S11, p. 122]). Since
where M 0 (t) =
j âˆ†M
j=0
j=0
fn ) is uniformly bounded, the above condition is satisfied when the following simpler conditions are
(âˆ†M
satisfied (see [S11, pp. 139-141]):
P
(i) For each u > 0 n eâˆ’u/n < +âˆ;
(ii) For some T < +âˆ, there exists a constant c(T ) < +âˆ such that supnâ‰¤jâ‰¤m(tn +T )

S 10

j
n

â‰¤ c(T ).

âˆ’

When n = (1 + n)âˆ’ , condition (i) is obviously verified, because we have limn n2 /eu(1+n) = 0. Finally,
condition (ii) is always satisfied when n is decreasing, as it is in the case n = (1 + n)âˆ’ . Indeed, we simply
have supnâ‰¤jâ‰¤m(tn +T ) j /n = n /n = 1.
a.s.

Theorem S5.2. In the above setting, if we have E[âˆ†Mn+1 âˆ†Mn+1 > |Fn ] âˆ’â†’ Î“ with Î“ a symmetric positive
definite matrix, then we have
1
d
âˆš Î¸N âˆ’â†’ N (0, Î£),
N
where Î£ = c2 Î“/2 when  âˆˆ (0, 1) and Î£ = c2 Î“ when  = 1.
a.s.

Proof. We have Î¸N âˆ’â†’ 0 and 0 belongs to the interior part of Î˜. Moreover, we have
a.s. 2
fn+1 âˆ†M
fn+1 > |Fn ] âˆ’â†’
E[âˆ†M
c Î“.

For the case  âˆˆ (1/2, 1], we refer to [S9, Th. 2.1] (with h = Id, Uâˆ— = c2 Î“ and Î³âˆ— = 1) and [S14, Th. 1]
(with H = âˆ’Id, Î³n = Ïƒn = n and so Î³0 = 1 and Î² = ). For the case  âˆˆ (0, 1/2], we refer to [S11, cap.10,
âˆš
Th. 2.1] (with A = âˆ’Id). The key assumption for applying this theorem is Î¸n / n tight. On the other
hand, in the considered setting, this last condition is satisfied because of [S11, Th. 4.1]. Note that the limit
distribution corresponds to the stationary distribution of the diffusion
dUt = (âˆ’Id + c()) Ut dt + cÎ“1/2 dWt ,
where W = (Wt )t is a standard Wiener process and
(
0 for  < 1
c() =
1/2 for  = 1.
Therefore the limit covariance matrix is determined by solving the associated Lyapunovâ€™s equation [S14],
that, in the considered case, simply is
2 (âˆ’Id + c()Id) Î£ = âˆ’c2 Î“.

Theorem S5.3. In the above setting, let (Âµn ) be another stochastic process taking values in Î˜ = [âˆ’1, 1]k ,
adapted to a filtration F and following the dynamics
1
1
Âµn+1 âˆ’ Âµn = âˆ’ (Âµn âˆ’ Î¸n ) + âˆ†Mn+1 .
n
n
a.s.

Suppose that E[âˆ†Mn+1 âˆ†Mn+1 > |Fn ] âˆ’â†’ Î“. If  âˆˆ (1/2, 1), then we have
!
âˆš
 

(c + 1)2 Î“
0
N ÂµN
d
âˆ’â†’
N
0,
.
2
âˆ’1/2
c
0
Î“
N Î¸ N
2
If  = 1, then we have
!
âˆš
 
N ÂµN
[(c + 1)2 + c2 ]Î“
d
âˆ’â†’ N 0,
âˆ’1/2
c(c + 1)Î“
N Î¸ N

c(c + 1)Î“
c2 Î“


.

Proof. The dynamics for the pair (Âµn , Î¸n )n is
(
Âµn+1 âˆ’ Âµn = âˆ’ n1 (Âµn âˆ’ Î¸n ) + n1 âˆ†Mn+1
fn+1 .
Î¸n+1 âˆ’ Î¸n = âˆ’n Î¸n + cn âˆ†Mn+1 = âˆ’n Î¸n + n âˆ†M
a.s.

with E[âˆ†Mn+1 âˆ†Mn+1 > |Fn ] âˆ’â†’ Î“. Therefore, when 1/2 <  < 1, the statement follows from [S13] (with
Q11 = Q22 = âˆ’Id, Q12 = Id, Q21 = 0, b = Î²0 = 1, a = , Î“11 = Î“, Î“22 = c2 Î“ and Î“12 = Î“21 = cÎ“).
In particular, the two blocks of the limit covariance matrix, say Î£Âµ and Î£Î¸ , are determined solving the
equations
1
1
(H + Id)Î£Âµ + Î£Âµ (H > + Id) = âˆ’Î“Âµ ,
2
2

S 11

âˆ’1
âˆ’1 > >
âˆ’1 > >
where H = Q11 âˆ’ Q12 Qâˆ’1
22 Q21 = âˆ’Id + 0 and Î“Âµ = Î“11 + Q12 Q22 Î“22 (Q22 ) Q12 âˆ’ Î“12 (Q22 ) Q12 âˆ’
2
2
Q12 Qâˆ’1
Î“
=
Î“
+
c
Î“
+
cÎ“
+
cÎ“
=
(c
+
1)
Î“,
and
22 21

Q22 Î£Î¸ + Î£Î¸ Q>
22 = âˆ’Î“22 .
When  = 1, we can conclude by [S14] or [S19] taking Xn = (Âµn , Î¸n )> . Indeed, in this case the
covariance matrix is given by
1
1
e
(H + Id)Î£ + Î£(H > + Id) = âˆ’Î“,
2
2
where




âˆ’Id
Id
cÎ“
e= Î“
H=
and Î“
.
0
âˆ’Id
cÎ“ c2 Î“
Therefore, if we split Î£ in blocks, say Î£Âµ , Î£Î¸ and Î£ÂµÎ¸ , we find the system
âˆ’Î£Âµ + 2Î£ÂµÎ¸ = âˆ’Î“
âˆ’Î£ÂµÎ¸ + Î£Î¸ = âˆ’cÎ“
âˆ’Î£Î¸ = âˆ’c2 Î“
and so the proof is concluded by solving this system.

S6

Stable convergence

This brief section contains some basic definitions and results concerning stable convergence. For more details, we refer the reader to [S5, S7, S10] and the references therein.
Let (â„¦, A, P ) be a probability space, and let S be a Polish space, endowed with its Borel Ïƒ-field. A kernel
on S, or a random probability measure on S, is a collection K = {K(Ï‰) : Ï‰ âˆˆ â„¦} of probability measures
on the Borel Ïƒ-field of S such that, for each bounded Borel real function f on S, the map
Z
Ï‰ 7â†’ Kf (Ï‰) = f (x) K(Ï‰)(dx)
is A-measurable. Given a sub-Ïƒ-field H of A, a kernel K is said H-measurable if all the above random
variables Kf are H-measurable. A probability measure Î½ can be identified with a constant kernel K(Ï‰) = Î½
for each Ï‰.
On (â„¦, A, P ), let (Yn )n be a sequence of S-valued random variables, let H be a sub-Ïƒ-field of A, and let
K be a H-measurable kernel on S. Then, we say that Yn converges H-stably to K, and we write Yn âˆ’â†’ K
H-stably, if
weakly
P (Yn âˆˆ Â· | H) âˆ’â†’ E [K(Â·) | H]
for all H âˆˆ H with P (H) > 0,
where K(Â·) denotes the random variable defined, for each Borel set B of S, as Ï‰ 7â†’ KIB (Ï‰) = K(Ï‰)(B). In
the case when H = A, we simply say that Yn converges stably to K and we write Yn âˆ’â†’ K stably. Clearly, if
Yn âˆ’â†’ K H-stably, then Yn converges in distribution to the probability distribution E[K(Â·)]. The H-stable
convergence of Yn to K can be stated in terms of the following convergence of conditional expectations:
E[f (Yn ) | H]

Ïƒ(L1 , Lâˆ )

âˆ’â†’

Kf

(S6.1)

for each bounded continuous real function f on S. In [S7] the notion of H-stable convergence is firstly
generalized in a natural way replacing in (S6.1) the single sub-Ïƒ-field H by a collection G = (Gn ) (called
conditioning system) of sub-Ïƒ-fields of A and then it is strengthened by substituting the convergence in
Ïƒ(L1 , Lâˆ ) by the one in probability (i.e. in L1 , since f is bounded). Hence, according to [S7], we say that
Yn converges to K stably in the strong sense, with respect to G = (Gn ), if
P

E [f (Yn ) | Gn ] âˆ’â†’ Kf
for each bounded continuous real function f on S.
We now conclude this section recalling some convergence results that we apply in our proofs.
From [S10, Th. 3.2] (see also [S7, Th. 5 and Cor. 7] or [S5, Th. 5.5.1 and Cor. 5.5.2]), we get:

S 12

Theorem S6.1. Given a filtration F = (Fn )n , let (YN,n )N,n be a triangular array of random variables
with values in Rk such that YN,n is Fn -measurable and E[YN,n |Fnâˆ’1 ] = 0. Suppose that the following two
conditions are satisfied:
(c1) E [ max1â‰¤nâ‰¤N |YN,n | ] â†’ 0 and
PN
> P
âˆ’â†’ V , where V is a random variable with values in the space of positive semidefinite
(c2)
n=1 YN,n YN,n
k Ã— k-matrices.
P
Then N
n=1 YN,n converges stably to the Gaussian kernel N (0, V ).
From [S7, Th. 5, Cor. 7, Rem. 4] or [S5, Th. 5.5.1, Cor. 5.5.2, Rem. 5.5.2]), we obtain:
Theorem S6.2. Let (Ln ) be a Rk -valued martingale with respect to the filtration F = (Fn ). Suppose that
a.s., L1

Ln âˆ’â†’ L for some Rk -valued random variable L and
(c1) ne E[supjâ‰¥n |Ljâˆ’1 âˆ’ Lj | ] âˆ’â†’ 0 and
P
P
(c2) n2e jâ‰¥n (Ljâˆ’1 âˆ’ Lj )(Ljâˆ’1 âˆ’ Lj )> âˆ’â†’ V , where V is a random variable with values in the space of
positive semidefinite k Ã— k-matrices.
Then

ne Ln âˆ’ L âˆ’â†’ N (0, V )

stably in strong sense w.r.t. F.

P Indeed, following [S7, Example 6], it is enough to observe that Ln âˆ’ L can be written as Ln âˆ’ L =
jâ‰¥n (Lj âˆ’ Lj+1 ).
Finally, the following result combines together a stable convergence and a stable convergence in the
strong sense [S4, Lemma 1].
Theorem S6.3. Suppose that Cn and Dn are S-valued random variables, that M and N are kernels on S,
and that G = (Gn )n is an (increasing) filtration satisfying for all n

S
Ïƒ(Cn )âŠ‚Gn and Ïƒ(Dn )âŠ‚Ïƒ
n Gn .
If Cn stably converges to M and Dn converges to N stably in the strong sense, with respect to G, then
[Cn , Dn ] âˆ’â†’ M âŠ— N

stably.

(Here, M âŠ— N is the kernel on S Ã— S such that (M âŠ— N )(Ï‰) = M (Ï‰) âŠ— N (Ï‰) for all Ï‰.)
This last result contains as a special case the fact that stable convergence and convergence in probability
combine well: that is, if Cn stably converges to M and Dn converges in probability to a random variable D,
then (Cn , Dn ) stably converges to M âŠ— Î´D , where Î´D denotes the Dirac kernel concentrated in D.

SR References
[S1] G. Aletti and I. Crimaldi. The rescaled PoÌlya urn: local reinforcement and chi-squared goodness of
fit test. arXiv:1906.10951, 2019.
[S2] G. Aletti and I. Crimaldi. Generalized rescaled PoÌlya urn and its statistical applications. Main Article
of this supplementary material, 2020.
[S3] G. Aletti, I. Crimaldi, and A. Ghiglietti. Networks of reinforced stochastic processes: asymptotics for
the empirical means. Bernoulli, 25(4B):3339â€“3378, 2019.
[S4] P. Berti, I. Crimaldi, L. Pratelli, and P. Rigo. A central limit theorem and its applications to multicolor
randomly reinforced urns. J. Appl. Probab., 48(2):527â€“546, 2011.
[S5] I. Crimaldi. Introduzione alla nozione di convergenza stabile e sue varianti (Introduction to the notion
of stable convergence and its variants), volume 57. Unione Matematica Italiana, Monograf s.r.l.,
Bologna, Italy., 2016. Book written in Italian.
[S6] I. Crimaldi, P. Dai Pra, and I. G. Minelli. Fluctuation theorems for synchronization of interacting
PoÌlyaâ€™s urns. Stochastic Process. Appl., 126(3):930â€“947, 2016.
[S7] I. Crimaldi, G. Letta, and L. Pratelli. A Strong Form of Stable Convergence, volume 1899, pages
203â€“225. Springer, 2007.

S 13

[S8] B. Delyon. Stochastic approximation with decreasing gain: Convergence and asymptotic theory.
Technical report, 2000.
[S9] G. Fort. Central limit theorems for stochastic approximation with controlled markov chain dynamics.
ESAIM: PS, 19:60â€“80, 2015.
[S10] P. Hall and C. C. Heyde. Martingale limit theory and its application. Academic Press Inc. [Harcourt
Brace Jovanovich Publishers], New York, 1980. Probability and Mathematical Statistics.
[S11] H. J. Kushner and G. G. Yin. Stochastic approximation and recursive algorithms and applications,
volume 35 of Applications of Mathematics (New York). Springer-Verlag, New York, second edition,
2003. Stochastic Modelling and Applied Probability.
[S12] M. MeÌtivier. Semimartingales. Walter de Gruyter and Co., Berlin, 1982.
[S13] A. Mokkadem and M. Pelletier. Convergence rate and averaging of nonlinear two-time-scale stochastic
approximation algorithms. Ann. Appl. Probab., 16(3):1671â€“1702, 08 2006.
[S14] M. Pelletier. Weak convergence rates for stochastic approximation with application to multiple targets
and simulated annealing. Ann. Appl. Probab., 8(1):10â€“44, 1998.
[S15] R. Pemantle. A time-dependent version of poÌlyaâ€™s urn. J. Theor. Probab., 3:627â€“637, 1990.
[S16] J. N. K. Rao and A. J. Scott. The analysis of categorical data from complex sample surveys: chi-squared
tests for goodness of fit and independence in two-way tables. J. Amer. Statist. Assoc., 76(374):221â€“230,
1981.
[S17] H. Robbins and D. Siegmund. A convergence theorem for non negative almost supermartingales and
some applications. In Optimizing Methods in Statistics, pages 233â€“257. Academic Press, 1971.
[S18] J. Sherman and W. J. Morrison. Adjustment of an inverse matrix corresponding to a change in one
element of a given matrix. Ann. Math. Statist., 21(1):124â€“127, 03 1950.
[S19] L.-X. Zhang. Central limit theorems of a recursive stochastic algorithm with applications to adaptive
designs. Ann. Appl. Probab., 26(6):3630â€“3658, 2016.

S 14

