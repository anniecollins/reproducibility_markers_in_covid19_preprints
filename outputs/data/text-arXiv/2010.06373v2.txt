Generalized Rescaled PoÌlya urn and its statistical applications
Giacomo Aletti

âˆ—

and

Irene Crimaldi

â€ 

December 18, 2020

Abstract

arXiv:2010.06373v2 [math.ST] 17 Dec 2020

We introduce the Generalized Rescaled PoÌlya (GRP) urn, that provides three different generative models for a chi-squared test of goodness of fit for the long-term probabilities of clusterized
data, with independence between clusters and correlation, due to a reinforcement mechanism, inside each cluster. We apply the proposed test to a â€œbigâ€ dataset of Twitter posts about COVID-19
pandemic: in a few words, for a classical Ï‡2 test the data result strongly significant for the rejection
of the null hypothesis (the daily sentiment rate remains constant), but, taking into account the
correlation among data, the introduced test leads to a different conclusion. Beside the statistical
application, we point out that the GRP urn is a simple variant of the standard Eggenberger-PoÌlya
urn, that, with suitable choices of the parameters, shows â€œlocalâ€ reinforcement, almost sure convergence of the empirical mean to a deterministic limit and different asymptotic behaviours of the
predictive mean.
Keywords: central limit theorem, chi-squared test, PoÌlya urn, preferential attachment, reinforcement learning, reinforced stochastic process, stochastic approximation, urn model.
MSC2010 Classification: 62L20, 60F05, 62F05, 62F03.

Contents
1 Introduction

2

2 The Generalized Rescaled PoÌlya (GRP) urn

4

3 Main theorem: goodness of fit result

6

4 Asymptotic results for the empirical means

7

5 Simulations
5.1 Simulations of Theorem 4.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.2 Simulations of Theorem 4.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

8
8
9

6 Statistical applications
11
6.1 Estimation of the parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
7 COVID-19 epidemic Twitter analysis
âˆ—
â€ 

19

ADAMSS Center, UniversitaÌ€ degli Studi di Milano, Milan, Italy, giacomo.aletti@unimi.it
IMT School for Advanced Studies, Lucca, Italy, irene.crimaldi@imtlucca.it

1

8 Proof of Theorem 4.2

21

Supplemental Materials

S1

S1 Proofs and intermediate results
S1
S1.1 Proof of Theorem 3.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . S 1
S1.2 A preliminary central limit theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . S 2
S1.3 Proof of Theorem 4.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . S 3
P
S2 Case n n < +âˆ
S3
S3 Computations regarding local reinforcement
S7
S3.1 Case Î± = Î² âˆˆ (0, 1) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . S 8
S3.2 Case Î± = Î² = 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . S 8
S3.3 Case 0 < Î± < Î² < (1 + Î±)/2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . S 9
S4 Technical results

S 10

S5 Some stochastic approximation results

S 12

S6 Stable convergence

S 14

1

Introduction

The standard Eggenberger-PoÌlya urn (see [23, 34]) has been widely studied and generalized (some
recent variants can be found in [3, 7, 8, 9, 11, 13, 16, 18, 19, 25, 26, 32, 33]). In its simplest form,
this model with k-colors works as follows. An urn contains N0 i balls of color i, for i = 1, . . . , k,
and, at each discrete time, a ball is extracted from the urn and then it is returned inside the urn
together with Î± > 0 additional balls of the same color. Therefore, if we denote by Nn i the number
of balls of color i in the urn at time n, we have
Nn i = Nnâˆ’1 i + Î±Î¾n i

for n â‰¥ 1,

where Î¾n i = 1 if the extracted ball at time n is of color i, and Î¾n i = 0 otherwise. The parameter
Î±
Pnregulates the reinforcement mechanism: the greater Î±, the greater the dependence of Nn i on
h=1 Î¾h i .
The â€œRescaledâ€ PoÌlya (RP) urn model, presented in [3], is characterized by the introduction
of the parameter Î², together with the initial parameters (b0 i )i=1,...,k and (B0 i )i=1,...,k , next to the
parameter Î± of the original model, so that
Nn i = b0 i + B n i

with

Bn i = Î²Bnâˆ’1 i + Î±Î¾n i

n â‰¥ 1.

Therefore, the urn initially contains b0 i +B0 i balls of color i and the parameter Î² â‰¥ 0, together with
Î± > 0, regulates the reinforcement mechanism. More precisely, the term Î²Bnâˆ’1 i links Nn i to the
â€œconfigurationâ€ at time nâˆ’1 through the â€œscalingâ€ parameter Î², and the term Î±Î¾n i links Nn i to the
outcome of the extraction at time n through the parameter Î±. Note that the case Î² = 1 corresponds
to the standard Eggenberger-PoÌlya urn with an initial number N0 i = b0 i + B0 i of balls of color
i. When Î² âˆˆ [0, 1), this variant of the Eggenberger-PoÌlya urn shows the following features: (i) a
â€œlocalâ€ reinforcement, i.e. a reinforcement mechanism mainly based on the last observations, (ii)
PN
a long-term almost sure convergence of the empirical mean n=1 Î¾n i /N to the deterministic limit

2

Pn
p0 i = b0 i / i=1 b0 i , and (iii) a chi-squared goodness of fit result for the long-term probabilities
{p0 1 , . . . , p0 k }. In particular, regarding the point (iii), we have that the chi-squared statistics
Ï‡2 = N

k
X
(b
p i âˆ’ p 0 i )2
i=1

p0 i

=

k
X
(Oi âˆ’ N p0 i )2
i=1

N p0 i

,

(1)

where N is the size of the sample, pbi = Oi /N , with Oi the number of observations equal to i in
1
the sample, is asymptotically distributed as Ï‡2 (k âˆ’ 1)Î» = Î“( kâˆ’1
2 , 2Î» ), with Î» > 1. This constant
Î», due to the presence of correlation among units, mitigate the effect of the sample size N in
(1), that multiplies the chi-squared distance between the observed frequencies and the expected
probabilities. Indeed, the observed value of the chi-squared distance has to be compared with the
â€œcriticalâ€ value Ï‡21âˆ’Î¸ (k âˆ’ 1)Î»/N , where Ï‡21âˆ’Î¸ (k âˆ’ 1) denotes the quantile of order 1 âˆ’ Î¸ of the
chi-squared distribution Ï‡2 (k âˆ’ 1). This aspect is important for the statistical applications in the
context of a â€œbig sampleâ€, when a small value of the chi-squared distance might be significant, and
hence a correction related to the correlation between observations is desirable (see, for instance,
[10, 12, 15, 24, 27, 30, 36, 40, 41, 43]).
In this work, we replace the two parameters Î± and Î² by two sequences (Î±n )n and (Î²n )n , in
order to give more flexibility to the model and capture different asymptotic behaviours. We call
this new variant of the Eggenberger-PoÌlya urn Generalized Rescaled PoÌlya (GRP) urn. Besides
the Eggenberger-PoÌlya urn and the RP urn, the GRP urn model also includes as special cases the
models illustrated in [28] and [38].
Referring to the above issues (ii)-(iii), we show that, with a suitable choice of the model parameters, we still have the almost sure convergence of pbi = Oi /N to the probability p0 i , together with a
chi-squared goodness of fit result, where the effect of the sample size N is weakened by the presence
of correlation. More precisely, we present a case where, as before, the chi-squared statistics (1) is
asymptotically distributed as Ï‡2 (k âˆ’ 1)Î», with Î» > 1, and another case where it is asymptotically
distributed as Ï‡2 (k âˆ’ 1)N 1âˆ’2e Î», where Î» > 0 may be smaller than 1, but e is always strictly smaller
than 1/2. In particular, in the second case, the critical value for the chi-squared distance becomes
Ï‡21âˆ’Î¸ (k âˆ’ 1)Î»/N 2e , where, although the constant Î» may be smaller than 1, the effect of the sample
size N is mitigated by the exponent 2e < 1. Summing up, the GRP urn provides three models
(the RP urn introduced and studied in [3] and the other two analyzed in the present work), each
of them presenting different limit features and all of them suitable as theoretical frameworks for
a chi-squared test of goodness of fit for the long-term probabilities of correlated data, generated
according to a reinforcement mechanism. It is worthwhile to underline that the results proven in
this work do not cover (and are not covered by) those obtained for the RP urn in [3]. Moreover, as
explained in the following Section 2, the techniques required here and in [3] are completely different.
Regarding issue (i), we show that the provided examples exhibit a broader sense local reinforcement, in the sense that the â€œweightâ€ of the observations is eventually increasing with time.
Finally, we underline that the almost sure convergence of the predictive mean Ïˆn i = E[Î¾n+1 i =
1| â€œpastâ€] is typically proven for urn models, in order to apply â€œstochastic approximation central
limit theoremsâ€, where it is assumed as an hypothesis. In the GRP this kind of convergence is
not always guaranteed (see, for instance, the RP urn model, where we have a random persistent
fluctuation of the predictive mean) or it is not easy to check if it holds or not (see Section 8 for
more details). Therefore, in these cases, different proof arguments are necessary.
The sequel of the paper is so structured. In Section 2 we set up our notation, we define the
Generalized Rescaled PoÌlya (GRP) urn and we discuss its relationships with previous models. In
Section 3 we provide the main result of this work, that is the almost sure convergence of the
empirical means to the deterministic limits p0 i and the goodness of fit result for the long-term
probabilities p0 i , together with comments and examples. In Section 4 we state two convergence
results for the empirical means, which are the basis for the proof of the main theorem, and Section
5 collects some related simulations. In Section 6 we describe a possible statistical application of
the GRP urn and the related results in the context of a big sample, inspired by [3, 12, 35]. More

3

precisely, it is a chi-squared test of goodness of fit for the long-term probabilities of clusterized data,
with independence between clusters and correlation, due to a reinforcement mechanism, inside each
cluster. We apply the proposed test to a dataset of Twitter posts about COVID-19 pandemic [2].
Given the null hypothesis that the daily sentiment rate of the posts is the same for all the considered days (suitably spaced days in the period February 20th - April 20th 2020), performing a
classical Ï‡2 test, the data result strongly significant for the rejection of the null hypothesis, while,
taking into account the correlation among posts sent in the same day, the proposed test leads to
a different conclusion. All the shown theoretical results are analytically proven. The proofs are
left to Section S1 in the Supplementary Material [1], except for the proof of Theorem 4.2, which
is methodologically new and emphasizes new techniques of martingale limit theory and so it is
illustrated in Section 8. Finally, in the Supplementary Material we also provide some complements,
some technical lemmas and some recalls about stochastic approximation theory and about stable
convergence. When necessary, the references to the Supplementary Material are preceded by an
â€œSâ€, so that (S1.2) will refer to the equation (S1.2) in [1].

2

The Generalized Rescaled PoÌlya (GRP) urn

In all the sequel, we suppose given two sequences of parameters (Î±n )nâ‰¥1 , with Î±n > 0 and
Pk
(Î²n )nâ‰¥0 with Î²n â‰¥ 0. Given a vector x = (x1 , . . . , xk )> âˆˆ Rk , we set |x| =
i=1 |xi | and
P
k
kxk2 = x> x = i=1 |xi |2 . Moreover we denote by 1 and 0 the vectors with all the components
equal to 1 and equal to 0, respectively.
The urn initially contains b0 i + B0 i > 0 distinct balls of color i, with i = 1, . . . , k. We set
b0 = (b0 1 , . . . , b0 k )> and B0 = (B0 1 , . . . , B0 k )> . We assume |b0 | > 0 and we set p0 = |bb00 | . At
each discrete time (n + 1) â‰¥ 1, a ball is drawn at random from the urn, obtaining the random
vector Î¾n+1 = (Î¾n+1 1 , . . . , Î¾n+1 k )> defined as
(
1 when the extracted ball at time n + 1 is of color i
Î¾n+1 i =
0 otherwise,
and the number of balls in the urn is so updated:
Nn+1 = b0 + Bn+1

with

Bn+1 = Î²n Bn + Î±n+1 Î¾n+1 ,

(2)

which gives (since |Î¾n+1 | = 1)
|Bn+1 | = Î²n |Bn | + Î±n+1 .
Therefore, setting

rnâˆ—

= |Nn | = |b0 | + |Bn |, we get
âˆ—
rn+1
= rnâˆ— + (Î²n âˆ’ 1)|Bn | + Î±n+1 ,

(3)

âˆ—
rn+1
âˆ’ rnâˆ— = |b0 |(1 âˆ’ Î²n ) âˆ’ rnâˆ— (1 âˆ’ Î²n ) + Î±n+1 .

(4)

that is
Moreover, setting F0 equal to the trivial Ïƒ-field and Fn = Ïƒ(Î¾1 , . . . , Î¾n ) for n â‰¥ 1, the conditional
probabilities Ïˆn = (Ïˆn 1 , . . . , Ïˆn k )> of the extraction process, also called predictive means, are
Ïˆn = E[Î¾n+1 |Fn ] =

b0 + Bn
Nn
=
|Nn |
rnâˆ—

n â‰¥ 0.

(5)

It is obvious that we have |Ïˆn | = 1. Moreover, when Î²n > 0 for all n, the probability Ïˆn i results
increasing with the number of times we observed the value i, that is the random variables Î¾n i are
generated according to a reinforcement mechanism: the probability that the extraction of color i

4

occurs has an increasing dependence on the number of extractions of color i occurred in the past
(see, e.g. [39]). More precisely, we have
Qnâˆ’1
Pn  Qnâˆ’1 
b0 + B0 j=0 Î²j + h=1 Î±h j=h Î²j Î¾h
Ïˆn =
(6)
Pn  Qnâˆ’1  .
Qnâˆ’1
|b0 | + |B0 | j=0 Î²j + h=1 Î±h j=h Î²j
Qnâˆ’1
The dependence of Ïˆn on Î¾h depends on the factor f (h, n) = Î±h j=h Î²j , with 1 â‰¤ h â‰¤ n, n â‰¥ 0.
In the case of the standard Eggenberger-PoÌlya urn, that corresponds to Î±n = Î± > 0 and Î²n = 1
for all n, each observation Î¾h has the same â€œweightâ€ f (h, n) = Î±. Instead, if the factor f (h, n)
increases with h, then the main contribution is given by the most recent extractions. We refer to
this phenomenon as â€œlocalâ€ reinforcement. For instance, this is the case when (Î±n ) is increasing
and Î²n = 1 for all n. Another case is when Î±n = Î± > 0 and Î²n < 1 for all n. The case Î²n = 0
for all n is an extreme case, for which Ïˆn depends only on the last extraction Î¾n (recall that
Qnâˆ’1
conventionally j=n = 1). For the next examples, we will show that they exhibit a broader sense
local reinforcement, in the sense that the â€œweightâ€ of the observations is eventually increasing with
time.
By means of (5), together with (2) and (3), we have
Ïˆn+1 âˆ’ Ïˆn = âˆ’

 Î±n+1

(1 âˆ’ Î²n )
|b0 | Ïˆn âˆ’ p0 + âˆ—
Î¾n+1 âˆ’ Ïˆn .
âˆ—
rn+1
rn+1

(7)

The particular case when Î²n = Î² = 0 for all n corresponds to a version of the so-called â€œmemory-1
senile reinforced random walkâ€ on a star-shaped graph introduced in [28]. The case Î±n = Î± > 0 and
Î²n = Î² = 1 for all n corresponds to the standard Eggenberger-PoÌlya urn with an initial number
N0 i = b0 i + B0 i of balls of color i. When (Î±n ) is a not-constant sequence, while Î²n = Î² = 1
for all n, the GRP urn coincides with the variant of the Eggenberger-PoÌlya urn introduced in [38]
(see also [39, Sec. 3.2]). Instead, when Î² 6= 1, the GRP urn does not fall in any variants of the
Eggenberger-PoÌlya urn discussed in [39, Sec. 3.2]. The case when Î±n = Î± > 0 and Î²n = Î² â‰¥ 0 for
all n corresponds to the Rescaled PoÌlya (RP) urn introduced and studied in [3]. When (Î²n ) is not
identically equal to 1, since the first term in the right hand of the above relation, the GRP urn does
not belong to the class of Reinforced Stochastic Processes (RSPs) studied in [4, 6, 5, 20, 21, 22].
Indeed, the RSPs are characterized by a â€œstrictâ€ reinforcement mechanism such that Î¾n i = 1 implies
Ïˆn i > Ïˆnâˆ’1 i and so, as a consequence, Ïˆn i has an increasing dependence on the number of times
we have Î¾h i = 1 for h = 1, . . . , n. When (Î²n ) is not identically equal to 1, the GRP urn does
not satisfy the â€œstrictâ€ reinforcement mechanism, because the first term is positive or negative
according to the sign of (1 âˆ’ Î²n ) and of (Ïˆn âˆ’ p0 ). Furthermore, we observe that equation (7)
recalls the dynamics of a RSP with a â€œforcing inputâ€ (see [4, 20, 42]), but the main difference relies
on the fact that the GRP urn model also allows for the two cases:
P 1âˆ’Î²n
P  1âˆ’Î²n 2
â€¢
=
+âˆ
and
= +âˆ,
âˆ—
âˆ—
n rn+1
n
rn+1


2
P
P 1âˆ’Î²n
n
< +âˆ and n 1âˆ’Î²
< +âˆ.
â€¢
n râˆ—
râˆ—
n+1

n+1

n)
Setting Î¸n = Ïˆn âˆ’ p0 and âˆ†Mn+1 = Î¾n+1 âˆ’ Ïˆn = Î¾n+1 âˆ’ p0 âˆ’ Î¸n and letting n = |b0 | (1âˆ’Î²
râˆ—
n+1

âˆ—
and Î´n = Î±n+1 /rn+1
, from (7) we obtain

Ïˆn+1 âˆ’ Ïˆn = âˆ’n (Ïˆn âˆ’ p0 ) + Î´n âˆ†Mn+1

(8)

Î¸n+1 âˆ’ Î¸n = âˆ’n Î¸n + Î´n âˆ†Mn+1 .

(9)

and so
Therefore, the asymptotic behaviour of (Î¸n ) depends on the two sequences (n )n and (Î´n )n .
PN
Finally, we observe that, setting Î¾ N = n=1 Î¾n /N and Âµn = Î¾ n âˆ’ p0 , we have the equality
1
1
Âµn+1 âˆ’ Âµn = âˆ’ (Âµn âˆ’ Î¸n ) + âˆ†Mn+1 ,
n
n

5

(10)

that links the asymptotic behaviour of (Âµn ) and the one of (Î¸n ).
Different kinds of sequences (n )n and (Î´n )n provide different kinds of asymptotic behaviour of
Î¸n , i.e. of the empirical mean Î¾ N . In Section 3, we provide two cases in which we have a long-term
PN
almost sure convergence of the empirical mean Oi /N = n=1 Î¾n i /N toward the constant p0i =
b0 i /|b0 |, together with a chi-squared goodness of fit result. In particular, the quantities p0 1 , . . . , p0 k
can be seen as a long-run probability distribution on the possible values (colors) {1, . . . , k}.
It is worthwhile to point out that the two cases studied in the present work do not include
(and are not included in) the case Î±n = Î± > 0 and Î²n = Î² âˆˆ [0, 1), studied in [3]. Moreover, the
techniques employed here and in [3] are completely different: when Î²n = Î² âˆˆ [0, 1) as in [3], the
jumps âˆ†Ïˆn do not vanish and the process Ïˆ = (Ïˆn )n converges to a stationary Markov chain and
so the appropriate Markov ergodic theory is employed; in this work, we have |âˆ†Ïˆn | = o(1), so that
the martingale limit theory is here exploited to achieve the asymptotic results. Obviously, the two
techniques are not exchangeable or adaptable from one contest to the other one.

3

Main theorem: goodness of fit result

Given a sample (Î¾1 , . . . , Î¾N ) generated by a GRP urn, the statistics
N
X

Oi = #{n = 1, . . . , N : Î¾n i = 1} =

Î¾n i ,

i = 1, . . . , k,

n=1

counts the number of times we observed the value i. The theorem below states,
PN under suitable
assumptions, the almost sure convergence of the empirical mean pbi = Oi /N = n=1 Î¾n i /N toward
the probability p0 i , together with a chi-squared goodness of fit test for the long-term probabilities
p0 1 , . . . , p0 k . More precisely, we prove the following result:
Theorem 3.1. Assume p0 i > 0 for all i = 1, . . . , k and suppose to be in one of the following cases:
a) n = (n + 1)âˆ’ and Î´n = cn , with  âˆˆ (0, 1] and c > 0, or
b) n = (n + 1)âˆ’ , Î´n âˆ¼ c(n + 1)âˆ’Î´ , with  âˆˆ (0, 1), Î´ âˆˆ (/2, ) and c > 0.
Define the constants e and Î» as
(
e=
and

1/2
1/2 âˆ’ ( âˆ’ Î´) < 1/2

ï£±
2
ï£´
ï£²(c + 1)
Î» = (c + 1)2 + c2 = [2c(c + 1) + 1]
ï£´
ï£³ c2
1+2(âˆ’Î´)

in case a)
in case b)
in case a) with  âˆˆ (0, 1) ,
in case a) with  = 1 ,
in case b) .

(11)

a.s.

Then pbi = Oi /N âˆ’â†’ p0 i and
1
N 1âˆ’2e

k
X
(Oi âˆ’ N p0 i )2
i=1
2

N p0 i

where W0 has distribution Ï‡ (kâˆ’1) = Î“

= N 2e

k
X
(b
pi âˆ’ p0 i )2
i=1

kâˆ’1 1
2 , 2)

p0 i

d

âˆ’â†’ Wâˆ— = Î»W0

N â†’âˆ

and, consequently, Wâˆ— has distribution Î“

kâˆ’1 1
2 , 2Î»



.

We note that Î» is a constant greater than 1 in case a); while, in case b), it is a strictly positive
quantity. Moreover, in case b), we have 0 < ( âˆ’ Î´) < /2 < 1/2 and so (1 âˆ’ 2e) = 2( âˆ’ Î´) âˆˆ (0, 1).
As a consequence, we have N 1âˆ’2e Î» > 1 for N large enough.
In the next two examples we show that it is possible to construct suitable sequences (Î±n )n and
(Î²n )n such that the corresponding sequences (n )n and (Î´n ) converge to zero with the same rate or
with different rates and satisfy the assumptions a) or b) of the above theorem, respectively.

6

Example 3.2. (Case n = (n + 1)âˆ’ and Î´n = cn , with  > 0 and c > 0 )
n)
Take Î±n+1 = c|b0 |(1 âˆ’ Î²n ), with Î²n âˆˆ [0, 1) and c > 0, that implies Î´n = Î±râˆ—n+1 = c |b0 |(1âˆ’Î²
= cn .
râˆ—
n+1

Set rnâˆ— = (1 + c)|b0 |(1 âˆ’ tn ) so that from (4) we obtain tn+1 = Î²n tn . Hence, we have
tn+1 = t0

n
Y
k=0

and so

Î²k =

n+1

n
c|b0 | âˆ’ |B0 | Y
Î²k
(1 + c)|b0 |
k=0

n
Y
âˆ—
rn+1
= (1 + c)|b0 | + |B0 | âˆ’ c|b0 |
Î²k .
k=0

Qâˆ

âˆ’â†’ r = (1+c)|b0 |+(|B0 |âˆ’c|b0 |)Î² âˆ— > 0. If we
Therefore, setting Î² = k=0 Î²k âˆˆ [0, 1), we get
âˆ—
âˆ—
choose |B0 | = c|b0 |, then rn = r = (1 + c)|b0 | for each n and so, setting Î²n = 1 âˆ’ (1 + c)(1 + n)âˆ’
with  > 0, we obtain n = (1 + n)âˆ’ and Î´n = cn . Taking  âˆˆ (0, 1], we have that n and Î´n satisfy
assumption a) of Theorem 3.1. Moreover, we have Î±n = c|b0 |(1+c)nâˆ’ and 1âˆ’Î²n = (1+c)(1+n)âˆ’
Qnâˆ’1
and so, for the behaviour of the factor f (h, n) = Î±h j=h Î²j in (6), we refer to Section S3.
âˆ—

rnâˆ—

âˆ—

Example 3.3. (Case n = (n + 1)âˆ’ and Î´n âˆ¼ c(n + 1)âˆ’Î´ , with 0 < Î´ <  < 1 and c > 0)
Take 0 < Î´ <  < 1 and set Î³ =  âˆ’ Î´ > 0, rnâˆ— = nÎ³ and (1 âˆ’ Î²n ) = |b0 |âˆ’1 (1 + n)âˆ’Î´ . We immediately
have
(1 âˆ’ Î²n )
= (1 + n)âˆ’Î´âˆ’Î³ = (n + 1)âˆ’
n = |b0 | âˆ—
rn+1
and (4) yields Î±n+1 = (n + 1)Î³ âˆ’ nÎ³ [1 âˆ’ |b0 |âˆ’1 (1 + n)âˆ’Î´ ] âˆ’ (1 + n)âˆ’Î´ , so that


Î±n+1
1 Î³ 
Î±n+1
Î´n = âˆ—
=
1
âˆ’
1
âˆ’
=
1 âˆ’ |b0 |âˆ’1 (1 + n)âˆ’Î´ âˆ’ (1 + n)âˆ’Î´âˆ’Î³
rn+1
(n + 1)Î³
n+1



= 1 âˆ’ 1 âˆ’ Î³(n + 1)âˆ’1 + O(nâˆ’2 ) 1 âˆ’ |b0 |âˆ’1 (1 + n)âˆ’Î´ âˆ’ (1 + n)âˆ’


= |b0 |âˆ’1 (1 + n)âˆ’Î´ 1 + Î³|b0 |(n + 1)âˆ’1+Î´ âˆ’ |b0 |(1 + n)âˆ’+Î´ âˆ’ Î³(n + 1)âˆ’1 + O(nâˆ’2+Î´ ) .
Setting c = |b0 |âˆ’1 > 0, we obtain n = (n + 1)âˆ’ and Î´n âˆ¼ c(n + 1)âˆ’Î´ . Taking Î´ âˆˆ (/2, ), we
have that n and Î´n satisfy assumption b) of Theorem 3.1. Moreover, we have Î±n = cnâˆ’(2Î´âˆ’) (1 +
Î³câˆ’1 nâˆ’1+Î´ âˆ’ câˆ’1 nâˆ’+Î´ âˆ’ Î³nâˆ’1 + O(nâˆ’2+Î´ )) and (1 âˆ’ Î²n ) = c(1 + n)âˆ’Î´ , with 0 < 2Î´ âˆ’  < Î´ <
Qnâˆ’1
(1 + 2Î´ âˆ’ )/2, and so, for the behaviour of the factor f (h, n) = Î±h j=h Î²j in (6), we refer to
Section S3.

4

Asymptotic results for the empirical means

Theorem 3.1 is a consequence of suitable convergence results for the empirical means Î¾ N in the
s
considered cases a) and b). In the sequel, we will use the symbol âˆ’â†’ in order to denote the stable
convergence (for a brief review on stable convergence, see Section S6).
The case when Î´n = cn in (8) is essentially
Approximation (SA)
P 2
P 2 covered by the Stochastic
theory. The most known case is when

<
+âˆ.
The
case

=
+âˆ is less usual in
n n
n n
literature, but it is well characterized in [31]. More precisely, leveraging the results collected in
Section S5, we prove in Section S1.3 the following result:
Theorem 4.1. Take n = (n + 1)âˆ’ and Î´n = cn , with  âˆˆ (0, 1] and c > 0, and set Î“ =
a.s.
diag(p0 ) âˆ’ p0 p0 > . Then Î¾ N âˆ’â†’ p0 and
âˆš
 s
N Î¾ N âˆ’ p0 âˆ’â†’ N (0, Î»Î“) ,
with Î» = (c + 1)2 when 0 <  < 1 and Î» = (c + 1)2 + c2 = 2c(c + 1) + 1 when  = 1.

7

The case when (n )n and (Î´n )n in (8) go to zero with different rates is typically neglected in
SA literature. To our best knowledge, it is taken into consideration only in [37], where the weak
convergence rate of the sequence (Ïˆn ) toward a certain point Ïˆ âˆ— is established under suitable
assumptions, given the event {Ïˆn â†’ Ïˆ âˆ— }. No result is given for the empirical mean Î¾ N , which
instead is the focus of the present paper, as shown in the following theorem (proven in Section 8):
Theorem 4.2. Take n = (n + 1)âˆ’ and Î´n âˆ¼ c(n + 1)âˆ’Î´ , with  âˆˆ (0, 1), Î´ âˆˆ (/2, ) and c > 0.
a.s.
Then Î¾ N âˆ’â†’ p0 and


 s
c2
1/2âˆ’(âˆ’Î´)
N
Î¾ N âˆ’ p0 âˆ’â†’ N 0,
Î“ ,
1 + 2( âˆ’ Î´)
with Î“ = diag(p0 ) âˆ’ p0 p0 > .

5

Simulations

In this section, we provide some simulations related to Theorem 4.1 and Theorem 4.2 stated above.
More precisely, we have simulated the model in the two different cases following Example 3.2 and
Example 3.3. In both cases, we have k = 3, b0 = p0 = (0.167, 0.333, 0.5) (in order to show possible
asymmetries in the convergence) and N in {100, 316, 1 000, 3 162, 10 000, 31 623, 100 000} (unformly
spaced in log-scale). Each sinulation is made with 1 000 independent replicas.
Firstly, we show for both the theorems the convergence of the empirical mean Î¾ N to p0 , by
PN
plotting the mean value of the empirical mean Î¾ N = n=1 Î¾n i /N , for i = 1, 2, 3 and the different
values of N , together with its standard deviation.
Then, we consider the CLT. To this regard, we point out that, as shown in the analytical proofs,
we have

N 1/2âˆ’(âˆ’Î´) Î¾ N âˆ’ p0 = DOM + REM ,
where DOM is a â€œdominant factorâ€ converging to the Gaussian distribution with the appropriate
variance, while REM is a remainder part which is eventually negligible.

Finally, we show that in any set of data the CLT is experimentally found for N 1/2âˆ’(âˆ’Î´) Î¾ N âˆ’ p0
with a Gaussian distribution of the form N (0, Î»Ì‚Î“), where Î»Ì‚ = Î»Ì‚(, Î´) is the standard deviation computed with the simulated data.

5.1

Simulations of Theorem 4.1

The results of Theorem 4.1 are here shown simulating the model by means of Example 3.2 with
n = Î´n = (1 + n) , where  âˆˆ {0.30, 0.40, 0.50, 0.85, 1.00} and c = 1.
Fig. 1 shows the convergence of the empirical means to p0 .
For what concerns the CLT, we observe that
âˆš
âˆš

 âˆš
N Î¾ N âˆ’ p0 = (c + 1) N Î¾ N âˆ’ Ïˆ N âˆ’1 âˆ’ N DN ,
(12)
âˆš
where the first term tends to N (0, (c + 1)2 Î“) and the remainder term N DN tends to 0 only when
 < 1. Indeed, when  = 1, we have a different value for the constant Î» in Theorem 4.1. These
facts are summarized in Fig. 2 and Fig 3.
âˆš

Finally, Fig. 4 shows the convergence in distribution of N Î¾ N âˆ’ p0 to a Gaussian distribution of the form N (0, Î»Ì‚Î“), where Î»Ì‚ = Î»Ì‚(, Î´) is the standard deviation computed with the simulated
data.

8

Figure 1: Convergence of the empirical mean to p0 : 1 000 independent simulations of the model
by means of Example 3.2 P
with k = 3, b0 = p0 = (0.167, 0.333, 0.5) and different values of  = Î´ and
plot of the mean value of N
n=1 Î¾n i /N , together with its standard deviation, for different values of N .

5.2

Simulations of Theorem 4.2

We have simulated the model following Example 3.3 with k = 3, b0 = p0 = (0.167, 0.333, 0.5)
(so that c = 1) and different values for the parameters  and Î´. The total number of performed
(independent) replications is again 1 000.
PN
In Fig. 5 we provide the mean value of the empirical mean Î¾ N = n=1 Î¾n i /N , for i = 1, 2, 3
and different values of N , together with its standard deviation. This plot shows the convergence of
the empirical means to p0 .
For what concerns the CLT, the proof given in Section 8 points out that
N


2
X
Î´nâˆ’1
d
N 1/2âˆ’(âˆ’Î´) Î¸ N âˆ’1 â‰ˆ N 0, N âˆ’1âˆ’2(âˆ’Î´)
Î“
â†’ N (0, Î»Î“) ,
2
n=1 nâˆ’1

where Î» = 1/[1 + 2( âˆ’ Î´)] (see the proof after (21)). Fig. 2 shows this convergence, while the
reminder term

N 1/2âˆ’(âˆ’Î´) Î¾ N âˆ’ p0 âˆ’ N 1/2âˆ’(âˆ’Î´) Î¸ N âˆ’1 ,
(13)
is shown to converge towards 0 in Fig. 3, with different speed.

Finally, Fig. 8 shows the convergence in distribution of N 1/2âˆ’(âˆ’Î´) Î¾ N âˆ’ p0 to a Gaussian
distribution of the form N (0, Î»Ì‚Î“), where Î»Ì‚ = Î»Ì‚(, Î´) is the standard deviation computed with
simulated data. For some values of  and Î´, the constant Î»Ì‚ is not equal to the theoretical value
1/[1 + 2( âˆ’ Î´)], because the term (13) goes to 0 slowly. However, it is worth to note that this
issue does not matter for the statistical application of the result, since, as described in Section 6,
we estimate the pair (Î· = 2( âˆ’ Î´), Î») from the data.

9

âˆš

Figure 2: Convergence in distribution of (c + 1) N Î¾ N i âˆ’ Ïˆ N âˆ’1 i : 1 000 independent simulations of the model by means of Example 3.2 with âˆš
k = 3, b0 = p0 =(0.167, 0.333, 0.5) and different
values of  = Î´ and plot of the distribution of (c+1) N Î¾ N i âˆ’ Ïˆ N âˆ’1 i for i = 1, 2, 3. In red: Standard
Gaussian distribution.
10

âˆš
Figure 3: Convergence to zero of the remaining term N DN in (12): 1 000 independent
simulations of the model by means of Example 3.3 with k = 3, b0 = p0 =âˆš(0.167, 0.333, 0.5) and
different values of  = Î´ and plot of the distribution of the three components N DN i .

6

Statistical applications

In a big sample the units typically can not be assumed independent and identically distributed,
but they exhibit a structure in clusters, with independence between clusters and with correlation
inside each cluster [12, 17, 29, 35, 44, 45]. The model and the related results presented in [3] and
in the present paper may be useful in the situation when inside each cluster the probability that
a certain unit chooses the value i is affected by the number of units in the same cluster that have
already chosen the value i, hence according to a reinforcement rule. Formally, given a â€œbigâ€ sample
{Î¾ n : n = 1, . . . , N }, we suppose that the N units are ordered so that we have the following L
clusters of units:
( `âˆ’1
)
`
X
X
C` =
Nl + 1, . . . ,
Nl ,
` = 1, . . . , L.
l=1

l=1

Therefore, the cardinality of each cluster C` is N` . We assume that the units in different clusters
are independent, that is
[Î¾1 , . . . , Î¾N1 ], . . . , [Î¾P`âˆ’1 Nl +1 , . . . , Î¾P`l=1 Nl ], . . . , [Î¾PLâˆ’1 Nl +1 , . . . , Î¾N ]
l=1

l=1

are L independent multidimensional random variables. Moreover, we assume that the observations
inside each cluster can be modeled as a GRP satisfying case a) or case b) of Theorem 3.1. Given
certain (strictly positive) intrinsic probabilities pâˆ—0 1 (`), . . . , pâˆ—0 k (`) for each cluster C` , we firstly
want to estimate the model parameters and then perform a test with null hypothesis
H0 :

p0 i (`) = pâˆ—0 i (`) âˆ€i = 1, . . . , k

based on the the statistics
Q` =

1
2(âˆ’Î´)

N`

k
X
Oi (`) âˆ’ N` pâˆ—0 i (`)
N` pâˆ—0 i (`)
i=1

2
,

11

with Oi (`) = #{n âˆˆ C` : Î¾n i = 1},

(14)

âˆš

Figure 4: Convergence in distribution of N Î¾ N i âˆ’ p0 i : 1 000 independent simulations of the
model by means of Example 3.3 with k = 3, b0 = p0 = (0.167, 0.333, 0.5) and different values of  = Î´
âˆš
and plot of the distribution of N âˆš Î¾N i âˆ’p0 i for i = 1, 2, 3. In red: Standard Gaussian distribution.
Î»Ì‚p0 i (1âˆ’p0 i )

12

Figure 5: Convergence of the empirical mean to p0 : 1 000 independent simulations of the model
by means of Example 3.3 P
with k = 3, b0 = p0 = (0.167, 0.333, 0.5) and different values of  and Î´ and
plot of the mean value of N
n=1 Î¾n i /N , together with its standard deviation, for different values of N .

1
and its corresponding asymptotic distribution Î“ kâˆ’1
2 , 2Î» , where Î» is given in (11). Note that we
can perform the above test for a certain cluster `, or we can consider all the clusters together using
PL
1
the aggregate statistics `=1 Q` and its corresponding distribution Î“( L(kâˆ’1)
, 2Î»
).
2
âˆ—
Regarding the probabilities p0 i (`), some possibilities are:
â€¢ we can take pâˆ—0 i (`) = 1/k for all i = 1, . . . , k if we want to test possible differences in the
probabilities for the k different values;
(1)
â€¢ we can suppose to have two different periods of times, and so two samples, say {Î¾n
: n=
P
(1)
(2)
âˆ—
1, . . . , N } and {Î¾n : n = 1, . . . , N }, take p0 i (`) = nâˆˆC` Î¾n i /N` for all i = 1, . . . , k, and
perform the test on the second sample in order to check possible changes in the intrinsic
long-run probabilities;
P
â€¢ we can take one of the clusters as benchmark, say `âˆ— , set pâˆ—0 i (`) = nâˆˆC`âˆ— Î¾n i /N`âˆ— for all
i = 1, . . . , k and ` 6= `âˆ— , and perform the test for the other L âˆ’ 1 clusters in order to check
differences with the benchmark cluster `âˆ— .

Finally, if we want to test possible differences in the clusters, then we can take pâˆ—0 i (`) = pâˆ—0 i =
PN
PL
n=1 Î¾n i /N for all ` = 1, . . . , L and perform the test using the aggregate statistics
`=1 Q` with
1
,
).
asymptotic distribution Î“( (Lâˆ’1)(kâˆ’1)
2
2Î»

6.1

Estimation of the parameters

The model parameters are , Î´ and c. However, as we have seen, the fundamental quantities are
Î· = 2( âˆ’ Î´) and Î» given in (11). Moreover, recall that in case a), we have Î· = 0 and Î» > 1 and, in
case b), we have Î· âˆˆ (0, 1) and Î» > 0. Therefore, according the considered model, the pair (Î·, Î»)

13

Figure 6: Convergence in distribution of N 1/2âˆ’(âˆ’Î´) Î¸N âˆ’1 i : 1 000 independent simulations of the
model by means of Example 3.3 with k = 3, b0 = p0 = (0.167, 0.333, 0.5) and different values of 
Î¸
and Î´ and plot of the distribution of N 1/2âˆ’(âˆ’Î´) âˆš N âˆ’1 i
for i = 1, 2, 3. In red: Standard Gaussian
Î»p0 i (1âˆ’p0 i )

distribution.
14

Figure 7: Convergence to zero of the difference (13): 1 000 independent simulations of the model
by means of Example 3.3 with k = 3, b0 = p0 = (0.167, 0.333, 0.5) and different values of  and Î´ and
plot of the distribution of the three components of (13).
belongs to S = {0} Ã— (1, +âˆ) âˆª (0, 1) Ã— (0, +âˆ). In order to estimate the pair (Î·, Î») âˆˆ S, we define
T` = N`Î· Q` =

k
X
Oi (`) âˆ’ N` pâˆ—0 i (`)
N` pâˆ—0 i (`)
i=1

2
.

Given the observed values t1 , . . . , tL , the log-likelihood function of Q` reads
L

ln(L(Î·, Î»)) = ln L(Î·, Î»; t1 , . . . , tL ) = âˆ’

kâˆ’1 X
kâˆ’1
L ln(Î») âˆ’
Î·
ln(N` ) âˆ’
2
2
`=1

1
2Î»

L
X

t`
N`Î·

+ R1 ,

`=1

where R1 is a remainder term that does not depend on (Î·, Î»). Now, we look for the maximum
likelihood estimator of the two parameters (Î·, Î»).
We immediately observe that, when all the clusters have the same cardinality, that is all the
N` are equal to a certain N0 , then we cannot hope to estimate Î· and Î», separately. Indeed, the
log-likelihood function becomes
ln(L(Î·, Î»)) = ln L(Î·, Î»; t1 , . . . , tL ) = âˆ’

i
kâˆ’1 h
L ln(Î») + Î· ln(N0 ) âˆ’
2

1
2Î»N0Î·

L
X

t` + R1 = f (Î»N0Î· ) .

`=1

dÎ· = PL t /(k âˆ’1)L.
This fact implies that it possible to estimate only the parameter (Î»N0Î· ) as Î»N
0
`=1 `
From now on, we assume that at least two clusters have different cardinality, that is at least a
pair of cardinalities N` are different. We have to find (if they exist!) the maximum points of the
function (Î·, Î») 7â†’ ln(L(Î·, Î»)) on the set S, which is not closed nor limited. First of all, we note that
ln(L(Î·, Î»)) â†’ âˆ’âˆ for Î» â†’ +âˆ and Î» â†’ 0. Thus, the log-likelihood function has maximum value
on the closure S of S and its maximum points are stationary points belonging to (0, 1) Ã— (0, +âˆ) or

15


Figure 8: Convergence in distribution of N 1/2âˆ’(âˆ’Î´) Î¾ N i âˆ’ p0 i : 1 000 independent simulations
of the model by means of Example 3.3 with k = 3, b0 = p0 = (0.167, 0.333, 0.5) and different values
of  and Î´ and plot of the distribution of N 1/2âˆ’(âˆ’Î´) âˆš Î¾N i âˆ’p0 i
for i = 1, 2, 3. In red: Standard
Î»Ì‚p0 i (1âˆ’p0 i )

Gaussian distribution.
16

they belong to {0, 1} Ã— (0, +âˆ). For detecting the points of the first type, we compute the gradient
of the log-likelihood function, obtaining
PL
PL t` ln(N` ) !
1
âˆ’ kâˆ’1
Î·
`=1 ln(N` ) + 2Î»
`=1
2
PL t` N`
âˆ‡(Î·, Î») ln L =
.
1
kâˆ’1
âˆ’ 2Î» L + 2Î»2 `=1 N Î·
`

Hence, the stationary points (Î·, Î») of the log-likelihood function are solutions of the system
ï£± PL t
PL
`
ï£´
Î· ln(N` )
ï£´
ln(N` )
ï£´ `=1 N`
ï£´
= `=1
P L t`
ï£´
ï£²
L
Î·
`=1 N`

PL t`
ï£´
ï£´
ï£´
`=1 N`Î·
ï£´
ï£´
ï£³Î» =
.
L(k âˆ’ 1)
In particular, we get that the stationary points are of the form (Î·, Î»(Î·)), with
PL t`
Î»(Î·) =

`=1 N`Î·

L(k âˆ’ 1)

.

(15)

In order to find the maximum points on the border, that is belonging to {0, 1} Ã— (0, +âˆ), we
observe that, fixed any Î·, the function
Î» 7â†’ âˆ’

kâˆ’1
L ln(Î») âˆ’
2

1
2Î»

L
X

t`
N`Î·

+ R2 ,

`=1

where R2 is a remainder term not depending on Î», takes its maximum value at the point Î»(Î·)
defined in (15).
Summing up, the problem of detecting the maximum points of the log-likelihood function on S
reduces to the study of the maximum points on [0, 1] of the function
Î· 7â†’ ln(L(Î·, Î»(Î·))) = âˆ’

L

L

`=1

`=1

X t  k âˆ’ 1 X
kâˆ’1
`
L ln
âˆ’
Î·
ln(N` ) + R3 ,
2
N`Î·
2

(16)

where R3 is a remainder term not depending on Î·. To this purpose, we note that we have
ï£®P
ï£¹
L
PL
t`
ln(N
)
ln(L(Î·, Î»(Î·))
k âˆ’ 1 ï£° `=1 N`Î· ln(N` )
`
ï£» = (k âˆ’ 1)L g(Î·) ,
d
=
L
âˆ’ `=1
PL t`
dÎ·
2
L
2
`=1 Î·
N`

where

PL
g(x) =

t`
`=1 N`x ln(N` )
PL t`
`=1 N`x

PL
âˆ’

ln(N` )
.
L

`=1

Setting
t`
N`x
P L tl
l=1 Nlx

p(x, `) =

and denoting by Ex [Â·] and by Eu [Â·] the mean value with respect to the discrete probability distribution {p(x, `) : ` = 1, . . . , L} on {N1 , . . . , NL } and with respect to the uniform discrete distribution
on {N1 , . . . , NL } respectively, the above function g can be written as
g(x) =

L
X
`=1

PL
p(x, `) ln(N` ) âˆ’

ln(N` )
= Ex [ln(N )] âˆ’ Eu [ln(N )] .
L

`=1

17

Moreover, we have

g 0 (x) =

âˆ’

PL

t`
`=1 N`x

 P
L
ln2 (N` )
`=1
P

t`
N`x

L
t`
`=1 N`x

=âˆ’

L
X

p(x, `) ln2 (N` ) +

L
X

`=1



+

P

L
t`
`=1 N`x

2
ln(N` )

2

p(x, `) ln(N` )

2

= âˆ’ V arx [ln(N )] ,

`=1

where V arx [Â·] denotes the variance with respect to the discrete probability distribution {p(x, `) :
` = 1, . . . , L} on {N1 , . . . , NL }. Since, we are assuming that at least two N` are different, we have
V arx [ln(N )] > 0 and so the function g is strictly decreasing. Finally, we observe that we have
PL

`=1 t`

Covu (ln(N ), T ) =
and

ln(N` )

L

PL

PL
âˆ’

`=1 t`

PL

L

PL

`=1 t`

L

PL t`
ln(N` )
`=1 N`
Covu (ln(N ),
=
âˆ’
= g(1)
,
L
L
L
L
where Covu (Â·, Â·) denotes the covariance with respect to the discrete joint distribution concentrated
on the diagonal and such that P {N = N` , T = t` } = 1/L with ` = 1, . . . , L. Hence, we distinguish
the following cases.
T
N)

t`
`=1 N`

ln(N` )

PL

ln(N` )
= g(0)
L

`=1

t`
`=1 N`

PL

`=1

First case: Covu (ln(N ), T ) â‰¤ 0
We are in the case when g(0) â‰¤ 0 and so the function (16) is strictly decreasing for Î·P> 0. Thus, its
L
b = Î»(0) = `=1 t` . Recall
maximum value on [0, 1] is assumed at Î·b = 0. Consequently, we have Î»
L(kâˆ’1)

b âˆˆ S and so Î»
b > 1. If the model fits well the data, this is a consequence. Indeed,
that we need (0, Î»)
d
b
b
b = Î» > 1. A value Î»
b â‰¤ 1 means
Î» is an unbiased estimator: Î» âˆ¼ Î“(L(k âˆ’ 1)/2, 1/(2Î»)) and so E[Î»]
a bad fit of the consider model to the data (the smaller the value of Î», the worse the fitting). Note
b = 1), the corresponding test statistics (14) and its distribution
that in the threshold case (b
Î· = 0, Î»
coincide with the classical ones used for independent observations.
T
Second case: Covu (ln(N ), T ) > 0 and Covu (ln(N ), N
)<0

We are in the case when g(0) > 0 and g(1) < 0. Hence, the function (16) has a unique stationary
t`
N`Î·b
L(kâˆ’1)

PL

b = Î»(b
point Î·b âˆˆ (0, 1), which is the maximum point. Consequently, we have Î»
Î·) =
b belongs to S.
The point (b
Î· , Î»)

`=1

> 0.

T
Third case: Covu (ln(N ), N
)â‰¥0

We are in the case when g(1) â‰¥ 0 and so the function (16) is strictly increasing on [0, 1]. Hence,
PL

t`

b = Î»(1) = `=1 N` . However, the point
its maximum point is at Î·b = 1, and, accordingly, we have Î»
L(kâˆ’1)
b does not belong to S and so, in this case, we conclude that we have a bad fit of the model to the
(1, Î»)
d

data. Note that, if the considered model fits well the data, then we have T /N âˆ¼ Î»e(Î·âˆ’1) ln(N ) Ï‡2 (k âˆ’
T
1) with Î· < 1 and, consequently, we expect Covu (ln(N ), N
) < 0. Moreover, a value Î· â‰¥ 1 in the
d

statistics (14) means a central limit theorem of the type N (1âˆ’Î·)/2 (Î¾ N âˆ’ p0 ) âˆ¼ N (0, CÎ“) with
(1 âˆ’ Î·)/2 â‰¤ 0. This is impossible since (Î¾ N âˆ’ p0 ) is bounded.

18

Figure 9: Plot of the function (16). Its maximum point gives the estimated value of the model
parameter Î·.

7

COVID-19 epidemic Twitter analysis

We illustrate the application of the above statistical methodology to a dataset containing posts
on the on-line social network Twitter about the COVID-19 epidemic. More precisely, the dataset
covers the period from February 20th (h. 11pm) to April to 20th (h. 10pm) 2020, including tweets
in Italian language. More details on the keywords used for the query can be found in [14]. For every
message, the relative sentiment has been calculated using the polyglot python module developed
in [46]. This module provides a numerical value v for the sentiment and we have fixed a threshold
T = 0.35 so that we have classified as a tweet with positive sentiment those with v > T and
as a tweet with negative sentiment those with v < âˆ’T . We have discarded tweets with a value
v âˆˆ [âˆ’T, T ].
We are in the case k = 2 and the random variables Î¾n = Î¾n 1 take the value 1 when the sentiment
of the post n is positive. We have partitioned the data so that each set Pd collect the messages of the
single day d, for d = 1(February 20st), . . . , 61(April 20th) and then, in order to obtain independent
clusters, we have set C` = P1+3(`âˆ’1) , for ` = 1, . . . , 21 = L. Therefore N` is the total number of
PL
tweets posted during the day 1 + 3(` âˆ’ 1) and N = `=1 N` = 699 450 is the sample size. Inside
each cluster, the â€œsentimentâ€ associated to each message is driven by a reinforcement mechanism,
that can be modeled by means of a GRP: the probability to have a tweet with positive sentiment is
increasing with the number of past tweets with positive sentiment and the reinforcement is mostly
driven by the most recent tweets (see [2]).
Our purpose is to test the null hypothesis H0 : p0 (`) = p0 for any `. Therefore, taking
PN
pâˆ—0 1 (`) = pâˆ—0 = n=1 Î¾n /N for each `, we have firstly estimated the model parameters and then
PL
we have performed the chi-squared test based on the aggregate statistics `=1 Q` and its corre1
, 2Î»
). The estimated values are Î·b = 0.4363572 and
sponding asymptotic distribution Î“( (Lâˆ’1)(kâˆ’1)
2
b
Î» = 2.728098 (in Fig. 9 we plot the function (16)).
The contingency table and the associated statistics for testing H0 is given in Table 1. The
obtained Ï‡2 -statistics for a usual Ï‡2 -test is 5507.803, which is significant at any level of
Pconfidence.
L
Under the proposed GRP model and the null hypothesis, the aggregate statistics
`=1 Q` has
Lâˆ’1 1
(asymptotic) distribution Î“( 2 , b ) and the corresponding p-value associated to the data is equal
2Î»
to 0.4579297. The null hypothesis that the daily sentiment rate of the posts is the same for all the
considered days is therefore strongly rejected with a classical Ï‡2 test, while the same hypothesis is
accepted if we take into account the reinforcement mechanism of correlation given in GRP model.
Finally, in Fig. 10 there are the values of the single statistics Q` . We have tested the inde-

19

Date
2020-02-20
2020-02-23
2020-02-26
2020-02-29
2020-03-03
2020-03-06
2020-03-09
2020-03-12
2020-03-15
2020-03-18
2020-03-21
2020-03-24
2020-03-27
2020-03-30
2020-04-02
2020-04-05
2020-04-08
2020-04-11
2020-04-14
2020-04-17
2020-04-20

Obs+
25
53564
29831
18220
16801
27906
41650
255
14193
12064
11571
13339
14798
12689
12714
13373
14889
12153
13406
13977
13753

Obsâˆ’
43
60476
37175
22184
14834
27030
34769
156
13562
10089
10026
9172
10039
10651
9300
10815
11987
10777
11430
11371
12393

Exp+
35.11
58886.18
34599.51
20863.18
16335.18
28366.99
39460.04
212.23
14331.69
11439.02
11151.92
11623.88
12824.94
12051.94
11367.24
12489.82
13877.81
11840.23
12824.42
13088.80
13500.86

Expâˆ’
32.89
55153.82
32406.49
19540.82
15299.82
26569.01
36958.96
198.77
13423.31
10713.98
10445.08
10887.12
12012.06
11288.06
10646.76
11698.18
12998.19
11089.77
12011.58
12259.20
12645.14

Ï‡2+
2.91
481.02
657.20
334.87
13.28
7.49
121.54
8.62
1.34
34.15
15.75
253.07
303.55
33.67
159.56
62.45
73.68
8.26
26.37
60.27
4.71

Ï‡2âˆ’
3.11
513.58
701.67
357.53
14.18
8.00
129.76
9.20
1.43
36.46
16.81
270.20
324.09
35.95
170.36
66.68
78.67
8.82
28.16
64.35
5.03

(c)

Ï‡2+
0.46
2.99
5.15
3.27
0.14
0.06
0.90
0.62
0.02
0.43
0.20
3.19
3.67
0.42
2.03
0.76
0.86
0.10
0.32
0.72
0.06

(c)

Ï‡2âˆ’
0.49
3.19
5.50
3.49
0.15
0.07
0.96
0.67
0.02
0.46
0.22
3.41
3.92
0.45
2.17
0.82
0.92
0.11
0.34
0.77
0.06

Table 1: Contingency table associated to COVID-Twitter data: Obs+ (Obsâˆ’ ) are the number of posts
with positive (negative) sentiment posted in the day ` reported in the first column (DataTime); Exp+
(Expâˆ’ ) corresponds to N` pâˆ—0 (resp. N` (1 âˆ’ pâˆ—0 )), where N` = Obs+ + Obsâˆ’ ; Ï‡2+ (Ï‡2âˆ’ ) is the quantity
(Obs+ âˆ’ Exp+ )2 /Exp+ (resp. (Obsâˆ’ âˆ’ Expâˆ’ )2 /Expâˆ’ ); Ï‡2+
Ï‡2âˆ’ /N`Î·b). The statistics Q` corresponds to Ï‡2+

(c)

+ Ï‡2âˆ’

(c)

(c)

(Ï‡2âˆ’

(c)

) is the quantity Ï‡2+ /N`Î·b (resp.

.

Figure 10: Plot of the Q` -series. The black line corresponds to the value of 95th-quantile of the
distribution Î“( 12 , 1b ), that is 10.48.
2Î»

20

Df
Ï‡2
pâˆ’value

1
3.454
0.063

2
3.624
0.163

3
4.209
0.240

4
4.640
0.326

5
5.065
0.408

6
7.103
0.311

7
8.660
0.278

8
8.812
0.358

9
10.360
0.322

10
12.852
0.232

Table 2: Summary of Ljungâ€“Box test for autocorrelation of Q` statistics with different numbers
of autocorrelation lags being tested. Df: number of lags under investigation; Ï‡2 : Ljungâ€“Box test
statistics, which is distributed as a Ï‡2 distribution with Df degrees of freedom under the null hypothesis
of independence; pâˆ’value: pâˆ’value of the Ljungâ€“Box test
pendence of the timed sequence {Q` } with a Ljungâ€“Box test and we give the results in Table 2.
The strong emotional involvement of those days had a â€œmixing effectsâ€ that cancelled possible
significant autocorrelation during different 3-delayed days.

8

Proof of Theorem 4.2

PN
PN
For all the sequel, we set Ïˆ N âˆ’1 = n=1 Ïˆnâˆ’1 /N and Î¸ N âˆ’1 = n=1 Î¸nâˆ’1 /N . To the proof of
Theorem 4.2, we premise some intermediate results.
Lemma 8.1. Under the same assumptions of Theorem 4.2, we have E[kÎ¸n k2 ] = O(nâˆ’2Î´ ) â†’ 0.
Proof. We observe that, starting from (8), we get
kÎ¸n+1 k2 = Î¸n+1 > Î¸n+1 = (1 âˆ’ n )2 kÎ¸n k2 + Î´n2 kâˆ†Mn+1 k2 + 2(1 âˆ’ n )Î´n Î¸n > âˆ†Mn+1
and so
E[kÎ¸n+1 k2 |Fn ] = (1 âˆ’ n )2 kÎ¸n k2 + Î´n2 E[kâˆ†Mn+1 k2 |Fn ] .

(17)

2

Hence, setting xn = E[kÎ¸n k ], we get
xn+1 = (1 âˆ’ 2n )xn + 2n xn + Î´n2 E[kâˆ†Mn+1 k2 ]


Î´2
= (1 âˆ’ 2n )xn + n n xn + n E[kâˆ†Mn+1 k2 ]
n
= (1 âˆ’ 2n )xn + 2n Î¶n ,
with 0 â‰¤ Î¶n =



n x n +

2
Î´n
2
n E[kâˆ†Mn+1 k ]



/2. Applying Lemma S4.4 (with Î³n = 2n ), we find

that lim supn xn â‰¤ lim supn Î¶n . On the other hand, since (âˆ†Mn+1 )n is uniformly bounded and
2
2
2n /Î´n2 âˆ¼ câˆ’2 nâˆ’2(âˆ’Î´) â†’ 0, we have Î¶n = O(n + Î´n2 âˆ’1
n ) = O(Î´n /n ) and so xn = O(Î´n /n ). We can
2 âˆ’2Î´
2
.
conclude recalling that Î´n /n âˆ¼ c n
Lemma 8.2. Under the same assumptions of Theorem 4.2, we have
Î¸ N âˆ’1 =

N
N âˆ’1
1 X
1 X Î´n
Î¸nâˆ’1 =
âˆ†Mn+1 + RN ,
N n=1
N n=0 n



a.s.
where RN âˆ’â†’ 0 and N e E |RN | âˆ’â†’ 0 with e = 1/2 âˆ’ ( âˆ’ Î´) âˆˆ (0, 1/2).
Proof. By (9), we have
Î¸n = âˆ’

1
Î´n
(Î¸n+1 âˆ’ Î¸n ) + âˆ†Mn+1 .
n
n

21

(18)

Therefore, we can write
N
âˆ’1
X

N
âˆ’1
X

N âˆ’1

X Î´n
1
Î¸n = âˆ’
(Î¸n+1 âˆ’ Î¸n ) +
âˆ†Mn+1


n=0
n=0 n
n=0 n

 NX

âˆ’1 
N
âˆ’1
X
Î¸N
Î¸0
1
1
Î´n
=âˆ’
âˆ’
âˆ’
âˆ’
Î¸n +
âˆ†Mn+1 ,
N âˆ’1
0



nâˆ’1
n
n=1
n=0 n

where the second equality is due to the Abel transformation for a series. It follows the decomposition
(18) with



N âˆ’1 
1
Î¸N
Î¸0
1 X
1
1
RN = âˆ’
âˆ’
âˆ’
âˆ’
Î¸n .
(19)
N N âˆ’1
0
N n=1 nâˆ’1
n
Since |Î¸n | = O(1), we have
|RN | =

O(N âˆ’1 âˆ’1
N âˆ’1 )

+O N

âˆ’1

N
âˆ’1
X

!
|âˆ’1
nâˆ’1

âˆ’

âˆ’1
n |

n=1

PN âˆ’1
âˆ’1
âˆ’1
âˆ’1
Note that n=1 |âˆ’1
nâˆ’1 âˆ’n | = 0 âˆ’N âˆ’1 when (n ) is decreasing and so the last term in the above
âˆ’(1âˆ’)
expression is O(N âˆ’1 âˆ’1
)â†’
N âˆ’1 ). Therefore, since  < 1 by assumption, we have |RN | = O(N
0.
Regarding the
 last statement of the lemma, we observe that, from what we have proven before,
we obtain N e E |RN | = O(N eâˆ’(1âˆ’) ) = O(N Î´âˆ’1/2 ) â†’ 0 when Î´ < 1/2. However, in the considered
cases 1) and 2), we might have Î´ â‰¥ 1/2. Therefore, we need other arguments in order to prove the
last statement. To this purpose, we observe that, by Lemma 8.1, we have E[ |Î¸n | ] = O(n/2âˆ’Î´ )
and so, by (19), we have
!
N âˆ’1


1 X âˆ’1
âˆ’1 /2âˆ’Î´
e
âˆ’(1âˆ’e) 3/2âˆ’Î´
|
âˆ’ n |n
N E |RN | = O(N
N
)+O
N 1âˆ’e n=1 nâˆ’1
!
N âˆ’1
1 X âˆ’1
âˆ’1 /2âˆ’Î´
âˆ’(1âˆ’)/2
|
âˆ’ n |n
.
= O(N
)+O
N 1âˆ’e n=1 nâˆ’1
Moreover, we have
N
âˆ’1
X
n=1

âˆ’1 /2âˆ’Î´
|âˆ’1
=
nâˆ’1 âˆ’ n |n

N
âˆ’1
X

[(n âˆ’ 1) âˆ’ n ] n/2âˆ’Î´ =

N
âˆ’1
X

nâˆ’1+/2âˆ’Î´ âˆ¼ N 3/2âˆ’Î´ = o(N 1âˆ’e ) ,

n=1

n=1

because e = 1/2 âˆ’ ( âˆ’ Î´) and  < 1. Summing up, we have N e E[|RN |] = O(N âˆ’(1âˆ’)/2 ) + o(1) â†’
0.
a.s.

a.s.

Lemma 8.3. Under the same assumptions of Theorem 4.2, we have Î¸ N âˆ’1 âˆ’â†’ 0, that is Ïˆ N âˆ’1 âˆ’â†’
a.s.
a.s.
p0 . In particular, when  âˆˆ (1/2, 1) and Î´ âˆˆ (1/2, ), we have Î¸N âˆ’â†’ 0, that is ÏˆN âˆ’â†’ p0 .
Note that, when  âˆˆ (1/2, 1) and Î´ âˆˆ (1/2, ), we have the typical asymptotic behaviour of
the predictive mean of an urn process, that is its almost sure convergence. In the complementary
case, it seems to us not so easy to check if this type of convergence holds true. Therefore, for
the proof of Theorem 4.2 in this last case, we will employ a different technique, which is based on
the L2 -estimate of Lemma 8.1 for the predictive mean ÏˆN and the almost sure convergence of the
corresponding empirical mean Ïˆ N âˆ’1 .
Proof. Let us distinguish the following two cases:

22

1)  âˆˆ (1/2, 1) and Î´ âˆˆ (1/2, ) or
2)  âˆˆ (0, 1) and Î´ âˆˆ (/2, min{, 1/2}] \ {}.
For the case 1), we observe that, by (17), we have
E[kÎ¸n+1 k2 |Fn ] â‰¤ (1 + 2n )E[kÎ¸n k2 |Fn ] + Î´n2 E[kâˆ†Mn+1 k2 |Fn ].
P 2
Therefore,
since (âˆ†Mn+1 )n is uniformly bounded and, in case 1), we have
n n < +âˆ and
P 2
2
Î´
<
+âˆ,
the
sequence
(kÎ¸
k
)
is
a
bounded
non-negative
almost
supermartingale.
As a
n
n
n n
consequence, it converges almost surely to a certain random variable. This limit random variable
is necessarily equal to 0 because, by Lemma 8.1, we have E[kÎ¸n k2 ] = O(nâˆ’2Î´ ) â†’ 0. Hence, we
have the almost sure convergence of Î¸N to 0 and, consequently, the almost sure convergence of
Î¸ N âˆ’1 to 0 follows by Lemma S4.2 and Remark S4.3 (with cn = n and vN,n = n/N ), because
E[Î¸nâˆ’1 |Fnâˆ’2 ] = (1 âˆ’ nâˆ’2 )Î¸nâˆ’2 â†’ 0 almost surely.
a.s.
For the case 2), we use Lemma 8.2, that gives the decomposition (18), with RN âˆ’â†’ 0. Indeed,
PN âˆ’1 Î´n
by this decomposition, it is enough to prove that the term n=0 n âˆ†Mn+1 /N converges almost
surely to 0. To this purpose, we observe that, if we set
Ln =

n
X
1 Î´jâˆ’1
j=1

j jâˆ’1

âˆ†Mj ,

then (Ln ) is a square integrable martingale. Indeed, we have
+âˆ
2
X
1 Î´nâˆ’1
E[kâˆ†Mn k2 ] = O
2 2
n
nâˆ’1
n=1

+âˆ
X
n=1

Therefore, (Ln ) converges almost surely, that is we have
Applying Lemma S4.1 (with vN,n = n/N ), we find

!

1
n1+2e

< +âˆ .

1 Î´nâˆ’1
n n nâˆ’1 âˆ†Mn

P

< +âˆ almost surely.

N âˆ’1
N
X
1 X Î´n
1 Î´nâˆ’1
a.s.
âˆ†Mn+1 =
vN,n
âˆ†Mn âˆ’â†’ 0
N n=0 n
n

nâˆ’1
n=1
a.s.

and so Î¸ N âˆ’1 âˆ’â†’ 0.
Proof of Theorem 4.2. Set e = 1/2 âˆ’ ( âˆ’ Î´) âˆˆ (0, 1/2) and Î» = c2 /[2(1 âˆ’ e)] = c2 /[1 + 2( âˆ’ Î´)].
Moreover, let us distinguish the following two cases:
1)  âˆˆ (1/2, 1) and Î´ âˆˆ (1/2, ) or
2)  âˆˆ (0, 1) and Î´ âˆˆ (/2, min{, 1/2}] \ {}.
Almost sure convergence: In case 1), by Lemma 8.3, ÏˆN converges almost surely to p0 . Therefore,
the almost sure convergence of Î¾ N to p0 follows by Lemma S4.2 and P
Remark S4.3 (with cP
n = n and
vN,n = n/N ), because E[Î¾n+1 |Fn ] = Ïˆn â†’ p0 almost surely and n E[kÎ¾n k2 ]nâˆ’2 â‰¤ n nâˆ’2 <
+âˆ.
In case 2), we use a different argument. Take Î³ âˆˆ [0, e) and set
Ln =

n
X
j=1

1 Î´jâˆ’1
âˆ†Mj .
j 1âˆ’Î³ jâˆ’1

Then (Ln ) is a square integrable martingale, because we have
+âˆ
X
n=1

1
n2âˆ’2Î³

2
Î´nâˆ’1
E[kâˆ†Mn k2 ] = O
2
nâˆ’1

23

+âˆ
X
n=1

1
n1+2eâˆ’2Î³

!
< +âˆ .

P
1 Î´nâˆ’1
Therefore, (Ln ) converges almost surely, that is we have n n1âˆ’Î³
nâˆ’1 âˆ†Mn < +âˆ almost surely.
1âˆ’Î³
1âˆ’Î³âˆ’+Î´
1âˆ’Î³
By Lemma S4.1 (with vN,n = (n/N )
nâˆ’1 /Î´nâˆ’1 âˆ¼ n
/N
), we get
N
âˆ’1
X

1
N 1âˆ’Î³

N
X

âˆ†Mn+1 =

n=0

vN,n

n=1

1
n1âˆ’Î³

Î´nâˆ’1
a.s.
âˆ†Mn âˆ’â†’ 0.
nâˆ’1

Therefore, we have

N Î³ Î¾ N âˆ’ Ïˆ N âˆ’1 =

1
N 1âˆ’Î³

N
âˆ’1
X

a.s.

âˆ†Mn+1 âˆ’â†’ 0,

n=0

âˆ’Î³



that is Î¾ N âˆ’ Ïˆ N âˆ’1 = o(N ) for each Î³ âˆˆ [0, e). Recalling Lemma 8.3, we obtain in particular
that Î¾ N converges almost surely to p0 .
Second order asymptotic behaviour: We have
âˆš


N e Î¾ N âˆ’ p0 = N e ÂµN = N eâˆ’1/2 N ÂµN âˆ’ Î¸ N âˆ’1 + N e Î¸ N âˆ’1 .
(20)
Moreover, by Lemma 8.1, we have
N âˆ’1
N
X
1 X
E[|Î¸n |] = O(N âˆ’1
n/2âˆ’Î´ ) = O(N âˆ’1âˆ’Î´+/2+1 ) = O(N /2âˆ’Î´ ) â†’ 0 ,
N n=0
n=1
N âˆ’1
N
X
1 X
2
âˆ’1
E[kÎ¸n k ] = O(N
nâˆ’2Î´ ) = O(N âˆ’1âˆ’2Î´++1 ) = O(N âˆ’2Î´ ) â†’ 0 ,
N n=0
n=1

and so Theorem S1.1 holds true with V = Î“ (see Remark S1.2). Therefore, the first term in the
right side of (20) converges in probability to 0 because e < 1/2. Hence, if we prove that
s

N e Î¸ N âˆ’1 âˆ’â†’ N (0, Î»Î“) ,

(21)

then the proof is concluded.
In order to prove (21), we observe that, by decomposition (18) in Lemma 8.2, we have
N e Î¸ N âˆ’1 =

N
X

YN,n + N e RN ,

n=1
1 Î´nâˆ’1
N 1âˆ’e nâˆ’1 âˆ†Mn


and N e RN converges in probability to 0 (because N e E |RN |] â†’
PN
0). Therefore, it is enough to prove that the term n=1 YN,n stably converges to the Gaussian
kernel N (0, Î»Î“), with Î» = c2 /[2(1 âˆ’ e)] = c2 /[1 + 2( âˆ’ Î´)]. To this purpose, we observe that
PN
E[YN,n |Fnâˆ’1 ] = 0 and so n=1 YN,n converges stably to N (0, Î»Î“) if the conditions (c1) and (c2) of
Theorem S6.1, with V = Î»Î“, hold true. Regarding (c1), we note that Î´nâˆ’1 /nâˆ’1 âˆ¼ cnâˆ’Î´ = cn1/2âˆ’e
and so we have
where YN,n =

max |YN,n | â‰¤ N âˆ’(1âˆ’e) max

1â‰¤nâ‰¤N

1â‰¤nâ‰¤N

Î´nâˆ’1
Î´nâˆ’1
|Î¾n âˆ’ Ïˆnâˆ’1 | â‰¤ N âˆ’(1âˆ’e) max
= O(N âˆ’1/2 ) â†’ 0 .
1â‰¤nâ‰¤N nâˆ’1
nâˆ’1

Condition (c2) means
1
N 2(1âˆ’e)
We note that N âˆ’2(1âˆ’e)

PN

N
2
X
Î´nâˆ’1
P
(Î¾n âˆ’ Ïˆnâˆ’1 )(Î¾n âˆ’ Ïˆnâˆ’1 )> âˆ’â†’ Î»Î“.
2

n=1 nâˆ’1

2
2
n=1 Î´nâˆ’1 /nâˆ’1

2
â†’ Î», because Î´nâˆ’1
/2nâˆ’1 âˆ¼ c2 n1âˆ’2e , and

E[(Î¾n âˆ’ Ïˆnâˆ’1 )(Î¾n âˆ’ Ïˆnâˆ’1 )> |Fnâˆ’1 ] = diag(Ïˆnâˆ’1 ) âˆ’ Ïˆnâˆ’1 Ïˆnâˆ’1 > .

24

(22)

Therefore, in case 1), condition (22) immediately follows by the almost sure convergence of Ïˆn to p0 .
2
It is enough to apply Lemma S4.2 and Remark S4.3 with cn = n and vN,n = nÎ´nâˆ’1
/(N 2(1âˆ’e) 2nâˆ’1 ) âˆ¼
2 1+2(âˆ’Î´)
2âˆ’2e
2
2(1âˆ’e)
c n
/N
= c (n/N )
. In case 2), we apply again Lemma S4.2 with the above cn
and vN,n , but we note that Ïˆn = Î¸n + p0 and so condition (S4.6) in Lemma S4.2, with V = Î»Î“,
is equivalent to
1
N 2âˆ’2e

N
âˆ’1
X
n=0

Î´n2
P
Î¸n âˆ’â†’ 0
2n

1

and

N 2âˆ’2e

N
âˆ’1
X
n=0

Î´n2
P
Î¸n Î¸n > âˆ’â†’ 0kÃ—k .
2n

These two convergences hold true because, by Lemma 8.1, we have
N
âˆ’1
X

1
N 2âˆ’2e
1
N 2âˆ’2e

n=0

N
âˆ’1
X
n=0

N
X
Î´n2
âˆ’2+2e
E[|Î¸n |] = O(N
nâˆ’2Î´+2âˆ’Î´+/2 ) = O(N âˆ’2+2eâˆ’3Î´+5/2+1 ) = O(N âˆ’Î´+/2 ) â†’ 0 ,
2n
n=1

N
X
Î´n2
2
âˆ’2+2e
E[kÎ¸
k
]
=
O(N
nâˆ’2Î´+2âˆ’2Î´+ ) = O(N âˆ’2+2eâˆ’4Î´+3+1 ) = O(N âˆ’2Î´+ ) â†’ 0 .
n
2n
n=1

Therefore, in both cases 1) and 2), conditions c1) and c2) of Theorem S6.1 are satisfied and so
PN
n=1 YN,n stably converges to the Gaussian kernel N (0, Î»Î“).
Declaration
Both authors equally contributed to this work.
Acknowledgments
Giacomo Aletti is a member of the Italian Group â€œGruppo Nazionale per il Calcolo Scientificoâ€ of
the Italian Institute â€œIstituto Nazionale di Alta Matematicaâ€ and Irene Crimaldi is a member of the
Italian Group â€œGruppo Nazionale per lâ€™Analisi Matematica, la ProbabilitaÌ€ e le loro Applicazioniâ€
of the Italian Institute â€œIstituto Nazionale di Alta Matematicaâ€.
Funding Sources
Irene Crimaldi is partially supported by the Italian â€œProgramma di AttivitaÌ€ Integrataâ€ (PAI),
project â€œTOol for Fighting FakEsâ€ (TOFFE) funded by IMT School for Advanced Studies Lucca.

References
[1] Generalized rescaled PoÌlya urn and its statistical applications. Supplementary Material of this
article (2020)
[2] Aletti Giacomo, C.I., Saracco, F.: A model for the twitter sentiment curve. arXiv:2011.05933
(2020)
[3] Aletti, G., Crimaldi, I.: The rescaled PoÌlya urn: local reinforcement and chi-squared goodness
of fit test. arXiv:1906.10951 (2019)
[4] Aletti, G., Crimaldi, I., Ghiglietti, A.: Synchronization of reinforced stochastic processes with
a network-based interaction. Ann. Appl. Probab. 27(6), 3787â€“3844 (2017). DOI 10.1214/
17-AAP1296. URL https://doi.org/10.1214/17-AAP1296
[5] Aletti, G., Crimaldi, I., Ghiglietti, A.: Networks of reinforced stochastic processes: asymptotics
for the empirical means. Bernoulli 25(4B), 3339â€“3378 (2019)
[6] Aletti, G., Crimaldi, I., Ghiglietti, A.: Interacting reinforced stochastic processes: Statistical
inference based on the weighted empirical means. Bernoulli 26(2), 1098â€“1138 (2020)

25

[7] Aletti, G., Ghiglietti, A., Paganoni, A.M.: Randomly reinforced urn designs with prespecified
allocations. J. Appl. Probab. 50(2), 486â€“498 (2013). DOI 10.1239/jap/1371648956. URL
https://doi.org/10.1239/jap/1371648956
[8] Aletti, G., Ghiglietti, A., Rosenberger, W.F.: Nonparametric covariate-adjusted responseadaptive design based on a functional urn model. Ann. Statist. 46(6B), 3838â€“3866 (2018).
DOI 10.1214/17-AOS1677. URL https://doi.org/10.1214/17-AOS1677
[9] Aletti, G., Ghiglietti, A., Vidyashankar, A.N.: Dynamics of an adaptive randomly reinforced
urn. Bernoulli 24(3), 2204â€“2255 (2018). DOI 10.3150/17-BEJ926. URL https://doi.org/
10.3150/17-BEJ926
[10] Bergh, D.: Sample size and chi-squared test of fitâ€” a comparison between a random sample approach and a chi-square value adjustment method using swedish adolescent data. In:
Q. Zhang, H. Yang (eds.) Pacific Rim Objective Measurement Symposium (PROMS) 2014
Conference Proceedings, pp. 197â€“211. Springer Berlin Heidelberg, Berlin, Heidelberg (2015)
[11] Berti, P., Crimaldi, I., Pratelli, L., Rigo, P.: Asymptotics for randomly reinforced urns with
random barriers. J. Appl. Probab. 53(4), 1206â€“1220 (2016). DOI 10.1017/jpr.2016.75. URL
https://doi.org/10.1017/jpr.2016.75
[12] Bertoni, D., Aletti, G., Ferrandi, G., Micheletti, A., Cavicchioli, D., Pretolani, R.: Farmland
use transitions after the cap greening: a preliminary analysis using markov chains approach.
Land Use Policy 79, 789 â€“ 800 (2018). DOI https://doi.org/10.1016/j.landusepol.2018.09.012.
URL http://www.sciencedirect.com/science/article/pii/S0264837718308676
[13] Caldarelli, G., Chessa, A., Crimaldi, I., Pammolli, F.: Weighted networks as randomly reinforced urn processes. Phys. Rev. E 87, 020106 (2013)
[14] Caldarelli G. de Nicola R., P.M.P.M., F., S.: Analysis of online misinformation during the
peak of the covid-19 pandemics in italy. arXiv: 2010.01913 (2020)
[15] Chanda, K.C.: Chi-squared tests of goodness-of-fit for dependent observations. In: Asymptotics, Non-Parametrics and Time Series, Statist. Textbooks Monogr., vol. 158, pp. 743â€“756.
Dekker (1999)
[16] Chen, M.R., Kuba, M.: On generalized poÌlya urn models. J. Appl. Probab. 50(4), 1169â€“1186
(2013). DOI 10.1239/jap/1389370106. URL https://doi.org/10.1239/jap/1389370106
[17] Chessa, A., Crimaldi, I., Riccaboni, M., Trapin, L.: Cluster analysis of weighted bipartite
networks: A new copula-based approach. PLOS ONE 9(10), 1â€“12 (2014). DOI 10.1371/
journal.pone.0109507. URL https://doi.org/10.1371/journal.pone.0109507
[18] Collevecchio, A., Cotar, C., LiCalzi, M.: On a preferential attachment and generalized poÌlyaâ€™s
urn model. Ann. Appl. Probab. 23(3), 1219â€“1253 (2013). DOI 10.1214/12-AAP869. URL
https://doi.org/10.1214/12-AAP869
[19] Crimaldi, I.: Central limit theorems for a hypergeometric randomly reinforced urn. J. Appl.
Probab. 53(3), 899â€“913 (2016). DOI 10.1017/jpr.2016.48. URL https://doi.org/10.1017/
jpr.2016.48
[20] Crimaldi, I., Dai Pra, P., Louis, P.Y., Minelli, I.G.: Synchronization and functional central
limit theorems for interacting reinforced random walks. Stochastic Processes and their Applications 129(1), 70â€“101 (2019)
[21] Crimaldi, I., Dai Pra, P., Minelli, I.G.: Fluctuation theorems for synchronization of interacting
PoÌlyaâ€™s urns. Stochastic Process. Appl. 126(3), 930â€“947 (2016). DOI 10.1016/j.spa.2015.10.
005. URL https://doi.org/10.1016/j.spa.2015.10.005
[22] Dai Pra, P., Louis, P.Y., Minelli, I.G.: Synchronization via interacting reinforcement. J. Appl.
Probab. 51(2), 556â€“568 (2014). DOI 10.1239/jap/1402578643. URL http://dx.doi.org/10.
1239/jap/1402578643

26

[23] Eggenberger, F., PoÌlya, G.: UÌˆber die statistik verketteter vorgaÌˆnge. ZAMM - Journal of Applied Mathematics and Mechanics / Zeitschrift fuÌˆr Angewandte Mathematik und Mechanik
3(4), 279â€“289 (1923). DOI 10.1002/zamm.19230030407. URL https://onlinelibrary.
wiley.com/doi/abs/10.1002/zamm.19230030407
[24] Gasser, T.: Goodness-of-fit tests for correlated data. Biometrika 62(3), 563â€“570 (1975). DOI
10.1093/biomet/62.3.563
[25] Ghiglietti, A., Paganoni, A.M.: Statistical properties of two-color randomly reinforced urn
design targeting fixed allocations. Electron. J. Statist. 8(1), 708â€“737 (2014). DOI 10.1214/
14-EJS899. URL https://doi.org/10.1214/14-EJS899
[26] Ghiglietti, A., Vidyashankar, A.N., Rosenberger, W.F.: Central limit theorem for an adaptive
randomly reinforced urn model. Ann. Appl. Probab. 27(5), 2956â€“3003 (2017). DOI 10.1214/
16-AAP1274. URL https://doi.org/10.1214/16-AAP1274
[27] Gleser, L.J., Moore, D.S.: The effect of dependence on chi-squared and empiric distribution
tests of fit. The Annals of Statistics 11(4), 1100â€“1108 (1983). URL http://www.jstor.org/
stable/2241300
[28] Holmes, M., Sakai, A.: Senile reinforced random walks. Stochastic Processes and their Applications 117(10), 1519â€“1539 (2007)
[29] Ieva, F., Paganoni, A.M., Pigoli, D., Vitelli, V.: Multivariate functional clustering for the
morphological analysis of electrocardiograph curves. Journal of the Royal Statistical Society.
Series C (Applied Statistics) 62(3), 401â€“418 (2013). URL http://www.jstor.org/stable/
24771812
[30] Knoke, D., Bohrnstedt, G.W., Potter Mee, A.: Statistics for Social Data Analysis. F.E.Peacock
Publishers (2002)
[31] Kushner, H.J., Yin, G.G.: Stochastic approximation and recursive algorithms and applications,
Applications of Mathematics (New York), vol. 35, second edn. Springer-Verlag, New York
(2003). Stochastic Modelling and Applied Probability
[32] Laruelle, S., PageÌs, G.: Randomized urn models revisited using stochastic approximation.
Ann. Appl. Proba. 23(4), 1409â€“1436 (2013)
[33] Lasmar, N., Mailler, C., Selmi, O.: Multiple drawing multi-colour urns by stochastic approximation. J. Appl. Probab. 55(1), 254â€“281 (2018)
[34] Mahmoud, H.M.: PoÌlya urn models. Texts in Statistical Science Series. CRC Press, Boca
Raton, FL (2009)
[35] Micheletti, A., Aletti, G., Ferrandi, G., Bertoni, D., Cavicchioli, D., Pretolani, R.: A weighted
Ï‡2 test to detect the presence of a major change point in non-stationary Markov chains (2019).
Submitted for publication
[36] Pan, W.: Goodness-of-fit tests for GEE with correlated binary data. Scand. J. Statist. 29(1),
101â€“110 (2002). DOI 10.1111/1467-9469.00091
[37] Pelletier, M.: Weak convergence rates for stochastic approximation with application to multiple
targets an simulated annealing. Ann. Appl. Probab. 8(1), 10â€“44 (1998)
[38] Pemantle, R.: A time-dependent version of poÌlyaâ€™s urn. J. Theor. Probab. 3, 627â€“637 (1990)
[39] Pemantle, R.: A survey of random processes with reinforcement. Probab. Surveys 4, 1â€“79
(2007). DOI 10.1214/07-PS094. URL https://doi.org/10.1214/07-PS094
[40] Radlow, R., Alf Jr., E.F.: An alternate multinomial assessment of the accuracy of the Ï‡2 test
of goodness of fit. Journal of the American Statistical Association 70(352), 811â€“813 (1975).
DOI 10.1080/01621459.1975.10480306

27

[41] Rao, J.N.K., Scott, A.J.: The analysis of categorical data from complex sample surveys: chisquared tests for goodness of fit and independence in two-way tables. J. Amer. Statist. Assoc.
76(374), 221â€“230 (1981)
[42] Sahasrabudhe, N.: Synchronization and fluctuation theorems for interacting Friedman urns.
J. Appl. Probab. 53(4), 1221â€“1239 (2016). DOI 10.1017/jpr.2016.76. URL http://dx.doi.
org/10.1017/jpr.2016.76
[43] Tang, M.L., Pei, Y.B., Wong, W.K., Li, J.L.: Goodness-of-fit tests for correlated paired binary
data. Stat. Methods Med. Res. 21(4), 331â€“345 (2012). DOI 10.1177/0962280210381176
[44] Tharwat, A.: Independent component analysis: An introduction. Applied Computing
and Informatics (2018). DOI https://doi.org/10.1016/j.aci.2018.08.006. URL http://www.
sciencedirect.com/science/article/pii/S2210832718301819
[45] Xu, D., Tian, Y.: A comprehensive survey of clustering algorithms. Annals of Data Science
2(2), 165â€“193 (2015)
[46] Y., C., S., S.: Building sentiment lexicons for all major languages. In: Proceedings of the 52nd
Annual Meeting of the Association for Computational Linguistics (Short Papers), pp. 383â€“389
(2014)

28

Supplemental Materials
In this document we collect some proofs, complements, technical results and recalls, useful for [S01].
Therefore, the notation and the assumptions used here are the same as those used in that paper.

S1

Proofs and intermediate results

We here collect some proofs omitted in the main text of the paper [S01].

S1.1

Proof of Theorem 3.1

The proof is based on Theorem 4.1 (for case a)) and Theorem 4.2 (for case b)). The almost
sure convergence of Oi /N immediately follows since Oi /N = Î¾ N i . In order to prove the stated
convergence in distribution, we mimic the classical proof for the Pearson chi-squared test based on
the Sherman Morison formula (see [S18]), but see also [S16, Corollary 2].
We start recalling the Sherman Morison formula: if A is an invertible square matrix and we
have 1 âˆ’ v > Aâˆ’1 u 6= 0, then
(A âˆ’ uv > )âˆ’1 = Aâˆ’1 +

Aâˆ’1 uv > Aâˆ’1
.
1 âˆ’ v > Aâˆ’1 u

âˆ—
Given the observation Î¾n = (Î¾n 1 , . . . , Î¾n k )> , we define the â€œtruncatedâ€ vector Î¾n
= (Î¾nâˆ— 1 , . . . , Î¾nâˆ— kâˆ’1 )> ,
given by the first k âˆ’ 1 components of Î¾n . Theorem 4.1 (for case a)) and Theorem 4.2 (for case b))
give the second order asymptotic behaviour of (Î¾n ), that immediately implies

N

e



âˆ—
Î¾N

âˆ—

âˆ’p



PN
=

âˆ—
n=1 (Î¾n âˆ’
N 1âˆ’e

pâˆ— )

d

âˆ’â†’ N (0, Î“âˆ— ),

(S1.1)

where pâˆ— is given by the first k âˆ’ 1 components of p0 and Î“âˆ— = Î»(diag(pâˆ— ) âˆ’ pâˆ— pâˆ— T ). By assumption p0 i > 0 for all i = 1, . . . , k and so diag(pâˆ— ) is invertible with inverse diag(pâˆ— )âˆ’1 =
1
) and, since (diag(pâˆ— )âˆ’1 )pâˆ— = 1 âˆˆ Rkâˆ’1 , we have
diag( p01 1 , . . . , p0 kâˆ’1
1 âˆ’ pâˆ— T diag(pâˆ— )âˆ’1 pâˆ— = 1 âˆ’

kâˆ’1
X

p0 i =

i=1

k
X

p0 i âˆ’

i=1

kâˆ’1
X

p0 i = p0 k > 0.

i=1

Therefore we can use the Sherman Morison formula with A = diag(pâˆ— ) and u = v = pâˆ— , and we
obtain

1
1
1
1
(Î“âˆ— )âˆ’1 = (diag(pâˆ— ) âˆ’ pâˆ— pâˆ— T )âˆ’1 =
diag( p01 1 , . . . , p0 kâˆ’1
)+
11> .
(S1.2)
Î»
Î»
p0 k
Pk
Pkâˆ’1
Now, since i=1 (Î¾ N i âˆ’ p0 i ) = 0, then Î¾ N k âˆ’ p0 k = i=1 (Î¾ N i âˆ’ p0 i ) and so we get
k
X
(Oi âˆ’ N p0 i )2
i=1

N p0 i

k
X
(Î¾

kâˆ’1

h X (Î¾ âˆ’ p )2
âˆ’ p0 i )2
(Î¾
âˆ’ p0 k )2 i
0i
Ni
=N
+ Nk
p0 i
p0 i
p0 k
i=1
i=1
P
kâˆ’1
h kâˆ’1
X (Î¾ âˆ’ p0 i )2
( i=1 (Î¾ N i âˆ’ p0 i ))2 i
Ni
=N
+
p0 i
p0 k
i=1
=N

=N

Ni

kâˆ’1
X
i1 ,i2


1
1 
(Î¾ N i1 âˆ’ p0 i1 )(Î¾ N i2 âˆ’ p0 i2 ) Ii1 ,i2
+
,
p0 i1
p0 k
=1

S1

where Ii1 i2 is equal to 1 if i1 = i2 and equal to zero otherwise. Finally, from the above equalities,
recalling (S1.1) and (S1.2), we obtain
k
X
(Oi âˆ’ N p0 i )2

1
N 1âˆ’2e

i=1

N p0 i

âˆ—

âˆ—

d

= Î»N 2e (Î¾ N âˆ’ pâˆ— )> (Î“âˆ— )âˆ’1 (Î¾ N âˆ’ pâˆ— ) âˆ’â†’ Î»W0 = Wâˆ— ,

where 1 âˆ’ 2e â‰¥ 0 and W0 is a random variable with distribution Ï‡2 (k âˆ’ 1) = Î“((k âˆ’ 1)/2, 1/2),
where Î“(a, b) denotes the Gamma distribution with density function
f (w) =

ba aâˆ’1 âˆ’bw
w
e
.
Î“(a)

As a consequence, Wâˆ— has distribution Î“((k âˆ’ 1)/2, 1/(2Î»)).

S1.2

A preliminary central limit theorem

The following preliminary central limit theorem is useful for the proofs of the other central limit
theorems stated in [S01] and in Section S2.
Theorem S1.1. If
N
1 X
P
diag(Ïˆnâˆ’1 ) âˆ’ Ïˆnâˆ’1 Ïˆnâˆ’1 > âˆ’â†’ V ,
N n=1

(S1.3)

where V is a random variable with values in the space of positive semidefinite k Ã— k-matrices, then
âˆš
 s
 âˆš
N ÂµN âˆ’ Î¸ N âˆ’1 = N Î¾ N âˆ’ Ïˆ N âˆ’1 âˆ’â†’ N (0, V ) .
Proof. We can write
âˆš

N


1
1 X
N Î¾ N âˆ’ Ïˆ N âˆ’1 = âˆš N Î¾ N âˆ’ Ïˆ N âˆ’1 = âˆš
(Î¾n âˆ’ Ïˆnâˆ’1 )
N
N n=1
N
N
X
1 X
YN,n ,
âˆ†Mn =
=âˆš
N n=1
n=1

PN
with YN,n = N âˆ’1/2 âˆ†Mn . For the convergence of n=1 YN,n , we observe that E[YN,k |Fkâˆ’1 ] = 0
and so, by Theorem S6.1, it converges stably to N (0, V ) if the conditions (c1) and (c2)âˆš
hold true.
Regarding (c1), we note that max1â‰¤nâ‰¤N |YN,n | â‰¤ âˆš1N max1â‰¤nâ‰¤N |Î¾n âˆ’ Ïˆnâˆ’1 | = O(1/ N ) â†’ 0.
Condition (c2) means
N
X
n=1

YN,n Y >
N,n =

N
1 X
P
(Î¾n âˆ’ Ïˆnâˆ’1 )(Î¾n âˆ’ Ïˆnâˆ’1 )> âˆ’â†’ V.
N n=1

The above convergence holds P
true by Assumption (S1.3) and
P Lemma S4.2 (with cn = n and
vN,n = n/N ). Indeed, we have nâ‰¥1 E[kÎ¾n âˆ’ Ïˆnâˆ’1 k2 ]/n2 â‰¤ nâ‰¥1 nâˆ’2 < +âˆ and
E[(Î¾n âˆ’ Ïˆnâˆ’1 )(Î¾n âˆ’ Ïˆnâˆ’1 )> |Fnâˆ’1 ] = diag(Ïˆnâˆ’1 ) âˆ’ Ïˆnâˆ’1 Ïˆnâˆ’1 > .

Remark S1.2. Recalling that Ïˆn = Î¸n +p0 , the convergence (S1.3) with V = Î“ = diag(p0 )âˆ’p0 p0 > ,
means
N
N
1 X
1 X
P
P
Î¸ N âˆ’1 =
Î¸nâˆ’1 âˆ’â†’ 0
and
Î¸nâˆ’1 Î¸nâˆ’1 > âˆ’â†’ 0kÃ—k ,
N n=1
N n=1
where 0kÃ—k is the null matrix with dimension k Ã— k.

S2

S1.3

Proof of Theorem 4.1

By Lemma S4.2 (with cn = n and vN,n = n/N ), Remark S4.3 and Theorem S5.1, we immediately
get Î¾ N â†’ p0 almost
surely. Indeed, we have E[Î¾n+1 |Fn ] = Ïˆn â†’ p0 almost surely and
P
P
2 âˆ’2
âˆ’2
E[kÎ¾
k
]n
â‰¤
< +âˆ.
n
nâ‰¥1
nâ‰¥1 n
Regarding the central limit theorem for Î¾ N , we have to distinguish the two cases 1/2 <  â‰¤ 1 or
0 <  â‰¤ 1/2. In the first case, the result follows from Theorem S5.3, because (10) and the fact that
E[âˆ†Mn+1 âˆ†Mn+1 > |Fn ] = diag(Ïˆnâˆ’1 ) âˆ’ Ïˆnâˆ’1 Ïˆnâˆ’1 > â†’ Î“ almost surely; while for the second
case the result follows from Theorem S1.1. Indeed, we have
âˆš
 âˆš
 âˆš

N Î¾ N âˆ’ p0 = N Î¾ N âˆ’ Ïˆ N âˆ’1 + N Ïˆ N âˆ’1 âˆ’ p0
âˆš
 âˆš
= (c + 1) N Î¾ N âˆ’ Ïˆ N âˆ’1 âˆ’ N DN ,
âˆš



where D N = c Î¾ N âˆ’ Ïˆ N âˆ’1 âˆ’ Ïˆ N âˆ’1 âˆ’ p0 . By Theorem S1.1, the term (c+1) N Î¾ N âˆ’ Ïˆ N âˆ’1
stably converges to N (0, (c + 1)2 Î“) (note that assumption (S1.3) is satisfied with V = Î“, âˆš
because
Ïˆn â†’ p0 almost surely). Therefore, in order to conclude, it is enough to show that N DN
converges in probability to 0. To this purpose, we observe that, by (8) with Î´n = cn , we have
Ïˆn âˆ’ Ïˆnâˆ’1 = nâˆ’1 [c(Î¾n âˆ’ Ïˆnâˆ’1 ) âˆ’ (Ïˆnâˆ’1 âˆ’ p0 )]
and so
DN =

N
1 X Ïˆn âˆ’ Ïˆnâˆ’1
.
N n=1
nâˆ’1

P+âˆ
Moreover, we note that n=1 (Ïˆn âˆ’ Ïˆnâˆ’1 ) = limN ÏˆN âˆ’ Ïˆ0 = p0 âˆ’ Ïˆ0 < +âˆ and, by Lemma
S4.1 (with vN,n = N âˆ’1 /nâˆ’1 ), we get
N âˆ’1

N
X
Ïˆn âˆ’ Ïˆnâˆ’1 a.s.
âˆ’â†’ 0.
nâˆ’1
n=1

For  â‰¤ 1/2, this fact implies
âˆš

N DN = âˆš

N
âˆ’1
X
1
Ïˆn âˆ’ Ïˆnâˆ’1 a.s.
N âˆ’1
âˆ’â†’ 0 .
nâˆ’1
N N âˆ’1
n=1

The proof is thus concluded.

S2

Case

P

n n

< +âˆ

P
In this section we provide some results regarding the case n n < +âˆ, even if, as we will see, this
case is not interesting for the chi-squared test of goodness of fit. Indeed, as shown in the following
result, the empirical mean almost surely converges to a random variable, which does not coincide
almost surely with a deterministic vector.
P+âˆ
a.s.
Theorem S2.1. If n=0 n < +âˆ, then Î¾ N âˆ’â†’ Ïˆâˆ , where Ïˆâˆ is a random variable, which is
not almost surely equal to a deterministic vector, that is P (Ïˆâˆ 6= q0 ) > 0 for all q0 âˆˆ Rk .
P+âˆ
Proof. When n=0 n < +âˆ, the sequence (Ïˆn ) is a (bounded) non-negative almost supermartingale (see [S17]) because, by (8), we have
E[Ïˆn+1 |Fn ] = Ïˆn (1 âˆ’ n ) + n p0 â‰¤ Ïˆn + n p0 .

S3

As a consequence, it converges almost surely (and in Lp with p â‰¥ 1) to a certain random variable
Ïˆ
fact follows from quasi-martingale theory [S12]: indeed, since
Pâˆ . An alternative proof of this P
E[
|E[Ïˆ
|F
]
âˆ’
Ïˆ
|
]
=
O(
n+1
n
n
n
n n ) < +âˆ, the stochastic process (Ïˆn ) is a non-negative
quasi-martingale and so it converges almost surely (and in Lp with p â‰¥ 1) to a certain random
variable Ïˆâˆ .
The almost sure convergence of Î¾ n to Ïˆâˆ follows by Lemma S4.2 and Remark
P S4.3 (with cn = n
and vN,n = n/N ), because E[Î¾n+1 |Fn ] = Ïˆn â†’ Ïˆâˆ almost surely and nâ‰¥1 E[kÎ¾n k2 ]nâˆ’2 â‰¤
P
âˆ’2
< +âˆ.
nâ‰¥1 n
In order to show that Ïˆâˆ is not almost surely equal to a deterministic vector, we set
yn = E[kÏˆn âˆ’ p0 k2 ] âˆ’ kE[Ïˆn âˆ’ p0 ]k2 =

k
X

V ar[Ïˆn i âˆ’ p0 i ]

i=1

and observe that, starting from (8), we get
Ïˆn+1 âˆ’ p0 = (1 âˆ’ n )(Ïˆn âˆ’ p0 ) + Î´n âˆ†Mn+1
and so
kE[Ïˆn âˆ’ p0 ]k2 = E[Ïˆn âˆ’ p0 ]> E[Ïˆn âˆ’ p0 ] = (1 âˆ’ n )2 kE[Ïˆn âˆ’ p0 ]k2
and
E[kÏˆn+1 âˆ’ p0 k2 ] = E[(Ïˆn+1 âˆ’ p0 )> (Ïˆn+1 âˆ’ p0 )]
= (1 âˆ’ n )2 E[kÏˆn âˆ’ p0 k2 ] + Î´n2 E[kâˆ†Mn+1 k2 ] .
Hence, we obtain
yn+1 = (1 âˆ’ n )2 yn + Î´n2 E[kâˆ†Mn+1 k2 ] = (1 âˆ’ 2n )yn + Î¶en

(S2.4)

with Î¶en = 2n yn + Î´n2 E[kâˆ†Mn+1 k2 ] â‰¥ 0. It follows that, given nÌƒ such that n < 1/2 for n â‰¥ nÌƒ, we
QN âˆ’1
have yN â‰¥ ynÌƒ n=nÌƒ (1 âˆ’ 2n ) for each N â‰¥ nÌƒ and so
!
+âˆ
+âˆ
Y
X
E[kÏˆâˆ âˆ’p0 k2 ]âˆ’kE[Ïˆâˆ âˆ’p0 ]k2 = yâˆ = lim yN â‰¥ ynÌƒ
(1âˆ’2n ) = ynÌƒ exp
ln(1 âˆ’ 2n ) .
N â†’+âˆ

n=nÌƒ

n=nÌƒ

P+âˆ

P+âˆ
The above exponential is strictly greater than 0 because n=nÌƒ ln(1 âˆ’ 2n ) âˆ¼ âˆ’2 n=nÌƒ n > âˆ’âˆ.
Therefore, if ynÌƒ > 0, then we have yâˆ > 0. This means that Ïˆâˆ âˆ’ p0 , and consequently Ïˆâˆ ,
is not almost surely equal to a deterministic vector, that is P (Ïˆâˆ 6= q0 ) > 0 for all q0 âˆˆ Rk . If
e then, by (S2.4), we get
ynÌƒ = 0, that is if ÏˆnÌƒ is almost surely equal to a deterministic vector Ïˆ,
e 2] > 0 ,
ynÌƒ+1 = Î´n2 E[kâˆ†Mn+1 k2 ] = Î´nÌƒ2 E[kÎ¾nÌƒ+1 âˆ’ Ïˆk
e is different from a vector of the canonical base of Rk by means of
because Î´n > 0 for each n and Ïˆ
the assumption b0 i + B0 i > 0 and equality (5). It follows that we can repeat the above argument
replacing nÌƒ by nÌƒ + 1 and conclude that Ïˆâˆ is not almost surely equal to a deterministic vector.
As a consequence of the above theorem, if we aim atP
having the almost sure convergence of
+âˆ
Î¾ N to a deterministic vector, we have to avoid the case n=0 n < +âˆ. However, for the sake
of completeness, we provide a second-order convergence result also in this case. First, we note
that Theorem S1.1 still holds true with V = diag(Ïˆâˆ ) âˆ’ Ïˆâˆ Ïˆâˆ > . Indeed, assumption (S1.3) is
satisfied by Lemma S4.2 and Remark S4.3 (with cn = n and vN,n = n/N ), because of the almost
sure convergence of Ïˆn to Ïˆâˆ . Moreover, we have the following theorem:
Theorem S2.2. Suppose to be in one of the following two cases:

S4

a)

PN

n=1

âˆš
âˆš
PN
nnâˆ’1 = o( N ) and n=1 nÎ´nâˆ’1 = o( N );

b) n = (n + 1)âˆ’ and Î´n âˆ¼ c(n + 1)âˆ’Î´ with c > 0, Î´ âˆˆ (1/2, 1) and  > Î´ + 1/2 ( = +âˆ
included, that means n = 0 for all n).
Set e = 1/2 and Î» = 1 in case a) and e = Î´ âˆ’ 1/2 âˆˆ (0, 1/2) and Î» = c2 /[2(1 âˆ’ e)] = c2 /(3 âˆ’ 2Î´) in
case b). Then, we have
 s
N e Î¾ N âˆ’ ÏˆN âˆ’â†’ N (0, Î»Î“) ,
where Î“ = diag(Ïˆâˆ ) âˆ’ Ïˆâˆ Ïˆâˆ > .
When (ÏˆN âˆ’ Ïˆâˆ ) = oP (N âˆ’e ), we also have
 s
N e Î¾ N âˆ’ Ïˆâˆ âˆ’â†’ N (0, Î»Î“) .
Note that case a) covers the case n = (n + 1)âˆ’ and Î´n âˆ¼ c(n + 1)âˆ’Î´ with c > 0 and min{, Î´} >
3/2.
The case n = 0 (that is Î²n = 1) for all n corresponds to the case considered in [S15], but in
that paper the author studies only the limit Ïˆâˆ and he does not provide second-order convergence
results.
Proof. We have

N e Î¾ N âˆ’ ÏˆN =

1
N 1âˆ’e

1
N 1âˆ’e
=


N Î¾ N âˆ’ N ÏˆN =

N
X

(Î¾n âˆ’ Ïˆnâˆ’1 ) +

n=1

1
N 1/2âˆ’e

N
X

YN,n +

n=1

N 1âˆ’e

N 1âˆ’e

[Î¾n âˆ’ Ïˆnâˆ’1 + n(Ïˆnâˆ’1 âˆ’ Ïˆn )]

n=1

N
X

1

N
X

N
X

1

nnâˆ’1 (Ïˆnâˆ’1 âˆ’ p0 ) âˆ’

n=1

N
X

1
N 1âˆ’e

nÎ´nâˆ’1 âˆ†Mn

n=1

ZN,n + QN ,

n=1

where
YN,n =

âˆ†Mn
Î¾n âˆ’ Ïˆnâˆ’1
âˆš
= âˆš ,
N
N

ZN,n = âˆ’

and
QN =

1
N 1âˆ’e

N
X

nÎ´nâˆ’1 (Î¾n âˆ’ Ïˆnâˆ’1 )
nÎ´nâˆ’1 âˆ†Mn
=
N 1âˆ’e
N 1âˆ’e

nnâˆ’1 (Ïˆnâˆ’1 âˆ’ p0 ).

n=1

PN
In both cases a) and b), we have n=1 nnâˆ’1 = o(N 1âˆ’e ) and so QN converges almost surely to 0.
PN
Moreover, by Theorem S1.1, n=1 YN,n stable converges to N (0, V ) with V = Î“ = diag(Ïˆâˆ ) âˆ’
PN
Ïˆâˆ Ïˆâˆ > . Therefore it is enough to study the convergence of n=1 ZN,n . To this purpose, we
PN
observe that, if we are in case a), then n=1 ZN,n converges almost surely to 0 and so
âˆš
 s
N Î¾ N âˆ’ ÏˆN âˆ’â†’ N (0, Î“).
PN
Otherwise, if we are in case b), we observe that E[ZN,n |Fnâˆ’1 ] = 0 and so n=1 ZN,n converges
stably to N (0, Î»Î“) if the conditions (c1) and (c2) of Theorem S6.1, with V = Î»Î“, hold true.âˆš Re1
garding (c1), we observe that max1â‰¤nâ‰¤N |ZN,n | â‰¤ N 1âˆ’e
max1â‰¤nâ‰¤N nÎ´nâˆ’1 |Î¾n âˆ’Ïˆnâˆ’1 | = O(1/ N ).
Regarding condition (c2), that is
N
X
n=1

ZN,n ZN,n > =

1
N 2(1âˆ’e)

N
X

P

2
n2 Î´nâˆ’1
(Î¾n âˆ’ Ïˆnâˆ’1 )(Î¾n âˆ’ Ïˆnâˆ’1 )> âˆ’â†’

n=1

S5

c2
Î“,
2(1 âˆ’ e)

we observe that it holds true even almost surely, because
c2 /(3 âˆ’ 2Î´) and

1
N 2(1âˆ’e)

PN

n=1

2
n2 Î´nâˆ’1
â†’ c2 /[2(1 âˆ’ e)] =

a.s.

E[(Î¾n âˆ’ Ïˆnâˆ’1 )(Î¾n âˆ’ Ïˆnâˆ’1 )> |Fnâˆ’1 ] = diag(Ïˆnâˆ’1 ) âˆ’ Ïˆnâˆ’1 Ïˆnâˆ’1 > âˆ’â†’ Î“
2
(see Lemma S4.2 and Remark S4.3 with cn = n and vN,n = n3 Î´nâˆ’1
/N 2(1âˆ’e) âˆ¼ c2 (n/N )3âˆ’2Î´ ).
Therefore, we have
 s

N e Î¾ N âˆ’ ÏˆN âˆ’â†’ N 0, c2 (3 âˆ’ 2Î´)âˆ’1 Î“ .

Finally, we observe that


N e Î¾ N âˆ’ Ïˆâˆ = N e Î¾ N âˆ’ ÏˆN + N e (ÏˆN âˆ’ Ïˆâˆ ) .
Therefore, when (ÏˆN âˆ’ Ïˆâˆ ) = oP (N âˆ’e ), we have
 s
N e Î¾ N âˆ’ Ïˆâˆ âˆ’â†’ N (0, Î»Î“) .

An example of the case a) of Theorem S2.2 with (ÏˆN âˆ’ Ïˆâˆ ) = oP (N âˆ’e ) is the RP urn with
Î±n = Î± > 0 and Î²n = Î² > 1 (see [S02]). Indeed, in this case, we have n âˆ¼ c Î² âˆ’n and Î´n âˆ¼ cÎ´ Î² âˆ’n ,
where c > 0 and cÎ´ > 0 are suitable constants, and (ÏˆN âˆ’ Ïˆâˆ ) = O(Î² âˆ’N ). We conclude this
section with other two examples regarding the case n = 0 (that is Î²n = 1) for all n.
Example S2.3. (Case n = 0 and Î´n âˆ¼ c(n + 1)âˆ’Î´Pwith c > 0 and Î´ > 3/2)
n
If n = 0 for all n, then we have rnâˆ— = |b0 | + |B0 | + h=1 Î±h . Therefore, if we take Î±n = nâˆ’Î´ , with
P+âˆ
âˆ—
Î´ > 3/2, then rnâˆ— converges to the constant râˆ— = |b0 | + |B0 | + h=1 hâˆ’Î´ and Î´n = Î±n+1 /rn+1
âˆ¼
âˆ’Î´
âˆ—
cÎ±n+1 = c(n + 1) , with c = P
1/r . Moreover, since Î´ > 3/2, assumption a) of Theorem S2.2 is
satisfied. We also observe that n Î´n2 < +âˆ and so Ïˆâˆ i is not concentrated on {0, 1} and has no
atoms in (0, 1) (see [S15, Th. 2 and Th. 3]). More precisely, we have
Ïˆâˆ =

b0 + B0 +
|b0 | +

P+âˆ

n=1 Î±n Î¾n
P+âˆ
|B0 | + n=1 Î±n

and so
ÏˆN âˆ’ Ïˆâˆ =
PN
P
PN
P
(b0 + B0 + n=1 Î±n Î¾n ) nâ‰¥N +1 Î±n âˆ’ (|b0 | + |B0 | + n=1 Î±n ) nâ‰¥N +1 Î±n Î¾n
=
PN
P+âˆ
(|b0 | + |B0 | + n=1 Î±n )(|b0 | + |B0 | + n=1 Î±n )
ï£«
ï£¶
X

Oï£­
Î±n ï£¸ = O N 1âˆ’Î´ .
nâ‰¥N +1

Since Î´ > 3/2, we get (ÏˆN âˆ’ Ïˆâˆ ) = o(N âˆ’1/2 ). This fact can also be obtained as a consequence
of Theorem S2.5 below. Indeed, this theorem states that the rate of convergence of ÏˆN to Ïˆâˆ is
N âˆ’(Î´âˆ’1/2) .
Note that, since Î²n = 1 for all n, the factor f (h, n) in (6) coincides with Î±h and so, in this case,
it is decreasing.
Example S2.4. (Case n = 0 and Î´n âˆ¼ c(n + 1)âˆ’Î´ with c > 0 and Î´ âˆˆ (1/2, 1)) P
n
As in the P
previous example, since n = 0 for all n, we have rnâˆ— = |b0 | + |B0 | + h=1 Î±h . Let us
n
Î±
âˆ—
set An = h=1 Î±h = exp(bn ) with b > 0 and Î± âˆˆ (0, 1/2), which brings to rn âˆ¼ An â†‘ +âˆ and

S6

Î±n = exp(bnÎ± ) âˆ’ exp(b(n âˆ’ 1)Î± ) and
Pnâˆ’1
Î±h
Î±n
Î´nâˆ’1 =
âˆ¼ 1 âˆ’ Ph=1
n
|b0 | + |B0 | + An
h=1 Î±h
= 1 âˆ’ exp [b ((n âˆ’ 1)Î± âˆ’ nÎ± )]



= bnÎ± 1 âˆ’ (1 âˆ’ nâˆ’1 )Î± + O n2Î± (1 âˆ’ (1 âˆ’ nâˆ’1 )Î± )2 = bnÎ± Î±nâˆ’1 + O(nâˆ’2 ) + O(nâˆ’(2âˆ’2Î±) )
= bÎ±nâˆ’(1âˆ’Î±) + O(nâˆ’(2âˆ’Î±) ) + O(nâˆ’(2âˆ’2Î±) ) = bÎ±nâˆ’(1âˆ’Î±) + O(nâˆ’2(1âˆ’Î±) ),
so that Î´ = (1 âˆ’ Î±) âˆˆ (1/2, 1) and c = bÎ± > 0. Hence,
have Î´n âˆ¼ c(n + 1)âˆ’Î´ and assumption b)
P we
2
of Theorem S2.2 is satisfied. We also observe that n Î´n < +âˆ and so Ïˆâˆ i is not concentrated on
{0, 1} and has no atoms in (0, 1) (see [S15, Th. 2 and Th. 3]). Moreover, by Theorem S2.5 below,
we get that N e (Ïˆ N âˆ’ Ïˆâˆ ) âˆ’â†’N 0, c2 (2e)âˆ’1 Î“ , where e = Î´ âˆ’ 1/2. Hence, applying Theorem
S6.3, we obtain
 s

N e Î¾ N âˆ’ Ïˆâˆ âˆ’â†’ N 0, c2 [2e(1 âˆ’ e)]âˆ’1 Î“ .
Finally, note that, as before, since Î²n = 1 for all n, the factor f (h, n) in (6) coincides with Î±h and
so, in this case, `(h) = ln(f (h, n)) = ln(Î±h ) âˆ¼ ln(Î´hâˆ’1 ) + bhÎ± âˆ¼ bhÎ± âˆ’ bÎ±(1 âˆ’ Î±) ln(h). Hence,
there exists hâˆ— such that h 7â†’ `(h) is increasing for h â‰¥ hâˆ— . Since maxhâ‰¤hâˆ— `(h) â‰¤ C, for a suitable
constant C, the contributions of the observations until hâˆ— are eventually smaller than those with
h â‰¥ hâˆ— , that are increasing with h.
Theorem S2.5. For n = 0 for all n and Î´n âˆ¼ c(n + 1)âˆ’Î´ with c > 0 and 1/2 < Î´ â‰¤ 1, we have

1
stably in the strong sense w.r.t. F,
N Î´âˆ’ 2 (Ïˆ N âˆ’ Ïˆâˆ ) âˆ’â†’N 0, c2 (2Î´ âˆ’ 1)âˆ’1 Î“
where Î“ = diag(Ïˆâˆ ) âˆ’ Ïˆâˆ Ïˆâˆ > .
Proof. We want to apply Theorem S6.2. To this purpose, we recall that, when n = 0 for all n, the
process (Ïˆn ) is a martingale with respect to F. Moreover, it converges almost surely and in mean
to Ïˆâˆ . Therefore, in order to conclude, it is enough to check conditions (c1) and (c2) of Theorem
S6.2. Regarding the first condition, we note that
N Î´âˆ’1/2 sup |Ïˆn âˆ’ Ïˆn+1 | = N Î´âˆ’1/2 sup Î´n |âˆ†Mn+1 | = O(N Î´âˆ’1/2âˆ’Î´ ) = O(N âˆ’1/2 ) âˆ’â†’ 0.
nâ‰¥N

nâ‰¥N

Finally, regarding the second condition, we observe that
X
X
N 2Î´âˆ’1
(Ïˆn âˆ’ Ïˆn+1 )(Ïˆn âˆ’ Ïˆn+1 )> âˆ¼ N 2Î´âˆ’1 c2
(n + 1)âˆ’2Î´ (âˆ†Mn+1 )(âˆ†Mn+1 )>
nâ‰¥N

nâ‰¥N
a.s.

âˆ’â†’

2

c
Î“,
(2Î´ âˆ’ 1)

where the almost sure convergence follows from [S06, Lemma 4.1] and the fact that
a.s.

E[(âˆ†Mn+1 )(âˆ†Mn+1 )> |Fn ] = E[(Î¾n+1 âˆ’ Ïˆn )(Î¾n+1 âˆ’ Ïˆn )> |Fn ] âˆ’â†’ Î“.

S3

Computations regarding local reinforcement

Suppose Î±n âˆ¼ anâˆ’Î± for n â‰¥ 1 and (1 âˆ’ Î²n ) âˆ¼ b(n + 1)âˆ’Î² for n â‰¥ 0. In the following subsections
Qnâˆ’1
we study the behaviour of the factor f (h, n) = Î±h j=h Î²j in some particular cases that cover the
cases of the two examples in Section 3. Specifically, for all the considered cases, we set `(h, n) =
Qnâˆ’1
Pnâˆ’1
ln(Î±h j=h Î²j ) = ln(Î±h ) + j=h ln(Î²j ) for n â‰¥ h and we prove that there exists hâˆ— such that
maxhâ‰¤hâˆ— `(h, n) â‰¤ `(hâˆ— , n) and h 7â†’ `(h, n) is increasing for h â‰¥ hâˆ— . This means that the weights
f (h, n) of the observations until hâˆ— are smaller than those with h â‰¥ hâˆ— and the contribution of the
observation for h â‰¥ hâˆ— is increasing with h.

S7

S3.1

Case Î± = Î² âˆˆ (0, 1)

Suppose Î±n = anâˆ’Î± and 1 âˆ’ Î²n = b(n + 1)âˆ’Î± , with a, b > 0 and Î± âˆˆ (0, 1). For n â‰¥ h, we have
`(h + 1, n) âˆ’ `(h, n) = ln(a(h + 1)âˆ’Î± ) âˆ’ ln(ahâˆ’Î± ) âˆ’ ln(1 âˆ’ b(h + 1)âˆ’Î± )



1
b
Î±
b
= âˆ’Î± ln 1 +
âˆ’ ln 1 âˆ’
=âˆ’ +
.
h
(h + 1)Î±
h (h + 1)Î±
Since Î± < 1, there exists h0 such that the function h 7â†’ `(h, n) is monotonically increasing for
âˆ’Î±
h â‰¥ h0 . Now, fix Î· > 0 and let j0 such that j â‰¥ j0 implies ln(Î²j ) â‰¤ âˆ’ bj
1+Î· . Then take hâˆ— â‰¥
max(h0 , j0 ) + 1 and h â‰¤ h0 âˆ’ 1. For hâˆ— large enough, we get
`(hâˆ— , n) âˆ’ `(h, n) = ln(Î±hâˆ— ) âˆ’ ln(Î±h ) âˆ’

hX
âˆ— âˆ’1

âˆ’Î±
ln(Î²j ) = ln(ahâˆ’Î±
)âˆ’
âˆ— ) âˆ’ ln(ah

j=h

hX
âˆ— âˆ’1

ln(Î²j )

j=h

hX
âˆ— âˆ’1

bj âˆ’Î±
1+Î·
j=max(h0 ,j0 )
Z hâˆ— âˆ’1
b
xâˆ’Î± dx
â‰¥ âˆ’Î± ln(hâˆ— ) + C1 +
1 + Î· max(h0 ,j0 )


b
= âˆ’Î± ln(hâˆ— ) + C1 +
(hâˆ— âˆ’ 1)1âˆ’Î± âˆ’ max(h0 , j0 )1âˆ’Î±
(1 + Î·)(1 âˆ’ Î±)
b
= C2 âˆ’ Î± ln(hâˆ— ) +
(hâˆ— âˆ’ 1)1âˆ’Î± â‰¥ 0 .
(1 + Î·)(1 âˆ’ Î±)
â‰¥ ln(hâˆ’Î±
âˆ— )+

Therefore, taking hâˆ— large enough, we have maxhâ‰¤hâˆ— `(h, n) = maxhâ‰¤h0 âˆ’1 `(h, n)âˆ¨maxh0 â‰¤hâ‰¤hâˆ— `(h, n) â‰¤
`(hâˆ— , n).

S3.2

Case Î± = Î² = 1

Suppose Î±n = anâˆ’1 and 1 âˆ’ Î²n = b(n + 1)âˆ’1 , with a > 0 and b > 1. For n â‰¥ h, we have
`(h + 1, n) âˆ’ `(h, n) = ln(a(h + 1)âˆ’1 ) âˆ’ ln(ahâˆ’1 ) âˆ’ ln(1 âˆ’ b(h + 1)âˆ’1 )


b 
bâˆ’1
1
âˆ’ ln 1 âˆ’
=
+ o(hâˆ’1 ).
= âˆ’ ln 1 +
h
(h + 1)
h+1
Since b > 1, we can argue as in the previous subsection. Therefore, there exists h0 such that the
function h 7â†’ `(h, n) is monotonically increasing for h â‰¥ h0 . Now, fix Î· = (b âˆ’ 1)/(b + 1) > 0 and
âˆ’1
let j0 such that j â‰¥ j0 implies ln(Î²j ) â‰¤ âˆ’ bj
1+Î· . Then take hâˆ— â‰¥ max(h0 , j0 ) + 1 and h â‰¤ h0 âˆ’ 1.

S8

For hâˆ— large enough, we get
`(hâˆ— , n) âˆ’ `(h, n) = ln(Î±hâˆ— ) âˆ’ ln(Î±h ) âˆ’

hX
âˆ— âˆ’1

ln(Î²j ) =

ln(ahâˆ’1
âˆ— )

âˆ’ ln(ah

âˆ’1

)âˆ’

j=h

hX
âˆ— âˆ’1

ln(Î²j )

j=h

hX
âˆ— âˆ’1

bj âˆ’1
1+Î·
j=max(h0 ,j0 )
Z hâˆ— âˆ’1
b
xâˆ’1 dx
â‰¥ âˆ’ ln(hâˆ— ) + C1 +
1 + Î· max(h0 ,j0 )

b 
= âˆ’ ln(hâˆ— ) + C1 +
ln(hâˆ— âˆ’ 1) âˆ’ ln(max(h0 , j0 ))
(1 + Î·)
bâˆ’1âˆ’Î·
= C2 +
ln(hâˆ— ) âˆ’ O(1/hâˆ— )
(1 + Î·)
b(b âˆ’ 1)
= C2 +
ln(hâˆ— ) âˆ’ O(1/hâˆ— ) â‰¥ 0 .
2b
â‰¥ ln(hâˆ’1
âˆ— )+

Therefore, taking hâˆ— large enough, we have maxhâ‰¤hâˆ— `(h, n) = maxhâ‰¤h0 âˆ’1 `(h, n)âˆ¨maxh0 â‰¤hâ‰¤hâˆ— `(h, n) â‰¤
`(hâˆ— , n).

S3.3

Case 0 < Î± < Î² < (1 + Î±)/2

Suppose



c2
c3
c1
+ O(1/n2âˆ’Î² )
Î±n = anâˆ’Î± 1 + 1âˆ’Î² + Î²âˆ’Î± +
n
n
n

and 1 âˆ’ Î²n = b(n + 1)âˆ’Î² , with a, b > 0, 0 < Î± < Î² < (1 + Î±)/2 and c1 , c2 , c3 âˆˆ R. Set
Î³ = Î² âˆ’ Î± âˆˆ (0, 1/2). For n â‰¥ h, we have
`(h + 1, n) âˆ’ `(h, n) = ln(a(h + 1)âˆ’Î± ) âˆ’ ln(ahâˆ’Î± ) âˆ’ ln(1 âˆ’ b(h + 1)âˆ’Î² )

+ ln 1 + c1 /(h + 1)1âˆ’Î² + c2 /(h + 1)Î³ + c3 /(h + 1) + O(1/h2âˆ’Î² ) (S3.5)

âˆ’ ln 1 + c1 /h1âˆ’Î² + c2 /hÎ³ + c3 /h + O(1/h2âˆ’Î² ) .
Now, we aim at obtaining a series expansion with a reminder term of the type o(1/hÎ² ). Since
Î² < 1, the first three terms of the right-hand side of the above equation give
 1


b
b
ln(a(h+1)âˆ’Î± )âˆ’ln(ahâˆ’Î± )âˆ’ln(1âˆ’b(h+1)âˆ’Î² ) = âˆ’Î± ln 1+ âˆ’ln 1âˆ’
=
+o(hâˆ’Î² ).
h
(h + 1)Î²
(h + 1)Î²
We deal now with the last two terms of (S3.5). We recall that
ln(1 + x) = x âˆ’

x2
x3
xj
+
+ Â· Â· Â· + (âˆ’1)jâˆ’1
+ o(xj ) ,
2
3
j

and therefore, since 2 âˆ’ Î² = 1 + 1 âˆ’ Î² > 1 > Î² and j(1 âˆ’ Î²) > Î² and jÎ³ = j(Î² âˆ’ Î±) > Î² for j large
enough, there are only a finite number J0 of terms with an order Ï„j â‰¤ Î². In other words, we can
write

ln 1 + c1 /(h + 1)1âˆ’Î² + c2 /(h + 1)Î³ + c3 /(h + 1) + O(1/n2âˆ’Î² )

âˆ’ ln 1 + c1 /h1âˆ’Î² + c2 /hÎ³ + c3 /h + O(1/n2âˆ’Î² )
=

J0
X
j=1

Cj (h + 1)âˆ’Ï„j âˆ’

J0
X

Cj hâˆ’Ï„j + o(1/hÎ² )

j=1

S9

=

=

J0
X

J0
X




Cj (h + 1)âˆ’Ï„j âˆ’ hâˆ’Ï„j + o(1/hÎ² ) =
Cj hâˆ’Ï„j (1 + hâˆ’1 )âˆ’Ï„j âˆ’ 1 + o(1/hÎ² )

j=1

j=1

J0
X


Cj hâˆ’Ï„j (Ï„j hâˆ’1 + o(1/h) + o(1/hÎ² ) = o(1/hÎ² ) .

j=1

Summing up, we have
`(h + 1, n) âˆ’ `(h, n) =

b
+ o(hâˆ’Î² ).
(h + 1)Î²

Then there exists h0 such that the function h 7â†’ `(h, n) is monotonically increasing for h â‰¥ h0 .
âˆ’Î²
Now, fix Î· > 0 and let j0 such that j â‰¥ j0 implies ln(Î²j ) â‰¤ âˆ’ bj
1+Î· . Then take hâˆ— â‰¥ max(h0 , j0 ) + 1
and h â‰¤ h0 âˆ’ 1. Since Î² < (1 + Î±)/2, we have Î±n = anâˆ’Î± (1 + O(1/nÎ³ )) and so, for hâˆ— large enough,
we get
`(hâˆ— , n) âˆ’ `(h, n) = ln(Î±hâˆ— ) âˆ’ ln(Î±h ) âˆ’

hX
âˆ— âˆ’1

ln(Î²j )

j=h
âˆ’Î±
= ln(ahâˆ’Î±
) + ln(1 + O(hâˆ’Î³
âˆ— ) âˆ’ ln(ah
âˆ— )) + C1 âˆ’

hX
âˆ— âˆ’1

ln(Î²j )

j=h
hX
âˆ— âˆ’1

âˆ’Î³
â‰¥ ln(hâˆ’Î±
âˆ— ) + ln(1 + O(hâˆ— )) + C1 +

j=max(h0 ,j0 )

â‰¥ âˆ’Î± ln(hâˆ— ) +

O(hâˆ’Î³
âˆ— )

b
+ C2 +
1+Î·

Z

hâˆ— âˆ’1

bj âˆ’Î²
1+Î·

xâˆ’Î² dx

max(h0 ,j0 )



b
(hâˆ— âˆ’ 1)1âˆ’Î² âˆ’ max(h0 , j0 )1âˆ’Î²
(1 + Î·)(1 âˆ’ Î²)
b
= C3 + O(hâˆ’Î³
(hâˆ— âˆ’ 1)1âˆ’Î² â‰¥ 0 .
âˆ— ) âˆ’ Î± ln(hâˆ— ) +
(1 + Î·)(1 âˆ’ Î²)

= âˆ’Î± ln(hâˆ— ) + O(hâˆ’Î³
âˆ— ) + C2 +

Therefore, taking hâˆ— large enough, we have maxhâ‰¤hâˆ— `(h, n) = maxhâ‰¤h0 âˆ’1 `(h, n)âˆ¨maxh0 â‰¤hâ‰¤hâˆ— `(h, n) â‰¤
`(hâˆ— , n).

S4

Technical results

We recall the generalized Kronecker lemma [S03, Corollary A.1]:
Lemma S4.1. (Generalized Kronecker Lemma)
Let {vN,n : 1 â‰¤ n â‰¤ N } and (zn )n be respectively a triangular array and a sequence of complex
numbers such that vN,n 6= 0 and
lim vN,n = 0,
N

and

P

n zn

lim vn,n exists finite,
n

is convergent. Then limN

N
X

|vN,n âˆ’ vN,nâˆ’1 | = O(1)

n=1

PN

n=1

vN,n zn = 0.

The above corollary is useful to get the following result for complex random variables, which
slightly extends the version provided in [S03, Lemma A.2]:

S 10

Lemma S4.2. Let H = (Hn )n be a filtration and (Yn )n a H-adapted sequence of complex
random

P
variables. Moreover, let (cn )n be a sequence of strictly positive real numbers such that n E |Yn |2 /c2n <
+âˆ and let {vN,n , 1 â‰¤ n â‰¤ N } be a triangular array of complex numbers such that vN,n 6= 0 and
lim vN,n = 0,
N

lim vn,n exists finite,
n

N
X

|vN,n âˆ’ vN,nâˆ’1 | = O(1) .

n=1

Suppose that
N
X

vN,n

n=1

E[Yn |Hnâˆ’1 ] P
âˆ’â†’ V,
cn

where V is a suitable random variable. Then

PN

n=1

(S4.6)
P

vN,n Yn /cn âˆ’â†’ V .

If the convergence in (S4.6) is almost sure, then also the convergence of
V is almost sure.

PN

n=1

vN,n Yn /cn toward

Proof. Consider the martingale (Mn )n defined by
Mn =
It is bounded in L2 since
that means

P

n

E[|Yn |2 ]
c2n

n
X
Yj âˆ’ E[Yj |Hjâˆ’1 ]
.
cj
j=1

< +âˆ by assumption and so it is almost surely convergent,

X Yn (Ï‰) âˆ’ E[Yn |Hnâˆ’1 ](Ï‰)
cn

n

< +âˆ

for Ï‰ âˆˆ B with P (B) = 1. Therefore, fixing Ï‰ âˆˆ B and setting zn = Yn (Ï‰)âˆ’E[Ycnn |Hnâˆ’1 ](Ï‰) , by Lemma
S4.1, we get
N
X
Yn (Ï‰) âˆ’ E[Yn |Hnâˆ’1 ](Ï‰)
vN,n
lim
= 0,
N
cn
n=1
that is

N
X
n=1

vN,n

Yn âˆ’ E[Yn |Hnâˆ’1 ] a.s.
âˆ’â†’ 0.
cn

In order to conclude, it is enough to observe that
N
X
n=1

vN,n

N
N
X
Yn
Yn âˆ’ E[Yn |Hnâˆ’1 ] X
E[Yn |Hnâˆ’1 ]
=
vN,n
+
vN,n
cn
cn
cn
n=1
n=1

and use assumption (S4.6).
PN |v |
PN v
a.s.
Remark S4.3. If we have n=1 N,n
= O(1), limN n=1 N,n
cn
cn = Î» âˆˆ C and E[Yn |Hnâˆ’1 ] âˆ’â†’ Y ,
then (S4.6) is satisfied with almost sure convergence and V = Î»Y . Indeed, if we denote by A an
event such that P (A) = 1 and limn E[Yn |Hnâˆ’1 ](Ï‰) = Y (Ï‰) for each Ï‰ âˆˆ A, then we can fix Ï‰ âˆˆ A,
set wn = E[Yn |Hnâˆ’1 ](Ï‰) and w = Y (Ï‰), and apply the generalized Toeplitz lemma [S03, Lemma
A.1] (with zN,n = vN,n /(cn Î») and s = 1 when Î» 6= 0 and with zN,n = vN,n /cn and s = 0 when
PN
Î» = 0) in order to get n=1 vN,n wcnn â†’ Î»Y almost surely.
The proof of the following lemma can be found in [S08]. We here rewrite the proof only for the
readerâ€™s convenience.

S 11

Lemma S4.4. ([S08], Lemma 18)
Let xn , Î¶n , Î³n be non-negative sequences such that Î³n â†’ 0,

P

n

Î³n = +âˆ and

xn â‰¤ (1 âˆ’ Î³n )xnâˆ’1 + Î³n Î¶n .
Then lim supn xn â‰¤ lim supn Î¶n .
Proof. Take L > lim supn Î¶n and nâˆ— large enough so that Î¶n < L and Î³n â‰¤ 1 when n â‰¥ nâˆ— . Then,
using that (x + y)+ â‰¤ x+ + y + , we have for n â‰¥ nâˆ—
+

(xn âˆ’ L)+ â‰¤ ((1 âˆ’ Î³n )(xnâˆ’1 âˆ’ L) + Î³n (Î¶n âˆ’ L))

â‰¤ (1 âˆ’ Î³n )(xnâˆ’1 âˆ’ L)+ + Î³n (Î¶n âˆ’ L)+
â‰¤ (1 âˆ’ Î³n )(xnâˆ’1 âˆ’ L)+ .
P
+
Since
= 0. This is enough to
n Î³n = +âˆ, the above inequality implies that limn (xn âˆ’ L)
conclude, because we can choose L arbitrarily close to lim supn Î¶n .

S5

Some stochastic approximation results

Consider a stochastic process (Î¸n ) taking values in Î˜ = [âˆ’1, 1]k , adapted to a filtration F = (Fn )n
and following the dynamics
Î¸n+1 = (1 âˆ’ n )Î¸n + cn âˆ†Mn+1 ,

(S5.7)

where c > 0, (âˆ†Mn+1 )n is a uniformly bounded martingale difference sequence with respect to F
P
fn+1 = câˆ†Mn+1 ,
and n = (n + 1)âˆ’ with  âˆˆ (0, 1] so that n â†’ 0 and n n = +âˆ. Setting âˆ†M
equation (S5.7) becomes
fn+1 .
Î¸n+1 = (1 âˆ’ n )Î¸n + n âˆ†M
Then:
a.s.

Theorem S5.1. In the above setting, we have Î¸N âˆ’â†’ 0 .
Proof. We have the following two cases:
P
â€¢  âˆˆ (1/2, 1] so that n 2n < +âˆ or
P
â€¢  âˆˆ (0, 1/2] so that n 2n = +âˆ.
For the first case, we refer to [S11, Cap. 5, Th. 2.1]. For the second case, we refer to [S11, Cap. 5,
fn ) are uniformly bounded, the key assumption to be
Th. 3.1]). In this case, since (Î¸n ) and (âˆ†M
verified in order to apply [S11, Cap. 5, Th. 3.1] is the â€œrate of changeâ€ condition (see [S11, p. 137]),
that is
lim sup sup |M 0 (N + t) âˆ’ M 0 (N )| = 0,
a.s.
N

tâˆˆ[0,1]

Pm(t)âˆ’1

fj+1 and m(t) = inf{n : t < tn+1 = Pn j } (see [S11, p. 122]).
where M 0 (t) = j=0 j âˆ†M
j=0
fn ) is uniformly bounded, the above condition is satisfied when the following simpler
Since (âˆ†M
conditions are satisfied (see [S11, pp. 139-141]):
P
(i) For each u > 0 n eâˆ’u/n < +âˆ;
(ii) For some T < +âˆ, there exists a constant c(T ) < +âˆ such that supnâ‰¤jâ‰¤m(tn +T )

j
n

â‰¤ c(T ).
âˆ’

When n = (1 + n)âˆ’ , condition (i) is obviously verified, because we have limn n2 /eu(1+n) = 0.
Finally, condition (ii) is always satisfied when n is decreasing, as it is in the case n = (1 + n)âˆ’ .
Indeed, we simply have supnâ‰¤jâ‰¤m(tn +T ) j /n = n /n = 1.

S 12

a.s.

Theorem S5.2. In the above setting, if we have E[âˆ†Mn+1 âˆ†Mn+1 > |Fn ] âˆ’â†’ Î“ with Î“ a symmetric positive definite matrix, then we have
1
d
âˆš Î¸N âˆ’â†’ N (0, Î£),
N
where Î£ = c2 Î“/2 when  âˆˆ (0, 1) and Î£ = c2 Î“ when  = 1.
a.s.

Proof. We have Î¸N âˆ’â†’ 0 and 0 belongs to the interior part of Î˜. Moreover, we have
>

a.s. 2
fn+1 âˆ†M
fn+1 |Fn ] âˆ’â†’
E[âˆ†M
c Î“.

For the case  âˆˆ (1/2, 1], we refer to [S09, Th. 2.1] (with h = Id, Uâˆ— = c2 Î“ and Î³âˆ— = 1) and [S14,
Th. 1] (with H = âˆ’Id, Î³n = Ïƒn = n and so Î³0 = 1 and Î² = ). For the case  âˆˆ (0, 1/2], we
refer to [S11, cap.10, Th. 2.1] (with A = âˆ’Id). The key assumption for applying this theorem is
âˆš
Î¸n / n tight. On the other hand, in the considered setting, this last condition is satisfied because
of [S11, Th. 4.1]. Note that the limit distribution corresponds to the stationary distribution of the
diffusion
dUt = (âˆ’Id + c()) Ut dt + cÎ“1/2 dWt ,
where W = (Wt )t is a standard Wiener process and
(
0 for  < 1
c() =
1/2 for  = 1.
Therefore the limit covariance matrix is determined by solving the associated Lyapunovâ€™s equation
[S14], that, in the considered case, simply is
2 (âˆ’Id + c()Id) Î£ = âˆ’c2 Î“.

Theorem S5.3. In the above setting, let (Âµn ) be another stochastic process taking values in Î˜ =
[âˆ’1, 1]k , adapted to a filtration F and following the dynamics
1
1
Âµn+1 âˆ’ Âµn = âˆ’ (Âµn âˆ’ Î¸n ) + âˆ†Mn+1 .
n
n
a.s.

Suppose that E[âˆ†Mn+1 âˆ†Mn+1 > |Fn ] âˆ’â†’ Î“. If  âˆˆ (1/2, 1), then we have
!
âˆš
 

(c + 1)2 Î“
0
N ÂµN
d
2
âˆ’â†’
N
0,
.
âˆ’1/2
c
0
N Î¸ N
2 Î“
If  = 1, then we have
âˆš

N ÂµN
âˆ’1/2
N Î¸ N

!
d

âˆ’â†’ N




0,

[(c + 1)2 + c2 ]Î“
c(c + 1)Î“

c(c + 1)Î“
c2 Î“


.

Proof. The dynamics for the pair (Âµn , Î¸n )n is
(
Âµn+1 âˆ’ Âµn = âˆ’ n1 (Âµn âˆ’ Î¸n ) + n1 âˆ†Mn+1
fn+1 .
Î¸n+1 âˆ’ Î¸n = âˆ’n Î¸n + cn âˆ†Mn+1 = âˆ’n Î¸n + n âˆ†M
a.s.

with E[âˆ†Mn+1 âˆ†Mn+1 > |Fn ] âˆ’â†’ Î“. Therefore, when 1/2 <  < 1, the statement follows from
[S13] (with Q11 = Q22 = âˆ’Id, Q12 = Id, Q21 = 0, b = Î²0 = 1, a = , Î“11 = Î“, Î“22 = c2 Î“ and

S 13

Î“12 = Î“21 = cÎ“). In particular, the two blocks of the limit covariance matrix, say Î£Âµ and Î£Î¸ , are
determined solving the equations
1
1
(H + Id)Î£Âµ + Î£Âµ (H > + Id) = âˆ’Î“Âµ ,
2
2
âˆ’1
âˆ’1 > >
âˆ’1 > >
where H = Q11 âˆ’ Q12 Qâˆ’1
22 Q21 = âˆ’Id + 0 and Î“Âµ = Î“11 + Q12 Q22 Î“22 (Q22 ) Q12 âˆ’ Î“12 (Q22 ) Q12 âˆ’
âˆ’1
2
2
Q12 Q22 Î“21 = Î“ + c Î“ + cÎ“ + cÎ“ = (c + 1) Î“, and

Q22 Î£Î¸ + Î£Î¸ Q>
22 = âˆ’Î“22 .
When  = 1, we can conclude by [S14] or [S19] taking Xn = (Âµn , Î¸n )> . Indeed, in this case
the covariance matrix is given by
1
1
e
(H + Id)Î£ + Î£(H > + Id) = âˆ’Î“,
2
2
where


H=

âˆ’Id
0

Id
âˆ’Id




e=
and Î“

Î“
cÎ“


cÎ“
.
c2 Î“

Therefore, if we split Î£ in blocks, say Î£Âµ , Î£Î¸ and Î£ÂµÎ¸ , we find the system
âˆ’Î£Âµ + 2Î£ÂµÎ¸ = âˆ’Î“
âˆ’Î£ÂµÎ¸ + Î£Î¸ = âˆ’cÎ“
âˆ’Î£Î¸ = âˆ’c2 Î“
and so the proof is concluded by solving this system.

S6

Stable convergence

This brief section contains some basic definitions and results concerning stable convergence. For
more details, we refer the reader to [S05, S07, S10] and the references therein.
Let (â„¦, A, P ) be a probability space, and let S be a Polish space, endowed with its Borel Ïƒ-field.
A kernel on S, or a random probability measure on S, is a collection K = {K(Ï‰) : Ï‰ âˆˆ â„¦} of
probability measures on the Borel Ïƒ-field of S such that, for each bounded Borel real function f
on S, the map
Z
Ï‰ 7â†’ Kf (Ï‰) =

f (x) K(Ï‰)(dx)

is A-measurable. Given a sub-Ïƒ-field H of A, a kernel K is said H-measurable if all the above
random variables Kf are H-measurable. A probability measure Î½ can be identified with a constant
kernel K(Ï‰) = Î½ for each Ï‰.
On (â„¦, A, P ), let (Yn )n be a sequence of S-valued random variables, let H be a sub-Ïƒ-field of
A, and let K be a H-measurable kernel on S. Then, we say that Yn converges H-stably to K, and
we write Yn âˆ’â†’ K H-stably, if
weakly

P (Yn âˆˆ Â· | H) âˆ’â†’ E [K(Â·) | H]

for all H âˆˆ H with P (H) > 0,

where K(Â·) denotes the random variable defined, for each Borel set B of S, as Ï‰ 7â†’ KIB (Ï‰) =
K(Ï‰)(B). In the case when H = A, we simply say that Yn converges stably to K and we write
Yn âˆ’â†’ K stably. Clearly, if Yn âˆ’â†’ K H-stably, then Yn converges in distribution to the probability

S 14

distribution E[K(Â·)]. The H-stable convergence of Yn to K can be stated in terms of the following
convergence of conditional expectations:
E[f (Yn ) | H]

Ïƒ(L1 , Lâˆ )

âˆ’â†’

Kf

(S6.8)

for each bounded continuous real function f on S. In [S07] the notion of H-stable convergence
is firstly generalized in a natural way replacing in (S6.8) the single sub-Ïƒ-field H by a collection
G = (Gn ) (called conditioning system) of sub-Ïƒ-fields of A and then it is strengthened by substituting
the convergence in Ïƒ(L1 , Lâˆ ) by the one in probability (i.e. in L1 , since f is bounded). Hence,
according to [S07], we say that Yn converges to K stably in the strong sense, with respect to
G = (Gn ), if
P
E [f (Yn ) | Gn ] âˆ’â†’ Kf
for each bounded continuous real function f on S.
We now conclude this section recalling some convergence results that we apply in our proofs.
From [S10, Th. 3.2] (see also [S07, Th. 5 and Cor. 7] or [S05, Th. 5.5.1 and Cor. 5.5.2]), we get:
Theorem S6.1. Given a filtration F = (Fn )n , let (YN,n )N,n be a triangular array of random
variables with values in Rk such that YN,n is Fn -measurable and E[YN,n |Fnâˆ’1 ] = 0. Suppose that
the following two conditions are satisfied:
(c1) E [ max1â‰¤nâ‰¤N |YN,n | ] â†’ 0 and
PN
> P
(c2)
n=1 YN,n YN,n âˆ’â†’ V , where V is a random variable with values in the space of positive
semidefinite k Ã— k-matrices.
PN
Then n=1 YN,n converges stably to the Gaussian kernel N (0, V ).
From [S07, Th. 5, Cor. 7, Rem. 4] or [S05, Th. 5.5.1, Cor. 5.5.2, Rem. 5.5.2]), we obtain:
Theorem S6.2. Let (Ln ) be a Rk -valued martingale with respect to the filtration F = (Fn ).
a.s., L1

Suppose that Ln âˆ’â†’ L for some Rk -valued random variable L and
(c1) ne E[supjâ‰¥n |Ljâˆ’1 âˆ’ Lj | ] âˆ’â†’ 0 and
P
P
(c2) n2e jâ‰¥n (Ljâˆ’1 âˆ’ Lj )(Ljâˆ’1 âˆ’ Lj )> âˆ’â†’ V , where V is a random variable with values in the
space of positive semidefinite k Ã— k-matrices.
Then

ne Ln âˆ’ L âˆ’â†’ N (0, V )

stably in strong sense w.r.t. F.

Indeed,P
following [S07, Example 6], it is enough to observe that Ln âˆ’ L can be written as
Ln âˆ’ L = jâ‰¥n (Lj âˆ’ Lj+1 ).
Finally, the following result combines together a stable convergence and a stable convergence in
the strong sense [S04, Lemma 1].
Theorem S6.3. Suppose that Cn and Dn are S-valued random variables, that M and N are kernels
on S, and that G = (Gn )n is an (increasing) filtration satisfying for all n
S
Ïƒ(Cn )âŠ‚Gn and Ïƒ(Dn )âŠ‚Ïƒ ( n Gn ) .
If Cn stably converges to M and Dn converges to N stably in the strong sense, with respect to G,
then
[Cn , Dn ] âˆ’â†’ M âŠ— N
stably.
(Here, M âŠ— N is the kernel on S Ã— S such that (M âŠ— N )(Ï‰) = M (Ï‰) âŠ— N (Ï‰) for all Ï‰.)

S 15

This last result contains as a special case the fact that stable convergence and convergence in
probability combine well: that is, if Cn stably converges to M and Dn converges in probability to a
random variable D, then (Cn , Dn ) stably converges to M âŠ— Î´D , where Î´D denotes the Dirac kernel
concentrated in D.

References
[S01] Generalized rescaled PoÌlya urn and its statistical applications. Main Article of this supplementary material (2020)
[S02] Aletti, G., Crimaldi, I.: The rescaled PoÌlya urn: local reinforcement and chi-squared goodness
of fit test. arXiv:1906.10951 (2019)
[S03] Aletti, G., Crimaldi, I., Ghiglietti, A.: Networks of reinforced stochastic processes: asymptotics for the empirical means. Bernoulli 25(4B), 3339â€“3378 (2019)
[S04] Berti, P., Crimaldi, I., Pratelli, L., Rigo, P.: A central limit theorem and its applications to
multicolor randomly reinforced urns. J. Appl. Probab. 48(2), 527â€“546 (2011). DOI 10.1239/
jap/1308662642. URL https://doi.org/10.1239/jap/1308662642
[S05] Crimaldi, I.: Introduzione alla nozione di convergenza stabile e sue varianti (Introduction
to the notion of stable convergence and its variants), vol. 57. Unione Matematica Italiana,
Monograf s.r.l., Bologna, Italy. (2016). Book written in Italian
[S06] Crimaldi, I., Dai Pra, P., Minelli, I.G.: Fluctuation theorems for synchronization of interacting PoÌlyaâ€™s urns. Stochastic Process. Appl. 126(3), 930â€“947 (2016). DOI 10.1016/j.spa.2015.
10.005. URL https://doi.org/10.1016/j.spa.2015.10.005
[S07] Crimaldi, I., Letta, G., Pratelli, L.: A Strong Form of Stable Convergence, vol. 1899, pp.
203â€“225. Springer (2007)
[S08] Delyon, B.: Stochastic approximation with decreasing gain: Convergence and asymptotic
theory. Technical report (2000)
[S09] Fort, G.: Central limit theorems for stochastic approximation with controlled markov chain
dynamics. ESAIM: PS 19, 60â€“80 (2015). DOI 10.1051/ps/2014013. URL https://doi.org/
10.1051/ps/2014013
[S10] Hall, P., Heyde, C.C.: Martingale limit theory and its application. Academic Press Inc.
[Harcourt Brace Jovanovich Publishers], New York (1980). Probability and Mathematical
Statistics
[S11] Kushner, H.J., Yin, G.G.: Stochastic approximation and recursive algorithms and applications, Applications of Mathematics (New York), vol. 35, second edn. Springer-Verlag, New
York (2003). Stochastic Modelling and Applied Probability
[S12] MeÌtivier, M.: Semimartingales. Walter de Gruyter and Co., Berlin (1982)
[S13] Mokkadem, A., Pelletier, M.: Convergence rate and averaging of nonlinear two-time-scale
stochastic approximation algorithms. Ann. Appl. Probab. 16(3), 1671â€“1702 (2006). DOI
10.1214/105051606000000448. URL https://doi.org/10.1214/105051606000000448
[S14] Pelletier, M.: Weak convergence rates for stochastic approximation with application to multiple targets an simulated annealing. Ann. Appl. Probab. 8(1), 10â€“44 (1998)
[S15] Pemantle, R.: A time-dependent version of poÌlyaâ€™s urn. J. Theor. Probab. 3, 627â€“637 (1990)
[S16] Rao, J.N.K., Scott, A.J.: The analysis of categorical data from complex sample surveys:
chi-squared tests for goodness of fit and independence in two-way tables. J. Amer. Statist.
Assoc. 76(374), 221â€“230 (1981)

S 16

[S17] Robbins, H., Siegmund, D.: A convergence theorem for non negative almost supermartingales
and some applications. In: Optimizing Methods in Statistics, pp. 233â€“257. Academic Press
(1971)
[S18] Sherman, J., Morrison, W.J.: Adjustment of an inverse matrix corresponding to a change
in one element of a given matrix. Ann. Math. Statist. 21(1), 124â€“127 (1950). DOI 10.1214/
aoms/1177729893. URL https://doi.org/10.1214/aoms/1177729893
[S19] Zhang, L.X.: Central limit theorems of a recursive stochastic algorithm with applications to
adaptive designs. Ann. Appl. Probab. 26(6), 3630â€“3658 (2016)

S 17

