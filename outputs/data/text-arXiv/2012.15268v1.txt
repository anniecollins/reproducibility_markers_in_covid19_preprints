arXiv:2012.15268v1 [q-bio.QM] 30 Dec 2020

UMAP-assisted K-means clustering of large-scale
SARS-CoV-2 mutation datasets
Yuta Hozumi1 , Rui Wang1 , Changchuan Yin2 , and Guo-Wei Wei1,3,4 *
1
Department of Mathematics,
Michigan State University, MI 48824, USA.
2
Department of Mathematics, Statistics, and Computer Science,
University of Illinois at Chicago, Chicago, IL 60607, USA
3
Department of Electrical and Computer Engineering,
Michigan State University, MI 48824, USA.
4
Department of Biochemistry and Molecular Biology,
Michigan State University, MI 48824, USA.

Abstract
Coronavirus disease 2019 (COVID-19) caused by severe acute respiratory syndrome coronavirus 2
(SARS-CoV-2) has a worldwide devastating effect. The understanding of evolution and transmission of
SARS-CoV-2 is of paramount importance for the COVID-19 control, combating, and prevention. Due to
the rapid growth of both the number of SARS-CoV-2 genome sequences and the number of unique mutations, the phylogenetic analysis of SARS-CoV-2 genome isolates faces an emergent large-data challenge.
We introduce a dimension-reduced k-means clustering strategy to tackle this challenge. We examine the
performance and effectiveness of three dimension-reduction algorithms: principal component analysis
(PCA), t-distributed stochastic neighbor embedding (t-SNE), and uniform manifold approximation and
projection (UMAP). By using four benchmark datasets, we found that UMAP is the best-suited technique
due to its stable, reliable, and efficient performance, its ability to improve clustering accuracy, especially
for large Jaccard distanced-based datasets, and its superior clustering visualization. The UMAP-assisted
k-means clustering enables us to shed light on increasingly large datasets from SARS-CoV-2 genome isolates.

Key words: PCA, t-SNE, UMAP, SARS-CoV-2, COVID-19

Contents
1

Introduction

2

Methods
2.1 Sequence and alignment . .
2.2 SNP position based features
2.3 Jaccard based representation
2.4 K-means clustering . . . . .
* Corresponding

1

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

author. E-mail: weig@msu.edu

i

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

2
2
3
3
3

2.5
2.6
2.7
3

Principal component analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
t-SNE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
UMAP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

Validation
3.1 Validation data . . . . . . . . . . . . . .
3.2 Validation results . . . . . . . . . . . . .
3.2.1 Coil 20 . . . . . . . . . . . . . . .
3.2.2 Facebook Network . . . . . . . .
3.2.3 MNIST . . . . . . . . . . . . . . .
3.2.4 Jaccard distanced-based MNIST
3.3 Efficiency comparison . . . . . . . . . .

4
4
5

.
.
.
.
.
.
.

6
7
7
8
9
10
12
14

4

SARS-CoV-2 mutation clustering
4.1 World SARS-CoV-2 mutation clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.2 United States SARS-CoV-2 mutation clustering . . . . . . . . . . . . . . . . . . . . . . . . . . .

14
14
17

5

Discussion

18

6

Conclusion

20

7

Appendix

23

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

ii

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

1

Introduction

Beginning in December 2019, coronavirus disease 2019 (COVID-19) caused by severe acute respiratory
syndrome coronavirus 2 (SARS-CoV-2) has become one of the most deadly global pandemic in history.
The COVID-19 infections in the US and other nations are still spiking. As of October 30, 2020, the World
Health Organization (WHO) has reported 44,888,869 confirmed cases of COVID-19 and 1,178,145 confirmed
deaths. The virus has spread to Africa, Americas, Eastern Mediterranean, Europe, South-East Asia and
Western Pacific [1]. To prevent further damage to our livelihood, we must control its spread through testing, social distancing, tracking the spread, and developing effective vaccines, drugs, diagnostics, and treatments.
SARS-CoV-2 is a positive-sense single-strand RNA virus that belongs to the Nidovirales order, coronaviridae family and betacoronavirus genus [20]. To effectively track the virus, testing patients with suspected exposure to COVID-19 and sequencing the strand via PCR (polymerase chain reaction) are important. From sequencing, we can analyze patterns in mutation and predict transmission pathways. Without understanding such pathways, current efforts to find effective medicines and vaccines could become
futile because mutations may change viral genome or lead to resistance. As of October 30, 2020, there
are 89627 available sequences with 23763 unique single nucleotide polymorphisms (SNPs) with respect to
the first SARS-CoV-2 sequent collected in December 2019 [35] according to our mutation tracker https:
//users.math.msu.edu/users/weig/SARS-CoV-2 Mutation Tracker.html.
A popular method for understanding mutational trends is to perform phylogenetic analysis, where one
clusters mutations to find evolution patterns and transmission pathways. Phylogenetic analysis has been
done on the Nidovirales family [2,2,9,10,12,15] to understand genetic evolutionary pathways, protein level
changes [6,12,30,31], large scale variants [30â€“32,34] and global trends [3,27,29]. Commonly used techniques
for phylogenetic analysis include tree based methods [21] and K-means clustering. Both methods belong to
unsupervised machine learning techniques, where ground truth is unavailable. These approaches provide
valuable information for exploratory research. A main issue with phylogenetic tree analysis is that as the
number of samples increase, its computation becomes unpractical, making it unsuitable for large genome
datasets. In contrast, K-means scales well with sample size increase, but does not perform well when the
sample size is too small. Jaccard distance is commonly used to compare genome sequences [36] because
it offers a phylogenetic or topological difference between samples. However, the tradeoff to the Jaccard
distance is that its feature dimension is the same as its number of samples, suggesting that for a large sample size, the number of features is also large. Since K-means clustering relies on computing the distance
between the center of the cluster and each sample, having a large feature space can result in expensive computation, large memory requirement, and poor clustering performance. This become a significant problem
as the number of SARS-CoV-2 genome isolates from patients has reached 150,000 at this point. There is a
pressing need for efficient clustering methods for SARS-CoV-2 genome sequences.
One technique to address this challenge is to perform dimensional reduction on the K-means input
dataset so that the task becomes manageable. Commonly used dimension reduction algorithms focus on
two aspects: 1) the pairwise distance structure of all the data samples and 2) preservation of the local distances over the global distance. Techniques such as principal component analysis (PCA) [11], Sammon
mapping [23], and multidimensional scaling (MDS) [8] aim to preserve the pairwise distance structure of
the dataset. In contrast, the t-distributed stochastic neighbor embedding (t-SNE) [16, 17], uniform manifold approximation and projection (UMAP) [4, 18], Laplacian eigenmaps [5], and LargeVis [26] focus on
the preservation of local distances. Among them, PCA, t-SNE, and UMAP are the most frequently used
algorithms in the applications of cell biology, bioinformatics, and visualization [4].
PCA is a popular method used in exploratory studies, aiming to find the directions of the maximum
variance in high-dimensional data and projecting them onto a new subspace to obtain low-dimensional

1

feature spaces while preserving most of the variance. The principal components of the new subspace can
be interpreted as the directions of the maximum variance, which makes the new feature axes orthogonal to
each other. Although PCA is able to cover the maximum variance among features, it may lose some information if one chooses an inappropriate number of principal components. As a linear algorithm, PCA performs poorly on the features with nonlinear relationship. Therefore, in order to present high-dimensional
data on low dimensional and nonlinear manifold, some nonlinear dimensional reduction algorithms such
as t-SNE and UMAP are employed. T-SNE is a nonlinear method that can preserve the local and global
structures of data. There are two main steps in t-SNE. First, it finds a probability distribution of the high
dimensional dataset, where similar data points are given higher probability. Second, it finds a similar
probability distribution in the lower dimension space, and the difference between the two distributions is
minimized. However, t-SNE computes pairwise conditional probabilities for each pair of samples and involves hyperparameters that are not always easy to tune, which makes it computationally complex. UMAP
is a novel manifold learning technique that also captures a nonlinear structure, which is competitive with
t-SNE for visualization quality and maintains more of the global structure with superior run-time performance [18]. UMAP is built upon the mathematical work of Belkin and Niyogi on Laplacian eigenmaps,
aiming to address the importance of uniform data distributions on manifolds via Riemannian geometry
and the metric realization of fuzzy simplicial sets by David Spivak [25]. Similar to t-SNE, UMAP can optimize the embedded low-dimensional representation with respect to fuzzy set cross-entropy loss function
by using stochastic gradient descent. The embedding is found by finding a low-dimensional projection of
the data that closely matches the fuzzy topological structure of the original space. The error between two
topological spaces will be minimized by optimizing the spectral layout of data in a low dimensional space.
In this work, we explore efficient computational methods for the SARS-CoV-2 phylogenetic analysis of
large volumn of SARS-CoV-2 genome sequences. Specifically, we are interested in developing a dimensionreduction assisted clustering method. To this end, we compare the effectiveness and accuracy of PCA,
t-SNE and UMAP for dimension reduction in association with the K-means clustering. To quantitatively
evaluate the performance, we recast supervised classification problems with labels into a K-means clustering problems so that the accuracy of K-means clustering can be evaluated. As a result, the accuracy and
performance of PCA, t-SNE and UMAP-assisted K-means clustering can be compared. By choosing the
different dimensional reduction ratios, we examine the performance of these methods in K-means settings
on four standard datasets. We found that UMAP is the most efficient, robust, reliable, and accurate algorithm. Based on this finding, we applied the UMAP-assisted K-means technique to large scale SARS-CoV-2
datasets generated from a Jaccard distance representation and a SNP position-based representation to further analyze its effectiveness, both in terms of speed and scalability. Our results are compared with those
in the literature [31] to shed new light on SARS-CoV-2 phylogenetics.

2
2.1

Methods
Sequence and alignment

The SARS-CoV-2 sequences were obtained from GISAID databank ( www.gisaid.com). Only complete
genome sequences with collection date, high coverage, and without â€™NNNNNNâ€™ in the sequences were
considered. Each sequence was aligned to the reference sequence [35] using a multiple sequence alignment
(MSA) package Clustal Omega [24]. A total of 23763 complete SARS-CoV-2 sequences are analyzed in this
work.

2

2.2

SNP position based features

Let N be the number of SNP profiles with respect to the SARS-CoV-2 reference genome sequence, and let
M be the number of unique mutation sites. Denote Vi as the position based feature of the ith SNP profile.
Vi = [vi1 , vi2 , ..., viM ],
is a 1 Ã— M vector. Here

(
vij

=

i = 1, 2, ..., N

1, mutation site
0, otherwise.

(1)

(2)

We compile this into an N Ã— M position based feature,
S(i, j) = vij

(3)

where each row represents a sample. Note that S(i, j) is a binary representation of the position and is
sparse.

2.3

Jaccard based representation

The Jaccard distance measures the dissimilarity between two sets. It is widely used in the phylogenetic
studies of SNP profiles. In this work, we utilize Jaccard distance to compare SNP profiles of SARS-CoV-2
genome isolates.
Let A and B be two sets. Consider the Jaccard index between A and B, denoted J(A, B), as the cardinality of the intersection divided by the cardinality of the union
J(A, B) =

Aâˆ©B
Aâˆ©B
=
.
AâˆªB
A + B âˆ’ Aâˆ©B

(4)

The Jaccard distance between the two sets is defined by subtracting the Jaccard index from 1:
dJ (A, B) = 1 âˆ’ J(A, B) =

AâˆªB âˆ’ Aâˆ©B
AâˆªB

(5)

We assume there are N SNP profiles or genome isolates that have been aligned to the reference SARSCoV-2 genome. Let Si , i = 1, ..., N , be the set with the position of the mutation of the ith sample. The
Jaccard distance between two sets Si and Sj is given by dJ (Si , Sj ). Taking the pairwise distance between
all the samples, we can construct the Jaccard based representation, resulting in an N Ã— N distance matrix D
D(i, j) = dJ (Si , Sj )

(6)

This distance defines a metric over the collections of all finite sets [14].

2.4 K-means clustering
K-means clustering is one of the most popular unsupervised learning methods in machine learning, where
it aims to cluster or partition a data {x1 , ..., xN }, xi âˆˆ RM into k clusters, {C1 , ..., Ck }, k â‰¤ N .
K-means clustering begins with selecting k points as k cluster centers, or centroids. Then, each point in
the dataset is assigned to the nearest centroid. The centroids are then updated by minimizing the withincluster sum of squares (WCSS), which is defined as
k
X
X
i=1 xi âˆˆCk

kxi âˆ’ Âµk k22 .
3

(7)

Here, k Â· k2 denotes the l2 norm and Âµk is the average of the data point in cluster k
Âµk =

1 X
xi .
|Ck |

(8)

xi âˆˆCk

This method, however, only finds the optimal centroid, given a fixed number of clusters k. In applications, we are interested in finding the optimal number of clusters as well. In order to obtain the best k
clusters, elbow method was used. The optimal number of clusters can be determined via the elbow method
by plotting the WCSS against the number of clusters, and choosing the inflection point position as the
optimal number of clusters.

2.5

Principal component analysis

Principal component analysis (PCA) is one the most commonly used dimensional reduction techniques
for the exploratory analysis of high-dimensional data [11]. Unlike other methods, there is no need for any
assumptions in the data. Therefore, it is a useful method for new data, such as SARS-CoV-2 SNPs data. PCA
is conducted by obtaining one component or vector at a time. The first component, termed the principal
component, is the direction that maximizes the variance. The subsequent components are orthogonal to
earlier ones.
Let {xi }N
i=1 be the input dataset, with N being the number of samples or data points. For each xi , let
xi âˆˆ RM , where M is the number of features or data dimension. Then, we can cast the data as a matrix
X âˆˆ RN Ã—M . PCA seeks to find a linear combination of the columns of X with maximum variance.
n
X

aj xj = Xa,

(9)

j=1

where a1 , a2 , ..., an are constants. The variance of this linear combination is defined as
var(Xa) = aT Sa,

(10)

where S is the covariance matrix for the dataset. Note that we compute the eigenvalue of the covariance
matrix. The maximum variance can be computed iteratively using Rayleighâ€™s quotient
a(1) = arg max
a

aT X T Xa
.
aT a

(11)

The subsequent components can be computed by maximizing the variance of
XÌ‚k = X âˆ’

kâˆ’1
X

Xaj aTj

(12)

j=1

where k represents the kth principal component. Here, k âˆ’ 1 principal components are subtracted from
the original matrix X. Therefore, the complexity of the method scales linearly with the number of components one seeks to find. In applications, we hope that the first few components give rise to a good PCA
representation of the original data matrix X.

2.6

t-SNE

The t-distributed stochastic neighbor embedding (t-SNE) is a nonlinear dimensional reduction algorithm
that is well suited for reducing high dimensional data into the two- or three-dimensional space. There are
two main stages in t-SNE. First, it constructs a probability distribution over pairs of data such that a pair
4

of near data points is assigned with a high probability, while a pair of farther away points is given a low
probability. Second, t-SNE defines a probability distribution in the embedded space that is similar to that in
the original high-dimensional space, and aims to minimize the Kullback-Leibler (KL) divergence between
them [16].
Let {x1 , x2 , ..., xN |xi âˆˆ RM } be a high dimensional input dataset. Our goal is to find an optimal low
dimensional representation {y1 , ..., yN |yi âˆˆ Rk }, such that k << M . The first step in t-SNE is to compute
the pairwise distribution between xi and xj , defined as pij . However, we find the conditional probability
of xj , given xi :
exp(âˆ’kxi âˆ’ xj k2 /2Ïƒi2 )
pj|i = P
i 6= j,
(13)
2 ,
2
m6=i exp(âˆ’kxi âˆ’ xm k /2Ïƒi )
setting pi|i = 0, and the denominator normalizes the probability. Here, Ïƒi is the predefined hyperparameter
called perplexity. A smaller Ïƒi is used for a denser dataset. Notice that this conditional probability is
symmetric when the perplexity is fixed, i.e. pi|j = pj|i . Then, define the pairwise probability as
pij =

pj|i + pij
.
2N

(14)

In the second step, we learn a k-dimensional embedding {y1 , ..., yN |yi âˆˆ Rk }. To this end, t-SNE calculates a similar probability distribution qij defined as
1
1+kyi âˆ’yj k2

qij = P P
m

1
l6=m 1+kym âˆ’yl k2

,

i 6= j

(15)

and setting qii = 0. Finally, the low dimensional embedding {y1 , ..., yN |yi âˆˆ Rk } is found by minimizing
the KL-divergence via a standard gradient descent method
KL(P |Q) =

X
i,j

pij log

pij
,
qij

(16)

where P and Q are the distributions for pij and qij , respectively. Note that the probability distributions in
Eqs. (13) and (15) can be replaced by using many other delta sequence kernel of positive type [33].

2.7

UMAP

Uniform manifold approximation and projection (UMAP) is a nonlinear dimensional reduction method,
utilizing three assumptions: the data is uniformly distributed on Riemannian manifold, Riemannian metric
is locally constant, and the manifold if locally connected. Unlike t-SNE which utilizes probabilistic model,
UMAP is a graph-based algorithm. Its essentially idea is to create a predefined k-dimensional weighted
UMAP graph representation of each of the original high-dimensional data point such that the edge-wise
cross-entropy between the weighted graph and the original data is minimized. Finally, the k-dimensional
eigenvectors of the UMAP graph are used to represent each of the original data point. In this section,
a computational view of UMAP is presented. For a more theoretical account, the reader is referred to
Ref. [18].
Similar to t-SNE, UMAP considers the input data X = {x1 , x2 , ..., xN }, xi âˆˆ RM and look for an optimal
low dimensional representation {y1 , ..., yN |yi âˆˆ Rk }, such that k < M . The first stage is the construction of
weighted k-neighbor graphs. Let define a metric d : X Ã— X â†’ R+ . Let k << M be a hyperparemeter, and
compute the k-nearest neighbors of each xi under a given metric d. For each xi , let
Ïi = min{d(xi , xj )|1 â‰¤ j â‰¤ k, d(xi , xj ) > 0}

5

(17)

where Ïƒi is defined via
k
X
j=1


exp

âˆ’ max(0, d(xi , xj ) âˆ’ Ïi )
Ïƒi


= log2 k.

(18)

One chooses Ïi to ensure at least one data point is connected to xi and having edge weight of 1, and set
Ïƒi as a length scale parameter. One defines a weighted directed graph GÌ„ = (V, E, Ï‰), where V is the set of
vertices (in this case, the data X), E is the set of edges E = {(xi , xj )|1 â‰¤ h â‰¤ k, 1 â‰¤ i â‰¤ N }, and Ï‰ is the
weight for edges


âˆ’ max(0, d(xi , xj ) âˆ’ Ïi )
Ï‰(xi , xj ) = exp
.
(19)
Ïƒi
UMAP tries to define an undirected weighted graph G from directed graph GÌ„ via symmetrization. Let A
be the adjacency matrix of the graph GÌ„. A symmetric matrix can be obtained
B = A + AT âˆ’ A âŠ— AT ,

(20)

where T is the transpose and âŠ— denotes the Hadamard product. Then, the undirected weighted Laplacian
G (the UMAP graph) is defined by its adjacency matrix B.
In its realization, UMAP evolves an equivalent weighted graph H with a set of points {yi }i=1,Â·Â·Â· ,N ,
utilizing attractive and repulsive forces. The attractive and repulsive forces at coordinate yi and yj are
given by
2(bâˆ’1)

âˆ’2abkyi âˆ’ yj k2
w(xi , xj )(yi âˆ’ yj ), and
1 + kyi âˆ’ yj k22
2b
(1 âˆ’ w(xi , xj ))(yi âˆ’ yj )
( + kyi âˆ’ yj k22 )(1 + akyi âˆ’ yj k2b
2 )

(21)
(22)

where a, b are hyperparemeters, and  is taken to be a small value such that the denominator does not
k
become 0. The goal is to find the optimal low-dimensional coordinates {yi }N
i=1 , yi âˆˆ R , that minimizes the
edge-wise cross entropy with the original data at each point. The evolution of the UMAP graph Laplacian
G can be regarded as a discrete approximation of the Laplace-Beltrami operator on a manifold defined by
the data [7]. Implementation and further detail of UMAP can be found in Ref. [18].
UMAP may not work well if the data points is non-uniform. If part of the data points have k important
neighbors while other part of the data points have k 0 >> k important neighbors, the k-dimensional UMAP
will not work efficiently. Currently, there is no algorithm to automatically determine the critic minimal
kmin for a given dataset. Additionally, weights w(xi , xj ) and force terms can be replaced by other functions
that are easier to evaluate [33]. The metric d can be selected as Euclidean distance, Manhattan distance,
Minkowski distance, and Chebyshev distance, depending on applications.

3

Validation

K-means clustering is one of the unsupervised learning algorithms, suggesting that neither the accuracy
nor the root-mean-square error can be calculated to evaluate the performance of the K-means clustering explicitly. Additionally, K-means clustering can be problematic for high-dimensional large datasets.
Dimension-reduced K-means clustering is an efficient approach. To evaluate its accuracy and performance,
we convert supervised classification problems with known salutations into dimension-reduced K-means
clustering problems. In doing so, we apply the K-means clustering to the classification dataset by setting
the number of clusters equals to the number of the real categories. Next, in each cluster, we will take the
data with the dominant label as the test for all samples and then calculate the K-means clustering accuracy
for the whole dataset.
6

3.1

Validation data

In this work, we will consider the following classification datasets to test the performance of the clustering
methods: Coil 20, Facebook large page-page network, MNIST, and Jaccard distanced-based MNIST.
â€¢ Coil 20: Coil 20 [19] is a dataset with 1440 gray scale images, consisting of 20 different objects, each
with 72 orientation. Each image is of size 128 Ã— 128, which was treated as a 16384 dimensional vector
for dimensional reduction
â€¢ Facebook Network: Facebook large page-page network [22] is a page-page webgraph of verified
Facebook sites. Each node represents a facebook page, and the links are the mutual links between
sites. This is a binary dataset with 22,470 nodes; hence the sample size and feature size are both
22,470. Jaccard distance was computed between each nodes for the feature space.
â€¢ MNIST: MNIST [13] is a hand written digit dataset. Each image is a grey scale of size 28 Ã— 28, which
was treated as a 784 dimensional vector for the feature space, each with a integer value in [0, 255].
Standard normalization was used before performing dimensional reduction. There are 70,000 sample,
with 10 different labels.
â€¢ Jaccard distanced-based MNIST: The above dataset was converted to a Jaccard distance-based dataset.
This is to simulate position based mutational dataset, where 1 indicates a mutation in a particular position. Jaccard distance was used to construct the feature space, hence for each sample, the feature
size is 70,000. This dataset can be viewed as an additional validation on our Jaccard distance representation.

3.2

Validation results

In the present work, we implement three popular dimensional reduction methods, PCA, UMAP, and tSNE, for the dimension reduction and compare their performance in K-means clustering. For a uniform
comparison, we reduce the dimensions of the samples by a set of ratios. The minimum between the number
of features and the number of samples was taken as base of the reduction. For the Coil 20 dataset, since the
numbers of samples and features were 1440 and 16384, respectively, dimension-reductions were based on
1440. For the Facebook Network, since the numbers of samples and features were both 22,470, dimensionreductions were based on 22,470. For the MNIST dataset, since the numbers of samples and features were
respectively 70,000 and 784, dimension-reductions were based on 784. Finally, for the Jaccard distancedbased MNIST dataset, since the numbers of samples and features were both 70,000, dimension-reductions
were based on 70,000. Note that for the Jaccard distanced-based MNIST data, more aggressive ratios were
used because the original feature size is huge, i.e., 70,000. The standard ratios of 2, 4, and 8, etc do not
sufficiently reduce the dimension for effective K-means computation. For the purpose of visualization,
two-dimensional reduction algorithms are applied to each reduction scheme. In order to validate PCA,
UMAP, and t-SNE assisted K-means clustering, we observed their performance using labeled datasets.
K-nearest neighbors (K-NN) was used to find the baseline of the reduction, which reveals how much
information can be preserved in the feature after applying a dimensional reduction algorithm. For k-NN,
10 fold cross-validation was performed.
Notably, K-means clustering is an unsupervised learning algorithm, which does not have labels to evaluate the clustering performance explicitly. However, we can assess the K-means clustering accuracy via
labeled datasets that has ground truth. In doing so, we choose the number of K as the original number
of classes. Then, we can compared the k-means clustering results with the ground truth. Therefore, the
accuracy can reveal the performance of the proposed dimension-reduction-assisted (k-means) clustering
method. For the classification problem, we assume the training set is {(xi , yi )|xi âˆˆ Rm , yi âˆˆ Z}ni=1 with
the |{yi }ni=1 | = k. Here n, m, and k represent the number of samples, the number of features {xi }, and the
7

number of labels {yi }, respectively. We set the number of clusters equals to the number of labels k. After
applying the K-means clustering algorithm, we get k different clusters {cj }kj=1 . In each cluster, we define
the predictor of the K-means clustering in the cluster cj to be :
yÌ‚(cj ) = max{Fj (y1 ), Â· Â· Â· , Fj (yk )},

(23)

where Fj (yi ), Â· Â· Â· , Fj (yk ) are the appearance frequencies of each label in the cluster cj . Then the clustering
accuracy can be defined as:
P
1{yi =yÌ‚i }
Accuracy = i
,
(24)
n
where {yÌ‚i } are predicted labels. Moreover, other evaluation metrics such as precision, recall, and receiver
operating characteristic (ROC) can also be defined accordingly.
3.2.1

Coil 20

Figure 1: Comparison of different dimensional reduction algorithms on Coil 20 dataset. Total 20 different labels are in the Coil 20
dataset, and we use the ground truth label to color each data points. (a) Feature size is reduced to dimension 2 by PCA. (b) Feature
size is reduced to dimension 2 by t-SNE. (c) Feature size is reduced to dimension 2 by UMAP.

Figure 1 shows the performance of PCA-assisted, UMAP-assisted and t-SNE-assisted clustering of the
Coil 20 dataset. For each case, the dataset were reduced to dimension 2 using default parameters, and the
plots were colored with the ground truth of the Coil 20 dataset. It can be seen that PCA does not present
good clustering, whereas UMAP and t-SNE show very good clusters.
Table 1: Accuracy of k-NN of the Coil 20 dataset without applying any reduction algorithms, as well as the accuracy of k-NN assisted
by PCA, UMAP and t-SNE with different dimensional reduction ratio. The sample size, feature size, and the number of labels of the
Coil 20 dataset are 1440, 16384, and 20, respectively.

Dataset

k-NN accuracy
w/o reduction

Coil 20
(1440,16384,20)

0.956

Reduced
dimension
720 (1/2)
360 (1/4)
180 (1/8)
90 (1/16)
45 (1/32)
22 (1/64)
14 (1/100)
7 (1/200)
3
2

PCA
accuracy
0.955
0.957
0.973
0.977
0.980
0.985
0.730
0.985
0.850
0.730

UMAP
accuracy
0.668
0.861
0.867
0.860
0.861
0.868
0.851
0.870
0.863
0.853

t-SNE
accuracy
0.850
0.889
0.881
0.885
0.875
0.743
0.878
0.845
0.959
0.948

Table 1 shows the accuracy of k-NN clustering of the Coil 20 dataset assisted by PCA, t-SNE, and UMAP
with different dimensional reduction radio. The Coil 20 dataset has 1,440 samples, 16,384 features, and 20
different labels. For PCA, the sklearn implementation on python was used with standard parameters. Note
that for all methods, dimensions were reduced to 3 and 2 for a comparison. For t-SNE, Multicore-TSNE [28]
8

was used because it offers up to 8 core processor, which is not available in the sklearn implementation, and it
is the fastest performing t-SNE algorithm. For UMAP, we used standard parameters [18]. It can be seen that
when we reduce the dimension to 3, t-SNE performs best. Moreover, when the dimensional reduction ratio
is 1/100, PCA and UMAP also perform well. Notably, the k-NN accuracy for the data without applying
any dimensional reduction algorithm is 0.956, indicating that UMAP does not provide the best clustering
performance on the Coil 20 dataset. However, PCA and t-SNE will preserve the information of the original
data with a dimensional reduction ratio larger than 1/100, and t-SNE even performs better for dimensional
three on the Coil 20 dataset.
Table 2: Accuracy of K-means clustering of the Coil 20 dataset without applying any reduction algorithms, as well as the accuracy
of K-means assisted by PCA, UMAP and t-SNE with different dimensional reduction ratio. The sample size, feature size, and the
number of labels of the Coil 20 dataset are 1440, 16384, and 20, respectively.

Dataset

K-means accuracy
w/o reduction

Coil 20
(1440,16384,20)

0.626

Reduced
dimension
720 (1/2)
360 (1/4)
180 (1/8)
90 (1/16)
45 (1/32)
22 (1/64)
14 (1/100)
7 (1/200)
3
2

PCA
accuracy
0.64
0.678
0.633
0.642
0.666
0.673
0.631
0.591
0.561
0.537

UMAP
accuracy
0.301
0.800
0.822
0.799
0.800
0.819
0.817
0.819
0.800
0.801

t-SNE
accuracy
0.798
0.718
0.648
0.681
0.615
0.151
0.154
0.360
0.780
0.828

Table 2 describes the accuracy of K-means clustering of Coil 20 assisted by PCA, UMAP, and t-SNE
with different dimensional reduction ratio. For consistency, we use the same set of standard parameters as
k-NN. For the Coil 20 dataset, the accuracy of K-means clustering assisted by UMAP has the best performance. When the reduced dimension is 2048 (ratio 1/8), UMAP will result in a relatively high K-means
accuracy (0.822). Moreover, although PCA performs best on k-NN accuracy, it performs poorly on the
K-means accuracy, indicating that PCA is not a suitable dimensional reduction algorithm on the Coil 20
dataset. Furthermore, the highest accuracy of K-means clustering is 0.828, which is calculated from the
t-SNE-assisted algorithm. However, the t-SNE-assisted accuracy under different reduction ratio changes
dramatically. When the ratio is 1/64, the t-SNE-assisted accuracy is only 0.151, indicating that t-SNE is
sensitive to the hyper-parameters settings. In contrast, the performance of UMAP is highly stable under all
dimension-reduction ratios.
Note that dimension-reduced k-means clustering methods outperform the original k-means clustering.
Therefore, the proposed dimension-reduced k-means clustering methods not only improve the k-means
clustering efficiency, but also achieve better accuracy.
3.2.2

Facebook Network

Figure 2 shows the visualization performance of PCA-assisted, UMAP-assisted, and t-SNE-assisted clustering of the Facebook Network. For each case, the dataset was reduced to dimension 2 using default
parameters, and the plots were colored with the ground truth of the Facebook Network. Figure 2 shows
that the PCA-based data is located distributively, while the t-SNE- and UMAP-based data show clusters.
Table 3 shows the accuracy of k-NN clustering of the Facebook Network assisted by PCA, t-SNE, and
UMAP with different dimensional reduction radio. The Facebook Network dataset has 22,470 samples with
4 different labels, and the feature size of the Facebook Network is also 22,470. For each algorithm, we use the
same settings as the Coil 20 dataset. Without applying any dimensional reduction method, The Facebook
9

Figure 2: Comparison of different dimensional reduction algorithms on the Facebook Network dataset. Total 4 different labels are in
the Facebook Network dataset, and we use the ground truth label to color each data points. (a) Feature size is reduced to dimension 2
by PCA. (b) Feature size is reduced to dimension 2 by t-SNE. (c) Feature size is reduced to dimension 2 by UMAP.

Network has 0.755 k-NN accuracy. The reduced feature from PCA has the best k-NN performance when the
reduction ratio is 1/2. UMAP has a better performance compared to PCA and t-SNE when the reduction
ratio is smaller than 1/16.
Table 3: Accuracy of k-NN of the Facebook Network without applying any reduction algorithms, as well as the accuracy of k-NN
assisted by PCA, UMAP and t-SNE with different dimensional reduction ratio. The sample size, feature size, and the number of labels
of the Facebook Network are 22470, 22470, and 4, respectively.

Dataset

Facebook Network
(22470, 22470, 4)

k-NN accuracy
w/o reduction

0.755

Reduced
dimension
11235 (1/2)
5617 (1/4)
2808 (1/8)
1404 (1/16)
702 (1/32)
351 (1/64)
224 (1/100)
112 (1/200)
44 (1/500)
22 (1/1000)
3
2

PCA
accuracy
0.756
0.755
0.754
0.751
0.751
0.746
0.733
0.721
0.714
0.690
0.552
0.501

UMAP
accuracy
0.360
0.669
0.754
0.816
0.814
0.815
0.814
0.819
0.816
0.815
0.801
0.786

t-SNE
accuracy
0.307
0.316
0.355
0.707
0.669
0.690
0.676
0.633
0.709
0.643
0.741
0.732

Table 4 describes the accuracy of K-means clustering of the Facebook Network assisted by PCA, UMAP
and t-SNE with different dimensional reduction ratio. PCA, UMAP, and t-SNE all have very poor performance, which may be caused by the smaller number of labels. The highest accuracy 0.427 is observed in
the t-SNE-assistant algorithm with dimension 2.
Similar to the last case, UMAP-based and t-SNE-based dimension-reduced k-means clustering methods
outperform the original k-means clustering with the full feature dimension. Therefore, it is useful to carry
out dimension reduction before k-means clustering for large datasets.
3.2.3

MNIST

Figure 3 shows the performance of PCA-assisted, UMAP-assisted and t-SNE-assisted clustering of the
MNIST dataset. The sample size of the MNIST dataset is 70000, which has 784 features with 10 different digit labels. For each case, the dataset was reduced to dimension 2 using default parameters, and the
plots were colored with the ground truth of the MNIST dataset. In Figure 3, by applying the UMAP algorithm, the clear clusters can be detected for the MNIST dataset. The t-SNE offers a reasonable clustering at
dimension 2 too. However, the PCA does not provide a good clustering.

10

Table 4: Accuracy of K-means clustering of the Facebook Network without applying any reduction algorithms, as well as the accuracy
of K-means assisted by PCA, UMAP and t-SNE with different dimensional reduction ratio. The sample size, feature size, and the
number of labels of the Facebook Network are 22470, 22470, and 4, respectively.

Dataset

K-means accuracy
w/o reduction

Facebook Network
(22470, 22470, 4)

0.374

Reduced
dimension
11235 (1/2)
5617 (1/4)
2808 (1/8)
1404 (1/16)
702 (1/32)
351 (1/64)
224 (1/100)
112 (1/200)
44 (1/500)
22 (1/1000)
3
2

PCA
accuracy
0.331
0.331
0.331
0.331
0.331
0.331
0.331
0.331
0.331
0.331
0.332
0.358

UMAP
accuracy
0.306
0.307
0.411
0.397
0.401
0.400
0.400
0.400
0.400
0.401
0.351
0.345

t-SNE
accuracy
0.306
0.299
0.314
0.313
0.306
0.308
0.327
0.306
0.313
0.306
0.344
0.427

Figure 3: Comparison of different dimensional reduction algorithms on the MNIST dataset. Total 10 different labels are in the MNIST
dataset, and we use the ground truth label to color each data points. (a) Feature size is reduced to dimension 2 by PCA. (b) Feature
size is reduced to dimension 2 by t-SNE. (c) Feature size is reduced to dimension 2 by UMAP.

Table 5: Accuracy of k-NN of the MNIST dataset without applying any reduction algorithms, as well as the accuracy of k-NN assisted
by PCA, UMAP and t-SNE with different dimensional reduction ratio. The sample size, feature size, and the number of labels of the
MNIST dataset are 70000, 784, and 10, respectively.

Dataset

k-NN accuracy
w/o reduction

MNIST
(70000, 784, 10)

0.948

Reduced
dimension
392 (1/2)
196 (1/4)
98 (1/8)
49 (1/16)
24 (1/32)
12 (1/64)
7 (1/100)
3
2

PCA
accuracy
0.951
0.956
0.960
0.961
0.953
0.926
0.846
0.513
0.323

UMAP
accuracy
0.937
0.938
0.937
0.937
0.937
0.937
0.936
0.929
0.919

t-SNE
accuracy
0.696
0.846
0.893
0.886
0.842
0.676
0.940
0.938
0.928

Table 5 shows the accuracy of k-NN clustering of the MNIST dataset assisted by PCA, t-SNE, and UMAP
with different dimensional reduction radios. For each algorithm, we use the same settings as the Coil
20 dataset. Without applying any dimensional reduction algorithms, the accuracy of k-NN is 0.948. By
applying PCA/UMAP with the reduction ratio greater than 1/64, the accuracy of PCA/UMAP-assisted
11

Table 6: Accuracy of K-means clustering of the MNIST dataset without applying any reduction algorithms, as well as the accuracy
of K-means assisted by PCA, UMAP and t-SNE with different dimensional reduction ratio. The sample size, feature size, and the
number of labels of the MNIST dataset are 70000, 784, and 10, respectively.

Dataset

K-means accuracy
w/o reduction

MNIST
(70000, 784, 10)

0.494

Reduced
dimension
392 (1/2)
196 (1/4)
98 (1/8)
49 (1/16)
24 (1/32)
12 (1/64)
7 (1/100)
3
2

PCA
accuracy
0.487
0.492
0.498
0.496
0.501
0.489
0.464
0.365
0.300

UMAP
accuracy
0.665
0.667
0.673
0.718
0.697
0.682
0.677
0.727
0.712

t-SNE
accuracy
0.122
0.113
0.113
0.113
0.114
0.138
0.740
0.537
0.593

k-NN is at the same level without using any dimensional reduction algorithm. However, in contract with
UMAP and t-SNE, when the reduced dimension is 2 or 3, PCA performs poorly. This indicates that the PCA
may not be suitable for dimension-reduction for datasets with a large sample size.
Table 6 describes the accuracy of K-means clustering of the MNIST dataset assisted by PCA, UMAP, and
t-SNE with different dimensional reduction ratios. By applying PCA, the accuracy of K-means is around
0.45. The t-SNE method performance is quite unstable, from very poor (0113) to the best (0.740), and to a
relatively low value of 0.593. In contrast, we can see a stable and improved accuracy from using UMAP at
various reduction ratios, indicating that the reduced feature generated by UMAP can better represent the
clustering properties of the MNIST dataset compared to the PCA and t-SNE.
As observed early, the present UMAP and t-SNE-assisted k-means clustering methods also significantly
out-perform the original k-means clustering for this dataset.
3.2.4

Jaccard distanced-based MNIST

Figure 4: Comparison of different dimensional reduction algorithms on the Jaccard distanced-based MNIST dataset. Total 10 different
labels are in the Jaccard distanced-based MNIST dataset, and we use the ground truth label to color each data points. (a) Feature size
is reduced to dimension 2 by PCA. (b) Feature size is reduced to dimension 2 by t-SNE. (c) Feature size is reduced to dimension 2 by
UMAP.

Out last validation dataset is Jaccard distanced-based MNIST. This dataset can be treated as a test on
the Jaccard distance-based data representation. Figure 4 shows the performance of PCA-assisted, UMAPassisted, and t-SNE-assisted clustering of the Jaccard distanced-based MNIST dataset. The dataset was
reduced to dimension 2 using default parameters for visualization, and the plots were colored with the
ground truth of the Jaccard distanced-based MNIST dataset. From Figure 4, we can see that UMAP provides
the clearest clusters compared to PCA and t-SNE when the dimension is reduced to 2. The performance of
t-SNE is reasonable while PCA does not give a good clustering.

12

Table 7: Accuracy of k-NN of the Jaccard distanced-based MNIST dataset without applying any reduction algorithms, as well as the
accuracy of k-NN assisted by PCA, UMAP and t-SNE with different dimensional reduction ratio. The sample size, feature size, and
the number of labels of the Jaccard distanced-based MNIST dataset are 70000, 70000, and 10, respectively.

Dataset

k-NN accuracy
w/o reduction

Jaccard distanced-based MNIST
(70000, 70000, 10)

0.958

Reduced
dimension
7000 (1/10)
3500 (1/20)
1750 (1/40)
875 (1/80)
437 (1/160)
218 (1/320)
109 (1/640)
70 (1/1000)
35 (1/2000)
17 (1/5000)
7 (1/10000)
3
2

PCA
accuracy
0.958
0.958
0.958
0.958
0.958
0.958
0.958
0.958
0.956
0.938
0.867
0.487
0.313

UMAP
accuracy
0.958
0.966
0.967
0.967
0.968
0.968
0.968
0.968
0.968
0.968
0.967
0.965
0.960

t-SNE
accuracy
NA
NA
NA
NA
0.718
0.701
0.873
0.915
0.872
0.916
0.942
0.939
0.924

Table 8: Accuracy of K-means clustering of the Jaccard distanced-based MNIST dataset without applying any reduction algorithms,
as well as the accuracy of K-means assisted by PCA, UMAP and t-SNE with different dimensional reduction ratio. The sample size,
feature size, and the number of labels of the Jaccard distanced-based MNIST dataset are 70000, 70000, and 10, respectively.

Dataset

K-means accuracy
w/o reduction

Jaccard distanced-based MNIST
(70000, 70000, 10)

0.555

Reduced
dimension
7000 (1/10)
3500 (1/20)
1750 (1/40)
875 (1/80)
437 (1/160)
218 (1/320)
109 (1/640)
70 (1/1000)
35 (1/2000)
17 (1/5000)
7 (1/10000)
3
2

PCA
accuracy
0.436
0.436
0.436
0.435
0.435
0.435
0.435
0.436
0.435
0.436
0.431
0.364
0.261

UMAP
accuracy
0.329
0.693
0.792
0.793
0.793
0.793
0.794
0.793
0.794
0.793
0.793
0.798
0.791

t-SNE
accuracy
NA
NA
NA
NA
0.114
0.156
0.114
0.113
0.116
0.113
0.737
0.635
0.635

Table 7 shows the accuracy of k-NN clustering of Jaccard distanced-based MNIST assisted by PCA, tSNE, and UMAP with different dimensional reduction radios. For each algorithm, we use the same settings
as the Coil 20 dataset. Notably, the k-NN accuracy for the data without applying any dimensional reduction
algorithm is 0.958, which is at the same level as the PCA algorithm with a reduction ratio greater than
1/5000. Moreover, we can find that UMAP performs well compared to PCA and t-SNE, indicating that
after applying UMAP, the reduced feature still preserves most of the valued information of the Jaccard
distanced-based MNIST dataset. The stability and persistence of UMAP at various reduction ratios are the
most important features.
Table 8 describes the accuracy of K-means clustering of the Jaccard distanced-based MNIST dataset
assisted by PCA, UMAP, and t-SNE with different dimensional reduction ratio. For consistency, we will use
the same standard parameters as k-NN. Similar to the MNIST dataset, the accuracy of K-means clustering

13

assisted by UMAP still has the best performance. When the reduced dimension is 3, UMAP will result
in the highest K-means accuracy 0.798. Noticeably, although PCA performs well on k-NN accuracy, it
has the lowest K-mean accuracy, indicating that PCA is not a suitable dimensional reduction algorithm,
especially for those datasets with a large number of samples. To be noted, the t-SNE accuracy at four
reduced dimensions are not available due to the extremely long running time.
In a nutshell, PCA, UMAP, and t-SNE can all perform well for k-NN. However, for the Coil 20 dataset,
UMAP performs slightly poorly, whereas the t-SNE performs well, which may be caused by a lack of data
size. In order to train UMAP, it needs a suitable data size. The Coil 20 dataset has 20 labels, each with only
72 samples. This may not be enough to train UMAP properly. However, even in this case, UMAP performance is still very stable at various reduction ratios and is the best method in terms of reliability, which
become the major advantages of UMAP. Another strength of UMAP comes from its dimension-reduction
for K-means clustering. In most cases, UMAP can improve K-means clustering accuracy, especially for the
Jaccard distanced-based MNIST dataset. Furthermore, UMAP can generate a very clear and elegant visualization of clusters with low dimensional reduction value such as 2. Additionally, UMAP performed better
than PCA and t-SNE for a larger dataset (MNIST and Jaccard distanced-based MNIST). Especially for the
Jaccard distanced-based MNIST data, where Jaccard distance was used as the metric, UMAP performed
best, which indicates the merit of using UMAP for Jaccard distanced-based datasets, such as COVID-19
SNP datasets. Furthermore, the accuracies for k-NN classification and K-means clustering are both improved on the Jaccard distance-based MNIST dataset compared to the original MNIST dataset, which provides convincing evidence that the Jaccard distance representation will help improve the performance of
the clustering on the SARS-CoV-2 mutation dataset in the following sections.

3.3

Efficiency comparison

It is important to understand the computational time behaviors of various methods. To this end, we compare computational time for three dimension-reduction techniques. Figure 5 depicts the computational time
of three methods for the four datasets under various reduction ratios. The green, orange, and blue lines represent the computational time of t-SNE, UMAP, and PCA, respectively. Some points in green line of Figure 5
(d) are not available, which due to the extremely long running time. PCA performed best in most cases,
except for the Coil 20 dataset, where UMAP had comparable computational time. This behavior is expected
because PCA is a linear transformation, and its time should scale linearly with the number of components
in the lower dimensional space. UMAP and t-SNE were slower than PCA, but it is evident from MNIST
and Jaccard distanced-based MNIST datasets that UMAP scales better with the increase in the number of
samples. Note that for Jaccard distanced-based MNIST, a higher dimension was not computed because the
computational time was too long. For Facebook Network, UMAP is outperforming t-SNE; however, for
higher dimensions, t-SNE computed faster. Nonetheless, from our baseline test Table 3, t-SNE does not
perform well, indicating instability. Faster computation time may indicate too fast of a convergence, which
leads to poor embedding.

4
4.1

SARS-CoV-2 mutation clustering
World SARS-CoV-2 mutation clustering

We gather data submitted to GISAID up to October 30, 2020, and the total number of samples is 89627. We
first get the SNP information by applying the multiple sequence alignment, which leads to 23763 unique
SNPs. Next, we calculate the pairwise Jaccard distance of our dataset in order to generate the Jaccard
distance-based features. Here, the number of rows is the number of samples (89627), and the number of
columns is the feature size (89627). As we mentioned in Section 2.3, the Jaccard distance-based feature is

14

30000

PCA time
UMAP time
t-SNE time

15000
Time

Time

20000
10000
0

PCA time
UMAP time
t-SNE time

20000

10000
5000

0.0

0.1

0.2
0.3
Reduction ratio

0.4

0

0.5

0.0

(a) Coil 20 time

12500

0.5

0.08

0.10

PCA time
UMAP time
t-SNE time

20000
15000
10000

5000

5000

2500
0

0.4

Time

Time

7500

0.2
0.3
Reduction ratio

(b) Facebook Network time

PCA time
UMAP time
t-SNE time

10000

0.1

0.0

0.1

0.2
0.3
Reduction ratio

0.4

0

0.5

(c) MNIST time

0.00

0.02

0.04
0.06
Reduction ratio

(d) Jaccard distanced-based MNIST time

Figure 5: Computational time of each reduction ratio. The green, orange and blue lines represent the computational time of t-SNE,
UMAP, and PCA, respectively. Not surprisingly, PCA performs the best in the majority of cases, except for the Coil 20 dataset. UMAP
and t-SNE perform worse than PCA, but UMAP scales better when there are more samples, as evident from MNIST and Jaccard
distanced-based MNIST datasets. Note that for Jaccard distanced-based MNIST, the higher dimension was not computed because the
computational time was too long.

a square matrix. However, due to the large size of samples and features, applying K-means clustering directly on the feature of the size of 89627Ã—89627 is a very time-consuming process. Considering that UMAP
outperforms the other two dimensional reduction algorithms (PCA and t-SNE) on the Jaccard distancebased MNIST dataset, we employ UMAP to reduce our original feature with the size of 89627Ã—89627 to
89627Ã—200. To be noted, UMAP is a reliable and stable algorithm, which performs consistently in clustering at various reduction ratios. Therefore, there is no need to use the same reduction dimension of 200 and
one can also choose a different reduction dimension value to generate similar results.
With the reduced dimension feature that has the size of 89627Ã—200, we split our SARS-CoV-2 dataset
into different clusters by applying the K-means clustering methods. After comparing the WCSS under a
different number of clusters, we find that there are 6 clusters forming within the SARS-CoV-2 population
based on the elbow method. Table 11 shows the top 25 single mutations of each cluster. In order to understand the relationship, we also analyzed the commutation occurring in each cluster (Table 9). From Table 11
and Table 9 we see the following:
Table 9: The frequency and occurrence percentage of SARS-CoV-2 co-mutations from each clusters in the world.
Cluster Co-mutations
Frequency Occurrence percentage
Cluster 1 [241, 1163, 3037, 7540, 14408, 16647, 18555, 22992, 23401, 23403, 28881, 28882, 28883]
776
0.463
Cluster 2 [241, 3037, 14408, 23403]
8640
0.925
Cluster 3 [241, 1059, 3037, 14408, 23403, 25563]
8878
0.662
Cluster 4 [241, 3037, 14408, 23403, 28881, 28882, 28883]
14913
0.829
Cluster 5 [241, 3037, 14408, 23403]
17412
0.969
Cluster 6 [241, 1163, 3037, 7540, 14408, 16647, 18555, 22992, 23401, 23403, 28881, 28882, 28883]
1352
0.771

â€¢ Though Clusters 1 and 6 seem similar from the top 25 single mutations, the co-mutations tells a different story. The same co-mutations have a higher frequency in Cluster 6, indicating that the co-mutation
has higher number of descendants.

15

â€¢ Clusters 2 and 5 have high frequency of [241, 3037, 14408, 23403] mutations, but Cluster 5 has a clear
co-mutation descendent with high frequency.
â€¢ Cluster 3 has a unique combination of mutation that is only popular in Cluster 3.
Table 12 shows the cluster distributions of samples from 25 countries. Here, we use the ISO 3166-1
alpha-2 codes as the country code. The listed countries are the United Kingdom (UK), the United States
(US), Australia (AU), India (IN), Switzerland (CH), Netherlands (NL), Canada (CA), France (FR), Belgium
(BE), Singapore (SG), Spain (ES), Russia (RU), Portugal (PT), Denmark (DK), Sweden (SE), Austria (AT),
Japan (JP), South Africa (ZA), Iceland (IS), Brazil (BR), Saudi Arabia (SA), Norway (NO), China (CN), Italy
(IT), and Korea (KR). From Table 12, we can see the following:
â€¢ SNP profiles from UK are dominated in Clusters 5 and 4.
â€¢ Clusters 1 and 6â€™s SNP profiles are predominantly found in AU.
â€¢ SNP profiles from US are found mostly in Clusters 3 and 5.
â€¢ Most countryâ€™s SNP profiles are found in Clusters 2 - 5, with some having slightly higher numbers,
but not as significant as the UK, US and AU.

Figure 6: Cluster distribution of the global SARS-CoV-2 mutation dataset. Using Highchart, the world map was colored, according to
the dominant cluster. Clusters 1, 2, 3, 4, 5, and 6 were colored with light blue, blue, green, red, purple and yellow, respectively. For
example, United States have SNP profiles from all clusters, but Cluster 5 (purple) is the dominant type in the US. Only countries with
more than 25 sequenced data available on GISAID were considered. Countries with fewer than 25 samples are labeled grayed.

Notably, in Table 9, Cluster 2 and Cluster 5 have the same co-mutations with a relatively large frequency,
while Cluster 1 and Cluster 6 share the same co-mutations with a relatively low frequency, which indicate
that Cluster 2 and Cluster 5 share the same â€œrootâ€ with a large size, while Cluster 1 and Cluster 6 share
the same â€œrootâ€ with a smaller size in the 200-dimensional (200D) space. However, we cannot visualize
the distribution of our reduced dataset in the 200D space. Therefore, benefit from the stable and reliable
performance of UMAP at various reduction ratios, we reduce the dimension of our original dataset to
2, which enables us to observe the distribution of the dataset in the two-dimensional (2D) space. Figure 7
visualizes the distribution of our dataset with 6 distinct clusters with 2D UMAP. It can be seen that 2 clusters
(i.e., Cluster 2â€™ and Cluster 3â€™) share a small â€œrootâ€ located in the middle of the figure, and Cluster 4â€™ and
Cluster 5â€™ share another large â€œrootâ€ that also located in the middle of the figure.

16

Cluster 60

Cluster 50

Cluster 40

Cluster 30

Cluster 20

Cluster 10

Figure 7: 2D UMAP visualization of the world SARS-CoV-2 mutation dataset with 6 distinct clusters. Red, orange, yellow, light blue,
blue, and dark blue represent for Clusters 1â€™, 2â€™, 3â€™, 4â€™, 5â€™, and 6â€™, respectively.

4.2

United States SARS-CoV-2 mutation clustering

In addition to analyzing the clustering in the world, SNP profiles of SARS-CoV-2 from the United States
(US) were considered. In this section, the US dataset has 10279 unique single mutations and 22390 samples.
Therefore, the dimension of the Jaccard distance-based dataset is 22390Ã—22390. After applying the UMAP,
we reduce the dimension of the original dataset to be 22390Ã—200. Following the similar K-means clustering
processes as we did for the world dataset, we find that there are 6 predominant clusters forming in the
United States. Figure 8 show the US map with the cluster statistic. Here, Highchart was used to generate
the plot with the pie chart. Each states were colored based on the dominant cluster.
Table 13 shows the top 25 mutations from each clusters in the United States. The states with more than 50
samples are listed. Table 10 shows the common occurring co-mutations, and we can observe the following:
â€¢ Cluster F have high frequency of co-mutations [241, 3037, 14408, 23403, 28881, 28882, 28883], which is
a descendent of common co-mutations of Cluster 4 [241, 3037, 14408, 23403, 28881, 28882, 28883] from
Table 13.
â€¢ Clusters A, B, C, and D have frequent co-mutations [241, 1059, 3037, 14408, 23403, 25563], which are
also frequent co-mutations of Cluster 3.
Table 10: The frequency and occurrence percentage of SARS-CoV-2 co-mutations from each clusters in US clusters.

Cluster
Cluster A
Cluster B
Cluster C
Cluster D
Cluster E
Cluster F

Co-mutations
[241, 1059, 3037, 14408, 23403, 25563]
[241, 1059, 3037, 14408, 23403, 25563]
[241, 1059, 3037, 14408, 23403, 25563]
[241, 1059, 3037, 14408, 23403, 25563, 27964]
[8782, 17747, 17858, 18060, 28144]
[241, 3037, 14408, 23403, 28881, 28882, 28883]

Frequency
3116
5763
8878
1225
1109
2575

Occurrence percentage
0.465
0.605
0.662
0.864
0.743
0.932

Notably, in Table 10, Clusters A, B, and C have the same high-frequency co-mutations, indicating that
these three clusters may share the same â€œrootâ€ in the 200D space. However, it is impossible to show the
distribution of each cluster in the 200D space. Considering the stability and reliability of UMAP at various
reduction ratios, we employ UMAP to the original US dataset with reduced dimension 2, aiming to observe
the distribution of the dataset in the 2D space. Figure 9 illustrates the 2D visualization of the US dataset
17

Figure 8: Cluster distribution of United States SARS-CoV-2 mutation dataset. Using Highchart, the US map was colored, according to
the dominant cluster. Clusters A, B, C, D, E, and F were colored with light blue, blue, green, red, purple, and yellow, respectively. For
example, United States have SNP profiles from all clusters, but Cluster E (purple) is the dominant type in the US. Only those countries
that have more than 25 sequenced data available on GISAID were considered in the plot.

with 6 distinct clusters. We can see that there are 3 clusters (Clusters Aâ€™, Bâ€™, and Fâ€™) share the same â€œrootâ€
located in the middle of the figure, while the other 3 clusters (Clusters Câ€™, Dâ€™, and Eâ€™) are not. This confirms
our deduction about why Clusters A, B, and C have the same high-frequency co-mutations in Table 10.

5

Discussion

In this section, we compared our past results [31] with our new method to gain a different perspective in
clustering with the SNP profiles of COVID-19. In our previous work, a total of 8309 unique single mutations
are detected in 15140 SARS-CoV-2 isolates. Here, we also calculate the pairwise distance among 15140 SNP
profiles and set the number of clusters to be six. Table 15 shows the cluster distribution of samples from
the 15 countries [31]. The listed countries are the United States (US), Canada (CA), Australia (AU), United
Kingdom (UK), Germany (DE), France (FR), Italy (IT), Russia (RU), China (CN), Japan (JP), Korean (KR),
India (IN), Spain (ES), Saudi Arabia (SA), and Turkey (TR), and we use Cluster I, II, III, IV, V, and VI to
represent six clusters without applying any dimensional reduction algorithm. Table 16 lists the cluster
distribution of samples from the same 15 countries, where we use Ip , IIp , IIIp , IVp , Vp , and VIp to represent
six clusters performed by PCA with the reduction ratio to be 1/160. Table 17 lists the cluster distribution
of samples from the same 15 countries, where we use Iu , IIu , IIIu , IVu , Vu , and VIu to represent six clusters
performed by UMAP with the reduction ratio setting to be 1/160. Noticeably, the SNP profile is focused in
Cluster Iu , whereas in the non-reduced version, the samples are more spread out. This may be caused by
the large number of features, making computed distance between the centroid and each data too similar,
and leading to samples being placed in incorrect clusters.
18

Cluster F0

Cluster E0

Cluster D0

Cluster C0

Cluster B0

Cluster A0

Figure 9: The 2D UMAP visualization of the US SARS-CoV-2 mutation dataset with 6 distinct clusters. Red, orange, yellow, light blue,
blue, and dark blue represent for Cluster Aâ€™, Bâ€™, Câ€™, Dâ€™, Eâ€™, and Fâ€™, respectively.

Not surprisingly, PCA and the original method for [31] has nearly identical result. It has been shown
in [31] that PCA is the continuous solution of the cluster indicators in the K-means clustering method. On
the other hand, UMAP shows a slightly different result. In the PCA method, the distribution is more spread
out. In addition, the top occurrence for each country is higher for UMAP. On the other hand, we see that
there are more samples in Cluster Iu for UMAP, which may indicate that mutations in Cluster Iu are the
main strand.
Moreover, Figure 10 illustrates the 2D visualizations of the US dataset up to June 01, 2020, with 6 distinct
clusters by applying two different dimensional reduction algorithms. We can see that the data distribute
disorderly under both PCA- and UMAP-assisted K-means clustering algorithms. Specifically, the PCAassisted algorithm has a really poor clustering performance, while the UMAP-assisted algorithm forms
more clear and better clusters than the PCA-assisted algorithm, which is consistent with our previous analysis in Section 3.1.

Figure 10: 2D visualizations of the US SARS-CoV-2 mutation dataset up to June 01, 2020 with 6 distinct clusters by applying two
different dimensional reduction algorithms. (a) 2D PCA visualization. Red, orange, yellow, light blue, blue, and dark blue represent
for Cluster I0p , II0p , III0p , IV0p , V0p , and VI0p , respectively. (b) 2D UMAP visualization. Red, orange, yellow, light blue, blue, and dark blue
represent for Cluster I0u , II0u , III0u , IV0u , V0u , and VI0u , respectively.

19

6

Conclusion

The rapid global spread of coronavirus disease 2019 (COVID-19) caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) has led to genetic mutation stimulated by genetic evolution and adaptation. Up to October 30, 89627 complete SARS-CoV-2 sequences, and a total of 23763 unique SNPs have
been detected. Our previous work traced the COVID-19 transmission pathways and analyzed the distribution of the subtypes of SARS-CoV-2 across the world based on 15,140 complete SARS-CoV-2 sequences.
The K-means clustering separated the sequences into six distinguished clusters. However, considering the
tremendous increase in the number of available SARS-CoV-2 sequences, an efficient and reliable dimensional reduction method is urgently required. Therefore, the objective of the present work is to explore the
best suited dimension reduction algorithm based on their performance and effectiveness. Here, a linear algorithm PCA and two non-linear algorithms, t-distributed stochastic neighbor embedding (t-SNE) and uniform manifold approximation and projection (UMAP), have been discussed. To evaluate the performance
of dimension reduction techniques in clustering, which is an unsupervised problem, we first cast classification problems into clustering problems with labels. Next, by setting different reduction ratios, we test
the effectiveness and accuracy of PCA, t-SNE, and UMAP for k-NN and K-means using four benchmark
datasets. The results show that overall, UMAP outperforms other two algorithms. The major strengths
of UMAP is that UMAP-assisted k-NN classification and UMAP-assisted K-means clustering at various
dimension reduction ratios have a consistent performance in terms of accuracy, which proves that UMAP
is a stable and reliable dimension reduction algorithm. Moreover, compared to the K-means clustering accuracy that does not involve any dimensional reduction, UMAP-assisted K-means clustering can improve
the accuracy for most cases. Furthermore, when the dimension is reduced to two, the UMAP clustering visualization is clear and elegant. Additionally, UMAP is a relatively efficient algorithm compared to t-SNE.
Although PCA is a faster algorithm, its major limitation is its poor performance in accuracy. To be noted,
UMAP performs better than PCA and t-SNE for the dataset with a large number of samples, indicating it
is the best suited dimensional reduction algorithm for our SARS-CoV-2 mutation dataset. Moreover, we
apply the UMAP-assisted K-means clustering to the world SARS-CoV-2 mutation dataset (up to October
30, 2020), which displays six distinct clusters. Correspondingly, the same approaches are also applied to
the United States SARS-CoV-2 mutation dataset (up to October 30, 2020), resulting in six different clusters
as well. Furthermore, we provide a new perspective by utilizing UMAP-assisted K-means clustering to
analyze our previous SARS-CoV-2 mutation datasets, and the 2D visualization of UMAP-assisted K-means
clustering of our previous world SARS-CoV-2 mutation dataset (up to June 01, 2020) forms more clear
clusters than the PCA-assisted K-means clustering. Finally, one of our four datasets was generated by the
Jaccard distance representation, which improves both kNN classification and k-means clustering accuracies
on the original dataset.

Acknowledgment
This work was supported in part by NIH grant GM126189, NSF Grants DMS-1721024, DMS-1761320, and
IIS1900473, Michigan Economic Development Corporation, Bristol-Myers Squibb, and Pfizer. The authors
thank The IBM TJ Watson Research Center, The COVID-19 High Performance Computing Consortium, and
NVIDIA for computational assistance.

20

References
[1] Weekly operational update on COVID-19, 30 october 2020, 2020.
[2] I. Alam, A. A. Kamau, M. Kulmanov, Å. Jaremko, S. T. Arold, A. Pain, T. Gojobori, and C. M. Duarte.
Functional pangenome analysis shows key features of e protein are preserved in sars and SARS-CoV-2.
Frontiers in cellular and infection microbiology, 10:405, 2020.
[3] Y. Bai, D. Jiang, J. R. Lon, X. Chen, M. Hu, S. Lin, Z. Chen, X. Wang, Y. Meng, and H. Du. Comprehensive evolution and molecular characteristics of a large number of SARS-CoV-2 genomes reveal its
epidemic trends. International Journal of Infectious Diseases, 100:164â€“173, 2020.
[4] E. Becht, L. McInnes, J. Healy, C.-A. Dutertre, I. W. Kwok, L. G. Ng, F. Ginhoux, and E. W. Newell.
Dimensionality reduction for visualizing single-cell data using umap. Nature biotechnology, 37(1):38â€“
44, 2019.
[5] M. Belkin and P. Niyogi. Laplacian eigenmaps and spectral techniques for embedding and clustering.
Advances in neural information processing systems, 14:585â€“591, 2001.
[6] J. Chen, R. Wang, M. Wang, and G.-W. Wei. Mutations strengthened SARS-CoV-2 infectivity. arXiv
preprint arXiv:2005.14669, 2020.
[7] J. Chen, R. Zhao, Y. Tong, and G.-W. Wei. Evolutionary de Rham-Hodge method. arXiv preprint
arXiv:1912.12388, 2019.
[8] M. A. Cox and T. F. Cox. Multidimensional scaling. In Handbook of data visualization, pages 315â€“347.
Springer, 2008.
[9] P. Forster, L. Forster, C. Renfrew, and M. Forster. Phylogenetic network analysis of SARS-CoV-2
genomes. Proceedings of the National Academy of Sciences, 117(17):9241â€“9243, 2020.
[10] Y.-N. Gong, K.-C. Tsao, M.-J. Hsiao, C.-G. Huang, P.-N. Huang, P.-W. Huang, K.-M. Lee, Y.-C. Liu, S.L. Yang, R.-L. Kuo, et al. SARS-CoV-2 genomic surveillance in Taiwan revealed novel ORF8-deletion
mutant and clade possibly associated with infections in middle east. Emerging Microbes & Infections,
9(1):1457â€“1466, 2020.
[11] I. T. Jolliffe and J. Cadima. Principal component analysis: a review and recent developments.
Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences,
374(2065):20150202, 2016.
[12] S. M. Kasibhatla, M. Kinikar, S. Limaye, M. M. Kale, and U. Kulkarni-Kale. Understanding evolution of
SARS-CoV-2: A perspective from analysis of genetic diversity of rdrp gene. Journal of Medical Virology,
2020.
[13] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278â€“2324, 1998.
[14] M. Levandowsky and D. Winter. Distance between sets. Nature, 234(5323):34â€“35, 1971.
[15] X. Li, J. Zai, Q. Zhao, Q. Nie, Y. Li, B. T. Foley, and A. Chaillon. Evolutionary history, potential intermediate animal host, and cross-species analyses of SARS-CoV-2. Journal of medical virology, 92(6):602â€“611,
2020.
[16] G. C. Linderman, M. Rachh, J. G. Hoskins, S. Steinerberger, and Y. Kluger. Fast interpolation-based
t-SNE for improved visualization of single-cell rna-seq data. Nature methods, 16(3):243â€“245, 2019.
[17] L. v. d. Maaten and G. Hinton. Visualizing data using t-SNE. Journal of machine learning research,
9(Nov):2579â€“2605, 2008.

21

[18] L. McInnes, J. Healy, and J. Melville. Umap: Uniform manifold approximation and projection for
dimension reduction. arXiv preprint arXiv:1802.03426, 2018.
[19] S. A. Nene, S. K. Nayar, and H. Murase. Columbia object image library (COIL-20), 1996.
[20] C. S. G. of the International et al. The species severe acute respiratory syndrome-related coronavirus:
classifying 2019-ncov and naming it SARS-CoV-2. Nature Microbiology, 5(4):536, 2020.
[21] R. D. Page. Space, time, form: viewing the tree of life. Trends in Ecology & Evolution, 27(2):113â€“120,
2012.
[22] B. Rozemberczki, C. Allen, and R. Sarkar. Multi-scale attributed node embedding. 2019.
[23] J. W. Sammon. A nonlinear mapping for data structure analysis. IEEE Transactions on computers,
100(5):401â€“409, 1969.
[24] F. Sievers, A. Wilm, D. Dineen, T. J. Gibson, K. Karplus, W. Li, R. Lopez, H. McWilliam, M. Remmert,
J. SoÌˆding, et al. Fast, scalable generation of high-quality protein multiple sequence alignments using
Clustal Omega. Molecular systems biology, 7(1):539, 2011.
[25] D. I. Spivak.
Metric realization of fuzzy simplicial sets.
Self published notes, available online at https://www. semanticscholar. org/paper/METRIC-REALIZATION-OF-FUZZY-SIMPLICIAL-SETSSpivak/a73fb9d562a3850611d2615ac22c3a8687fa745e, 2012.
[26] J. Tang, J. Liu, M. Zhang, and Q. Mei. Visualizing large-scale and high-dimensional data. In Proceedings
of the 25th international conference on world wide web, pages 287â€“297, 2016.
[27] Y. Toyoshima, K. Nemoto, S. Matsumoto, Y. Nakamura, and K. Kiyotani. SARS-CoV-2 genomic variations associated with mortality rate of COVID-19. Journal of human genetics, pages 1â€“8, 2020.
[28] D. Ulyanov. Multicore-TSNE. https://github.com/DmitryUlyanov/Multicore-TSNE, 2016.
[29] L. van Dorp, M. Acman, D. Richard, L. P. Shaw, C. E. Ford, L. Ormond, C. J. Owen, J. Pang, C. C. Tan,
F. A. Boshier, et al. Emergence of genomic diversity and recurrent mutations in SARS-CoV-2. Infection,
Genetics and Evolution, page 104351, 2020.
[30] R. Wang, J. Chen, K. Gao, Y. Hozumi, C. Yin, and G.-W. Wei. Characterizing SARS-CoV-2 mutations in
the United States. arXiv preprint arXiv:2007.12692, 2020.
[31] R. Wang, J. Chen, Y. Hozumi, C. Yin, and G.-W. Wei. Decoding asymptomatic COVID-19 infection and
transmission. The journal of physical chemistry letters, 11:10007â€“10015, 2020.
[32] R. Wang, Y. Hozumi, C. Yin, and G.-W. Wei. Decoding SARS-CoV-2 transmission, evolution and ramification on COVID-19 diagnosis, vaccine, and medicine. arXiv preprint arXiv:2004.14114, 2020.
[33] G. Wei. Wavelets generated by using discrete singular convolution kernels. Journal of Physics A: Mathematical and General, 33(47):8577, 2000.
[34] M. Worobey, J. Pekar, B. B. Larsen, M. I. Nelson, V. Hill, J. B. Joy, A. Rambaut, M. A. Suchard, J. O.
Wertheim, and P. Lemey. The emergence of SARS-CoV-2 in Europe and North America. Science,
370(6516):564â€“570, 2020.
[35] F. Wu, S. Zhao, B. Yu, Y.-M. Chen, W. Wang, Z.-G. Song, Y. Hu, Z.-W. Tao, J.-H. Tian, Y.-Y. Pei, et al. A
new coronavirus associated with human respiratory disease in China. Nature, 579(7798):265â€“269, 2020.
[36] T. Zhou, K. C. Chan, Y. Pan, and Z. Wang. An approach for determining evolutionary distance in
network-based phylogenetic analysis. In International Symposium on Bioinformatics Research and Applications, pages 38â€“49. Springer, 2008.

22

7

Appendix
Table 11: Clusters distribution of the top 25 single mutations of SARS-CoV-2 in the world, collected up to October 30, 2020.

Top
Top 1
Top 2
Top 3
Top 4
Top 5
Top 6
Top 7
Top 8
Top 9
Top 10
Top 11
Top 12
Top 13
Top 14
Top 15
Top 16
Top 17
Top 18
Top 19
Top 20
Top 21
Top 22
Top 23
Top 24
Top 25

Position
23403
14408
3037
241
28881
28882
28883
25563
1059
22227
26801
21255
6286
29645
445
28932
1163
22992
11083
18555
16647
23401
7540
27944
204

Cluster 1
1676
1676
1676
1669
1676
1676
1676
0
0
1
6
1
0
0
0
0
1676
1675
2
1675
1676
1675
1676
0
6

Cluster 2
9301
9265
9264
9065
64
15
13
24
23
5
15
4
20
2
1
16
4
27
179
13
5
4
0
1
6

Cluster 3
13355
13392
13350
13176
28
16
4
13357
10919
25
14
8
5
9
0
10
0
11
415
37
9
3
0
5
11

23

Cluster 4
17711
17728
17731
17549
16509
16508
16513
1022
72
71
97
119
66
74
53
52
271
930
462
15
3
17
3
49
15

Cluster 5
36351
36306
36342
36128
14771
14735
14737
7702
4797
9211
9094
9101
9092
9069
9057
9031
4566
3292
6046
3216
3200
3192
3170
6308
5454

Cluster 6
1753
1753
1753
1380
1753
1753
1753
0
0
0
17
0
1
0
0
1
1753
1753
32
1752
1753
1753
1753
0
0

Table 12: Cluster distributions of SARS-CoV-2 sequences from top 25 countries with the highest number of sequences as of October
30, 2020. The top 25 countries are the United Kingdom (UK), the United States (US), Australia (AU), India (IN), Switzerland (CH),
Netherlands (NL), Canada (CA), France (FR), Belgium (BE), Singapore (SG), Spain (ES), Russia (RU), Portugal (PT), Denmark (DK),
Sweden (SE), Austria (AT), Japan (JP), South Africa (ZA), Iceland (IS), Brazil (BR), Saudi Arabia (SA), Norway (NO), China (CN), Italy
(IT), and Korea (KR).

Top
Top 1
Top 2
Top 3
Top 4
Top 5
Top 6
Top 7
Top 8
Top 9
Top 10
Top 11
Top 12
Top 13
Top 14
Top 15
Top 16
Top 17
Top 18
Top 19
Top 20
Top 21
Top 22
Top 23
Top 24
Top 25

Country
UK
US
AU
IN
CH
NL
CA
FR
BE
SG
ES
RU
PT
DK
SE
AT
JP
ZA
IS
BR
SA
NO
CN
IT
KR

Cluster 1
16
1
1652
0
0
0
1
5
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0

Cluster 2
2926
1798
247
415
580
397
193
220
285
35
406
62
114
96
84
142
25
72
137
30
5
42
18
120
10

Cluster 3
1112
8620
346
138
114
80
447
451
66
50
14
29
31
359
182
108
26
11
67
14
193
23
8
11
21

24

Cluster 4
9530
2209
286
683
411
283
166
176
269
115
81
539
334
32
219
180
236
299
54
154
81
61
43
129
26

Cluster 5
22959
9760
4328
708
827
942
572
223
274
675
365
101
166
89
79
129
242
134
162
194
98
224
258
66
260

Cluster 6
48
0
1700
0
0
0
2
2
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0

Table 13: Clusters distribution of the top 25 single mutations of SARS-CoV-2 in the United States, collected up to October 30, 2020.

Top
Top 1
Top 2
Top 3
Top 4
Top 5
Top 6
Top 7
Top 8
Top 9
Top 10
Top 11
Top 12
Top 13
Top 14
Top 15
Top 16
Top 17
Top 18
Top 19
Top 20
Top 21
Top 22
Top 23
Top 24
Top 25

Position
23403
14408
3037
241
25563
1059
27964
28881
28882
28883
28144
8782
18060
17858
17747
20268
10319
28854
19839
29870
36
29784
15933
11083
11916

Cluster A
6686
6688
6674
6562
4906
4235
4
16
9
0
8
1
14
3
1
1125
0
897
0
237
32
4
0
142
1

Cluster B
8110
8100
8097
8033
6405
5831
1880
408
401
399
728
728
299
296
308
389
1104
528
199
776
694
157
153
590
822

Cluster C
1418
1418
1417
1403
1417
1417
1402
2
1
0
0
0
0
0
0
0
381
17
0
36
22
0
0
32
9

25

Cluster D
486
486
486
470
485
485
0
2
1
1
0
0
0
0
0
0
0
0
1
9
117
0
0
12
0

Cluster E
3
3
2
2
0
1
1
1
0
0
1486
1482
1469
1460
1420
0
0
0
0
55
192
1
0
21
0

Cluster F
2757
2752
2756
2695
8
3
1
2733
2732
2738
2
1
8
2
3
0
0
0
1058
113
105
794
705
55
12

Table 14: Cluster statistics for states with more than 50 SARS-CoV-2 genome samples.

State
New Hampshire
New York
Kansas
Wisconsin
Missouri
North Dakota
Alaska
Arizona
South Dakota
Utah
District of Columbia
Hawaii
Kentucky
Maine
Delaware
Wyoming
Arkansas
Oregon
South Carolina
Texas
Tennessee
Washington
Minnesota
New Mexico
Mississippi
Vermont
Indiana
New Jersey
California

Cluster A
330
1461
425
328
944
310
55
442
260
266
232
79
91
59
44
12
24
41
10
35
45
18
69
40
53
22
6
27
21

Cluster B
1743
918
1461
1189
592
701
505
267
385
236
222
148
89
85
87
47
49
69
33
71
27
72
17
28
28
42
62
5
14

Cluster C
25
108
302
20
16
183
319
28
56
100
7
3
36
0
10
5
22
6
71
0
26
4
0
10
0
0
1
5
0

26

Cluster D
442
2
0
0
2
0
0
1
0
0
5
0
0
11
0
11
0
0
4
1
1
1
0
0
0
1
0
0
0

Cluster E
870
210
3
12
28
121
1
16
14
46
15
1
2
4
17
41
0
10
7
12
14
5
7
0
1
8
0
5
0

Cluster F
1082
451
332
192
73
41
7
104
140
30
53
16
11
17
10
41
46
12
0
2
7
6
4
11
1
3
0
9
15

Table 15: The world wide clusters from SARS-CoV-2 genome data available up to June 01, 2020. The listed countries are the United
States (US), Canada (CA), Australia (AU), United Kingdom (UK), Germany (DE), France (FR), Italy (IT), Russia (RU), China (CN),
Japan (JP), Korean (KR), India (IN), Spain (ES), Saudi Arabia (SA), and Turkey (TR). [31].

Country
US
CA
AU
UK
DE
FR
IT
RU
CN
JP
KR
IN
ES
SA
TR

Cluster I
844
12
163
539
10
41
26
10
8
0
0
93
27
14
25

Cluster II
311
29
149
875
20
85
24
27
3
3
0
69
100
31
3

Cluster III
488
17
410
908
21
14
9
1
215
68
28 0
141
74
9
24

Cluster IV
156
16
135
1532
38
12
17
109
1
20
0
10
25
1
9

Cluster V
1813
19
146
119
42
82
0
3
1
3
0
3
3
2
0

Cluster VI
975
41
77
3
0
0
0
0
25
0
0
0
2
0
0

Table 16: The world wide clusters from SARS-CoV-2 genome data available up to June 01, 2020 using PCA embedding with reduction
ratio of 1/160.

Country
US
CA
AU
UK
DE
FR
IT
RU
CN
JP
KR
IN
ES
SA
TR

Cluster Ip
915
14
164
543
10
46
26
10
8
0
0
95
27
30
27

Cluster IIp
489
17
414
908
21
14
9
1
213
68
28
141
74
9
24

Cluster IIIp
239
27
143
857
20
80
24
27
3
3
0
67
100
15
1

27

Cluster IVp
156
16
136
1546
38
12
17
109
1
20
0
10
25
1
9

Cluster Vp
1813
19
146
119
42
82
0
3
1
3
0
3
3
2
0

Cluster VIp
975
41
77
3
0
0
0
0
24
0
0
0
2
0
0

Table 17: The world wide clusters from SARS-CoV-2 genome data available up to June 01, 2020 using UMAP embedding with reduction ratio of 1/160

Country
US
CA
AU
UK
DE
FR
IT
RU
CN
JP
KR
IN
ES
SA
TR

Cluster Iu
2446
71
784
2171
57
163
13
92
178
36
18
232
205
56
56

Cluster IIu
1096
15
94
115
40
45
1
2
28
0
0
3
2
0
1

Cluster IIIu
90
9
64
828
14
10
35
49
6
11
0
7
12
1
4

28

Cluster IVu
751
35
18
2
0
0
0
0
10
0
1
0
0
0
0

Cluster Vu
110
1
83
534
5
11
5
0
22
47
9
2
7
0
0

Cluster VIu
94
3
37
326
15
5
22
7
6
0
0
72
5
0
0

