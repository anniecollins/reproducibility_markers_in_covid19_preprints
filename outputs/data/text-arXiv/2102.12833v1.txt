Diffusion Earth Moverâ€™s Distance and Distribution Embeddings

Alexander Tong * 1 Guillaume Huguet * 2 3 Amine Natik * 2 3 Kincaid MacDonald 4 Manik Kuchroo 5 1
Ronald Coifman 4 Guy Wolf â€  2 3 Smita Krishnaswamy â€  5 1

Abstract

arXiv:2102.12833v1 [cs.LG] 25 Feb 2021

We propose a new fast method of measuring distances between large numbers of related high
dimensional datasets called the Diffusion Earth
Moverâ€™s Distance (EMD). We model the datasets
as distributions supported on common data graph
that is derived from the affinity matrix computed
on the combined data. In such cases where the
graph is a discretization of an underlying Riemannian closed manifold, we prove that Diffusion
EMD is topologically equivalent to the standard
EMD with a geodesic ground distance. Diffusion
EMD can be computed in OÌƒ(n) time and is more
accurate than similarly fast algorithms such as
tree-based EMDs. We also show Diffusion EMD
is fully differentiable, making it amenable to future uses in gradient-descent frameworks such as
deep neural networks. Finally, we demonstrate
an application of Diffusion EMD to single cell
data collected from 210 COVID-19 patient samples at Yale New Haven Hospital. Here, Diffusion
EMD can derive distances between patients on
the manifold of cells at least two orders of magnitude faster than equally accurate methods. This
distance matrix between patients can be embedded into a higher level patient manifold which
uncovers structure and heterogeneity in patients.
More generally, Diffusion EMD is applicable to
all datasets that are massively collected in parallel
in many medical and biological systems.

1. Introduction
With the profusion of modern high dimensional, high
throughput data, the next challenge is the integration and
Equal contribution ; â€  Equal senior-author contribution. 1 Dept.
of Comp. Sci., Yale University, New Haven, CT, USA 2 Dept. of
Math. & Stat., UniversiteÌ de MontreÌal, MontreÌal, QC, Canada
3
Mila â€“ Quebec AI Institute, MontreÌal, QC, Canada 4 Dept. of
Math., Yale University, New Haven, CT, USA 5 Department of
Genetics, Yale University, New Haven, CT, USA. Correspondence
to: Smita Krishnaswamy <smita.krishnaswamy@yale.edu>.
*

Preprint posted to arXiv, Copyright 2021 by the author(s).

analysis of collections of related datasets. Examples of
this are particularly prevalent in single cell measurement
modalities where data (such as mass cytometry, or single
cell RNA sequencing data) can be collected in a multitude of
patients, or in thousands of perturbation conditions (Shifrut
et al., 2018). These situations motivate the organization
and embedding of datasets, similar to how we now organize
data points into low dimensional embeddings, e.g., with
PHATE (Moon et al., 2019), tSNE (van der Maaten & Hinton, 2008), or diffusion maps (Coifman & Lafon, 2006)).
The advantage of such organization is that we can use the
datasets as rich high dimensional features to characterize
and group the patients or perturbations themselves. In order
to extend embedding techniques to entire datasets, we have
to define a distance between datasets, which for our purposes are essentially high dimensional point clouds. For this
we propose a new form of Earth Moverâ€™s Distance (EMD),
which we call Diffusion EMD1 , where we model the datasets
as distributions supported on a common data affinity graph.
We provide two extremely fast methods for computing Diffusion EMD based on an approximate multiscale kernel
density estimation on a graph.
Optimal transport is uniquely suited to the formulation of
distances between entire datasets (each of which is a collection of data points) as it generalizes the notion of the shortest
path between two points to the shortest set of paths between
distributions. Recent works have applied optimal transport
in the single-cell domain to interpolate lineages (Schiebinger
et al., 2019; Yang & Uhler, 2019; Tong et al., 2020), interpolate patient states (Tong & Krishnaswamy, 2020), integrate
multiple domains (Demetci et al., 2020), or similar to this
work build a manifold of perturbations (Chen et al., 2020).
All of these approaches use the standard primal formulation
of the Wasserstein distance. Using either entropic regularization approximation and the Sinkhorn algorithm (Cuturi,
2013) to solve the discrete distribution case or a neural
network based approach in the continuous formulation (Arjovsky et al., 2017). We will instead use the dual formulation through the well-known Kantorovich-Rubinstein dual
to efficiently compute optimal transport between many distributions lying on a common low-dimensional manifold
1
Python implementation is available at https://github.
com/KrishnaswamyLab/DiffusionEMD.

Diffusion Earth Moverâ€™s Distance and Distribution Embedding

in a high-dimensional measurement space. This presents
both theoretical and computational challenges, which are
the focus of this work.
Specifically, we will first describe a new Diffusion EMD
that is an L1 distance between density estimates computed
using multiple scales of diffusion kernels over a graph. Using theory on the HoÌˆlder-Lipschitz dual norm on continuous
manifolds (Leeb & Coifman, 2016), we show that as the
number of samples increases, Diffusion EMD is equivalent
to the Wasserstein distance on the manifold. This formulation reduces the computational complexity of computing
K-nearest Wasserstein-neighbors between m distributions
over n points from O(m2 n3 ) for the exact computation to
OÌƒ(mn) with reasonable assumptions on the data. Finally,
we will show how this can be applied to embed large sets of
distributions that arise from a common graph, for instance
single cell datasets collected on large patient cohorts.
Our contributions include: 1. A new method for computing
EMD for distributions over graphs called Diffusion EMD.
2. Theoretical analysis of the relationship between Diffusion EMD and the snowflake of a standard EMD. 3. Fast
algorithms for approximating Diffusion EMD. 4. Demonstration of the differentiability of this framework. 5. Application of Diffusion EMD to embedding massive multi-sample
biomedical datasets.

2. Preliminaries
We now briefly review optimal transport definitions and
classic results from diffusion geometry.
Notation. We say that two elements A and B are equivalent if there exist c, C > 0 such that cA â‰¤ B â‰¤ CA, and
we denote A ' B. The definition of A and B will be clear
depending on the context.
Optimal Transport. Let Âµ, Î½ be two probability distributions on a measurable space â„¦ with metric d(Â·, Â·), Î (Âµ, Î½)
be the set of joint probability distributions Ï€ on the space
â„¦ Ã— â„¦, where for any subset Ï‰ âŠ‚ â„¦, Ï€(Ï‰ Ã— â„¦) = Âµ(Ï‰)
and Ï€(â„¦ Ã— Ï‰) = Î½(Ï‰). The 1-Wasserstein distance Wd also
known as the earth moverâ€™s distance (EMD) is defined as:
Z
Wd (Âµ, Î½) := inf
d(x, y)Ï€(dx, dy). (1)
Ï€âˆˆÎ (Âµ,Î½)

â„¦Ã—â„¦

When Âµ, Î½ are discrete distributions with n points, then
Eq. 1 is computable in O(n3 ) with a network-flow based
algorithm (PeyreÌ & Cuturi, 2019).
Let k Â· kLd denote the Lipschitz norm w.r.t. d, then the dual
of Eq. 1 is:
Z
Z
Wd (Âµ, Î½) = sup
f (x)Âµ(dx) âˆ’
f (y)Î½(dy). (2)
kf kLd â‰¤1

â„¦

â„¦

This formulation is known as the Kantorovich dual with
f as the witness function. Since it is in general difficult
to optimize over the entire space of 1-Lipschitz functions,
many works optimize the cost over a modified family of
functions such as functions parameterized by clipped neural networks (Arjovsky et al., 2017), functions defined
over trees (Le et al., 2019), or functions defined over Haar
wavelet bases (Gavish et al., 2010).
Data Diffusion Geometry Let (M, dM ) be a connected
Riemannian manifold, we denote by âˆ† the Laplace-Beltrami
operator on M. For all x, y âˆˆ M let ht (x, y) be the heat
kernel, which is the minimal solution of the heat equation:


âˆ‚
âˆ’ âˆ†x ht = 0,
(3)
âˆ‚t
with initial condition limtâ†’0 ht (x, y) = Î´y (x), where x 7â†’
Î´y (x) is the Dirac function centered at y, and âˆ†x is taken
with respect to the x argument of ht . Note that as shown
in Grigorâ€™yan et al. (2014), the heat kernel captures the
local intrinsic geometry of M in the sense that as t â†’ 0,
log ht (x, y) ' âˆ’d2M (x, y)/4t.
Here, in Sec. 4.2 (Theorem 1) we discuss another topological equivalent of the geodesic distance with a diffusion distance derived from the heat operator Ht := eâˆ’tâˆ† that characterizes the solutions of the heat equation
(Eq. 3), and is
R
related to the heat kernel via Ht f = ht (Â·, y)f (y)dy (see
Lafferty et al., 2005; Coifman & Lafon, 2006; Grigorâ€™yan
et al., 2014, for further details).
It is often useful (particularly in high dimensional data) to
consider data as sampled from a lower dimensional manifold
embedding in the ambient dimension. This manifold can
be characterized by its local structure and in particular, how
heat propagates along it. Coifman & Lafon (2006) showed
how to build such a propagation structure over discrete data
by first building a graph with affinities
2

(K )ij := eâˆ’kxi âˆ’xj k2 /

(4)

then considering the density P
normalized operator M :=
Qâˆ’1 K Qâˆ’1 , where Qii := j (K )ij . Lastly, a Markov
diffusion operator is defined by
X
P := D âˆ’1 M , where Dii :=
(M )ij .
(5)
j

Both D and Q are diagonal matrices. By the law of large
numbers, the operator P admits a natural continuous equivalent PÌƒ , i.e., for n i.i.d. points, the sums modulo n converge
to the integrals. Moreover, in Coifman & Lafon (2006, Prop.
t/
3) it is shown that limâ†’0 PÌƒ = eâˆ’tâˆ† = Ht . In conclusion, the operator P converges to PÌƒ as the sample size
increases and PÌƒ provide an approximation of the Heat kernel on the manifold. Henceforth, we drop the subscript of

Diffusion Earth Moverâ€™s Distance and Distribution Embedding

P to lighten the notation, further we will use the notation
P for the operator as well as for the matrix (it will be clear
in the context).

3. EMD through the L1 metric between
multiscale density estimates
The method of efficiently approximating EMD that we will
consider here is the approximation of EMD through density
estimates at multiple scales. Previous work has considered
densities using multiscale histograms over images (Indyk &
Thaper, 2003), wavelets over images and trees (Shirdhonkar
& Jacobs, 2008; Gavish et al., 2010) and densities over hierarchical clusters (Le et al., 2019). Diffusion EMD also
uses a hierarchical set of bins at multiple scales but with
smooth bins determined by the heat kernel, which allows us
to show equivalence to EMD with a ground distance of the
manifold geodesic. These methods are part of a family that
we call multiscale earth moverâ€™s distances that first compute
a set of multiscale density estimates or histograms where
L1 differences between density estimates realizes an effective witness function and have (varying) equivalence to the
Wasserstein distance between the distributions. This class
of multiscale EMDs are particularly useful in computing
embeddings of distributions where the Wasserstein distance
is the ground distance, as they are amenable to fast nearest
neighbor queries. We explore this application further in
Sec. 4.5 and these methods in Sec. B.1 of the Appendix.

with V = âˆªm
j=1 Xj and edge weights determined by the
Gaussian kernel (see Eq. 4), where we identify edge existence with nonzero weights. Then, we associate each Xi
(t)
with a density measure Âµi : V â†’ [0, 1], over the entire
data graph. To compute such measures, we first create indicator vectors for the individual datasets on it, let 1Xi âˆˆ
{0, 1}n be a vector where for each v âˆˆ V, 1Xi (v) = 1 if
and only if v âˆˆ Xi . We then derive a kernel density estimate by applying the diffusion operator constructed via
Eq. 5 over the graph G to these indicator functions to get
scale-dependent estimators
(t)

Âµi :=

4.2. Diffusion Earth Moverâ€™s Distance Formulation
We define the Diffusion Earth Moverâ€™s Distance between
two datasets Xi , Xj âˆˆ X as

d
Let X = {X1 , X2 , . . . , Xm }, âˆªm
j=1 Xj âŠ† M âŠ‚ R
P, be
a collection of datasets with ni = |Xi | and n =
i ni .
Assume that the Xi â€™s are independently sampled from a
common underlying manifold (M, dM ) which is a Riemannian closed manifold (compact and without boundary)
immersed in a (high dimensional) ambient space Rd , with
geodesic distance dM . Further, assume that while the underlying manifold is common, each dataset is sampled from
a different distribution over it, as discussed below. Such
collections of datasets arise from several related samples of
data, for instance single cell data collected on a cohort of
patients with a similar condition.

Here, we consider the datasets in X as representing distributions over the common data manifold, which we represent in
the finite setting as a common data graph GX = (V, E, w)

K
X

kTÎ±,k (Xi ) âˆ’ TÎ±,k (Xj )k1

(7)

k=0

4. Diffusion Earth Moverâ€™s Distance

4.1. Data Graphs and Density Estimates on Graphs

(6)

where the scale t is the diffusion time, which can be considered as a meta-parameter (e.g., as used in Burkhardt et al.,
2020) but can also be leveraged in multiscale estimation of
distances between distributions as discussed here. Indeed,
as shown in Burkhardt et al. (2020), at an appropriately
tuned single scale, this density construction yields a discrete
version of kernel density estimation.

WÎ±,K (Xi , Xj ) :=

We now present the Diffusion EMD, a new Earth Moverâ€™s
distance based on multiscale diffusion kernels as depicted in
Fig. 1. We first show how to model multiple datasets as distributions on a common data graph and perform multiscale
density estimates on this graph.

1 t
P 1Xi ,
ni

where 0 < Î± < 1/2 is a meta-parameter used to balance
long- and short-range distances, which in practice is set
close to 1/2, K is the maximum scale considered here, and
(
(2k+1 )
(2k )
2âˆ’(Kâˆ’kâˆ’1)Î± (Âµi
âˆ’ Âµi ) k < K
TÎ±,k (Xi ) :=
(2K )
Âµi
k=K
(8)
Further, to set K, we note that if the Markov process governed by P converges (i.e., to its stationary steady state) in
polynomial time w.r.t. |V |, then one can ensure that beyond
K = O(log |V |), all density estimates would be essentially
indistinguishable as shown by the following lemma, whose
proof appears in the Appendix:
Lemma 1. There exists a K = O(log |V |) such that
(2K )

K

Âµi
' P 2 1Xi ' Ï†0 for every i = 1, . . . , n, where
Ï†0 is the trivial eigenvector of P associated with the eigenvalue Î»0 = 1.
We now provide a theoretical justification of the Diffusion
EMD defined via Eq. 7 by following the relation established
in Leeb & Coifman (2016) between heat kernels and the
EMD on manifolds. Leeb & Coifman (2016) define the

Diffusion Earth Moverâ€™s Distance and Distribution Embedding

Figure 1. Diffusion EMD first embeds datasets into a common data graph G, then takes multiscale diffusion KDEs for each of the datasets.
These multiscale KDEs are then used to compute the Diffusion Earth Moverâ€™s Distance between the datasets that can be used in turn to
create graphs and embeddings (PHATE (Moon et al., 2018) shown here) of the datasets.

following ground distance for EMD over M by leveraging
the geometric information gathered from the L1 distances
between kernels at different scales.
Definition 1. The diffusion ground distance between x, y âˆˆ
M is defined as
X
2âˆ’kÎ± kh2âˆ’k (x, Â·) âˆ’ h2âˆ’k (y, Â·)k1 ,
DÎ± (x, y) :=
kâ‰¥0

for Î± âˆˆ (0, 1/2), the scale parameter K â‰¥ 0 and ht (Â·, Â·)
the heat kernel on M.
Note that DÎ± (Â·, Â·) is similar to the diffusion distance defined in Coifman & Lafon (2006), which was based on L2
notions rather than L1 here. Further, the following result
from Leeb & Coifman (2016, see Sec. 3.3) shows that the
distance DÎ± (Â·, Â·) is closely related to the intrinsic geodesic
one dM (Â·, Â·).
Theorem 1. Let (M, dM ) be a closed manifold with
geodesic distance dM , and let Î± âˆˆ (0, 1/2). The metric
DÎ± (Â·, Â·) defined via Def. 1 is equivalent to dM (Â·, Â·)2Î± .

Theorem 2. The EMD between two distributions Âµ, Î½ on a
closed Riemannian manifold (M, dM ) w.r.t. the diffusion
ground distance DÎ± (Â·, Â·), defined via Def. 1, given by
Z
WDÎ± (Âµ, Î½) = inf
DÎ± (x, y)Ï€(dx, dy), (10)
Ï€âˆˆÎ (Âµ,Î½)

MÃ—M

cH . That is WD ' W
cH , where Ht is
is equivalent to W
t
Î±
t
the Heat operator on M.
Proof. In Proposition 15 of Leeb & Coifman (2016), it is
shown that M is separable w.r.t. DÎ± (Â·, Â·), hence we can use
the Kantorovich-Rubinstein theorem. We let Î›Î± , the space
of functions that are Lipschitz w.r.t. DÎ± (Â·, Â·) and kÂ·kÎ›âˆ—Î± , the
norm of its dual space Î›âˆ—Î± . The norm is defined by
Z
kT kÎ›âˆ—Î± := sup
f dT.
kf kÎ›Î± â‰¤1

M

In Theorem 4 of Leeb & Coifman (2016), it is shown that
cH are equivalent to the norm kÂ·k âˆ— .
both WDÎ± and W
t
Î›Î±
t/

The previous theorem justifies why in practice we let Î±
close to 1/2, because we want the snowflake distance
d2Î±
M (Â·, Â·) to approximate the geodesic distance of M. The
notion of equivalence established by this result is such that
DÎ± (x, Â·) ' d(x, Â·)2Î± . It is easy to verify that two equivalent
metrics induce the same topology. We note that while here
we only consider the Heat kernel, a similar result holds (see
Theorem 4 in the Appendix) for a more general family of
kernels, as long as they satisfy certain regularity conditions.
For a family of operators (At )tâˆˆR+ we define the following
metric on distributions; let Âµ and Î½ be two distributions:
cA (Âµ, Î½) = kA1 (Âµ âˆ’ Î½)k
W
(9)
t
1
X
+
2âˆ’kÎ± k(A2âˆ’(k+1) âˆ’ A2âˆ’k )(Âµ âˆ’ Î½)k1 .
kâ‰¥0

The following result shows that applying this metric for the
cH yields an equivalent
family of operators Ht to get W
t
of the EMD with respect to the diffusion ground distance
DÎ± (Â·, Â·).

We consider the family of operators (P )tâˆˆR+ , which
is related to the continuous equivalent of the stochastic
matrix defined in Eq. 5. In practice, we use this family
of operators to approximate the heat operator Ht . Indeed,
when we take a small value of , as discussed in section 2,
we have from Coifman & Lafon (2006) that this is a valid
approximation.
Corollary 2.1. Let P be the continuous equivalent of the
stochastic matrix in Eq. 5. For  small enough, we have:
c t/ ' WD .
W
Î±
P


(11)

Eq. 11 motivates our use of Eq. 7 to compute the Diffusion
EMD. The idea is to take only the first K terms in the
c t/ and then choosing  := 2âˆ’K would
infinite sum W
P
give us exactly Eq. 7. We remark that the summation order
of Eq. 7 is inverted compared to Eq. 11, but in both cases the
largest scale has the largest weight. Finally, we state one last
theorem that brings our distance closer to the Wasserstein
w.r.t. dM (Â·, Â·); we refer the reader to the Appendix for its
proof.

Diffusion Earth Moverâ€™s Distance and Distribution Embedding

Theorem 3. Let Î± âˆˆ (0, 1/2) and (M, dM ) be a closed
manifold with geodesic dM . The Wasserstein distance w.r.t.
the diffusion ground distance DÎ± (Â·, Â·) is equivalent to the
Wasserstein distance w.r.t the snowflake distance dM (Â·, Â·)2Î±
on M, that is WDÎ± ' Wd2Î±
.
M
Corollary 3.1. For each 1 â‰¤ i, j â‰¤ m, let Xi , Xj âˆˆ X
be two datasets with size ni and nj respectively, and let
Âµi and Âµj be the continuous distributions corresponding to
the ones of Xi and Xj , let K be the largest scale and put
N = min(K, ni , nj ). Then, for sufficiently big N â†’ âˆ
(implying sufficiently small  = 2âˆ’K â†’ 0):
WÎ±,K (Xi , Xj ) ' Wd2Î±
(Âµi , Âµj ),
M

(12)

for all Î± âˆˆ (0, 1/2).
In fact we can summarize our chain of thought as follows
(a)
c t/ (Âµi , Âµj )
WÎ±,K (Xi , Xj ) ' W
P


(c)

' WDÎ± (Âµi , Âµj )

(b)

cH (Âµi , Âµj )
'W
t

(d)

' Wd2Î±
(Âµi , Âµj ),
M

where the approximation (a) is due to the fact that the discrete distributions on Xi and Xj converge respectively to Âµi
and Âµj when min(ni , nj ) â†’ âˆ. Further, WÎ±,K (Xi , Xj )
c t/ (Âµi , Âµj ) as in Eq. 9
approximate the infinite series W
P
when K â†’ âˆ, note also that we take  = 2âˆ’K so that the
largest scale in Eq. 7 is exactly 2K . The approximation in
(b) comes from the approximation of the heat operator as in
Coifman & Lafon (2006), (c) comes from Theorem 2 and
(d) comes from Theorem 1.
4.3. Efficient Computation of Dyadic Scales of the
Diffusion Operator
The most computationally intensive step of Diffusion EMD
requires computing dyadic scales of the diffusion operator P times Âµ to estimate the density of Âµ(t) at multiple
scales. Computed naively, by first powering P then right
multiplying Âµ, may take up to 2K matrix multiplications
which is infeasible for even moderately sized graphs. We
assume two properties of P that makes this computation
efficient in practice. First, that P is sparse with order OÌƒ(n)
non-zero entries. This applies when thresholding K or
when using a K-nearest neighbors graph to approximate
the manifold (van der Maaten & Hinton, 2008; Moon et al.,
2019). Second, that P t is low rank for large powers of t.
While there are many ways to approximate dyadic scales of
P , we choose from two methods depending on the number
of distributions m compared to the number of points in the
graph n. When m  n, we use a method based on Chebyshev approximation of polynomials of the eigenspectrum
of P . This method is efficient for sparse P and a small
number of distributions (Shuman et al., 2011). For more

detail on the error incurred by using Chebyshev polynomials we refer the reader to Trefethen (2013, Chap. 3). In
practice, this requires for the approximating polynomial of
J terms, computation of mJ (sparse) matrix vector multiplications for a worst case time complexity of O(Jmn3 ),
but in practice is OÌƒ(Jmn) where J is a small constant (see
Fig. 5(e)). However, while asymptotically efficient, when
m  n in practice this can be inefficient as it requires many
multiplications of the form P Âµ.
In the case where m  n, approximating powers of P and
applying these to the n Ã— m collection of distributions Âµ
once is faster. This method is also useful when the full set
of distributions is not known and can be applied to new
distributions one at a time in a data streaming model. A
K
naive approach for computing P 2 would require K dense
n Ã— n matrix multiplications. However, as noted in Coifman
& Maggioni (2006), for higher powers of P , we can use a
much smaller basis. We use an algorithm based on interpolative decomposition (Liberty et al., 2007; Bermanis et al.,
2013) to reduce the size of the basis, and subsequently the
computation time, at higher scales. In this algorithm we first
k
determine the approximate rank of P 2 using an estimate
of the density of the eigenspectrum of P as in Dong et al.
(2019). We then alternate steps of downsampling the basis
k
to the specified rank of P 2 with (randomized) interpolative
decomposition with steps of powering P on these bases.
Informally, the interpolative decomposition selects a representative set of points that approximate the basis well. In the
worst case this algorithm can take OÌƒ(mn3 ) time to compute
the diffusion density estimates, nevertheless with sparse P
with a rapidly decaying spectrum, this algorithm is OÌƒ(mn)
in practice. For more details see Sec. C of the Appendix.
4.4. Subsampling Density Estimates
The density estimates created for each distribution are both
large and redundant with each distribution represented by a
vector of K Ã— n densities. However, as noted in the previous
k
section, P 2 can be represented on a smaller basis, especially for larger scales. Intuitively, the long time diffusions
of nodes that are close to each other are extremely similar.
Interpolative decomposition (Liberty et al., 2007; Bermanis
et al., 2013) allows us to pick a set of points to center our
diffusions kernels such that they approximately cover the
graph up to some threshold on the rank. In contrast, in other
multiscale EMD methods the bin centers or clusters are
determined randomly, making it difficult to select the number of centers necessary. Furthermore, the relative number
of centers at every scale is fixed, for example, Quadtree or
Haar wavelet based methods (Indyk & Thaper, 2003; Gavish
k
et al., 2010) use 2d centers at every scale, and a clustering
based method (Le et al., 2019) selects C k clusters at every
scale for some constant C. Conversely, in Diffusion EMD,

Diffusion Earth Moverâ€™s Distance and Distribution Embedding
k

by analyzing the behavior of P 2 , we intelligently select the
number of centers needed at each scale based on the approxk
imate rank of P 2 up to some tolerance at each scale. This
does away with the necessity of a fixed ratio of bins at every
scale, allowing adaptation depending on the structure of the
manifold and can drastically reduce the size representations
(see Fig. 5(c)). For the Chebyshev polynomials method, this
subsampling is done post computation of diffusion scales,
k
and for the method based on approximating P 2 directly
the subsampling happens during computation. To this point,
we have described a method to embed distributions on a
graph into a set of density estimates whose size depends on
the data, and the spectrum decay of P . We will now explore
how to use these estimates for exploring the Diffusion EMD
metric between distributions.
4.5. Diffusion EMD Based Embeddings of Samples
Our main motivation for a fast EMD computed on related
datasets is to examine the space of the samples or datasets
themselves, i.e., the higher level manifold of distributions.
In terms of the clinical data, on which we show this method,
this would be the relationship between patients themselves,
as determined by the EMD between their respective singlecell peripheral blood datasets. Essentially, we create a kernel
matrix KX and diffusion operator PX between datasets
where the samples are nodes on the associated graph. This
diffusion operator PX can be embedded using diffusion
maps (Coifman & Lafon, 2006) or visualized with a method
like PHATE (Moon et al., 2018) that collects the information
into two dimensions as shown in Sec. 5. We note this higher
level graph can be a sparse KNN graph, particularly given
that a diffusion operator on the graph can allow for global
connections to be reformed via t-step path probabilities.
Multiscale formulations of EMD as in Eq. 8 are especially
effective when searching for nearest neighbor distributions
under the Wasserstein metric (Indyk & Thaper, 2003; Backurs et al., 2020) as this distance forms a normed space, i.e.,
a space where the metric is induced by the L1 norm of the
distribution vectors and their differences. Data structures
such as kd-trees, ball trees, locality sensitive hashing, are
able to take advantage of such normed spaces for sub-linear
neighbor queries. This is in contrast to network-flow or
Sinkhorn type approximations that require a scan through
all datapoints for each nearest neighbor query as this metric
is not derived from a norm.
4.6. Gradients of the Earth Moverâ€™s Distance
One of the hindrances in the use of optimal transport-based
distances has been the fact that it cannot be easily incorporated into deep learning frameworks. Gradients with respect
to the EMD are usually found using a trained Lipschitz
discriminator network as in Wasserstein-GANs (Arjovsky

et al., 2017), which requires unstable adversarial training, or
by taking derivatives through a small number of iterations of
the Sinkhorn algorithm (Frogner et al., 2015; Bonneel et al.,
2016; Genevay et al., 2018; Liu et al., 2020), which scales
with O(n2 ) in the number of points. Tree based methods
that are linear in the number of points do not admit useful gradients due to their hard binning over space, giving a
gradient of zero norm almost everywhere.
We note that, given the data diffusion operator, the computation of Diffusion EMD is differentiable and, unlike
Tree-based EMD, has smooth bins and therefore a non-zero
gradient norm near the data. Further, computation of the
gradient only requires powers of the diffusion operator multiplied by the indicator vector describing the distribution on
the graph. In fact, as mentioned in the supplementary material (Sect. C.1) for each v âˆˆ V , the gradient of the Diffusion
EMD âˆ‚WÎ±,K (Xi , Xj )/âˆ‚v depends mainly on the gradients
k
âˆ‚P2 /âˆ‚v for 0 â‰¤ K which can be expressed in terms of the
gradient of the Gaussian kernel âˆ‚K /âˆ‚v. This last quantity
is easy to compute. In Sect. C.1 of the Appendix, we give
an exact process on computing the gradient of the diffusion
EMD.

Figure 2. Wasserstein distance of indicator distributions 1x , x âˆˆ
[0, 1] from 10.5 computed using linear EMD methods L2 distance:
(a) ClusterTree (b) QuadTree and (c) Diffusion EMD.

5. Results
In this section, we first evaluate the Diffusion EMD on two
manifolds where the ground truth EMD with a geodesic
ground distance is known, a swiss roll dataset and spherical
MNIST (Cohen et al., 2017). On these datasets where we
have access to the ground truth geodesic distance we show
that Diffusion EMD is both faster and closer to the ground
truth than comparable methods. Then, we show an application to a large single cell dataset of COVID-19 patients
where the underlying metric between cells is thought to be
a manifold (Moon et al., 2018; Kuchroo et al., 2020). We
show that the manifold of patients based on Diffusion EMD

Diffusion Earth Moverâ€™s Distance and Distribution Embedding

by capturing the graph structure, better captures the disease
state of the patients.
Experimental Setup. We consider four baseline methods for approximating EMD: QuadTree(D) (Backurs et al.,
2020) which partitions the dataspace in half in each dimension up to some specified depth D, ClusterTree(C, D) (Le
et al., 2019) which recursively clusters the data with C clusters up to depth D using the distance between clusters to
weight the tree edges, and the convolutional Sinkhorn distance (Solomon et al., 2015) with the same graph as used in
Diffusion EMD. QuadTree and ClusterTree are fast to compute. However, because they operate in the ambient space,
they do not represent the geodesic distances on the manifold
in an accurate way. The convolutional Sinkhorn method
represents the manifold well but is significantly slower even
when using a single iteration. For more details on related
work and the experimental setup see Sections B and D of
the Appendix respectively.

Figure 3. Swiss roll dataset embeddings of m = 1000 distributions
with n = 10, 000 total points rotated into 10D colored by ground
truth 2D sheet axes. Diffusion EMD recreates the manifold better
in similar time.

Figure 4. Accuracy of methods measured via P@10 (left) and
Spearman coefficient (right), against their (log scaled) computation
time in seconds on the swiss roll dataset. Variations of methods
are over Chebyshev approximation order for Diffusion EMD, # of
trees for tree methods, and number of iterations for conv. Sinkhorn.
Diffusion EMD is more accurate than tree methods and orders of
magnitude faster than conv. Sinkhorn even with a single iteration.

Swiss roll data. The first application we explore is to
a dataset where we have distributions on a manifold for
which the geodesic distance is easily computable. In this
way we can compare to the ground truth EMD between
distributions. We generate m = 100 Gaussians on the swiss

Figure 5. Ablation study of major parameters for Chebyshev polynomial approximation on the swiss roll dataset. Mean and std.
over 10 runs over (a) values of the maximum scale K, (b) the rank
threshold in interpolative decomposition, (c) the total number of
centers in the L1 representation which drops with decomposition,
(d-e) performance against the # of scales, and the order of the
polynomial, both are very stable after a certain point, and (f) time
vs. the Chebyshev order.

roll with 100 points each for a total of n = 10, 000 points
on the graph. We compare each method on two metrics,
the 10-nearest neighbor accuracy measured (P@10) where
a P@10 of 1 means that the 10-nearest neighbors are the
same as the ground truth. We compare the rankings of
nearest neighbors over the entire dataset using the SpearmanÏ correlation coefficient, which measures the similarity of
nearest neighbor rankings. This coefficient ranges between
-1 for inversely ranked lists and 1 for the same ranks. This
measures rankings over the entire dataset equally rather
than only considering the nearest neighbors. Visually, we
show embeddings of the swiss roll in Fig. 3, where the 2D
manifold between distributions is best captured by Diffusion
EMD given a similar amount of time.
In Fig. 4, we investigate the time vs. accuracy tradeoff of a
number of fast EMD methods on the swiss roll. We compare
against the ground truth EMD which is calculated with the
exact EMD on the â€œunrolledâ€ swiss roll in 2D. We find that
Diffusion EMD is more accurate than tree methods for a
given amount of time and is much faster than the convolutional Sinkhorn method and only slightly less accurate.
To generate multiple models for each dataset we vary the
number of trees for tree methods, the Chebyshev order for
Diffusion EMD, and the number of iterations for convolutional Sinkhorn. We search over and fix other parameters
using a grid search as detailed in Sec. D of the Appendix.
In Fig. 5, we vary parameters of the Chebyshev polynomial
algorithm of Diffusion EMD. Regarding performance, we
find Diffusion EMD is stable to the number of scales chosen after a certain minimum maximum scale K, Chebyshev
polynomial order, and the number of scales used. By performing interpolative decomposition with a specified rank
k
threshold on P 2 we can substantially reduce the embedding size at a small cost to performance Fig. 5(b,c).

Diffusion Earth Moverâ€™s Distance and Distribution Embedding
Table 1. Classification accuracy, P@10, Spearman Ï and runtime
(in minutes) on 70,000 distributions from Spherical MNIST.

D IFF . EMD
C LUSTER
Q UAD

ACCURACY

P@10

S PEARMAN Ï

T IME

95.94
91.91
79.56

0.611
0.393
0.294

0.673
0.484
0.335

34 M
30 M
16 M

Spherical MNIST. Next, we use the Spherical MNIST
dataset to demonstrate the efficacy of the interpolative decomposition based approximation to Diffusion EMD, as
here m  n. Each image is treated as a distribution (of
pixel intensities) over the sphere. To evaluate the fidelity
of each embedding, we evaluate the 1-NN classification on
the embedding vectors in addition to P@10 and Spearman
coefficient in Tab. 1. Diffusion EMD creates embeddings
that better approximate true EMD over the sphere than tree
methods in a similar amount of time, which in this case also
gives better classification accuracy.

Figure 6. Embedding of 210 patients through different manifold
constructions. Visualizing patient eventual mortality and cell types
predictive of disease outcome on each manifold. Laplacian smoothness reported on each signal for each manifold.

Single cell COVID-19 patient data. The COVID-19 pandemic has driven biologists to generate vast amounts of
cellular data on hospitalized patients suffering from severe
disease. A major question in clinicians minds is determining
a priori which patients may be at risk for worse outcomes,
including requiring increased ventilatory support and increased risk of mortality. Certain cell types found in the
blood, such as CD16+ Neutrophils, T cells and non-classical
monocytes, have been associated with and predictive of mortality outcome. Ideally, a manifold of patients would find
these cellular populations to occupy a region of high mortality for CD16+ Neutrophils and non-classical monocytes
and low mortality for T cells. In order to construct a manifold of patients suffering from COVID-19, we analyzed 210
blood samples from 168 patients infected with SARS-CoV2 measured on a myeloid-specific flow cytometry panel, an
expanded iteration of a previously published dataset (Lucas
et al., 2020). We embedded 22 million cells from these pa-

tients into a common combined cell-cell graph with 27,000
nodes as defined in Section 4.1. We then computed Diffusion EMD and other methods on these datasets. Diffusion
EMD is computed by using indicator vectors for each patient
converted to density estimates as in Eq. 6.
On an informative embedding of patients, similar patients,
with similar features (such as mortality) would localize
on the manifold and thus the important features should be
smooth over the manifold. Furthermore, cell types which
are correlated with outcome either positively or negatively
should also be smooth and either correlated or anticorrelated
with outcome. To quantify this, we compute the smoothness
with respect to the patient manifold by using a Laplacian
quadratic form with respect to the 10-NN graph between
patients. Convolutional Sinkhorn does not scale to this
data, so we compare a patient manifold created with Diffusion EMD to ones created with QuadTree and ClusterTree.
Diffusion EMD is able to use the manifold of cells where
QuadTree and ClusterTree are built in the ambient space. In
Fig. 6 we visualize relevant signals over the patients using
PHATE (Moon et al., 2019) overlayed with the quadratic
smoothness of the signal over the graph. While the mortality
signal appeared enriched in the right branches of both the
Diffusion EMD and QuadTree manifolds, it did not localize
as well on the ClusterTree manifold. Both CD16+ Neutrophils and non-classical monocytes appeared smoother
over the Diffusion EMD manifold than the comparison manifolds. Since both cell types are associated with mortality,
it was interesting to see them both were enriched in high
mortality region of the Diffusion EMD manifold but not
the others. Finally, T cells, which are negatively correlated
with mortality appeared smoothly enriched in the Diffusion
EMD manifold in a region with no mortality. In QuadTree
and ClusterTree constructions, T cells appeared enriched
throughout the manifold, no localizing smoothly to a region with low mortality. These experiments show that the
patient manifold constructed with Diffusion EMD is more
informative, smoothly localizing key signals, such as patient
outcome and predictive cell types. For a more details see
Sec. D of the Appendix.

6. Conclusion
In this work we have introduced Diffusion EMD, a multiscale distance that uses heat kernel diffusions to approximate
the earth moverâ€™s distance over a data manifold. We showed
how Diffusion EMD can efficiently embed many samples
on a graph into a manifold of samples in OÌƒ(mn) time more
accurately than similarly efficient methods. This is useful in
the biomedical domain as we show how to embed COVID19 patient samples into a higher level patient manifold that
more accurately represents the disease structure between
patients. Finally, we also show how to compute gradients

Diffusion Earth Moverâ€™s Distance and Distribution Embedding

with respect to Diffusion EMD, which opens the possibility
of gradients of the earth moverâ€™s distance that scale linearly
with the dataset size.

Acknowledgements
This research was partially funded by IVADO PhD Excellence Scholarship [A.N.]; IVADO Professor startup & operational funds, IVADO Fundamental Research Proj. grant
PRF-2019-3583139727 [G.W.]; Chan-Zuckerberg Initiative
grants 182702 & CZF2019-002440 [S.K.]; and NIH grants
R01GM135929 & R01GM130847 [G.W., S.K.]. The content provided here is solely the responsibility of the authors
and does not necessarily represent the official views of the
funding agencies.

References
Arjovsky, M., Chintala, S., and Bottou, L. Wasserstein
GAN. In ICML, 2017.
Backurs, A., Dong, Y., Indyk, P., Razenshteyn, I., and Wagner, T. Scalable Nearest Neighbor Search for Optimal
Transport. ICML, 2020.
Benamou, J.-D., Carlier, G., Cuturi, M., Nenna, L., and
PeyreÌ, G. Iterative Bregman Projections for Regularized
Transportation Problems. SIAM J. Sci. Comput., 2014.
Bermanis, A., Averbuch, A., and Coifman, R. R. Multiscale data sampling and function extension. Applied and
Computational Harmonic Analysis, 34(1):15â€“29, January
2013. ISSN 10635203. doi: 10.1016/j.acha.2012.03.002.
Bonneel, N., PeyreÌ, G., and Cuturi, M. Wasserstein barycentric coordinates: Histogram regression using optimal
transport. ACM Transactions on Graphics (Proceedings
of SIGGRAPH 2016), 35(4), 2016.
Burkhardt, D. B., Stanley, J. S., Tong, A., Perdigoto, A. L.,
Gigante, S. A., Herold, K. C., Wolf, G., Giraldez, A. J.,
van Dijk, D., and Krishnaswamy, S. Quantifying the
effect of experimental perturbations in single-cell RNAsequencing data using graph signal processing. Technical
report, BiorXiv, 2020.
Chen, W. S., Zivanovic, N., van Dijk, D., Wolf, G., Bodenmiller, B., and Krishnaswamy, S. Uncovering axes
of variation among single-cell cancer specimens. Nat
Methods, 17(3):302â€“310, March 2020. ISSN 1548-7091,
1548-7105. doi: 10.1038/s41592-019-0689-z.

Coifman, R. R. and Maggioni, M. Diffusion wavelets. Applied and Computational Harmonic Analysis, 21(1):53â€“
94, July 2006. ISSN 10635203. doi: 10.1016/j.acha.2006.
04.004.
Cuturi, M. Sinkhorn Distances: Lightspeed Computation of
Optimal Transport. In Advances in Neural Information
Processing Systems 26, pp. 2292â€“2300, 2013.
Demetci, P., Santorella, R., Sandstede, B., Noble, W. S., and
Singh, R. Gromov-Wasserstein optimal transport to align
single-cell multi-omics data. Preprint, Bioinformatics,
April 2020.
Dong, K., Benson, A. R., and Bindel, D. Network Density
of States. Proc. 25th ACM SIGKDD Int. Conf. Knowl.
Discov. Data Min., pp. 1152â€“1161, July 2019. doi: 10.
1145/3292500.3330891.
Frogner, C., Zhang, C., Mobahi, H., Araya-Polo, M., and
Poggio, T. Learning with a Wasserstein Loss. Adv. Neural
Inf. Process. Syst. 28, 2015.
Gavish, M., Nadler, B., and Coifman, R. R. Multiscale
Wavelets on Trees, Graphs and High Dimensional Data:
Theory and Applications to Semi Supervised Learning.
In Proceedings of the 27th International Conference on
Machine Learning, Haifa, Israel, 2010.
Genevay, A., PeyreÌ, G., and Cuturi, M. Learning Generative
Models with Sinkhorn Divergences. In AISTATS, 2018.
Grigorâ€™yan, A. and Liu, L. Heat kernel and Lipschitzâ€“Besov
spaces. Forum Math., 27(6), January 2015. ISSN 09337741, 1435-5337. doi: 10.1515/forum-2014-0034.
Grigorâ€™yan, A., Hu, J., and Lau, K.-S. Heat Kernels on Metric Measure Spaces. In Geometry and Analysis of Fractals, volume 88, pp. 147â€“207, Berlin, Heidelberg, 2014.
Springer Berlin Heidelberg. ISBN 978-3-662-43919-7
978-3-662-43920-3. doi: 10.1007/978-3-662-43920-3 6.
Indyk, P. and Thaper, N. Fast image retrieval via embeddings. In 3rd International Workshop on Statistical and
Computational Theories of Vision, 2003.

Cohen, T., Geiger, M., KoÌˆhler, J., and Welling, M. Convolutional Networks for Spherical Signals. In ICML, 2017.

Kuchroo, M., Huang, J., Wong, P., Grenier, J.-C., Shung,
D., Tong, A., Lucas, C., Klein, J., Burkhardt, D., Gigante,
S., Godavarthi, A., Israelow, B., Oh, J. E., Silva, J., Takahashi, T., Odio, C. D., Fournier, J., Cruz, D., Ko, A. I.,
Wilson, F. P., Hussin, J., Wolf, G., and Krishnaswamy, S.
Multiscale PHATE Exploration of SARS-CoV-2 Data Reveals Multimodal Signatures of Disease. BioRxiv, 2020.

Coifman, R. R. and Lafon, S. Diffusion maps. Applied
and Computational Harmonic Analysis, 21(1):5â€“30, July
2006. ISSN 10635203. doi: 10.1016/j.acha.2006.04.006.

Lafferty, J., Lebanon, G., and Jaakkola, T. Diffusion kernels
on statistical manifolds. Journal of Machine Learning
Research, 6(1), 2005.

Diffusion Earth Moverâ€™s Distance and Distribution Embedding

Le, T., Yamada, M., Fukumizu, K., and Cuturi, M. TreeSliced Variants of Wasserstein Distances. In Advances in
Neural Information Processing Systems 33, 2019.
Leeb, W. Topics in Metric Approximation. PhD thesis, Yale
University, 2015.
Leeb, W. The mixed Lipschitz space and its dual for tree
metrics. Applied and Computational Harmonic Analysis,
44(3):584â€“610, May 2018. ISSN 10635203. doi: 10.
1016/j.acha.2016.06.008.
Leeb, W. and Coifman, R. HoÌˆlderâ€“Lipschitz Norms and
Their Duals on Spaces with Semigroups, with Applications to Earth Moverâ€™s Distance. J Fourier Anal Appl, 22
(4):910â€“953, August 2016. ISSN 1069-5869, 1531-5851.
doi: 10.1007/s00041-015-9439-5.
Liberty, E., Woolfe, F., Martinsson, P.-G., Rokhlin, V., and
Tygert, M. Randomized algorithms for the low-rank
approximation of matrices. Proceedings of the National
Academy of Sciences, 104(51):20167â€“20172, December
2007. ISSN 0027-8424, 1091-6490. doi: 10.1073/pnas.
0709640104.
Liu, R., Zou, J., and Balsubramani, A. Learning Transport Cost From Subset Correspondence. In International
Conference on Learning Representations, pp. 15, 2020.

PeyreÌ, G. and Cuturi, M. Computational Optimal Transport.
arXiv:1803.00567, 2019.
Sato, R., Yamada, M., and Kashima, H. Fast Unbalanced
Optimal Transport on a Tree. Adv. Neural Inf. Process.
Syst. 34, 2020.
Schiebinger, G., Shu, J., Tabaka, M., Cleary, B., Subramanian, V., Solomon, A., Gould, J., Liu, S., Lin, S.,
Berube, P., Lee, L., Chen, J., Brumbaugh, J., Rigollet,
P., Hochedlinger, K., Jaenisch, R., Regev, A., and Lander, E. S. Optimal-Transport Analysis of Single-Cell
Gene Expression Identifies Developmental Trajectories
in Reprogramming. Cell, 176(4):928â€“943.e22, 2019.
Shifrut, E., Carnevale, J., Tobin, V., Roth, T. L., Woo, J. M.,
Bui, C. T., Li, P. J., Diolaiti, M. E., Ashworth, A., and
Marson, A. Genome-wide CRISPR screens in primary
human t cells reveal key regulators of immune function.
Cell, 175(7):1958â€“1971.e15, December 2018. doi: 10.
1016/j.cell.2018.10.024. URL https://doi.org/
10.1016/j.cell.2018.10.024.
Shirdhonkar, S. and Jacobs, D. W. Approximate earth
moverâ€™s distance in linear time. In 2008 IEEE Conference
on Computer Vision and Pattern Recognition, Anchorage,
AK, USA, 2008. IEEE. ISBN 978-1-4244-2242-5. doi:
10.1109/CVPR.2008.4587662.

Lucas, C., , Wong, P., Klein, J., Castro, T. B. R., Silva, J.,
Sundaram, M., Ellingson, M. K., Mao, T., Oh, J. E.,
Israelow, B., Takahashi, T., Tokuyama, M., Lu, P.,
Venkataraman, A., Park, A., Mohanty, S., Wang, H.,
Wyllie, A. L., Vogels, C. B. F., Earnest, R., Lapidus,
S., Ott, I. M., Moore, A. J., Muenker, M. C., Fournier,
J. B., Campbell, M., Odio, C. D., Casanovas-Massana,
A., Herbst, R., Shaw, A. C., Medzhitov, R., Schulz,
W. L., Grubaugh, N. D., Cruz, C. D., Farhadian, S.,
Ko, A. I., Omer, S. B., and Iwasaki, A. Longitudinal analyses reveal immunological misfiring in severe
COVID-19. Nature, 584(7821):463â€“469, July 2020.
doi: 10.1038/s41586-020-2588-y. URL https://
doi.org/10.1038/s41586-020-2588-y.

Shuman, D. I., Vandergheynst, P., and Frossard, P. Chebyshev polynomial approximation for distributed signal processing. In 2011 International Conference on Distributed
Computing in Sensor Systems and Workshops (DCOSS),
pp. 1â€“8, Barcelona, Spain, June 2011. IEEE. ISBN 9781-4577-0512-0. doi: 10.1109/DCOSS.2011.5982158.

Moon, K. R., Stanley, J. S., Burkhardt, D., van Dijk, D.,
Wolf, G., and Krishnaswamy, S. Manifold learning-based
methods for analyzing single-cell RNA-sequencing data.
Current Opinion in Systems Biology, 7:36â€“46, February
2018. ISSN 24523100. doi: 10.1016/j.coisb.2017.12.008.

Tong, A., Huang, J., Wolf, G., van Dijk, D., and Krishnaswamy, S. TrajectoryNet: A Dynamic Optimal Transport Network for Modeling Cellular Dynamics. In Proceedings of the 37th International Conference on Machine Learning, 2020.

Moon, K. R., van Dijk, D., Wang, Z., Gigante, S., Burkhardt,
D. B., Chen, W. S., Yim, K., van den Elzen, A., Hirn,
M. J., Coifman, R. R., Ivanova, N. B., Wolf, G., and
Krishnaswamy, S. Visualizing structure and transitions
in high-dimensional biological data. Nat Biotechnol, 37
(12):1482â€“1492, 2019.

Trefethen, L. N. Approximation Theory and Approximation
Practice. Society for Industrial and Applied Mathematics,
2013.

Solomon, J., de Goes, F., Peyre, G., Paris-Dauphine, U.,
and Cuturi, M. Convolutional Wasserstein Distances:
Efficient Optimal Transportation on Geometric Domains.
In ACM Transactions on Graphics, 2015.
Tong, A. and Krishnaswamy, S. Interpolating optimal transport barycenters of patient manifolds, July 2020.

van der Maaten, L. and Hinton, G. E. Visualizing Data using
t-SNE. J. Mach. Learn. Res., 2008.

Diffusion Earth Moverâ€™s Distance and Distribution Embedding

Yang, K. D. and Uhler, C. Scalable Unbalanced Optimal
Transport Using Generative Adversarial Networks. In 7th
International Conference on Learning Representations,
pp. 20, 2019.

Diffusion Earth Moverâ€™s Distance and Distribution Embedding

Supplemental Material
We first analyze the theoretical framework of Diffusion EMD in Appendix A. Next we discuss related worki n Appendix B,
with a particular focus on multiscale methods for EMD. In Appendix C we provide further detail on the two algorithms for
computing Diffusion EMD, the first based on Chebyshev approximation, and the second based on directly powering the
diffusion operator while reducing the basis. We discuss gradients of Diffusion EMD in section C.1. Finally we provide
further experimental details in Appendix D.

A. General framework and proofs
A.1. General framework
We now recall some useful results from Leeb & Coifman (2016). We will only present an overview, for a rigorous exposition,
we suggest Leeb (2015), Leeb & Coifman (2016) and Grigorâ€™yan & Liu (2015).
We let Z be a sigma-finite measure space of dimension n, d(Â·, Â·) its intrinsic distance and Î¾ its associated sigma-finite
measure. In order to evaluate the EMD between two measures on Z, one would need to know d(Â·, Â·). Either directly, by
using 1, or implicitly, to define the space of Lipschitz functions with respect to d(Â·, Â·) in equation 2. One of the contributions
of this paper is to define a kernel based metric DÎ± (Â·, Â·), where the kernel depends on Î± âˆˆ (0, 1), and show that the metrics
DÎ± (Â·, Â·) and d(Â·, Â·) are closely related. Next, the objective is to use DÎ± (Â·, Â·) as the ground distance to compute the EMD in
its dual form. That is a norm between two distributions acting on the space of Lipschitz functions with respect to DÎ± (Â·, Â·).
To do so, the authors define Î›Î± the space of functions that are Lipschitz with respect to DÎ± (Â·, Â·), and its dual Î›âˆ—Î± ; the space
of measures acting on f âˆˆ Î›Î± . Further, in order to use the Kantorovichâ€“Rubinstein theorem, they show that the space Z is
separable with respect to the metric DÎ± (Â·, Â·). Thus, the norm of the dual space kÂ·kÎ›âˆ—Î± can be used to compute the EMD
(1)

(2)

with DÎ± (Â·, Â·) as the ground distance. Lastly, they define two norms kÂ·kÎ›âˆ—Î± and kÂ·kÎ›âˆ—Î± that are equivalent to kÂ·kÎ›âˆ—Î± on Î›âˆ— . In
practice, these norms are much faster to compute.
The metric DRÎ± (Â·, Â·) is defined using a family of kernels {at (Â·, Â·)}tâˆˆR+ on Z. For each kernel, we define an operator At as
(At f )(x) = Z at (x, y)f (y)dÎ¾(y). These kernels must respect some properties:
â€¢ The semigroup property: for all s, t > 0, At As = At+s ;
R
â€¢ The conservation property: Z at (x, y)dÎ¾(y) = 1;
R
â€¢ The integrability property: there exists C > 0 such that Z |at (x, y)|dÎ¾(y) < C, for all t > 0 and x âˆˆ Z.
Considering only the dyadic times, that is t = 2âˆ’k , we define the kernel pk (Â·, Â·) := a2âˆ’k (Â·, Â·) and the operator Pk = A2âˆ’k ,
for all k âˆˆ N. By leveraging the local geometric information gathered from the L1 distance between two measures
Dk (x, y) := kpk (x, Â·) âˆ’ pk (y, Â·)k1 ,
the authors define the following multiscale metric
DÎ± (x, y) :=

X

2âˆ’kÎ± Dk (x, y).

kâ‰¥0

To interpret Dk (Â·, Â·) in an intuitive way, consider the case where at (Â·, Â·) defines a random walk on Z. As a consequence,
at (x, B(y, r)) is the probability to move from x to a point in B(y, r) in t steps. Moreover, for any x âˆˆ Z, at (x, Â·) defines a
distribution, therefore Dk (x, y) is the L1 distance between two distributions induced by the points x and y. Since these
distributions depend on the number of steps t, it is clever to consider a distance that includes many scales, just like DÎ± (Â·, Â·).
Another property needs to be verified by the kernel at (Â·, Â·), this property depends on the distance DÎ± (Â·, Â·). Namely, the
geometric property: there exist C > 0 and Î± âˆˆ (0, 1) such that for all k âˆˆ N and x âˆˆ Z
Z
|pk (x, y)|DÎ± (x, y)dÎ¾(y) â‰¤ C2âˆ’kÎ± .
Z

We need to add three stronger regularity conditions on the kernel at (Â·, Â·), for DÎ± (Â·, Â·) to be closely related to the intrinsic
distance d(Â·, Â·):

Diffusion Earth Moverâ€™s Distance and Distribution Embedding

1. An upper bound on the kernel: there exist a non-negative, monotonic decreasing function Ï† : R+ â†’ R and Î² > 0 such
that for any Î³ < Î², the function verifies
Z
âˆ

Ï„ n+Î³âˆ’1 Ï†(Ï„ )dÏ„

0

and
n

|at (x, y)| â‰¤ tâˆ’ Î² Ï†



d(x, y)
t1/Î²


.

2. HoÌˆlder continuity estimate: there exist Î˜ > 0 sufficiently small, such that, for all t âˆˆ (0, 1] and all x, y âˆˆ Z, the
distance verifies d(x, y) â‰¤ t1/Î² and for all u âˆˆ Z the difference between kernels is bounded

Î˜ 

n
d(x, y)
d(x, y)
|at (x, u) âˆ’ at (y, u)| â‰¤ tâˆ’ Î²
Ï†
.
t1/Î²
t1/Î²
3. A local lower bound: there exist a monotonic decreasing function Ïˆ : R+ â†’ R and R > 0, such that, for all t âˆˆ (0, 1]
and all x, y where d(x, y) < R, we have


n
d(x, y)
.
|at (x, y)| â‰¥ tâˆ’ Î² Ïˆ
t1/Î²
It is shown that the heat kernel on a closed Riemannian manifold respects all these conditions (Leeb & Coifman, 2016, see
Sec. 3.3).
Definition 2. A distance d(Â·, Â·)b is a snowflake of a distance d(Â·, Â·) if b âˆˆ (0, 1). Moreover, the HoÌˆlder space is the space of
functions that are Lipschitz continuous w.r.t. a snowflake distance, hence the terminology used in Leeb & Coifman (2016).
We now state an important theorem from Leeb & Coifman (2016, Thm. 2).
Theorem 4. Consider a sigma-finite measure space Z of dimension n with metric d(Â·, Â·) and a measure Î¾ such that
Î¾(B(x, r)) ' rn . If the family of kernels {at (Â·, Â·)}tâˆˆR+ respect the condition 1,2 and 3, then, for 0 < Î± < min(1, Î˜/Î²),
the distance DÎ± (Â·, Â·) is equivalent to the thresholded snowflake distance min[1, d(Â·, Â·)Î±Î² ].
Remark. In our case, because we used the heat kernel on a closed Riemannian manifold M, we had DÎ± (Â·, Â·) ' dM (Â·, Â·)2Î±
(Thm. 1). This can be justified from Leeb & Coifman (2016, Cor. 2). Using the same notation as in the corollary, we define
C := maxx,y dM (x, y)2Î± (which is finite due to the assumptions on M), thus we can bound the constant B
B=

B
B
dM (x, y)2Î± â‰¥ dM (x, y)2Î± .
dM (x, y)2Î±
C

The previous theorem closely links the two considered distances. It also motivated the goal to compute the EMD w.r.t.
DÎ± (Â·, Â·). In Leeb & Coifman (2016)[Prop. 15], it is shown that Z is a separable space with respect to the metric DÎ± (Â·, Â·).
Hence, we can use the Kantorovich-Rubinstein theorem to express the EMD in its dual form.
First, we need to define the space of functions that are Lipschitz with respect to DÎ± (Â·, Â·). For a fix Î± âˆˆ (0, 1), we note this
space by Î›Î± . It corresponds to the set of functions f on Z such that the norm
kf kÎ›Î± := sup|f (x)| + sup
x

x6=y

|f (x) âˆ’ f (y)|
DÎ± (x, y)

âˆ—
1
is finite. Next, we need
R to define the spaceâˆ—dual to Î›Î± , which1 is noted by Î›Î± . For a function f âˆˆ Î›Î± and a L measure T ,
we define hf, T i := Z f dT . The space Î›Î± is the space of L measure with the norm

kT kÎ›âˆ—Î± :=

sup hf, T i.
kf kÎ›Î± â‰¤1

In practice, this norm would still be computationally expensive. However, the authors show that the norms
X
(1)
âˆ—
kT kÎ›âˆ—Î± := kP0âˆ— T k1 +
2âˆ’kÎ± (Pk+1
âˆ’ Pkâˆ— )T 1
kâ‰¥0
(2)
kT kÎ›âˆ—Î±

:=

kP0âˆ— T k1

+

X
kâ‰¥0

2âˆ’kÎ± k(Pkâˆ— âˆ’ P0âˆ— )T k1

Diffusion Earth Moverâ€™s Distance and Distribution Embedding

are equivalent to the norm kÂ·kÎ›âˆ— on Î›âˆ—Î± (Leeb & Coifman, 2016, Thm. 4). Where Pkâˆ— is the adjoint of Pk . Finally, using the
Î±
Kantorovich-Rubinstein theorem, we get
Z
(1)
(2)
inf
DÎ± (x, y)Ï€(dx, dy)kÂµ âˆ’ Î½kÎ›âˆ—Î± ' kÂµ âˆ’ Î½kÎ›âˆ—Î± ' kÂµ âˆ’ Î½kÎ›âˆ—Î± ,
Ï€âˆˆÎ (Âµ,Î½)

ZÃ—Z

where DÎ± (Â·, Â·) ' min[1, d(Â·, Â·)Î²Î± ] and d(Â·, Â·) is the ground distance of Z.
(1)

(2)

In conclusion, using kÂ·kÎ›âˆ—Î± or kÂ·kÎ›âˆ—Î± yields a norm equivalent to kÂ·kÎ›âˆ— . The norm kÂµ âˆ’ Î½kÎ›âˆ— is equal to the Wasserstein
Î±
Î±
distance between the distributions Âµ and Î½ with respect to the ground distance DÎ± (Â·, Â·).
A.2. Proofs of section 4.2
Lemma 1. Assuming that P converges (i.e. to its stationary distribution) in polynomial time w.r.t. |V |, then there exists
(2K )

K

a K = O(log |V |) such that Âµi
' P 2 1Xi â‰ˆ Ï†0 for every i = 1, . . . , n, where Ï†0 is the trivial eigenvector of P
associated with the eigenvalue Î»0 = 1.
P P
Proof. First we notice that P is reversible with respect to Ï€i = Dii / i j Dij . Since P is ergodic it converges to its
unique stationary distribution Ï€. Moreover, we assumed this convergence to be in polynomial time w.r.t. |V |, i.e. we define
1 X 2k
Pij âˆ’ Ï€j
2 j

âˆ†i (k) :=
and the mixing time for a  > 0

Ï„i () := min{k : âˆ†i (k 0 ) < , âˆ€ k 0 â‰¥ k}.
Then, by our assumption, there exist 2K = Ï„i () = O(|V |), thus K = O(log |V |). Intuitively, for all k â‰¥ K, each row of
k
k
the matrix P 2 is approximately equal to Ï€ (w.r.t. ), as a consequence P 2 1Xi â‰ˆ Ï†0 .

Theorem 3. Let Î± âˆˆ (0, 1/2) and (M, dM ) be a closed manifold with geodesic dM . The Wasserstein distance w.r.t. the
diffusion ground distance DÎ± (Â·, Â·) is equivalent to the Wasserstein distance w.r.t the snowflake distance dM (Â·, Â·)2Î± on M,
that is WDÎ± ' Wd2Î±
.
M
Proof. We will prove this for a more general framework, we let two metrics d1 (Â·, Â·) and d2 (Â·, Â·) on a sigma-finite measure
space Z, and let Âµ and Î½ be two measures. We assume d1 ' d2 , that is there exist two constants c, C > 0 such that for all
x, y âˆˆ Z we have c d1 (x, y) â‰¤ d2 (x, y) â‰¤ C d1 (x, y). Using the same notation as in Eq. 1, for all Ï€ âˆˆ Î (Âµ, Î½) we have
Z
Z
Z
c
d1 (x, y)Ï€(dx, dy) â‰¤
d2 (x, y)Ï€(dx, dy) â‰¤ C
d1 (x, y)Ï€(dx, dy)
ZÃ—Z

ZÃ—Z

ZÃ—Z

then
Z
c

Z
d1 (x, y)Ï€(dx, dy) â‰¤

inf
Ï€âˆˆÎ (Âµ,Î½)

ZÃ—Z

Z
d2 (x, y)Ï€(dx, dy) â‰¤ C

inf
Ï€âˆˆÎ (Âµ,Î½)

ZÃ—Z

inf
Ï€âˆˆÎ (Âµ,Î½)

d1 (x, y)Ï€(dx, dy)
ZÃ—Z

which is the same as cWd1 (Âµ, Î½) â‰¤ Wd2 (Âµ, Î½) â‰¤ CWd1 (Âµ, Î½), this proves that whenever d1 and d2 are equivalent then Wd1
and Wd2 are equivalent as well.
Corollary 2.1. Let P be the continuous equivalent of the stochastic matrix in Eq. 5. For  small enough, we have:
c t/ ' WD .
W
Î±
P


(13)

c t/ ' W
cH . First note that according to (Coifman & Lafon,
Proof. Note that by Theorem 2 it is equivalent to show that W
t
P


t/

2006) we have P

âˆ’ Ht

L2 (M)

â†’ 0 as  â†’ 0, note that since Vol(M) < âˆ (closed Manifold) and according to

Diffusion Earth Moverâ€™s Distance and Distribution Embedding
t/

Cauchy-Schwarz inequality we get P

âˆ’ Ht

L1 (M)

â†’ 0 as  â†’ 0. Generally speaking, convergence in L2 implies
t/

convergence in L1 when the manifold is closed. Now put D,t = P âˆ’ Ht , then we have kD,t kL1 (M) â†’ 0 as  â†’ 0.
Let Âµ and Î½ be two measures and let Î´ > 0, choose  > 0 small enough so that kDt, Î³k1 < Î´kÎ³k1 for all t > 0, where
Î³ = Âµ âˆ’ Î½,
X

cD (Âµ, Î½) = kD,1 Î³k +
W
2âˆ’kÎ± D,2âˆ’(k+1) âˆ’ D,2âˆ’k Î³ 1
,t
1
kâ‰¥0

â‰¤ kD,1 Î³k1 +

X

2âˆ’kÎ± D,2âˆ’(k+1) Î³

+ 2âˆ’kÎ± D,2âˆ’k Î³

1

1

kâ‰¥0

â‰¤ Î´kÎ³k1 + 2 Î´ kÎ³k1

X

2âˆ’kÎ±

kâ‰¥0


= Î´kÎ³k1 1 +

2
1 âˆ’ 2âˆ’Î±


.

This proves that for all t > 0, for all Î´ > 0 there exists an  > 0 sufficiently small such that
cD (Âµ, Î½) â‰¤ Î´kÂµ âˆ’ Î½k
W
,t
c t/ (Âµ, Î½) âˆ’ W
cH (Âµ, Î½) â‰¤ W
cD (Âµ, Î½), let
Note also that using the reverse triangle inequality we can easily show that W
t
,t
P
Î´ > 0 then for  > 0 small enough we get
cH (Âµ, Î½) âˆ’ Î´kÂµ âˆ’ Î½k â‰¤ W
c t/ (Âµ, Î½) â‰¤ W
cH (Âµ, Î½) + Î´kÂµ âˆ’ Î½k ,
W
t
t
1
1
P


according to (Wang et al., 1997) we can get lower bounds of the heat kernel (which implies lower bounds for the heat
cH (Âµ, Î½) â‰¥ kH1 (Âµ âˆ’ Î½)k â‰¥ CkÂµ âˆ’ Î½k for some C > 0. If Î´ < C/2 then for sufficiently small
operator), we have W
t
1
1
 > 0,
1c
c t/ (Âµ, Î½) â‰¤ 3 W
cH (Âµ, Î½)
WHt (Âµ, Î½) â‰¤ W
t
P
2
2
which completes the proof.
Corollary 3.1. For each 1 â‰¤ i, j â‰¤ m let Xi , Xj âˆˆ X be two datasets with size ni and nj respectively, and let Âµi and Âµj be
the continuous distributions corresponding to the ones of Xi and Xj , let K be the largest scale and put N = min(K, ni , nj ).
Then, for sufficiently big N â†’ âˆ (implying sufficiently small  = 2âˆ’K â†’ 0):
WÎ±,K (Xi , Xj ) ' Wd2Î±
(Âµi , Âµj ),
M

(14)

for all Î± âˆˆ (0, 1/2).
Proof. First define
(1/)

WÎ±,K, (Xi , Xj ) := Âµi

(1/)

âˆ’ Âµj

+
1

Kâˆ’1
X

(2âˆ’(k+1) /)

2âˆ’kÎ± (Âµi

(2âˆ’k /)

âˆ’ Âµi

(2âˆ’(k+1) /)

) âˆ’ (Âµj

(2âˆ’k /)

âˆ’ Âµj

)
1

k=0

, (15)

note that for  = 2âˆ’K we have WÎ±,K, (Xi , Xj ) = WÎ±,K (Xi , Xj ). Since ni â†’ âˆ and nj â†’ âˆ, then using Monte-Carlo
integration we get
lim WÎ±,K, (Xi , Xj ) = WÎ±,K, (Âµi , Âµj )
ni ,nj â†’âˆ

where WÎ±,K, (Âµi , Âµj ) has the same expression as in Eq. 15 (replacing the discrete measure Âµi â€™s with the continuous
measures Âµi â€™s),
WÎ±,K, (Âµi , Âµj ) = P1/ Âµi âˆ’ P1/ Âµj

+
1

Kâˆ’1
X
k=0

âˆ’(k+1)

2âˆ’kÎ± (P2

/

âˆ’k

Âµi âˆ’ P2

/

âˆ’(k+1)

Âµi ) âˆ’ (P2

/

âˆ’k

Âµj âˆ’ P2

/

Âµj )

1

Diffusion Earth Moverâ€™s Distance and Distribution Embedding

c t/ we have limKâ†’âˆ WÎ±,K, (Âµi , Âµj ) = W
c t/ (Âµi , Âµj ) and thus
note that by definition of W
P
P




c t/ (Âµi , Âµj )
lim WÎ±,K, (Xi , Xj ) = W
P

N â†’âˆ



combining this result with Corollary 2.1 yields WÎ±,K, (Xi , Xj ) ' WDÎ± (Âµi , Âµj ) for N large enough and  small enough.
Now if we take  = 2âˆ’K and apply Theorem 3 we get WÎ±,K (Xi , Xj ) ' Wd2Î±
(Âµi , Âµj ) as required.
M

B. Related Work
Wavelet-based linear time approximations of the earth moverâ€™s distance have been investigated by Indyk & Thaper (2003)
who used randomly shifted multi-scale grids to compute EMD between images. Shirdhonkar & Jacobs (2008) expanded this
to wavelets over Rn showing the benefit of using smooth bins over the data, and the relationship to the dual problem. Leeb
& Coifman (2016) investigated using wavelets over more general metric spaces. We build off of this theory and efficiently
approximate this distance on discrete manifolds represented by sparse kernels.
Another line of work in approximations to the Wasserstein distance instead works with tree metrics over the data (Leeb,
2018; Le et al., 2019; Backurs et al., 2020; Sato et al., 2020). These tree methods also linear time however are built in the
ambient dimension and do not represent graph distances as well as graph based methods as shown in Sec. 5. Averaging over
certain tree families can linked to approximating the EMD with a euclidean ground distance in Rn (Indyk & Thaper, 2003).
Many trees may be necessary to smooth out the effects of binning over a continuous space as is evident in Figure 2. Diffusion
EMD uses multiple scales of smooth bins to reduce this effect, thus giving a smoother distance which is approximately
equivalent to a snowflake of the L2 distance on the manifold.
A third line of work considers entropy regularized Wasserstein distances (Cuturi, 2013; Benamou et al., 2014; Solomon
et al., 2015). These methods show that the transportation plan of the regularized 2-Wasserstein distance is a rescaling of
the heat kernel accomplished by the iterative matrix rescaling algorithm known as the Sinkhorn algorithm. In particular,
Solomon et al. (2015) links the time parameter t in the heat kernel Ht to the entropy regularization parameter and performs
Sinkhorn scaling with Ht . While applying the heat kernel is efficient, to embed m distributions with the entropy regularized
Wasserstein distance is O(m2 ) as all pairwise distances must be considered. Next we explore the linear time algorithms
based on multiscale smoothing for compute the earth moverâ€™s distance.
B.1. Multiscale Methods for Earth Moverâ€™s Distance
Let a (possibly randomized) transformation T map distributions on â„¦ to a set of multiscale bins b, T : Âµ(â„¦) â†’ b, these
methods define a T such that
Wd (Âµ, Î½) â‰ˆ EkT (Âµ) âˆ’ T (Î½)k1 .

(16)

Where the approximation, the randomness, and the transform depend on the exact implementation. All of these methods
have two things in common, they smooth over multiple scales
Indyk & Thaper (2003) presented one of the early methods of this type. They showed that by computing averages over a
set of randomly placed grids at dyadic scales. These grids work well for images where the pixels form a discrete set of
coordinates.
Shirdhonkar & Jacobs (2008) showed how to generalize this work over images to more general bin types, more specifically
they showed how wavelets placed centered on a grid could replace the averages in Indyk & Thaper (2003). This work linked
the earth moverâ€™s distance in the continuous domain to that of the discrete through wavelets in Rd , showing that for some
wavelets the EMD approximation was better than the previous grid method. In Diffusion EMD we generalize these wavelets
to the graph domain using diffusion wavelets on the graph allowing Wasserstein distances with a geodesic ground distance.
Kolouri et al. (2016) in Sliced Wasserstein distances showed how the Wasserstein distance could be quickly approximated
by taking the Wasserstein distance along many one dimensional slices of the data. This can be thought of binning along
one dimension where the cumulative distribution function is represented along n bins (one for each point) with each bin
encompassing one more point than the last.

Diffusion Earth Moverâ€™s Distance and Distribution Embedding

Le et al. (2019) generalized the Sliced Wasserstein distance back to trees, giving a new method based on multi-level
clustering where the data is partitioned into smaller and smaller clusters where the bins are the clusters. They demonstrated
k
this method on high dimensional spaces where previous methods that had d2 bins at scale k were inefficiently using bins.
By clustering based on the data, their ClusterTree performs well in high dimensions. However, by clustering they lose
the convergence to the Wasserstein distance with Euclidean ground distance of previous methods for efficiency in high
dimensions.
Diffusion EMD uses smooth diffusion wavelets over the graph which can give geodesic distances unlike previous multiscale
methods. Furthermore, Diffusion EMD selects a number of bins that depends on the rank of the data at each dyadic scale,
which can lead to smaller representations depending on the data.
Table 2. Comparison of Multiscale Methods for Earth Moverâ€™s Distance
M ETHOD
I NDYK & T HAPER (2003)
S HIRDHONKAR & JACOBS (2008)
KOLOURI ET AL . (2016)
L E ET AL . (2019)
D IFFUSION EMD

S CALES

B INS PER SCALE

S MOOTH B INS

G EODESIC D ISTANCES

NO
NO
Y ES
Y ES
Y ES

NO
Y ES
NO
NO
Y ES

NO
NO
NO
NO
Y ES

d
k
d2
1
Ck

DYADIC
DYADIC
n
S PECIFIED
DYADIC

DATA D EPENDENT C ENTERS

2k

R ANK D EPENDENT

C. Algorithm Details
In this section we present two algorithms for computing the Diffusion EMD, the Chebyshev approximation method and
the interpolative decomposition method. The Chebyshev method is more effective when the number of distributions is
relatively small. The interpolative decomposition method is more effective when the number of distributions is large relative
to the size of the manifold. We also detail how to subsample the normed space based on the spectrum of P which can be
approximated quickly.
First we define the approximate rank up to precision Î´ of a matrix A âˆˆ RnÃ—n as:
RÎ´ (A) := # {i : Ïƒi (A)/Ïƒ0 (A) â‰¥ Î´}

(17)
k

Where Ïƒi (A) is the ith largest singular value of the matrix A. The approximate ranks of dyadic powers of P 2 are useful
for determining the amount of subsampling to do at each scale either after an application of the Chebyshev method or during
the computation of (approximate) dyadic powers of P . We note that based on the density of singular values of P , which is
quickly computable using the algorithm presented in Dong et al. (2019), the approximate rank of all dyadic powers of P can
be calculated without computing powers of P .
k

Chebyshev Approximation of Diffusion EMD We first note that P 2 can be computed spectrally using a filter on its
eigenvalues. Let M = D 1/2 P D âˆ’1/2 be the symmetric conjugate of P . Then M is symmetric and has eigenvalues lying
in the range âˆ’1 â‰¤ Î»0 â‰¤ Î»1 â‰¤ . . . â‰¤ Î»n â‰¤ 1. M can be decomposed into U Î£U T for orthonormal U and Î£ a diagonal
k
matrix of [Î»0 , Î»1 , . . . , Î»n ]. We then express P 2 as a filter on the eigenvalues of the M :
k

k

P 2 = D âˆ’1/2 U (Î£)2 U T D 1/2

(18)
k

We compute the first J Chebyshev polynomials of P j Âµ then use the polynomial filter on the eigenvalues h(Ïƒ) = Ïƒ 2
to compute diffusions of the distributions, and reweight these diffusion bins as specified in Algorithm 3. This algorithm
requires J sparse matrix multiplications of P and Âµ. For a total time complexity of OÌƒ(Jmn) when P is sparse.
Interpolative Decomposition Approximation of Diffusion EMD Our second method proceeds by directly approximatk
ing multiplication by the matrix P 2 . The naive solution of computing dyadic powers of P on the original basis quickly
k
leads to a dense n Ã— n matrix. Coifman & Maggioni (2006) observed that P 2 is of low rank, and therefore multiplication
k
by P 2 can be approximated on a smaller basis. In that work they introduced a method of iteratively reducing the size of the
k+1
k
k
basis using rank-revealing pivoted sparse QR decomposition, then computing P 2
= P 2 P 2 . By reducing the size of

Diffusion Earth Moverâ€™s Distance and Distribution Embedding

Algorithm 1 Interpolative Decomposition
Input: m Ã— n matrix A and number of subsamples k s.t. k < min{m,
pn}.
Output: m Ã— k matrix B and k Ã— n matrix C s.t. kAP âˆ’ BCk2 â‰¤ 4k(n âˆ’ k) + 1Ïƒk+1 (A), where B consists of k
columns of A.
Perform pivoted QR decomposition on A s.t. AP = QR, where P is a permutation matrix of A.
Where Q and R are decomposed as the following with Q1 is a m Ã— k matrix, Q2 is m Ã— (n âˆ’ k), etc.


R11
R12
Q = [Q1 | Q2 ];
R=
0kÃ—nâˆ’k R22
B â† Q1 R11
âˆ’1
C â† [Ik |R11
R12 ]

k

the basis and orthogonalizing the basis is kept small and P 2 is kept sparse on its corresponding basis. For sparse P this
gives an algorithm that is O(n log2 n) for computing dyadic powers of a sparse matrix.
Our algorithm follows the same lines as theirs however we use interpolative decomposition to reduce the size of the basis,
and do not do so at early stages where the size of the basis may still be a significant fraction of n. Interpolative decomposition
k
allows for an easy interpretation: we are selecting a well spaced set of points that acts as a basis for higher levels of P 2 .
For more details on interpolative decomposition we refer the reader to Liberty et al. (2007); Bermanis et al. (2013). The
k
algorithm in Coifman & Maggioni (2006) also computes powers of P 2 in the original basis by projecting the multiplication
k
in the reduced basis back to the original. We do not bother computing P 2 Âµ in the original basis, but instead use the reduced
basis directly to compare distributions against. Denote the basis at level k as Ï†k , which is really a subset of Ï†kâˆ’1 , then Ï†k
can be thought of as the centers of the bins at level k of our multiscale embedding. We only compare distributions at these
representative centers rather than at all datapoints. This is summarized in Algorithm 4, creating an embedding for each
distribution whose length is dependent on the rank of P .
Sampling the Diffusions at Larger Scales Interpolative decomposition is an integral part of Algorithm 4. However,
it can also be useful in reducing the size of the distribution representations of Algorithm 3 which uses the Chebyshev
approximation. Without sampling the Chebyshev algorithm centers a bin at every datapoint at every scale. However, for
k
larger scales this is unnecessary, and we can take advantage of the low rank of P 2 with interpolative decomposition. In
practice, we use the top 6 largest scales, in fact the distance shows very little sensitivity to small scales (see Fig. 5(d)). In
fact, this representation can be compressed without a significant loss in performance as shown in Fig. 5(b,c). We apply the
k
rank threshold to the spectrum of P 2 , which can be computed from the density of states algorithm described above, to
determine the number and location of centers to keep. With Î´ = 10âˆ’6 this reduces the number of centers from 60, 000 to
< 4, 000 with a small loss in performance. Rather than selecting a fixed number of centers per scale as in previous methods,
this allows us to vary the number of centers per scale as necessary to preserve the rank at each scale.
Time Complexity Here, we assume that P has at most OÌƒ(n) = O(n logc n) nonzero entries. In this case, interpolative
decomposition in Algorithm 1 has complexity O(k Â· m Â· n log n), and the randomized version in Algorithm 2 has complexity
O(km + k 2 n) (Liberty et al., 2007). Algorithm 3 has complexity O(Jmn logc n) which is dominated by calculation of the
J powers of P j Âµ. The computation time of Algorithm 4 is dominated by the first randomized interpolative decomposition
step. When we set Î³ = O(logc n), which controls when the application of RandID occurs, this gives a total time complexity
for Algorithm 4 of OÌƒ(mn) = O(n log2c n + mn logb n), where b and c are constants controlled by Î³. As Î³ increases, b
increases and c decreases. The first term is dominated by the first interpolative decomposition setup, and the second is the
k
calculation of Âµ(2 ) . It follows that when m  n, it is helpful to set Î³ larger for a better tradeoff of b and c. In practice we
set Î³ = n/10 which seems to offer a reasonable tradeoff between the two steps. For small m in practice Algorithm 3 is
significantly faster than Algorithm 4.
C.1. Gradients of the Diffusion EMD
In this section we will develop the computation of the gradient of the Diffusion EMD WÎ±,K (Xi , Xj ), we will show that its
gradient depend only on the gradient of the Gaussian kernel matrix K which is easy to compute. We start by recalling

Diffusion Earth Moverâ€™s Distance and Distribution Embedding

Algorithm 2 Randomized Interpolative Decomposition: RandID(A, j, k, l)
Input: m Ã— n matrix A, random sample sizes j, l, and number of p
subsamples k s.t. min{m, n} > j > k > l.
Output: m Ã— k matrix B and k Ã— n matrix C s.t. kA âˆ’ BCk2 â‰¤ 4k(n âˆ’ k) + 1Ïƒk+1 (A) w.h.p., where B consists of
k columns of A.
G1 â† N (0, 1)jÃ—m
W â† G1 A
Perform the (deterministic) interpolative decomposition on W in Algorithm 1 to get BÌƒ, C s.t. kW âˆ’ BÌƒDk2 small.
G2 â† N (0, 1)lÃ—j
W2 â† G2 W
BÌƒ2 â† G2 BÌƒ
Find the k indices of BÌƒ2 that are approximately equal to columns in W2 , [i1 , i2 , . . . , ik ].
b â† [b1 , b2 , . . . , bK ]
Algorithm 3 Chebyshev diffusion densities Tcheb (K, Âµ, K, Î±)
Input: n Ã— n graph kernel K, n Ã— m distributions Âµ, maximum scale K, and snowflake constant Î±.
Output: m Ã—
PKn distribution embeddings b
Q â† Diag( i Kij )
âˆ’1
K norm â† QP
KQâˆ’1
norm
D â† Diag( i Kij
)
âˆ’1/2
norm âˆ’1/2
M â†D
K
D
U Î£U T = M ; U orthogonal, Î£ Diagonal
0
Âµ(2 ) â† P Âµ
for k = 1 to K do
k
k
k
Âµ(2 ) â† P 2 Âµ â† D âˆ’1/2 U (Î£)2 U T D 1/2 Âµ
k
kâˆ’1
bkâˆ’1 â† 2(Kâˆ’kâˆ’1)Î± (Âµ(2 ) âˆ’ Âµ(2 ) )
end for
K
bK â† Âµ(2 )
b â† [b0 , b1 , . . . , bK ]

some basic facts about the gradient of matrices,
1. For a matrix P and a vector Âµ we have âˆ‡kP Âµk1 = sign(P Âµ)ÂµT âˆ‡P .
2. For an invertible matrix P , the gradient of the inverse of P is âˆ‡(P âˆ’1 ) = âˆ’P âˆ’1 âˆ‡(P )P âˆ’1 .
3. Let n âˆˆ Nâˆ— the gradient of P n is
âˆ‡P n =

nâˆ’1
X

P ` âˆ‡(P )P nâˆ’`âˆ’1 .

`=0

Using the definition of the Diffusion EMD, we have
K

âˆ‡WÎ±,K (Xi , Xj ) = âˆ‡ P2 (Âµj âˆ’ Âµi )

+
1

Kâˆ’1
X

2âˆ’(Kâˆ’kâˆ’1)Î± âˆ‡

k=0



P2

k+1

âˆ’ P2

k



(Âµj âˆ’ Âµi )

1

Kâˆ’1
 K

 k+1

X
K
k
k+1
k
= sign P2 (Âµj âˆ’ Âµi ) (Âµj âˆ’ Âµi )T âˆ‡(P2 ) +
2âˆ’(Kâˆ’kâˆ’1)Î± sign P2
âˆ’ P2 (Âµj âˆ’ Âµi ))T âˆ‡(P2
âˆ’ P2 ).
k=0

Diffusion Earth Moverâ€™s Distance and Distribution Embedding

Algorithm 4 Interpolative Decomposition diffusion densities Tid (K, Âµ, K, Î±, Î´, Î³)
Input: n Ã— n graph kernel K, n Ã— m distributions Âµ, maximum scale K, snowflake constant Î±, rank threshold Î´, and
basis recomputation threshold Î³.
Output: Distribution
embeddings b
P
Q â† Diag( i Kij )
âˆ’1
K norm â† QP
KQâˆ’1
norm
D â† Diag( i Kij
)
âˆ’1/2
norm âˆ’1/2
M â†D
K
D
0
Âµ(2 ) â† P Âµ
Âµ0 â† Âµ
n0 â† n
for k = 1 to K do
k
if RÎ´ (M 2 ) < Î³ then
k
k
Bk , Ck â† RandID(Mkâˆ’1 , RÎ´ (M 2 ) + 8, RÎ´ (M 2 ), 5)
k
nk â† RÎ´ (M 2 )
else
Bk , Ck , nk â† Bkâˆ’1 , Inkâˆ’1 , nkâˆ’1
end if
T
Mk â† Ck Mkâˆ’1 Mkâˆ’1
CkT
Âµk â† Ck Âµkâˆ’1
k
Âµ(2 ) â† D âˆ’1/2 Mk D 1/2 Âµk
k
kâˆ’1
bkâˆ’1 â† 2(Kâˆ’kâˆ’1)Î± (Âµ(2 ) âˆ’ Ck Âµ(2 ) )
end for
K
bK â† Âµ(2 )
b â† [b0 , b1 , . . . , bK ]

The last formula tells us that the gradient of WÎ±,K (Xi , Xj ) depends only on the gradient of the powers of P which in turn
can be expressed in terms of the gradients P . We have:
âˆ‡P = âˆ‡(Dâˆ’1 M Dâˆ’1 )
= âˆ‡(Dâˆ’1 )M + Dâˆ’1 âˆ‡(M )
= âˆ’Dâˆ’1 âˆ‡(D )Dâˆ’1 M + Dâˆ’1 âˆ‡(M )
= âˆ’Dâˆ’1 âˆ‡(diag(M 1))Dâˆ’1 M + Dâˆ’1 âˆ‡(M )
= âˆ’Dâˆ’1 diag(âˆ‡(M )1)Dâˆ’1 M + Dâˆ’1 âˆ‡(M ),
therefore in order to compute âˆ‡P we need to compute âˆ‡M ,
âˆ’1
âˆ‡M = âˆ‡(Qâˆ’1
 K Q )
âˆ’1
âˆ’1
âˆ’1
âˆ’1
âˆ’1
= âˆ‡(Qâˆ’1
 )K Q + Q âˆ‡(K )Q + Q K âˆ‡(Q )
âˆ’1
âˆ’1
âˆ’1
âˆ’1
âˆ’1
âˆ’1
âˆ’1
= âˆ’Qâˆ’1
 âˆ‡(Q )Q K Q + Q âˆ‡(K )Q âˆ’ Q K Q âˆ‡(Q )Q
âˆ’1
âˆ’1
âˆ’1
âˆ’1
âˆ’1
âˆ’1
âˆ’1
= âˆ’Qâˆ’1
 âˆ‡(diag(K 1))Q K Q + Q âˆ‡(K )Q âˆ’ Q K Q âˆ‡(diag(K 1))Q
âˆ’1
âˆ’1
âˆ’1
âˆ’1
âˆ’1
âˆ’1
âˆ’1
= âˆ’Qâˆ’1
 diag(âˆ‡(K )1)Q K Q + Q âˆ‡(K )Q âˆ’ Q K Q diag(âˆ‡(K )1)Q

hence the gradient of M can be written only in terms of the gradient of K . This process shows that the gradient of the
EMD diffusion can be expressed in terms of the gradient of the Gaussian kernel only, which may make it useful in future
applications where fast gradients of the earth moverâ€™s distance are necessary. For example, in distribution matching where
previous methods use gradients with respect to the Sinkhorn algorithm (Frogner et al., 2015), which scales with O(n2 ).
In this application, computation of gradients of Diffusion EMD would be O(n). What makes this possible is a smooth
kernel and smooth density bins over the graph. In contrast to Diffusion EMD, multiscale methods that use non-smooth bins

Diffusion Earth Moverâ€™s Distance and Distribution Embedding

have non-smooth gradients with respect to K , and so are not useful for gradient descent type algorithms. We leave further
investigation into the gradients of Diffusion EMD to future work.

D. Experiment details
We ran all experiments on a 36 core Intel(R) Xeon(R) CPU E5-2697 v4 @ 2.30GHz with 512GB of RAM. We note that
our method is extremely parallelizable, consisting of only matrix operations, thus a GPU implementation could potentially
speed up computation. For fair comparison we stick to CPU implementations and leave GPU acceleration to future work.
D.1. Metrics used
P@10. P@10 is a metric that measures the overlap in the 10-nearest neighbors between the nearest neighbors of the
EMD method and the ground truth nearest neighbors, which is calculated using exact OT on the geodesic ground distance.
P@10 is useful for distinguishing the quality of distance calculations locally, whereas the next metric measures the quality
more globally. This may be particularly important in applications where only the quality of nearest neighbors matters as in
Sect 4.5, or in applications of Wasserstein nearest neighbor classification (Indyk & Thaper, 2003; Backurs et al., 2020).
Spearman-Ï. The Spearmanâ€™s rank correlation coefficient (also known as Spearman-Ï) measures the similarity in the
order of two rankings. This is useful in our application because while the 1-Wasserstein and 2-Wasserstein may have large
distortion as metrics, they will have a high Spearman-Ï in all cases. Spearman-Ï is defined for two rankings of the data
where di is the difference in the rankings of each for each datapoint i as
P
6 d2i
Ï=1âˆ’
n(n2 âˆ’ 1)
This is useful for quantifying the global ranking of distributions in the graph, which shows the more global behavior of
EMD methods. Since locally manifolds with low curvature look very similar to Rd , where d is the intrinsic dimension of
the manifold, for distributions very close together on the manifold using a Euclidean ground distance may be acceptable,
however for distributions far away from each other according to a geodesic ground distance, approximation with a Euclidean
ground distance may perform poorly.
D.2. Line Example
In Fig 2 we used a cluster tree with 4 levels of 4 clusters for the cluster tree, and a quad tree of depth 4. These are mostly for
illustrative purposes so we there was no parameter search for ClusterTree or QuadTree. As the number of trees gets larger
ClusterTree and QuadTree start to converge to some smooth solution. ClusterTree has a number of bumps caused by the
number of clusters parameter that do not disappear even with many trees. Quadtree converges to a smooth solution as the
number of trees gets large in this low dimensional example.
D.3. Swiss Roll
On the swiss roll example we compared Diffusion EMD to ClusterTree, QuadTree, and the convolutional Sinkhorn method.
Here we describe the parameter setup in each of these methods for Figure 3, 4, and 5.
For Fig. 3 we chose parameters that approximately matched the amount of time between methods on this dataset to the
Diffusion EMD with default settings using the Chebyshev algorithm. This was 150 cluster trees of depth 8 with 3 cluster,
and 20 quad trees of depth 4. We noticed that the speed of quadtree diminishes exponentially with dimension.
For Fig. 4 we did a grid search over parameters for QuadTree and Cluster Tree to find the best performing settings of depth
and number of clusters. We searched over depth d âˆˆ [2 . . . 10] and number of clusters C âˆˆ [2 . . . 5] for ClusterTree. We
found a depth of 4 for QuadTree and 3 Clusters of depth 5 for ClusterTree gave the best tradeoff of performance vs. time on
this task. These parameters were tried while fixing the number of trees to 10. We fixed this depth and number of clusters for
subsequent experiments varying the number of trees in the range [1 . . . 100] for QuadTree and in the range [1 . . . 15] for
ClusterTree. The maximum of this range exceeded the time allotment of Diffusion EMD in both cases. For the convolutional
Sinkhorn method we fixed parameter t = 50 for Ht , and fixed the maximum number of iterations in the range [1 . . . 100].
This took orders of magnitude longer than the multiscale methods in all cases, with comparable performance to Diffusion
EMD on the P@10 metric and better performance using the Spearman-Ï metric. We note that a similar multi-step refinement

Diffusion Earth Moverâ€™s Distance and Distribution Embedding

scheme as implemented in Backurs et al. (2020) could be used here.
D.4. MNIST
To construct the Spherical MNIST dataset (Cohen et al., 2017), we projected the 70,000 images in classical MNIST onto a
spherical graph containing 25,088 nodes. Every pixel in each MNIST image was projected onto four nodes on the northern
hemisphere. These projections were treated as signals over a spherical graph.
We ran Diffusion EMD, ClusterTree, and QuadTree on this dataset to generate embeddings in L1 space. Diffusion EMD
was run with the diffusion wavelet approximations described in the paper, using an epsilon value of 0.001. To best compare
accuracy, we chose parameters for QuadTree and ClusterTree that approximately matched the runtime of Diffusion EMD.
For QuadTree, we used a depth of 20 and 10 clusters. For ClusterTree, we used a depth of 5. However, we note that both
ClusterTree and QuadTree experienced a variance of 10-30 minutes between subsequent runtimes with the same parameters,
probably due to differences of random initialization. We did not run Convolutional Sinkhorn on this dataset, as the projected
runtime exceeded 24 hours.
We calculated a ground truth EMD between 100 of the projected MNIST digits using the exact EMD implementation of
Python Optimal Transport. More accurate comparisons might be obtained using more points, but computation of this 100 by
100 distance matrix took over 24 hours, necessitating severe subsampling, and once again highlighting the need for fast and
accurate approximations of EMD.
For each algorithm, we obtained an accuracy score by running a kNN classifier on the embeddings using an even train/test
split considering only the nearest neighbors. We then computed the Spearman Ï and P@10 score between each method and
the exact EMD ground truth.
D.5. Single cell COVID-19 Dataset
One hundred sixty eight patients with moderate to severe COVID-19 (Marshall et al., 2020) were admitted to YNHH and
recruited to the Yale Implementing Medical and Public Health Action Against Coronavirus CT (IMPACT) study. From each
patient, blood samples were collected to characterize patient cellular responses across timepoints to capture the spectrum
of disease. In total, the composition of peripheral blood mononuclear cell (PBMC) was measured by a myeloid-centric
flow cytometry on 210 samples. Finally, clinical data was extracted from the electronic health record corresponding to each
biosample timepoint to allow for clinical correlation of findings as shown previously. In the main analysis, poor or adverse
outcomes were defined by patient death, while good outcomes were defined by patient survived. In order to analyze the 22
million cells generated in this study, we performed k-means clustering, setting k to 27,000, and took the resultant cluster
centroids as a summarization of the cellular state space as done previously in (Kuchroo et al., 2020).
To test the quality of the organization of patients using EMD methods, we test the Laplacian smoothness on the 10-nearest
neighbors graph created for each method. The Laplacian smoothness of a function on the nodes is denoted f T Lf where
L = D âˆ’ A is the (combinatorial) Laplacian of the adjacency matrix A. This is equivalent to computing the following sum:
X
(fi âˆ’ fj )2
(i,j)âˆˆE

where E is the edge set of the kNN graph. This measures the sum of squared differences of f along the edges of the kNN
graph, which measures the smoothness of f over the graph.
Various cell types have been shown to be associated with mortality from COVID-19 infection, including CD16+ Neutrophils,
T cells and non-classical monocytes. Previously, T cells-to-CD16+ Neutrophils ratios have been reported to be predict of
outcome, with low ratios predicting mortality and high ratios predicting survival (Li et al., 2020). Even outside of ratios,
the absolute counts to T cells (Chen & Wherry, 2020) and CD16+ Neutrophils (Meizlish et al., 2020) have been shown to
be associated with mortality. Finally, non-classical monocytes have also been shown to be enriched in patients with more
severe outcomes (Pence, 2020), further supporting our findings.

Supplement References
Chen, Z. and Wherry, E. J. T cell responses in patients with COVID-19. Nature Reviews Immunology, 20(9):529â€“536, July
2020. doi: 10.1038/s41577-020-0402-6. URL https://doi.org/10.1038/s41577-020-0402-6.

Diffusion Earth Moverâ€™s Distance and Distribution Embedding

Kolouri, S., Zou, Y., and Rohde, G. K. Sliced Wasserstein Kernels for Probability Distributions. In 2016 IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), pp. 5258â€“5267, Las Vegas, NV, USA, June 2016. IEEE. ISBN
978-1-4673-8851-1. doi: 10.1109/CVPR.2016.568.
Li, X., Liu, C., Mao, Z., Xiao, M., Wang, L., Qi, S., and Zhou, F. Predictive values of neutrophil-to-lymphocyte ratio on disease severity and mortality in COVID-19 patients: a systematic review and meta-analysis. Critical Care, 24(1), November
2020. doi: 10.1186/s13054-020-03374-8. URL https://doi.org/10.1186/s13054-020-03374-8.
Marshall, J. C., Murthy, S., Diaz, J., Adhikari, N. K., Angus, D. C., Arabi, Y. M., Baillie, K., Bauer, M., Berry, S.,
Blackwood, B., Bonten, M., Bozza, F., Brunkhorst, F., Cheng, A., Clarke, M., Dat, V. Q., de Jong, M., Denholm, J.,
Derde, L., Dunning, J., Feng, X., Fletcher, T., Foster, N., Fowler, R., Gobat, N., Gomersall, C., Gordon, A., Glueck, T.,
Harhay, M., Hodgson, C., Horby, P., Kim, Y., Kojan, R., Kumar, B., Laffey, J., Malvey, D., Martin-Loeches, I., McArthur,
C., McAuley, D., McBride, S., McGuinness, S., Merson, L., Morpeth, S., Needham, D., Netea, M., Oh, M.-D., Phyu, S.,
Piva, S., Qiu, R., Salisu-Kabara, H., Shi, L., Shimizu, N., Sinclair, J., Tong, S., Turgeon, A., Uyeki, T., van de Veerdonk,
F., Webb, S., Williamson, P., Wolf, T., and Zhang, J. A minimal common outcome measure set for COVID-19 clinical
research. The Lancet Infectious Diseases, 20(8):e192â€“e197, August 2020. doi: 10.1016/s1473-3099(20)30483-7. URL
https://doi.org/10.1016/s1473-3099(20)30483-7.
Meizlish, M. L., Pine, A. B., Bishai, J. D., Goshua, G., Nadelmann, E. R., Simonov, M., Chang, C.-H., Zhang, H., Shallow,
M., Bahel, P., Owusu, K., Yamamoto, Y., Arora, T., Atri, D. S., Patel, A., Gbyli, R., Kwan, J., Won, C. H., Cruz,
C. D., Price, C., Koff, J., King, B. A., Rinder, H. M., Wilson, F. P., Hwa, J., Halene, S., Damsky, W., Dijk, D. v., Lee,
A. I., and Chun, H. J. A neutrophil activation signature predicts critical illness and mortality in covid-19. medRxiv,
2020. doi: 10.1101/2020.09.01.20183897. URL https://www.medrxiv.org/content/early/2020/09/
02/2020.09.01.20183897.
Pence, B. D. Severe COVID-19 and aging: are monocytes the key? GeroScience, 42(4):1051â€“1061, June 2020. doi:
10.1007/s11357-020-00213-0. URL https://doi.org/10.1007/s11357-020-00213-0.
Wang, F.-Y. et al. Sharp explicit lower bounds of heat kernels. Annals of Probability, 25(4):1995â€“2006, 1997.

