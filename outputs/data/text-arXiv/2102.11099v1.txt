1

RCoNet: Deformable Mutual Information
Maximization and High-order Uncertainty-aware
Learning for Robust COVID-19 Detection

arXiv:2102.11099v1 [eess.IV] 22 Feb 2021

Shunjie Dong, Qianqian Yang, Yu Fu, Mei Tian, Cheng Zhuo, Senior Member, IEEE

Abstract‚ÄîThe novel 2019 Coronavirus (COVID-19) infection
has spread world widely and is currently a major healthcare
challenge around the world. Chest Computed Tomography (CT)
and X-ray images have been well recognized to be two effective
techniques for clinical COVID-19 disease diagnoses. Due to faster
imaging time and considerably lower cost than CT, detecting
COVID-19 in chest X-ray (CXR) images is preferred for efficient
diagnosis, assessment and treatment. However, considering the
similarity between COVID-19 and pneumonia, CXR samples
with deep features distributed near category boundaries are
easily misclassified by the hyper-planes learned from limited
training data. Moreover, most existing approaches for COVID19 detection focus on the accuracy of prediction and overlook
the uncertainty estimation, which is particularly important when
dealing with noisy datasets. To alleviate these concerns, we
propose a novel deep network named RCoNetks for robust COVID19 detection which employs Deformable Mutual Information Maximization (DeIM), Mixed High-order Moment Feature (MHMF)
and Multi-expert Uncertainty-aware Learning (MUL). With DeIM,
the mutual information (MI) between input data and the corresponding latent representations can be well estimated and
maximized to capture compact and disentangled representational
characteristics. Meanwhile, MHMF can fully explore the benefits
of using high-order statistics and extract discriminative features
of complex distributions in medical imaging. Finally, MUL
creates multiple parallel dropout networks for each CXR image to
evaluate uncertainty and thus prevent performance degradation
caused by the noise in the data. The experimental results show
that RCoNetks achieves the state-of-the-art performance on an
open source COVIDx dataset of 15134 original CXR images
across several metrics. Crucially, our method is shown to be
more effective than existing methods with the presence of noise
in the data.

Normal

Pneumonia

COVID-19

Fig. 1. Visual illustration of chest X-ray images, including normal, pneumonia
and COVID-19.

acid detection method by gene sequencing, is the accepted
standard for COVID-19 detection [3]. However, because of
the low accuracy of RT-PCR and limited medical test kits in
many hyper-endemic regions or countries, it is challenging to
detect every individual affected by COVID-19 rapidly [4], [5].
Therefore, alternative testing methods, which are faster and
more reliable than RT-PCR, are urgently needed to combat the
disease.
Since most COVID-19 positive patients were diagnosed
with pneumonia, radiological examinations could help detect
and assess the disease. Recently, chest computed tomography
(CT) has been shown to be efficient and reliable to achieve a
real-time clinical diagnosis of COVID-19, outperforming over
RT-PCR in terms of accuracy. Moreover, some deep learning
based methods have been proposed for COVID-19 detection
using chest CT images [6], [7], [8], [9]. For example, an
adaptive feature selection approach was proposed in [10] for
Index Terms‚ÄîChest X-rays, COVID-19, RCoNetks , DeIM, COVID-19 detection based on a trained deep forest model.
MHMF, MUL, Noisy Data, Uncertainty
In [11], an uncertainty vertex-weighted hypergraph learning
method was designed to identify COVID-19 from community
I. I NTRODUCTION
acquired pneumonia (CAP) using CT images. However, the
ORONAVIRUS disease 2019 (COVID-19) causes an on- routine use of CT, which is conducted via expensive equipments,
going pandemic that significantly impacts everyone‚Äôs life takes considerably more time than X-ray imaging and brings
since it was first reported, with hundreds of thousands of deaths a massive burden on radiology departments. Compared to CT,
and millions of infections emerging in over 200 countries [1], X-rays could significantly speed up disease screening, and
[2]. As indicated by the World Health Organization (WHO), hence become a preferred method for disease diagnosis.
Accordingly, deep learning based methods for detecting
due to its highly contagious nature and lack of corresponding
vaccines, the most effective method to control the spread of COVID-19 with chest X-ray (CXR) have been developed and
COVID-19 infection is to keep social distance and contact shown to be able to achieve accurate and speedy detection [12],
tracing. Hence, early and fast diagnosis of COVID-19 has [13]. For instance, a tailored convolution neural network
become significantly essential to control further spreading, and platform trained on open source dataset called COVIDNet
such that the patients could be hospitalized and receive proper in [14] was proposed for the detection of COVID-19 cases
from CXR. Oh et al. [15] proposed a novel probabilistic
treatment in time.
Since the emerge of COVID-19, reverse transcription gradient-weighted class activation map to enable infection
polymerase chain reaction (RT-PCR), as a viral nucleic segmentation and detection of COVID-19 on CXR images.

C

2

Fig. 1 shows three samples from the COVIDx dataset [14]
which contains three different classes: normal, pneumonia
and COVID-19. However, due to the similar pathological
information between pneumonia and COVID-19 in the early
stage, the CXR samples may have latent features distributed
near the category boundaries, which can be easily misclassified
by the hyper-plane learned from the limited training data.
Moreover, to the best of our knowledge, most of the existing
methods for COVID-19 detection are designed to extract
the lower-dimension latent representations which may not
be able to fully capture statistical characteristic of complex
distributions (i.e., non-Gaussian distribution). Furthermore,
quantifying uncertainty in COVID-19 detection is still a major
yet challenging task for doctors, especially with the presence
of noise in the training samples (i.e., label noise and image
noise).

II. BACKGROUND AND R ELATED W ORKS
In this section, we introduce related works on mutual
information estimation and uncertainty learning that lay the
foundation of this paper.
A. Mutual Information Estimation

Mutual information (MI), as a fundamental concept in
information theory, is widely applied to unsupervised feature
learning for quantifying the correlation between random
variables. MI has been exploited in a wide range of domains
and tasks, including biomedical sciences [17], blind source
separation (BSS, e.g., independent component analysis [18]),
feature selection [19], [20] and causal inference [21]. For
example, the object tracking task considered in [22] was treated
as a problem of optimizing the mutual information between
features extracted from a video with most color information
To address the above problems, we propose a novel deep net- removed and those from the original full-color video. Closely
work architecture, referred to as RCoNetks , for robust COVID- related work presented in [23] considered learning representa19 detection which, in particular, contains the following three tions to predict cross-modal correspondence by maximizing MI
modules, i.e., Deformable mutual Information Maximization between features from the multi-view encoders and the content
(DeIM), Mixed High-order Moment Feature (MHMF) and of the held-out view. Moreover, Mutual Information Neural
Multi-expert Uncertainty-aware Learning (MUL):
Estimation (MINE) proposed by [24] was designed to learn
a general-purpose estimator of the MI between continuous
‚Ä¢ The Deformable mutual Information Maximization (DeIM)
variables based on dual representations of the KL-divergence,
module estimates and maximizes the mutual information
which are scalable, flexible and, most crucially, trainable via
(MI) between input data and learned high-level representaback-propagation. Based on MINE, our proposal estimates and
tions, which pushes the model to learn the discriminative
maximizes the CXR image inputs and the corresponding latent
and compact features. We employ deformable convolution
representations to improve diagnosis performance.
layers in this module which are able to explore disentangled spatial features and mitigate the negative effect of
B. Uncertainty in Deep Learning
similar samples across different categories.
Aiming at combating the significant negative effects of
‚Ä¢ The Mixed High-order Moment Feature (MHMF) module,
uncertainty
in deep neural networks, uncertainty learning has
inspired by [16], fully explores the benefits of using a
been
getting
lots of research attention, which facilitates the
mix of high-order moment statistics to better characterize
reliability
assessment
and solves risk-based decision-making
the feature distributions in medical imaging.
problems
[25],
[26],
[27].
In recent years, various frameworks
‚Ä¢ The Multi-expert Uncertainty-aware Learning (MUL)
have
been
proposed
to
characterize
the uncertainty in the
creates multiple parallel dropout networks, each can be
model
parameters
of
deep
neural
networks,
referred to as model
treated as an expert, to derive multiple experts based
uncertainty,
due
to
the
limited
size
of
training
data [28], [29],
diagnosis similar to clinical practices, which improves the
which
can
be
reduced
by
collecting
more
training
data [26],
prediction accuracy. MUL also quantifies the prediction
[30],
[31].
Meanwhile,
another
kind
of
uncertainty
in deep
accuracy by obtaining the variance in prediction across
learning,
referred
to
as
data
uncertainty,
measures
the
noise
different experts.
inherent
in
given
training
data,
and
hence
cannot
be
eliminated
‚Ä¢ The experimental results show that our proposal achieves
the state-of-the-art performance in terms of most metrics by having more training data [32]. To combat these two
both on open source COVIDx dataset of 15134 original kinds of uncertainty, lots of works on various computer vision
tasks, i.e., face recognition [25], semantic segmentation [33],
CXR images and that of noisy setting.
object detection [34], person re-identification [35], etc., have
The remaining of this paper is organized as follows: In introduced deep uncertainty learning to improve the robustness
Section II, we review related works on mutual information of deep learning model and interpretability of discriminant. For
estimation and uncertainty learning as well. In Section III, face recognition task in [26], an uncertainty-aware probabilistic
after an overview of our proposed approach, we discuss the face embedding (PFE) was proposed to represent face images
main components of RCoNetks . In Section IV, we compare our as distributions by utilizing data uncertainty. Exploiting the
proposed architecture with the existing deep learning based advantage of Bayesian deep neural networks, one recent
methods evaluated on a public available dataset of CXR images study [36] leveraged the model uncertainty for analysis and
and also the same dataset but under noisy conditions. And we learning of face representations. To our knowledge, our proposal
also conduct extensive experiments to demonstrate the benefits is the first work that utilizes the high-order moment statistics
of DeIM, MHMF and MUL on the performance of the system. and multiple expert networks to estimate uncertainty for
Finally, we conclude this paper in Section V.
COVID-19 detection using CXR images.

3

ùë¨ùùç (ùëø)

Feature
Encoder
ùëø‚Ä≤

ùëø

ùìò(ùë¨ùùç (ùëø))

MHMF
module

DeIM
module

MUL
module

Multi-expert Uncertainty-aware Learning
COVID-19

‚ãÆ
Feature Encoder
Conv
STAGE1

Conv
STAGE2

CXR Samples

Conv
STAGE3

Conv
STAGE4

=

ùëø

Dropout

FC Layer

‚Ñí2

Dropout

FC Layer

‚Ñí3

‚Ä¶

‚Ä¶

‚Ä¶

Dropout

FC Layer

‚Ñíùë†

Different
mask

Shared
weight

ùìò(ùë¨ùùç (ùëø))

‚äï

=

ùëø‚Ä≤

Concat

ùë¨ùùç (ùëø)

Global
Discriminator

Concat

Global
Discriminator

ùë¨ùùç (ùëø)

‚äñ

ùúé

‚®Ä

‚Ñíùêº

ùúô2
Concat

‚®Ä

‚Ä¶

~

ùúô3

‚Ä¶
‚®Ä

ùúôùëò

ùìò(ùë¨ùùç (ùëø))

Pneumonia

Normal
Final Prediction
Conv

ùúô1

~

Shared Weight

‚ÑíùëÄ

Mixed Higher-order Moment Feature

Deformable Mutual Information Maximization

ùëø‚Ä≤

‚Ñí1

Deformable block (Conv STAGE1~ùüí)
Conv STAGE0

ùëø

FC Layer

Average

Conv
STAGE0

Dropout

~

Softplus

DeformableConv

‚äñSubtraction

GroupedConv

‚®Ä Multiplication

MaxPool

‚äï Addition

Batch Normalization

Fig. 2. The architecture of RCoNetks for COVID-19 detection.

III. M ETHOD
RCoNetks

In this section, we introduce the novel
for robust
COVID-19 detection, which incorporates Deformable mutual
Information Maximization (DeIM), Mixed High-order Moment
Feature (MHMF) and Multi-expert Uncertainty-aware Learning
(MUL), as illustrated in Fig. 2. k is the number of levels of
moment features that are combined in MHMF, and s is the
number of the expert network in MUL, which will be further
clarified in the sequel. The CXR images are first processed
by DeIM which consists of a stack of deformable convolution
layers, extracting discriminative features. The compact features
are then fed into MHMF module to generate high-order moment
latent features, reducing negative effects caused by similar
images. The proposed MUL utilizes the learned high-order
features to generate final diagnoses.

layers each followed by a batch normalization layer. Note that
we employ deformable convolutional layers which can better
extract spatial information of the irregular infected area compared to conventional convolutional layers. More specifically,
regular convolution operates on pre-defined rectangular grid
from an input image or a set of input feature maps, while the
deformable convolution operates on deformable grids that each
grid point is moved by a learnable offset. For example, the
receptive grid P of a regular convolution with kernel size 3 √ó 3
is fixed and can be given by:
P = {(‚àí1, ‚àí1), (‚àí1, 0), ..., (0, 1), (1, 1)},

(2)

while, for deformable convolution, the receptive grid is moved
by the learned offsets ‚àÜpn ‚àà R2 and the output is given as
follows:
X
b(p0 ) =
w(pn ) ¬∑ a(p0 + pn + ‚àÜpn ).
(3)
A. Deformable Mutual Information Estimation and MaximizaPn ‚ààP
tion
Due to the similarity between COVID-19 and pneumonia in
the latent space, we propose Deformable mutual Information
Maximization (DeIM) to extract discriminative and informative
features, reducing the negative influence caused by the lack
of distinctiveness in the deep features. In particular, we train
the model by maximizing the mutual information between the
input and corresponding latent representation.
We use a stack of five convolutional stages, as shown in
Fig. 2, to encode inputs into latent representations, which is
denoted by a differentiable parametric function Eœà :
Eœà : X ‚Üí Z,

(1)

where œà denotes the set of all the trainable parameters in
these layers, and X and Z denote the input and output spaces,
respectively.
The detailed architecture of each convolutional stage is
presented in Fig. 2, which consists of several convolutional

where b(p0 ) denotes the value at location p0 on the output
feature map b, pn enumerates the locations in P, w(pn )
represents the weight at location pn of the kernel, and a(¬∑) is
value at given location on the input feature map. We can see
that with the introduction of offsets ‚àÜpn , the receptive grid is
no longer fixed to be a rectangle, and instead is deformable.
We optimize Eœà by maximizing the mutual information
between the input and the output, i.e., I(X; Z), where Z ,
Eœà (X). The precise mutual information requires knowledge
probability density functions (PDFs) of X and Z, which is
intractable to obtain in practice. To overcome this issue, Mutual
Information Neural Estimation (MINE) proposed in [24]
estimates mutual information by using a lower-bound on the
Donsker-Varadhan representation [37] of the KL-divergence:
(DV )
I(X; Z) : = DKL (J||M) ‚â• IbŒ∏
(X; Z) :

= EJ [TŒ∏ (x, z)] ‚àí log EM [eTŒ∏ (x,z) ],

(4)

4

.

(7)

x0

The experiments have found that using the NCE estimator
outperforms the JSD estimator in some cases, but appears to
be quite similar most of the time.
The existing works [40] that implement these estimators use
some latent representation of x, which is then merged with
some randomly generated features to obtain ‚Äúfake‚Äù samples
e In contrast, we use the samples from other
that satisfy P = P.
categories as the ‚Äúfake‚Äù samples, i.e., x0 , instead. For example,
if the input is a pneumonia sample, then the fake sample is
either a normal or COVID sample. We note that this can push
the learned encoder to derive more distinguishable features for
samples from different categories.

40

60

80

0

75

0

10

130

4
10
0

0

10

60
260

1040
360

510

10

210

30

23
20

10

20

10

260

510
6190

236

30

130

10

40

51

1040

50

3

10

15
0

21
360
130

60

0
619

0

6

60

0
,Eœà
(x))

510

0

23

eTŒ∏,œà (x

0

order = 4

36

90
61

EP TŒ∏,œà (x, Eœà0 (x)) ‚àí EeP log

##
X

210

0
75

750

(DeN CE)
IbŒ∏,œà
(X; Eœà0 (X)) :=
"
"

60

order = 3

30 20

130

Since the encoder Eœà and the mutual information estimator TŒ∏
are optimized simultaneously with the same objective function,
we can share some layers between them, and replace the TŒ∏
with TŒ∏,œà to account for this fact.
Since we are primarily interested in maximizing the mutual
information rather than estimating the precise value, we can
alternatively use a Jensen-Shannon MI estimator (JSD) [38],
which offers more interpretable trade-off:
h

i
(DeJSD)
IbŒ∏,œà
(X; Eœà (X)) := EP ‚àí log 1 + e‚àíTŒ∏,œà (x,Eœà (x))
h 
i
0
‚àí EP√óeP log 1 + eTŒ∏,œà (x ,Eœà (x)) ,
(6)
where x is an input sample of an empirical probability
distribution P, x0 denotes a fake sample from distribution
e where P
e = P. This estimator is illustrated by th DeIM block
P,
shown in Fig. 2, which has the latent representation Eœà (x),
the input sample x and the fake sample x0 as input, and the
difference between the outputs of the two softplus operations
as the estimation of MI.
Another alternative MI estimator is called Noise-Contrastive
Estimator (NCE) [39], which is defined as:

0

10

40

0

(5)

Œ∏,œà

order = 2

10

10

b œà)
b = argmaxIb(DV ) (X; Eœà (X)).
(Œ∏,
Œ∏

order = 1
20

40

where J represents the joint probability of X and Z, i.e.,
J , P (X, Z), and M denotes the product of marginal
probabilities of X and Z, M , P (X)P (Z). TŒ∏ : X √ó Z ‚Üí R
denotes a global discriminator modeled by a neural network
(DV )
with parameters Œ∏, which is trained to maximize IbŒ∏
(X; Z)
to approximate the actual mutual information. Hence, we
can simultaneously estimate and maximize I(X; Eœà (X)) by
(DV )
maximizing IbŒ∏
(X; Z):

Fig. 3. Data points from three Gaussian distributions and the corresponding
moment feature of order 1 to 4
.

CXR samples we considered in this paper typically follow a
complex, non-Gaussian distribution [41], [42], which cannot
be fully captured by its first-order (mean) or second-order
statistics (variance).
We seek a better combination of different orders of statistics
to more precisely characterize the latent representation of the
CXR images. We illustrate the moment features of different
orders [16] in Fig. 3, where we plot 350 data points in R2
sampled from a distribution that combines three different
Gaussian distributions. We can observe that the high-order
moment features are more expressive of statistical characteristic
compared to low-order one. More specifically, it captures the
shape of the cloud of samples more accurately. Therefore,
we include the Mixed High-order Moment Feature (MHMF)
module in the proposed model, as shown in Fig. 2, which
outputs a combination of high-order moment features with
the latent representation Eœà (X) as input. This will potentially
solve the scattering problem, and, more importantly, capture the
subtle differences between CXR images of similar categories,
i.e., pneumonia and COVID-19 in our case.
We show how to obtain the complicated high-order moment
feature in the following. Define r-th order moment feature
as œÜr (a), where a ‚àà RH√óW √óC denotes a latent feature map
of dimension H √ó W √ó C. Lots of recent works adopt the
Kronecker product to compute high-order moment feature [42].
However, calculating Kronecker product of high dimensional
feature maps is significantly computational intensive, and hence
infeasible for real-world applications. Inspired by [43], [44],
[45], we approximate œÜr (a) by exploiting r random projectors
which relies on certain factorization schemes, such as Random
Maclaurin [46]. We use 1√ó1 convolution kernels as the random
projectors to estimate the expectations of high-order moment
features. That is,
œÜr (a) ‚âà K1 (a)

K2 (a)

¬∑¬∑¬∑

Kr (a) ‚àà RH√óW √óC , (8)

where represents the Hadamard (element-wise) product, and
K1 , K2 , . . . , Kr are 1 √ó 1 convolution kernels with random
weights.
Note that Random Maclaurin produces a estimator that
is independent of the input distribution, which causes the
B. Mixed High-order Moment Feature
estimated high-order moments to contain non-informative highThe presence of the image noise and label noise in CXR order moment components. We eliminate these components by
datasets may cause image latent representations generated by learning the weights of the projectors, i.e., the 1√ó1 convolution
deep neural networks to be scattered in the entire feature space. kernels, from the data. Also note that the Hadamard product of
To deal with this issue, [25], [26], [35] represent each image as a number of random projectors may end up with the estimated
a Gaussian distribution, that is defined by a mean (a standard high-order moment features to be similar to low-order ones.
feature vector) and a variance. However, the deep features of To solve this problem, we use a recursive way to estimate the

5

high-order moments instead,
œÜr (a) = œÜr‚àí1 (a)

Kr (a).

(9)

Since different order moments capture different informative
statistics, we design the MHMF module to keep the estimated
moments of different levels of order, as shown in Fig. 2, the
output of which is given as:
J (a) = [œÜ1 (a); œÜ2 (a); ¬∑ ¬∑ ¬∑ ; œÜr (a)] ‚àà RH√óW √órC .

(10)

Hence, J (a) is rich enough to capture the complicated
statistics, and produce discriminative features for the input
of different categories.
C. Multi-expert Uncertainty-aware Learning

where C is the total number of classes, yi,c is the c-th
element of yi , and ybi,c denotes the corresponding prediction.
Œªc represents the weight that controls how much the error on
class c contributes to the loss, c = 1, ..., C. Finally, the loss
LM of the whole MUL module is derived by averaging the
loss values of all the experts:
LM =

N
1 X j
L .
N j=1 e

(13)

We use the variance of classification loss Lje with regards
to the average loss LM to quantify the uncertainty, denoted by
œÉ, which is given as:
œÉ=

N
1 X
(LM ‚àí Lje )2 .
N j=1

(14)

The MHMF module, as described in section III-B, generates
mixed high-order moment features of each sample in the latent The proposed MUL module improves the diagnostic accuracy
space, which we aim to further exploit to derive compact and as the final prediction combines the results from multiple
disentangled information for COVID-19 detection. Meanwhile, experts, and also mitigates the negative effects caused by the
quantifying uncertainty in disease detection is undoubtedly noise in the data by introducing the dropout layers. Moreover,
significant to understand the confidence level of computer-based the experiments have revealed that the more experts in MUL
diagnoses. Motivated by the clinical practices, we present a module the faster the system converges during training.
novel neural network in this section, referred to as Multi-expert
Uncertainty-aware Learning (MUL), which takes in the mixed D. Training
high-order moment features and outputs the prediction and the
The whole architecture of RCoNetks is presented in Fig. 2,
quantification of the diagnostic uncertainty caused by the noise
where the CXR images are first processed by a stack of
in the data.
deformable convolution layers, then transformed to high-order
The structure of Multi-expert Uncertainty-aware Learning
moment latent features by the MHMF module, which are then
module is shown in Fig. 2, which consists of multiple dropout
fed to the MUL module to generate final diagnoses. The loss
layers that process the output from MHMF in parallel, each
used to optimize RCoNetks is given as follows
of which together with the following several fully connected
layers can be regarded as an expert for COVID-19 detection.
Ltotal = LM ‚àí Œ±LI ,
(15)
We note that each dropout layer uses different masks which
results in different subsets of latent information to be kept, where LM is the prediction loss given by Eq. (13) , and LI
while the following fully connected layers share the same denotes the mutual information between the input X and the
weights across different experts. The masks for the dropout latent representation Eœà (X) estimated by either Eq. (6) or
layers are generated randomly at each iteration during training, Eq. (7). Œ± is a positive hyper-parameter that governs how
but fixed during the inference time. We denote the input-output much LM and LI contribute to the total loss. During training,
function of each expert by Cej (¬∑), j = 1, ..., N , where N is the trainable parameters of the whole systems are updated
the total number of experts. Hence, we have the classification iteratively to minimize Ltotal , which is to jointly minimize the
prediction loss LM thus to improve the accuracy, and maximize
loss Lje of j-th expert given as follows:
the mutual information LI .
n
1X
j
j
Le =
Lw (Ce (J (Eœà (xi ))), yi ),
(11)
n i=1
IV. E XPERIMENTS AND R ESULTS
where n represents the total number of labeled CXR samples, A. Dataset
We use a public chest X-ray dataset, referred to as COVIDx,
and yi denotes the one-hot representation of the class label, i =
1, ..., n, and we recall that J (¬∑) denotes the MHMF operation to evaluate the proposed model, which is published by the
given in Eq. (10) and Eœà (¬∑) is the preprocessing step on authors of COVID-Net [14]. This dataset contains a total of
the CXR samples. Note that, the total number of COVID- 13975 CXR images from 13870 patients of 3 classes: (a) normal
19 cases is much smaller than non-COVID cases, i.e., normal (no infections); (b) pneumonia (non-COVID-19 pneumonia);
and pneumonia cases. This imbalance in the dataset leads to (c) COVID-19. It contains samples from five open source availa high ratio of false-negative classification. To mitigate this able data repositories https://github.com/lindawangg/COVIDnegative effect, we employ a weighted cross-entropy Lw (¬∑) Net/blob/master/docs/COVIDx.md. Three random CXR samples
of these three classes are shown in Fig. 1. To reduce the negative
given as follows:
effect caused by extremely unbalanced training samples, i.e.,
C
1 X
very limited number of COVID-19 positive cases compared to
Lw (b
yi , yi ) = ‚àí
Œªc ¬∑ yi,c log ybi,c ,
(12)
the other two categories, we further include other open-source
C c=1

6

TABLE I
D ETAILS OF PATIENT DATA USED FOR TRAINING AND TESTING

Data
Train
Test

Number of Patients Per Class
Normal
Pneumonia
COVID-19
7966
5451
207
885
594
31

Total Patients
13624
1510

CXR datasets from https://www.kaggle.com/c/rsna-pneumoniadetection-challenge/data. Following [14], [47], the dataset is
finally divided into 13624 training and 1510 test samples.
The numbers of samples from different categories used for
training and testing are summarized in Table I. Moreover, we
also adopted various data augmentation techniques to generate
more COVID-19 training samples, such as flipping, translation,
rotation using random five different angles, to tackle the data
imbalance issue such that the proposed model can learn an
effective mechanism of detecting COVID-19.

TABLE II
D ETAILS OF 10% NOISY PATIENT DATA USED FOR TRAINING .
Training Date
Normal
Pneumonia
COVID-19

‚Ä¢

‚Ä¢

Clean
7170
4906
187

Noise
796 (Peumonia+COVID-19)
545 (COVID-19+Normal)
20 (Peumonia+Normal)

Total
7966
5451
207

CoroNet [49]: A deep convolutional neural network model
based on Xception architecture pre-trained on ImageNet
dataset.
ReCoNet [47]: A residual image-based COVID-19 detection network that exploits a CNN-based multi-level
preprocessing filter block and a multi-task learning loss.

D. Implementation

We implement our RCoNetks using the PyTorch library and
apply ResNeXt [50] as the backbone network. We train the
model with the Adam optimizer with an initial learning rate
B. Evaluation Metrics
of 2 √ó 10‚àí4 and a weight decay factor of 1 √ó 10‚àí4 . All the
In our experiments, we use the following six metrics to experiments are run on an NVIDIA GeForce GTX 1080Ti
evaluate the COVID-19 detection performance of different GPU. We set the batch size to be 8, and resize all images to
224 √ó 224 pixels. The hyperparameter Œ± in the loss function
approaches:
given in Eq. (15) is set to be within the range of [0, 0.4].
‚Ä¢ Accuracy (ACC): ACC calculates the proportion
The
drops rate of each dropout layer in the MUL module is
of images that are correctly identified. ACC =
T P +T N
randomly
chosen from {0.1, 0.3, 0.5}. The loss weight Œªc for
T P +T N +F P +F N .
each
category,
which is used to calculate the weighted sum
‚Ä¢ Sensitivity (SEN ): SEN is the ratio of the positive cases
of
the
loss
as
given in Eq. (12), is set to be 1, 1, and 20
that have been correctly detected to all the positive cases.
TP
for
the
normal,
pneumonia, COVID-19 samples, respectively,
SEN = T P +F N .
corresponding
to
the number of training samples in each. We
‚Ä¢ Specificity (SP E): SP E is the ratio of the negative cases
adopt
5-fold
cross-validation
training that we randomly divide
that have been correctly classified to all the negative cases.
TN
the
training
sets
into
five
equal-size
subsets and train the model
SP E = T N +F P .
five
times
that
using
different
four
subsets
for training, and the
‚Ä¢ Balance (BAC): BAC is the mean value of SEN and
SEN +SP E
remaining
one
for
validation
each
time.
We
also evaluate our
SP E. BAC =
.
2
proposed
model
with
different
number
of
order
moments for
‚Ä¢ Positive Predictive Value (P P V ): P P V is the ratio of
the
MHMF
module
k,
and
different
number
of
experts
s.
correctly detected positive cases to all cases that are
TP
To
evaluate
the
performance
of
the
proposed
model
with
detected to be positive. P P V = T P +F P .
the
presence
of
label
noise,
we
derive
a
noisy
dataset
from
‚Ä¢ F1-score (F 1): F 1 uses a combination of accuracy and
sensitivity to calculate a balanced average result. F 1 = the given dataset in the following way: we randomly select
2√óACC√óSEN
a given percentage of training samples in each category, and
ACC+SEN .
assign wrong labels to these sample. In particular, to ensure
T N , T P , F N and F P represent the total number of true
that the fake COVID-19 samples are less than the real ones, we
negatives, true positives, false negatives, and false positives,
assign the COVID-19 labels to selected normal and pneumonia
respectively.
samples in a way the the number of normal and pneumonia
samples assigned with COVID-19 label equals to the number of
COVID-19 samples assigned with either normal and pneumonia
C. Compared Methods
label. We show a realization of the derived noisy dataset when
k
We compare the proposed RCoNets with the following five
the percentage of fake samples is set to be 10% in Table II.
existing deep learning methods for COVID-19 detection:
‚Ä¢ PbCNN [15]: A patch-based convolutional neural network
E. Results and Discussions
with a relatively small number of trainable parameters.
‚Ä¢ COVID-Net [14]: A tailored deep convolutional neural
Performance on Clean Data: The numerical results on the
network that uses a projection-expansion-projection design clean dataset without any artificial noise added are shown
pattern.
in Table III. The results are presented in the form of a ¬± b,
‚Ä¢ DenseNet-121 [48]: A densely connected convolutional
where a and b denote the average and variance values of each
network that connects each layer to every other layer in metric on five independent experiments, respectively. We can
a feed-forward fashion.
see that RCoNet54 , i.e., the proposed model with k = 4 levels

7

TABLE III
P ERFORMANCE COMPARISON OF DIFFERENT APPROACHES FOR COVID-19 DETECTION ON THE COVID X DATASET

Pneumonia

COVID-19

F1 (%)
87.37¬±2.14
93.20¬±0.85
96.74¬±1.04
95.60¬±0.95
97.43¬±0.59
95.91¬±0.56
96.63¬±0.58
97.35¬±0.38
97.61¬±0.48
97.63¬±0.71

Pneumonia

COVID-19

Prediction

(a) Clean

(b) 10% Noise

Param (M)
11.60
117.4
7.61
33.00
2.52
.73
6.74
6.75
6.77
6.77

FLOPs (G)
15.10
bf5.59
7.68
7.61
7.70
7.79
7.91
8.00

Actual
Normal

Pneumonia

COVID-19 Pneumonia

Actual
Normal

Prediction

COVID-19 Pneumonia

Actual
Normal

PPV (%)
88.65¬±1.52
94.73¬±0.97
96.05¬±1.00
95.00¬±1.03
97.17¬±0.76
95.86¬±0.62
96.94¬±0.53
97.59¬±0.91
97.93¬±0.74
97.10¬±0.91

Normal

BAC (%)
91.15¬±1.31
93.57¬±0.89
96.66¬±1.21
97.20¬±1.07
97.46¬±0.87
96.05¬±0.20
96.70¬±0.34
97.44¬±0.82
97.79¬±0.62
97.47¬±0.73

COVID-19 Pneumonia

COVID-19 Pneumonia

Actual

SPE (%)
96.40¬±2.10
95.76¬±2.04
97.23¬±1.01
97.50¬±1.93
97.53¬±1.28
96.38¬±0.29
96.91¬±0.74
97.62¬±0.40
98.24¬±0.39
97.18¬±0.63

Normal

SEN (%)
85.90¬±1.69
91.37¬±1.37
96.08¬±0.88
96.90¬±1.57
97.39¬±1.67
95.71¬±0.41
96.48¬±0.69
97.25¬±0.79
97.33¬±0.45
97.76¬±0.87

Normal

ACC (%)
88.90¬±1.63
95.10¬±1.34
97.40¬±1.67
95.00¬±1.58
97.48¬±1.05
96.12¬±0.33
96.78¬±0.57
97.46¬±0.43
97.89¬±0.53
97.50¬±0.62

Normal

Method
PbCNN [15]
COVID-Net [14]
DenseNet-121 [48]
CoroNet [49]
ReCoNet [47]
RCoNet14
RCoNet24
RCoNet34
RCoNet44
RCoNet54

COVID-19

Prediction

(c) 20% Noise

Normal

Pneumonia

COVID-19

Prediction

(d) 30% Noise

Fig. 4. Confusion matrices of the proposed RCoNetks trained on noisy dataset with different percentages of noisy samples.

TABLE IV
P ERFORMANCE COMPARISON OF DIFFERENT APPROACHES ON COVID X
DATASET WITH NOISY SAMPLES

Noise

10%

20%

30%

Method
PbCNN [15]
COVID-Net [14]
DenseNet-121 [48]
CoroNet [49]
ReCoNet [47]
RCoNet34
RCoNet44
RCoNet54
PbCNN [15]
COVID-Net [14]
DenseNet-121 [48]
CoroNet [49]
ReCoNet [47]
RCoNet34
RCoNet44
RCoNet54
PbCNN [15]
COVID-Net [14]
DenseNet-121 [48]
CoroNet [49]
ReCoNet [47]
RCoNet34
RCoNet44
RCoNet54

ACC (%)
83.22
91.03
91.97
89.45
91.63
92.78
92.98
92.01
78.42
82.51
82.16
82.33
83.26
84.18
84.30
84.34
67.76
71.98
72.74
71.87
73.26
74.56
74.69
74.88

SEN (%)
81.98
87.94
87.94
88.74
90.82
92.21
93.39
91.41
75.90
82.77
81.01
81.10
82.72
84.56
84.01
83.96
66.47
70.13
72.36
72.02
72.53
74.20
74.51
74.37

SPE (%)
89.01
90.62
92.17
90.06
91.16
93.51
93.12
92.76
80.29
81.95
82.21
81.89
83.17
85.79
85.99
85.21
70.61
71.55
72.96
71.54
73.11
75.54
76.94
75.21

of mixed moment features and s = 4 experts, achieves notable
performance improvement over the comparison methods in
terms of most metrics considered, including ACC, SPE, BAC,
PPV and F1 score. We note the performance of RCoNetks can
be further improved with a different set of k and s. For instance,
RCoNet54 achieves better SEN and F1 score than RCoNet44 .
The higher ACC and F1 score validate that RCoNetks is able
to obtain latent features, i.e., the mixed moment features of

different levels of order, that maintains inter-class separability
and intra-class compactness better than other models. Note
that RCoNet54 leads to a higher SEN than all other methods,
which is particularly important to COVID-19 detection, since
successfully detecting COVID-19 positive cases is the key to
control the spread of this super contagious disease. Moreover, it
can be observed that RCoNetks has smaller variance compared
to the others, which demonstrates the robustness and stability
of our model.
We also evaluate the complexity of the proposed model in
terms of numbers of parameters and computational cost, i.e.,
Float-point operations (FLOPs), which is presented in Table III.
It can be observed that the proposed model has much fewer
parameters than several existing methods, except ReCoNet.
However, we note that the FLOPs of RCoNetks is quite close
to that of ReCoNet, which means it takes a similar amount
of time to diagnose COVID-19 from CXR images by these
two model. We can also observe that the increase of k and
s, i.e., the number of mixed moment features and the number
of experts in MUL, only causes a small, or even neglectable,
amount of increase in the number of parameters and FLOPs
as well, which suggests that we can improve the performance
of the proposed model by optimizing k and s, without the
concern on the significant increase of the complexity.
Performance on Noisy Data: We further compare the
proposed model to the existing ones when there is noise present
in the training dataset. We generate three noisy training datasets
in the aforementioned way from the clean dataset with 10%,
20% and 30% samples with wrong labels, respectively. The
results, which we take the averages from five independent
experiments, are presented in Table IV. It can be easily seen
that the more fake samples we add the more it degrades

8

40
40

40

40

20

20

0

0

‚àí20

‚àí20

20

20
0

0

‚àí20

‚àí20

‚àí40
‚àí60

‚àí40
‚àí40

‚àí40
‚àí30

‚àí20

‚àí10

0

10

20

30

40

50

‚àí40

‚àí20

0

20

40

60

‚àí40

‚àí20

0

20

40

60

‚àí40

‚àí20

0

20

(a) baseline

(b) RCoNet-D

(c) RCoNet-M

(d) RCoNet-DM

(e) RCoNet24

(f) RCoNet34

(g) RCoNet44

(h) RCoNet54

40

Fig. 5. The t-SNE visualization of the latent features generated by different methods. Blue, green and red dots represent normal, pneumonia and COVID-19
samples, respectively.

0.5

Uncertainty

0.4

0.3
0.2
0.1

Actual: COVID-19
Prediction: COVID-19
Uncertainty:0.0094

Actual: COVID-19
Prediction: Normal
Uncertainty:0.4792

Fig. 6. Example CXR samples with their predictions and the corresponding
uncertainty levels by RCoNet44 .

the performance of all the methods. Note that the proposed
RCoNet44 still gets the state-of-the-art results in all considered
cases with different percentages of noisy samples in the training
dataset. Moreover, the performance gain over the existing
methods slightly increases with the ratio of noisy samples,
verifying that our model is more robust to the noise. Note
that the extreme case of 30% noisy samples leads to great
performance degradation of all the models. In practice, the
percentage of label noise is usually around 10% to 20%. We
present the confusion matrices in Fig. 4 to summarize the
prediction accuracy of different categories. We can observe
that, although with very limited number of COVID-19, our
model still maintains high accuracy of detecting COVID-19
cases, even with the presence of noisy samples.
Uncertainty Estimation: One remarkable advantage of
our model is the ability to quantify the uncertainty in the
final prediction, which is significantly crucial for COVID-19
detection. This is done by obtaining the variance in the output
of different experts in MUL as described in Section III-C. The
larger the variance is, the more different experts disagree with
each other, and, hence, the more uncertain the model is about

0
0%
10%
20%
30%
Percentage of noisy instances in the dataset

Fig. 7. Comparison on uncertainty level of the predictions by RCoNet44 .

the final prediction. We present two CXR samples in Fig. 6,
including the predictions and the corresponding uncertainty
level by RCoNetks . We can see that the correctly classified
CXR image has a low uncertainty level about its prediction,
i.e., 0.0094, and the misclassified CXR sample with a high
uncertainty level, i.e., 0.4792, suggests that an alternative way
of diagnosis should be sought to correct this prediction. This
greatly improves the reliability of the prediction by RCoNetks ,
and reduces the chance of misdiagnosis. We also show in Fig. 7
the average uncertainty levels of RCoNetks trained on clean
and noisy datasets with different ratios of noisy samples. It can
be observed that the uncertainty level increases almost linearly
with the percentage of noisy samples in the dataset, which
highlights the negative impact of noise on model training.
F. Analysis
We further numerically analyse the benefits of the three
key modules of RCoNetks , i.e., the DeIM, MHMF and MUL
modules in this section.
Effectiveness of DeIM: We utilize t-SNE method [51] to
visualize the latent features, presented in Fig. 5, which are

9

100

TABLE V
I MPACT OF THE MHMF AND MUL ON THE MODEL PERFORMANCE .
s=1
95.4
96.3
97.2
97.4
97.2
96.8

s=2
95.7
96.4
97.2
97.6
97.3
97.0

s=3
95.9
96.6
97.3
97.8
97.3
97.0

s=4
96.1
96.8
97.5
97.9
97.4
97.1

s=5
96.1
96.8
97.4
97.9
97.5
97.0

s=6
96.0
96.7
97.3
97.7
97.5
96.9

s=7
95.8
96.4
97.3
97.5
97.3
96.9

Accuracy

RCoNetks
k=1
k=2
k=3
k=4
k=5
k=6

95
90
85
80
75
70

0%

10%

20%

30%

65
0

0.001

0.002

0.005

0.01

0.02

0.03

0.1

0.4

Test Error

generated by the bottleneck layers of the baseline model, Fig. 8. The prediction accuracy by RCoNet44 with regards to different values
i.e., ResNeXt, RCoNetks and three variants of RCoNetks : (a) of Œ±.
RCoNet-D: a model contains only DeIM; (b) RCoNet-M: a
80
model contains only MUL; (c) RCoNet-DM: a model contains
70
COVID-Net
DeIM and MUL but not MHMF. Comparing the latent feature
ReCoNet
60
distribution by the baseline model shown in Fig. 5(a), and
CoroNet
50
that by RCoNet-D presented in Fig. 5(b), we can tell that the
RCoNet
introduction of DeIM leads to better class separation in the
40
latent space.
30
Effectiveness of MHMF: We can observe in Fig. 5(a) 20
Fig. 5(d) that the latent features of the COVID-19 samples,
10
generated by the models without MHMF, always distribute
0
around the category boundary, and are not quite separable
1
21
41
61
81 101 121 141 161 181 201 221 241 261
Epoch
from those of some pneumonia samples. Meanwhile, the latent
feature distributions presented in Fig. 5(e) - Fig. 5(h) derived by
Fig. 9. Comparison on the learning trajectories of different models.
the models with MHMF show significant separability between
different categories, which implies that MHMF can extract
discriminative features. We also include numerical results of
V. C ONCLUSIONS
RCoNetks , trained and tested on COVIDx dataset, with regards
In this paper, we proposed a novel deep network model,
to different values of k, i.e., the number of levels of the moment named RCoNetk , for robust COVID-19 detection, which
s
features to be mixed, and s, i.e., the number of experts, in contains three key
components, i.e., Deformable mutual InTable V in terms of accuracy. We can observe that, for a given formation Maximization (DeIM), Mixed High-order Moment
value of s, the accuracy increases first with the value of k Feature (MHMF) and Multi-expert Uncertainty-aware Learning
but decreases after k is larger than 4. It demonstrates that (MUL). DeIM estimates and maximizes the mutual information
including more levels of moment feature could improve the between input data and the latent representations simultaneously
model performance. However, the overly high-order moments to obtain the category separability in the latent space. We
may lead to performance degradation, which may be because proposed MHMF to overcome the limited expressive capability
these features are not useful for COVID detection.
of low-order statistics, and instead use a combination of both
Effectiveness of MUL: From Table V, we observe that, for low and high order moment features to extract more informative
a given value of k, accuracy increases first with the value of and discriminative features. MUL generates the final diagnosis
s but saturates around s = 5. This implies that having more and the uncertainty estimation, by combining the output of
experts in MUL can increase the prediction accuracy but it is multiple parallel dropout networks, each as an expert. We
not necessary to have too many.
numerically validated that the proposed RCoNet trained on
Parameter Sensitivity and Convergence: We evaluate how either the public COVIDx dataset or the noisy version of it,
sensitive the model performance in terms of accuracy to the outperforms the existing methods in terms of all the metrics
value of Œ±. We show the average accuracy of five independent considered. We note that these three modules can be easily
experiments by RCoNet44 trained on the dataset with different implemented into other frameworks for different tasks.
ratios of noisy samples in Fig. 8. As we can see, the larger
Œ±, which means the prediction loss, i.e., LM , contributes
R EFERENCES
less to the total loss, not necessarily leads to degradation in
[1] K. Zhang, X. Liu, J. Shen, Z. Li, Y. Sang, X. Wu, Y. Zha, W. Liang,
the accuracy. This means maximizing the mutual information
C. Wang, K. Wang et al., ‚ÄúClinically applicable ai system for accurate diagnosis, quantitative measurements, and prognosis of covid-19
between the input and the latent features could keep useful
pneumonia using computed tomography,‚Äù Cell, 2020.
information within the latent features, thus improving the
[2] Z. Han, B. Wei, Y. Hong, T. Li, J. Cong, X. Zhu, H. Wei, and W. Zhang,
prediction accuracy. We have also shown the learning curves
‚ÄúAccurate screening of covid-19 using attention based deep 3d multiple
instance learning,‚Äù IEEE Transactions on Medical Imaging, 2020.
of different models in Fig. 9, which shows that RCoNet44
[3] X. Mei, H.-C. Lee, K.-y. Diao, M. Huang, B. Lin, C. Liu, Z. Xie, Y. Ma,
converges slightly faster than the others, including COVID-Net,
P. M. Robson, M. Chung et al., ‚ÄúArtificial intelligence‚Äìenabled rapid
ReCoNet and CoroNet.
diagnosis of patients with covid-19,‚Äù Nature Medicine, pp. 1‚Äì5, 2020.

10

[4] W. Xie, C. Jacobs, J.-P. Charbonnier, and B. van Ginneken, ‚ÄúRelational
modeling for robust and efficient pulmonary lobe segmentation in ct
scans,‚Äù IEEE Transactions on Medical Imaging, 2020.
[5] X. Ouyang, J. Huo, L. Xia, F. Shan, J. Liu, Z. Mo, F. Yan, Z. Ding,
Q. Yang, B. Song et al., ‚ÄúDual-sampling attention network for diagnosis
of covid-19 from community acquired pneumonia,‚Äù IEEE Transactions
on Medical Imaging, 2020.
[6] H. X. Bai, R. Wang, Z. Xiong, B. Hsieh, K. Chang, K. Halsey, T. M. L.
Tran, J. W. Choi, D.-C. Wang, L.-B. Shi et al., ‚ÄúAi augmentation of
radiologist performance in distinguishing covid-19 from pneumonia of
other etiology on chest ct,‚Äù Radiology, p. 201491, 2020.
[7] A. A. Ardakani, A. R. Kanafi, U. R. Acharya, N. Khadem, and
A. Mohammadi, ‚ÄúApplication of deep learning technique to manage
covid-19 in routine clinical practice using ct images: Results of 10
convolutional neural networks,‚Äù Computers in Biology and Medicine, p.
103795, 2020.
[8] H. Kang, L. Xia, F. Yan, Z. Wan, F. Shi, H. Yuan, H. Jiang, D. Wu, H. Sui,
C. Zhang et al., ‚ÄúDiagnosis of coronavirus disease 2019 (covid-19) with
structured latent multi-view representation learning,‚Äù IEEE transactions
on medical imaging, 2020.
[9] D.-P. Fan, T. Zhou, G.-P. Ji, Y. Zhou, G. Chen, H. Fu, J. Shen, and
L. Shao, ‚ÄúInf-net: Automatic covid-19 lung infection segmentation from
ct images,‚Äù IEEE Transactions on Medical Imaging, 2020.
[10] L. Sun, Z. Mo, F. Yan, L. Xia, F. Shan, Z. Ding, W. Shao, F. Shi, H. Yuan,
H. Jiang et al., ‚ÄúAdaptive feature selection guided deep forest for covid-19
classification with chest ct,‚Äù arXiv preprint arXiv:2005.03264, 2020.
[11] D. Donglin, S. Feng, Y. Fuhua, X. Liming, M. Zhanhao, D. Zhongxiang,
S. Fei, L. Shengrui, W. Ying, S. Ying, H. Miaofei, G. Yaozong, S. He,
G. Yue, and S. Dinggang, ‚ÄúHypergraph learning for identification of
covid-19 with ct imaging,‚Äù 2020.
[12] Z. Y. Zu, M. D. Jiang, P. P. Xu, W. Chen, Q. Q. Ni, G. M. Lu, and L. J.
Zhang, ‚ÄúCoronavirus disease 2019 (covid-19): a perspective from china,‚Äù
Radiology, p. 200490, 2020.
[13] M. Siddhartha and A. Santra, ‚ÄúCovidlite: A depth-wise separable deep
neural network with white balance and clahe for detection of covid-19,‚Äù
arXiv preprint arXiv:2006.13873, 2020.
[14] L. Wang and A. Wong, ‚ÄúCovid-net: A tailored deep convolutional neural
network design for detection of covid-19 cases from chest x-ray images,‚Äù
arXiv preprint arXiv:2003.09871, 2020.
[15] Y. Oh, S. Park, and J. C. Ye, ‚ÄúDeep learning covid-19 features on cxr
using limited training data sets,‚Äù IEEE Transactions on Medical Imaging,
2020.
[16] E. Pauwels and J. B. Lasserre, ‚ÄúSorting out typicality with the inverse
moment matrix sos polynomial,‚Äù in Advances in Neural Information
Processing Systems, 2016, pp. 190‚Äì198.
[17] F. Maes, A. Collignon, D. Vandermeulen, G. Marchal, and P. Suetens,
‚ÄúMultimodality image registration by maximization of mutual information,‚Äù
IEEE transactions on Medical Imaging, vol. 16, no. 2, pp. 187‚Äì198,
1997.
[18] A. Hyv√§rinen and E. Oja, ‚ÄúIndependent component analysis: algorithms
and applications,‚Äù Neural networks, vol. 13, no. 4-5, pp. 411‚Äì430, 2000.
[19] N. Kwak and C.-H. Choi, ‚ÄúInput feature selection by mutual information
based on parzen window,‚Äù IEEE transactions on pattern analysis and
machine intelligence, vol. 24, no. 12, pp. 1667‚Äì1671, 2002.
[20] H. Peng, F. Long, and C. Ding, ‚ÄúFeature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy,‚Äù
IEEE Transactions on pattern analysis and machine intelligence, vol. 27,
no. 8, pp. 1226‚Äì1238, 2005.
[21] A. J. Butte and I. S. Kohane, ‚ÄúMutual information relevance networks:
functional genomic clustering using pairwise entropy measurements,‚Äù in
Biocomputing 2000. World Scientific, 1999, pp. 418‚Äì429.
[22] C. Vondrick, A. Shrivastava, A. Fathi, S. Guadarrama, and K. Murphy,
‚ÄúTracking emerges by colorizing videos,‚Äù in Proceedings of the European
Conference on Computer Vision (ECCV), 2018, pp. 391‚Äì408.
[23] R. Arandjelovic and A. Zisserman, ‚ÄúLook, listen and learn,‚Äù in Proceedings of the IEEE International Conference on Computer Vision, 2017,
pp. 609‚Äì617.
[24] M. I. Belghazi, A. Baratin, S. Rajeshwar, S. Ozair, Y. Bengio,
A. Courville, and D. Hjelm, ‚ÄúMutual information neural estimation,‚Äù
in International Conference on Machine Learning, 2018, pp. 531‚Äì540.
[25] J. Chang, Z. Lan, C. Cheng, and Y. Wei, ‚ÄúData uncertainty learning in
face recognition,‚Äù arXiv preprint arXiv:2003.11339, 2020.
[26] Y. Shi and A. K. Jain, ‚ÄúProbabilistic face embeddings,‚Äù in Proceedings
of the IEEE International Conference on Computer Vision, 2019, pp.
6902‚Äì6911.

[27] A. Kendall, V. Badrinarayanan, and R. Cipolla, ‚ÄúBayesian segnet: Model
uncertainty in deep convolutional encoder-decoder architectures for scene
understanding,‚Äù arXiv preprint arXiv:1511.02680, 2015.
[28] C. Blundell, J. Cornebise, K. Kavukcuoglu, and D. Wierstra, ‚ÄúWeight
uncertainty in neural networks,‚Äù arXiv preprint arXiv:1505.05424, 2015.
[29] Y. Gal, ‚ÄúUncertainty in deep learning,‚Äù University of Cambridge, vol. 1,
p. 3, 2016.
[30] D. J. MacKay, ‚ÄúA practical bayesian framework for backpropagation
networks,‚Äù Neural computation, vol. 4, no. 3, pp. 448‚Äì472, 1992.
[31] R. M. Neal, Bayesian learning for neural networks. Springer Science
& Business Media, 2012, vol. 118.
[32] A. Kendall and Y. Gal, ‚ÄúWhat uncertainties do we need in bayesian
deep learning for computer vision?‚Äù in Advances in neural information
processing systems, 2017, pp. 5574‚Äì5584.
[33] S. Isobe and S. Arai, ‚ÄúDeep convolutional encoder-decoder network with
model uncertainty for semantic segmentation,‚Äù in 2017 IEEE International
Conference on INnovations in Intelligent SysTems and Applications
(INISTA). IEEE, 2017, pp. 365‚Äì370.
[34] J. Choi, D. Chun, H. Kim, and H.-J. Lee, ‚ÄúGaussian yolov3: An accurate
and fast object detector using localization uncertainty for autonomous
driving,‚Äù in Proceedings of the IEEE International Conference on
Computer Vision, 2019, pp. 502‚Äì511.
[35] T. Yu, D. Li, Y. Yang, T. M. Hospedales, and T. Xiang, ‚ÄúRobust person
re-identification by modelling feature uncertainty,‚Äù in Proceedings of the
IEEE International Conference on Computer Vision, 2019, pp. 552‚Äì561.
[36] U. Zafar, M. Ghafoor, T. Zia, G. Ahmed, A. Latif, K. R. Malik, and
A. M. Sharif, ‚ÄúFace recognition with bayesian convolutional networks
for robust surveillance systems,‚Äù EURASIP Journal on Image and Video
Processing, vol. 2019, no. 1, p. 10, 2019.
[37] M. D. Donsker and S. S. Varadhan, ‚ÄúAsymptotic evaluation of certain
markov process expectations for large time, i,‚Äù Communications on Pure
and Applied Mathematics, vol. 28, no. 1, pp. 1‚Äì47, 1975.
[38] S. Nowozin, B. Cseke, and R. Tomioka, ‚Äúf-gan: Training generative
neural samplers using variational divergence minimization,‚Äù in Advances
in neural information processing systems, 2016, pp. 271‚Äì279.
[39] M. U. Gutmann and A. Hyv√§rinen, ‚ÄúNoise-contrastive estimation of
unnormalized statistical models, with applications to natural image
statistics,‚Äù Journal of Machine Learning Research, vol. 13, no. Feb,
pp. 307‚Äì361, 2012.
[40] P. Bachman, R. D. Hjelm, and W. Buchwalter, ‚ÄúLearning representations
by maximizing mutual information across views,‚Äù in Advances in Neural
Information Processing Systems, 2019, pp. 15 509‚Äì15 519.
[41] J. Xu, P. Ye, Q. Li, H. Du, Y. Liu, and D. Doermann, ‚ÄúBlind image
quality assessment based on high order statistics aggregation,‚Äù IEEE
Transactions on Image Processing, vol. 25, no. 9, pp. 4444‚Äì4457, 2016.
[42] C. Chen, Z. Fu, Z. Chen, S. Jin, Z. Cheng, X. Jin, and X.-S. Hua, ‚ÄúHomm:
Higher-order moment matching for unsupervised domain adaptation,‚Äù
arXiv preprint arXiv:1912.11976, 2019.
[43] P. Jacob, D. Picard, A. Histace, and E. Klein, ‚ÄúMetric learning with
horde: High-order regularizer for deep embeddings,‚Äù in Proceedings
of the IEEE International Conference on Computer Vision, 2019, pp.
6539‚Äì6548.
[44] H. J√©gou and O. Chum, ‚ÄúNegative evidences and co-occurences in image
retrieval: The benefit of pca and whitening,‚Äù in European conference on
computer vision. Springer, 2012, pp. 774‚Äì787.
[45] M. Opitz, G. Waltner, H. Possegger, and H. Bischof, ‚ÄúBier-boosting independent embeddings robustly,‚Äù in Proceedings of the IEEE International
Conference on Computer Vision, 2017, pp. 5189‚Äì5198.
[46] P. Kar and H. Karnick, ‚ÄúRandom feature maps for dot product kernels,‚Äù
pp. 583‚Äì591, 2012.
[47] S. Ahmed, M. H. Yap, M. Tan, and M. K. Hasan, ‚ÄúReconet: Multi-level
preprocessing of chest x-rays for covid-19 detection using convolutional
neural networks,‚Äù medRxiv, 2020.
[48] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, ‚ÄúDensely
connected convolutional networks,‚Äù in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 4700‚Äì4708.
[49] A. I. Khan, J. L. Shah, and M. M. Bhat, ‚ÄúCoronet: A deep neural
network for detection and diagnosis of covid-19 from chest x-ray images,‚Äù
Computer Methods and Programs in Biomedicine, p. 105581, 2020.
[50] S. Xie, R. Girshick, P. Dollar, Z. Tu, and K. He, ‚ÄúAggregated residual
transformations for deep neural networks,‚Äù pp. 5987‚Äì5995, 2017.
[51] J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang, E. Tzeng, and
T. Darrell, ‚ÄúDecaf: A deep convolutional activation feature for generic
visual recognition,‚Äù in International conference on machine learning,
2014, pp. 647‚Äì655.

