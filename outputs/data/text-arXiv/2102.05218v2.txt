FLOP: Federated Learning on Medical Datasets
using Partial Networks
Qian Yangâˆ—â™¯ , Jianyi Zhangâˆ— , Weituo Hao, Gregory P. Spell, Lawrence Carin

arXiv:2102.05218v2 [cs.LG] 23 Jun 2021

Duke University, USA

ABSTRACT

1

The outbreak of COVID-19 Disease due to the novel coronavirus
has caused a shortage of medical resources. To aid and accelerate the diagnosis process, automatic diagnosis of COVID-19 via
deep learning models has recently been explored by researchers
across the world. While different data-driven deep learning models
have been developed to mitigate the diagnosis of COVID-19, the
data itself is still scarce due to patient privacy concerns. Federated
Learning (FL) is a natural solution because it allows different organizations to cooperatively learn an effective deep learning model
without sharing raw data. However, recent studies show that FL
still lacks privacy protection and may cause data leakage. We investigate this challenging problem by proposing a simple yet effective
algorithm, named Federated Learning on Medical Datasets using
Partial Networks (FLOP), that shares only a partial model between
the server and clients. Extensive experiments on benchmark data
and real-world healthcare tasks show that our approach achieves
comparable or better performance while reducing the privacy and
security risks. Of particular interest, we conduct experiments on
the COVID-19 dataset and find that our FLOP algorithm can allow
different hospitals to collaboratively and effectively train a partially
shared model without sharing local patientsâ€™ data.

Automatic disease diagnosis using machine learning methods holds
immense promise, and innovations in this field may refine health
care systems and improve medical practice worldwide. For example,
human digestive system cancers â€” including esophageal, stomach
and colorectal cancers â€” account for about 2.8 million new cases
and 1.8 million deaths per year. Automatic detection, recognition,
and assessment of pathological findings based on images from inside the gastrointestinal (GI) tract will assist doctors in identifying
areas of concern and optimize use of scarce medical resources. Of
great concern in 2020 and into 2021, the global COVID-19 (â€œthe
coronavirus") pandemic has caused over 1.32 million deaths, with
infections and deaths still increasing [1]. As communities and organizations across the world continue making efforts to control the
pandemic, researchers seek to quicken COVID-19 early detection
by automatically classifying computed tomography (CT) scan slices
(images) of patientsâ€™ chests [7, 20, 29, 30, 40].
However, there are two major challenges towards utilizing these
medical images. One challenge is that, collectively, this data is
distributed across a large number of devices or clients located in different hospitals. When relying on data-driven deep learning models
to diagnose disease [11, 32], using only the local data isolated on
a single device will not be sufficient to train an effective model. A
second challenge is the necessity of using the data without compromising patientsâ€™ privacy and security. The leaking of private
data is not only a concern in public media, but also for the hospitals
which must protect patientsâ€™ privacy. To train deep learning models
on such data while not compromising patientsâ€™ privacy, federated
learning [22] has become a promising solution by sharing a model
between clients and a server, instead of sharing the data itself.
Recent improvements in federated learning include overcoming
the statistical challenge in training machine learning models over
distributed networks of devices [28, 39], improving security [4, 10],
and personalization [6, 28]. The conventional federated learning
framework is proved to prevent data leakage against a semi-honest
server, if gradients aggregation is operated with SMC [4] or Homomorphic Encryption [3]. However, recent empirical results in
[41] show that sharing a model may not fully protect privacy, and
gradients exchange will cause Deep Leakage [9, 38, 41]. In [41], the
authors showed that it is possible to obtain private training data
from the publicly shared gradients, including pixel-wise images
and token-wise sentences. One strategy to avoid deep leakage is by
compressing the gradients. Furthermore, the authors in [9] empirically show that federated averaging is also susceptible to attacks, by
successfully reconstructing training images from a convolutional
neural network. To overcome these vulnerabilities of federated

CCS CONCEPTS
â€¢ Computing methodologies â†’ Supervised learning by classification.

KEYWORDS
Federated Learning; Disease Diagnosis
ACM Reference Format:
Qian Yangâˆ—â™¯ , Jianyi Zhangâˆ— , Weituo Hao, Gregory P. Spell, Lawrence Carin.
2021. FLOP: Federated Learning on Medical Datasets using Partial Networks.
In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery
and Data Mining (KDD â€™21), August 14â€“18, 2021, Virtual Event, Singapore.
ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/3447548.3467185

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™21, August 14â€“18, 2021, Virtual Event, Singapore
Â© 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-8332-5/21/08. . . $15.00
https://doi.org/10.1145/3447548.3467185

INTRODUCTION

* Equal

contribution
author. Email: laraqianyang@gmail.com.

â™¯ Corresponding

Cloud Server

Client
(Hospital)

Client
(Hospital)

Client
(Hospital)

Client
(Hospital)

0
12

Input

Feature Extractor

Classifier

Figure 1: Overview of FLOP, allowing for collaboration
among hospitals with small, local datasets to train better machine learning models without loss of privacy.

learning, this paper exploits a new model structure, and we present
the attempt at sharing a partial model for federated learning on
medical datasets, which is still an unexplored field.
In this paper, we propose a simple yet effective algorithm called
Federated Learning on Medical Datasets using Partial Networks
(FLOP). Specifically, instead of sharing an entire model between a
server and clients in each round of training, clients share only a
part of the model for federated averaging and keep the last several
layers private. An overview of FLOP is shown in Figure 1, and our
contributions are as follows:
â€¢ studying the effects of sharing a partial model in a federated
learning framework on medical datasets;
â€¢ applying the FLOP algorithm to different model architectures
(3-layer CNN, VGG11, CovidNet, ResNet50, MobileNet-v2,
ResNetXt), and the presentation of extensive experiments
on both benchmark data (Fashion-MNIST, CIFAR-10) and
real-world medical data (COVIDx, Kvasir);
â€¢ showing empirically that FLOP allows for collaboration among
clients (such as hospitals) with small, local datasets to train
better machine learning models than the baseline algorithm
FedAvg [22] without loss of privacy.

2

RELATED WORK AND PRELIMINARIES

With the emergence of tighter privacy regulations in Europe and
around the world, researchers have started seeking solutions to
train machine learning models from user data without compromising user privacy. Federated learning decentralizes conventional
machine learning by removing the need to aggregate data into a
single location or server, and has become the most popular solution
to meet new data protection regulations [14, 22, 36]. Intuitively, the
mechanism of federated learning is as follows: clients download the
current model, train it on local data from the client, and then send
model updates to the server. The server aggregates and averages
the model updates from a set of clients to improve the shared model.
All the training data remains on the client devices throughout the
learning process.

More formally, we define a set of ğ‘† data owners {C1, . . . , Cğ‘† },
with the ğ‘–-th owner holding a data matrix Dğ‘– . Each row of the matrix
Dğ‘– denotes a data sample, and each column represents a particular
feature. The data are partitioned by sample identifiers, such as user
or device IDs. We denote the feature, label, and sample ID spaces as
X, Y, and Z, respectively. These constitute the complete training
dataset (Z, X, Y). Federated learning is a process whereby clients
collaboratively train a model M, while each Dğ‘– is held locally by
each data owner Cğ‘– .
Federated Learning can be classified into horizontal federated
learning, vertical federated learning, and federated transfer learning.
If the clients in federated learning share overlapping data features
but differ in data samples, we denote it as horizontal federated
learning [36]. The scenario in which clients share overlapping data
samples but differ in data features is known as vertical federated
learning. Federated transfer learning is the case in which there is
no overlap in both data samples or features. For example, when two
hospitals serve two different regions, the data samples associated
with a specific disease are likely different but with similar feature
spaces, as the disease is the same. Therefore, the two hospitals can
collaborate in designing better machine learning models through
horizontal federated learning, without loss of privacy.
The Federated Learning framework has been applied to many
healthcare tasks, such as predicting heart-related hospitalizations
[5] and understanding the genetic underpinnings of brain diseases
[27]. Recent works [19, 21] focusing on federated learning for
COVID-19 typically rely on sharing the full model between clients.
Moreover, they do not differentiate between IID and Non-IID data
distributions. However, [41] indicates that sharing a full model
will cause Deep Leakage [9, 38, 41]. To address these shortcomings,
this paper investigates a new model framework and presents the
attempt to share a partial model for federated learning on medical
datasets. We also analyze both IID and Non-IID data distribution
cases in our experiments.
The model architectures in this paper are based on Convolutional
Neural Networks (CNNs), which have achieved great empirical
success in computer vision [12, 18], natural language processing
[15, 37] and speech recognition [2, 23]. Although there are many
variations of the CNN architecture, a CNN for image classification
tasks is typically composed of two basic components: a feature
extractor and classifier. The feature extractor includes several convolutional layers followed by max-pooling and an activation function, while the classifier usually consists of fully connected layers.
Motivated by this observation, we note a natural way to incorporate a split CNN model into a federated learning architecture: a
shared feature extractor with general feature domain information
and private classifier with private label and task information.

3

METHOD

Federated learning addresses data collection/aggregation concerns
by communicating model updates only. In this paper, we strengthen
the data protection by splitting a model into two parts, choosing a
natural split for CNN architectures: a shared part with general feature domain information, and an unshared part with user-specific
task information. We summarize the proposed FLOP method in
Algorithm 1. Let M denote the full model, which is partitioned into

4

Algorithm 1 FLOP Algorithm
Model Training(M):

// Run on user ğ‘¢

procedure
Receive Mğ‘  from the server and let Mğ‘ ğ‘¢ = Mğ‘ 
Train the model Mğ‘¢ = [Mğ‘ ğ‘¢ , Mğ‘ğ‘¢ ] on local training set
Return Î”Mğ‘ ğ‘¢ to the server
end procedure
Model Update
procedure
Initialize Mğ‘ 
for each episode ğ‘¡ = 1, 2, ... do
Sample ğ‘š users ğ‘ˆğ‘¡
for each user ğ‘¢ âˆˆ ğ‘ˆğ‘¡ in parallel do
Send Mğ‘  to user ğ‘¢
end for
Receive Î”Mğ‘ ğ‘¢ from user ğ‘¢;
Ã
Update Mğ‘  = Mğ‘  âˆ’ ğ›½ |ğ‘ˆ1 |
Î”Mğ‘ ğ‘¢
ğ‘¡

ğ‘¢ âˆˆğ‘ˆğ‘¡

end for
end procedure

a shared part Mğ‘  and private part Mğ‘ . Altogether M = [Mğ‘  , Mğ‘ ].
For a particular local client ğ‘¢, we denote Î”Mğ‘ ğ‘¢ as the update of
Mğ‘ ğ‘¢ . Again considering the horizontal federated learning paradigm
for illustration: ğ‘ˆ clients with the same data structure collaboratively learn a machine learning model. The training process of our
algorithm is as follows.
â€¢ Step 1: Clients receive Mğ‘  from the server, train their own
model Mğ‘¢ = [Mğ‘ ğ‘¢ , Mğ‘ğ‘¢ ] locally, and send the gradients of
Mğ‘ ğ‘¢ back to server;
â€¢ Step 2: The server performs secure aggregation on the received updates from the participating clients;
â€¢ Step 3: The server sends the aggregated results of Mğ‘  back
to clients;
â€¢ Step 4: Clients update their Mğ‘ ğ‘¢ model with the results from
the server.
The above steps are iterated until the loss function converges,
concluding the training. The process and the algorithm are agnostic
to any specific models, and all the clients will obtain the final shared
model parameters.
Privacy is one of the key properties that federated learning aims
to ensure. There are different types of privacy attacks in federated
learning. Recent empirical results in [41] show that sharing a model
may not fully protect the privacy and gradients exchange will
cause Deep Leakage [9, 38, 41]. However, our FLOP framework
addresses this vulnerability because it only shares a partial model.
Furthermore, we can achieve guaranteed privacy by masking a
selection of gradients with encryption [3], differential privacy [26],
or secret sharing [4] techniques in step 1, which is out of the scope
of this paper.

EXPERIMENTS

In this section, we report the results of our FLOP algorithm for
different models. We build upon an open source federated learning
framework1 to implement our FLOP2 in the PyTorch deep learning
API. Our experiments are conducted on both real-world medical
datasets (COVIDx and Kvasir) and benchmark datasets (FashionMNIST [33] and CIFAR-10 [17]). Specifically, each client has a subdataset derived from the original full dataset. We describe the details
on generation of these sub-datasets for each client in Subsection
4.1, using the CIFAR-10 dataset as an example. On the two realworld medical datasets in Subsection 4.2, we use the CovidNet,
ResNet50, MobileNet-v2, and ResNetXt model architectures. For
the two benchmark datasets in Subsection 4.3, we verify the effectiveness of FLOP using the VGG-11 model architecture and a
3-layer CNN. The task in this paper is image classification, and the
datasets across clients follow a non-IID distribution in this section.
We discuss the non-IID results in Subsection 4.4 and also analyze
the results when datasets are IID in Subsection 4.5.

4.1

Construction of the Non-IID Datasets

In the federated learning setting, the training data on a given
client depend on the manner of device use by a particular user.
For example, different hospitals may experience different COVID19 caseloads, resulting in a varying proportion of COVID-19 cases.
Any particular clientâ€™s local dataset will not be representative of
the population distribution. Hence, the data distribution on each
client is likely to be Non-IID. We describe the construction of nonIID datasets across client devices, using the CIFAR-10 dataset as an
illustrative example. CIFAR-10 consists of 60, 000 color images in 10
classes, with 6, 000 images per class. The dataset is split into 50, 000
training images and 10, 000 test images. Supposing the number of
clients for Federated Learning is 50, we distribute to each client
1, 000 training images as its local dataset. The assignment of the
50, 000 CIFAR-10 training images to each client under the non-IID
setting is as follows:
â€¢ Step 1: The images are sorted such that all examples with
the same category label are together. We note that CIFAR-10
has a uniform class distribution of 5, 000 images for each of
its 10 classes. Thus, after this step, the sorting yields 5, 000
examples of the first class, 5, 000 of the second class and so
on for the remaining classes.
â€¢ Step 2: Set a number of â€œchunksâ€, and use these chunks to
subdivide each class. For example, if we set the number of
chunks for each class to be 25, then each chunk will have
5000/25 = 200 images. Then the entire training dataset will
have 250 chunks, and each client receives 5 chunks.
â€¢ Step 3: Randomly distribute the chunks uniformly to each
client. In our running example, each client selects 5 chunks
from the 250 chunks. The distribution scheme is as follows:
(i) The first client chooses the chunks from the first class
with the probability of ğœ† and from the rest classes with
the probability of 1 âˆ’ ğœ†. If we set ğœ† = 0.6, the first client
will select chunks randomly from the remaining classes
with probability 0.4.
1 https://github.com/AshwinRJ/Federated-Learning-PyTorch
2 https://github.com/jianyizhang123/FLOP

(ii) Similarly, the second client chooses the chunks from the
second class with 0.6 probability, and the tenth client
chooses the chunks from the last class with 0.6 probability.
(iii) After that, the eleventh client chooses chunks from the
first class again with 0.6 probability and from the other
classes with 0.4 probability.
(iv) The distribution follows this scheme for clients until each
has 5 chunks.
(v) Once a particular class runs out of images, the current
client will choose chunks from the remaining classes with
a normalized probability (normalized from the original
probability). For example, if there are no images in the first
class, and the original probability is 0.6 from the first class
and (1 âˆ’ 0.6)/9 from the other nine classes, the current
client will choose the chunks from each of the remaining
nine classes with 1/9.
Following the steps above, the clients will receive images with an
uneven distribution of classes. Hence, across the clients, the data
distribution in each client becomes Non-IID. We use this non-IID
dataset distribution scheme for all datasets mentioned in the paper.

4.2

Experiments on Medical Datasets

4.2.1

Dataset.

COVIDx. The task of Covid-19 diagnosis is image classification
with three classes: (i) Normal (No infection), (ii) Pneumonia (NonCOVID-19 infection, e.g., viral, bacterial, etc.), and (iii) COVID-19
(COVID-19 viral infection). COVIDx [31] is the open-access benchmark dataset with the largest number of COVID-19 positive patient
cases, and is the combination of five publicly available COVID19 data repositories: (1) COVID-19 Image Data Collection [8], (2)
COVID-19 Chest X-ray Dataset Initiative3 , (3) Actualmed COVID19 Chest X-ray Dataset4 , (4) COVID-19 radiography dataset5 , and (5)
RSNA Pneumonia Detection Challenges dataset 6 . We use COVIDx
as our training and test dataset. As these datasets are ever-updated
during the ongoing pandemic, we specify that for our experiments,
the dataset consists of 13,954 images for training and 1,579 for testing. The training dataset contains 7,966 Normal, 5,471 Pneumonia
and 517 COVID-19 images. The test dataset contains 885 Normal,
594 Pneumonia, and 100 COVID-19 images.
Kvasir. The Kvasir dataset [24] concerns image classification for
Gastrointestinal disease with eight classes. It includes images showing anatomical landmarks, pathological findings, or endoscopic
procedures in the GI tract, which are collected using endoscopic
equipment at Vestre Viken Health Trust (VV) in Norway. It consists
of 8,000 images in 8 classes and 1,000 images for each class (6,000 for
training and 2,000 for testing). The 8 classes show Anatomical Landmarks (Z-line, pylorus, cecum), Pathological Findings (esophagitis,
polyps, ulcerative colitis), and Polyp Removal (â€œdyed and lifted
polypâ€ and â€œdyed resection marginsâ€) in the GI tract.
3 https://github.com/agchung/Figure1-COVID-chestxray-dataset
4 https://github.com/agchung/Actualmed-COVID-chestxray-dataset
5 https://www.kaggle.com/tawsifurrahman/covid19-radiography-database
6 https://www.kaggle.com/c/rsna-pneumonia-detection-challenge/data

4.2.2 Model. For both medical datasets above, we apply our FLOP
framework on models below to verify the frameworkâ€™s efficacy.
COVID-Net [31] is a recently proposed deep convolutional neural network designed for the detection of COVID-19 cases from
chest X-ray (CXR) images. To compress the network structure, it
utilizes projection-expansion- projection-extension (PEPX) while
preserving the performance to a large extent.
MobileNet-v2 [25] is a new mobile model which improves the
state-of-the-art performance on several tasks, of which the architecture is based on an inverted residual structure. MobileNet-v2 uses
lightweight convolutions to process features in the intermediate
expansion layer.
ResNet50 [12] is a variant of the ResNet model, which utilizes
the the Residual Block to improve the performance of very deep
neural networks. It has been widely adopted in many computer
vision tasks.
ResNeXtâ€™s [34] topology is as the same as ResNet50. The difference is it uses a "split-transform-merge" strategy (branched paths
within a single module) to improve the performance.

4.2.3

Implementations.

Local Testing. The classical federated learning framework uses
FedAvg [22] to update the model Mğ‘  shared between server and
clients. The recent work in [21] further utilizes FedAvg to detect
COVID-19. By contrast, FLOP avoids Deep Leakage [9, 38, 41] and
protects patientsâ€™ privacy by only allowing the server access to a
partial model Mğ‘ ğ‘¢ from each client ğ‘¢. The server in [21] derives a
full model after each training round and tests it on the test dataset,
denoted as global testing. However, since the server in FLOP does
not maintain a globally shared full model, we instead test FLOP
with different Mğ‘¢ = [Mğ‘ ğ‘¢ , Mğ‘ğ‘¢ ] on each local client ğ‘¢, which is
denoted as local testing.
With a sub-dataset distributed to each client, we further randomly split the client-local dataset into local training and test. In
our experiments, we consider 5 clients in total. At each round, we
randomly select 2 clients (ğ‘¢ğ‘– , ğ‘– = 1, 2) from the 5 clients. The selected client ğ‘¢ğ‘– trains its model Mğ‘¢ğ‘– = [Mğ‘ ğ‘¢ , Mğ‘ğ‘¢ğ‘– ] on its own
training dataset for 3 epochs, and then sends Mğ‘ ğ‘¢ to the sever. After
the server aggregates and updates MÌƒğ‘  , all the clients derive a new
model MÌƒğ‘¢ğ‘– = [ MÌƒğ‘ ğ‘¢ , MÌƒğ‘ğ‘¢ğ‘– ], where MÌƒğ‘ ğ‘¢ = MÌƒğ‘  . Then they test MÌƒğ‘¢ğ‘–
on their own local test datasets. We average the accuracy and loss
over all the clients for the comparison.
We follow the previous work [21] to conduct our experiments
in a pseudo-distributed setting on 1 Ã— Nvidia RTX 2080 Ti GPU.
Clients use the Adam [16] optimizer with a learning rate ğ‘Ÿ = 2ğ‘’ âˆ’ 5
and weight decay ğ‘¤ = 1ğ‘’ âˆ’ 7. All other hyperparameters are as the
same as found in [21].
4.2.4

Results on COVIDx.

Analysis. While we initially expect that there may be a tradeoff
between model performance and privacy protection, we actually
find that our FLOP framework outperforms classical FedAvg on four
models (COVID-Net, MobileNet-v2, ResNet50, ResNeXt) by 0.5% âˆ¼
2%. We partition each client local dataset into a local training

Table 1: Local testing accuracy on COVIDx. We expect a possible tradeoff between protecting privacy and model performance;
however, we find FLOP improves local testing accuracy over FedAvg for four models by 0.5% âˆ¼ 2%.
COVID-Net
90.08Â±0.40
92.10Â±0.36

Local Test Loss

0.7

MobileNet-v2
90.07Â±1.99
91.16Â±1.76

FedAvg
FLOP

0.6
0.5
0.4
0.3
0.2

ResNet50
93.64Â±0.23
94.54Â±0.21

FedAvg
FLOP

0.6
0.5
0.4
0.3
0.2

0

20

40
60
Epochs

80

0.1

100

0

20

(a) CovidNet

80

100

(b) ResNet50
FedAvg
FLOP

0.6

40
60
Epochs

0.5
0.4
0.3

0.8
FedAvg
FLOP

0.7
Local Test Loss

Local Test Loss

0.7

0.2

ResNeXt
93.26Â±0.08
93.72Â±0.32

0.7
Local Test Loss

Framework
FedAvg
FLOP

0.6
0.5
0.4
0.3
0.2

0

20

40
60
Epochs

80

0.1

100

0

20

(c) MobileNet-v2

40
60
Epochs

80

100

(d) ResNeXt

Figure 2: Local testing loss on the COVIDx dataset, with testing as described in Section 4.2.3. Note that local testing loss for
FLOP decreases more rapidly than that of the FedAvg framework. X-axis: epoch; y-axis: averaged local testing loss.
CovidNet
Resnet50
MobileNet-v2
ResNeXt

0.6
0.5
0.4
0.3
0.2
0.1

0.7
Local Test Loss

Local Test Loss

0.7

CovidNet
Resnet50
MobileNet-v2
ResNeXt

0.6
0.5
0.4
0.3
0.2

0

20

40
60
Epochs

(a) FedAvg

80

100

0.1

0

20

40
60
Epochs

80

100

(b) FLOP

Figure 3: (a) Local testing loss for the four models altogether under FedAvg; (b) Local testing loss for the four models altogether
under FLOP. X-axis: epoch; y-axis: averaged local testing loss.

c

44.00% 33.00% 23.00%
(44/100) (33/100) (23/100)

n

0.4

p

5.08% 94.80% 0.11%
(45/885) (839/885) (1/885)

0.6

c

0.2

45.00% 28.00% 27.00%
(45/100) (28/100) (27/100)

p
c
Predict label

n

(a) CovidNet FedAvg

p
c
Predict label

p

47.00% 22.00% 31.00%
(47/100) (22/100) (31/100)

0.5
0.4

0.2

n

89.39% 8.42%
2.19%
(531/594) (50/594) (13/594)

p

8.81% 90.40% 0.79%
(78/885) (800/885) (7/885)

c

0.2

28.00% 10.00% 62.00%
(28/100) (10/100) (62/100)

n

0.7
0.6
0.5
0.4
0.3

c

45.00% 16.00% 39.00%
(45/100) (16/100) (39/100)

0.4

88.38% 9.26%
2.36%
(525/594) (55/594) (14/594)

p

2.03% 96.72% 1.24%
(18/885) (856/885) (11/885)

0.6

0.4

c

0.2

0.8

26.00% 13.00% 61.00%
(26/100) (13/100) (61/100)

p
c
Predict label

n

0.2

p
c
Predict label

(d) ResNet-50 FLOP

0.2

n

87.71% 9.60%
2.69%
(521/594) (57/594) (16/594)

p

3.73% 94.69% 1.58%
(33/885) (838/885) (14/885)

0.8

0.6

0.4

c

27.00% 8.00% 65.00%
(27/100) (8/100) (65/100)

n

0.2

94.44% 4.04%
1.52%
(561/594) (24/594) (9/594)

0.8

0.6

p

7.57% 90.62% 1.81%
(67/885) (802/885) (16/885)
0.4

c

32.00% 2.00% 66.00%
(32/100) (2/100) (66/100)

0.2

0.1

p
c
Predict label

(e) MobileNet-v2 FedAvg

0.6

n

(c) ResNet-50 FedAvg

0.1

n

5.20% 92.99% 1.81%
(46/885) (823/885) (16/885)

0.8

0.7

0.3

c

p

0.8

0.9

0.8

0.6

9.72% 88.59% 1.69%
(86/885) (784/885) (15/885)

91.75% 6.57%
1.68%
(545/594) (39/594) (10/594)

(b) CovidNet FLOP

Actual label

Actual label

91.25% 7.24%
1.52%
(542/594) (43/594) (9/594)

0.4

n

0.0

0.9

n

0.8

Actual label

0.6

88.05% 11.95% 0.00%
(523/594) (71/594) (0/594)

Actual label

4.52% 95.37% 0.11%
(40/885) (844/885) (1/885)

n

Actual label

p

0.8

Actual label

86.03% 13.80% 0.17%
(511/594) (82/594) (1/594)

Actual label

Actual label

n

n

n

p
c
Predict label

p
c
Predict label

n

(g) ResNeXt FedAvG

(f) MobileNet-v2 FLOP

p
c
Predict label

(h) ResNeXt FLOP

Figure 4: Confusion matrices on the COVIDx dataset. â€œnâ€ denotes â€œnormalâ€; â€œpâ€ â€” â€œPneumoniaâ€; and â€œcâ€ â€” â€œCOVID-19â€.
Table 2: Global testing accuracy on the COVIDx dataset
Framework
FedAvg
FLOP

COVID-Net
87.61Â±0.38
88.14Â±0.17

MobileNet-v2
86.98Â±3.15
87.01Â±2.93

2.00
1.50
1.25
1.00
0.75
0.50
0.25

FedAvg
FLOP

1.75
Local Test Loss

Local Test Loss

ResNeXt
90.26Â±0.12
90.31Â±0.13

2.00
FedAvg
FLOP

1.75

0.00

ResNet50
90.86Â±0.23
90.84Â±0.39

1.50
1.25
1.00
0.75
0.50
0.25

0

20

40
60
Epochs

80

100

(a) ResNet50

0.00

0

20

40
60
Epochs

80

100

(b) MobileNet-v2

Figure 5: Local testing loss on the Kvasir dataset. Our FLOP framework outperforms compared to the classical FedAvg framework. X-axis: epoch; Y-axis: averaged local testing loss.
set (70%) and local test set (30%) and present local testing losses
in Figure 2. We observe in Figure 2 that the local testing loss of
our FLOP algorithm converges faster than the classical FedAvg
algorithm and our FLOP obtains the better solutions than FedAvg.
We repeat the experiments with different random seeds and
record the best local testing accuracy. Their average and Standard

Deviation are reported in Table 1. We find that across all models
investigated, FLOP achieves higher local testing accuracy than FedAvg. In particular, FLOP on the CovidNet model improves the
classical FedAvg by over 2%. Among the four models, ResNet50
achieves the best results for both the classical FedAvg framework
and our FLOP framework. This is also evident in Figure 3, in which

Ablation Study. As mentioned earlier in Section 4.2.3, FLOP only
shares the model Mğ‘ ğ‘¢ between clients and the server. In the ablation
study, we simulate FedAvg [22] by averaging the sum of MÌƒğ‘¢ğ‘– over
all the clients and test it on the full test dataset, in order to obtain
a similar and comparable global testing accuracy as tested by FedAvg. Table 2 shows the global testing accuracy on the COVIDx
dataset. For all four models, the results of FLOP are comparable
with those of classical FedAvg, outperforming FedAvg for three
of the four models examined. With this performance and the increased privacy/security afforded by sharing only a partial model,
we advocate for FLOP as a superior federated learning framework
over FedAvg.
4.2.5 Results on Kvasir. As for the Kvasir dataset, we also split into
local training sets (80%) and local test sets (20%) for each client.
We train two models (ResNet50 and MoblileNet-V2) to verify the
effectiveness of the FLOP.
Table 3: Local testing accuracy for Kvasir
Framework
FedAvg
FLOP

ResNet50
88.85Â±2.39
95.05Â±1.26

MobileNet-v2
91.08Â±1.37
97.44Â±0.30

Table 4: Global testing accuracy for Kvasir
Framework
FedAvg
FLOP

ResNet50
82.46Â±2.13
82.83Â±2.73

MobileNet-v2
84.15Â±1.04
84.01Â±1.37

Local testing loss is shown in Figure 5. Again, we observe that
the local testing loss of FLOP converges faster than the classical

0.6
FedAvg
FLOP

0.5
Local Test Loss

we compare the local testing loss for the four models altogether
under either FedAvg or FLOP. The performance of COVID-Net is
similar to MobileNet-V2 under both frameworks. The local testing
losses of ResNet50 and ResNeXt decrease faster than the lighter
MobileNet-V2 and COVID-Net models. It is worth noting that
ResNet50 achieves the best performance with respect to both metrics of local testing accuracy and local testing loss.
We additionally note from Figure 2 that the local test loss for
FLOP decreases more stably than that of FedAvg as the models
train. The curve for classical FedAvg, shown in blue, fluctuates
dramatically while the curve for FLOP, shown in red, becomes
stable. This improvement can be observed clearer in Figure 3. All
curves in Figure 3b are more stable than the curves in 3a. We will
further analyze the reason that our FLOP method achieves better
results in Section 4.4.
To make the experimental results more comprehensive, we also
explore the sensitivity of the models to each label. By presenting
the confusion matrices for the models shown in Figure 4, we further
demonstrate that the accuracy for each label in our FLOP is comparable to or even better than the accuracy in FedAvg. Specifically,
the accuracy of Covid-19 label turns to be higher than the one in
FedAvg, and our FLOP does not sacrifice the privacy.

0.4
0.3
0.2
0.1
0.0

0

10

20
30
Epochs

40

50

Figure 6: Local testing loss for Fashion-MNIST. X-axis:
epoch; Y-axis: averaged local testing loss.

FedAvg. Furthermore, we also see FLOP outperform FedAvg with
respect to local testing accuracy, shown in Table 3. Our FLOP increases the accuracy by âˆ¼ 6% for both models.
An interesting result is that in this case, MobileNet-V2 achieves
better results than ResNet50, despite MobileNet-V2 being lighter
neural network than ResNet50. In the case of the Kvasir dataset, our
experiments suggest using MobileNet-V2 over ResNet50 to achieve
the best results for Federated learning.
Ablation Study. Similar to the experiments on COVIDx, we also
conduct an ablation study on Kvasir. Shown in Table 4, we present
the global test accuracy for these two models. Our framework is
still comparable to FedAvg while not sacrificing client privacy.

4.3

Experiments on Benchmark Datasets

To further verify the effectiveness of FLOP, we conduct experiments
on two benchmark datasets that are also publicly available: FashionMNIST and CIFAR-10.
Fashion-MNIST. Fashion-MNIST consists of a training set of
60, 000 images and a test set of 10, 000 images. Each example is
a 28 Ã— 28 grayscale image, associated with a label from 10 classes.
We use a 3-layer CNN model: two convolutional layers followed by
one linear layer.
Following the similar settings of the experiments on COVIDx
and Kvasir, the simulation on Fashion-MNIST is for 100 clients and
terminates after 50 rounds. For every round, we randomly select 15
clients from the 100 clients and perform 10 local epochs for training
on each client. We optimize using stochastic gradient descent (SGD)
with batch size 60. After each round, we record the local testing
accuracy and the local testing loss on each client. Then we average
them over all the clients and show the results in Figure 6.
CIFAR-10. CIFAR-10 consists of 60, 000 32 Ã— 32 colour images in
10 classes, with 50, 000 training images and 10, 000 test images. The
simulation on CIFAR-10 is for 50 clients in total and terminates after
100 rounds. For every round, we randomly select 20 clients from
the 50 clients and perform 5 local epochs on each client. The model
is VGG-11, and the last linear layer is not shared. The optimizer is
stochastic gradient descent (SGD) with batch size 100. After each
round, we record the local testing accuracy and the local testing
loss on each client. Then we average them over all the clients and
show the results in Figure 7.

Table 5: Local testing accuracy on COVIDx when the datasets of the clients are IID. We expect a possible tradeoff between
protecting privacy and model performance; however, we find FLOP improves local testing accuracy over FedAvg for all four
models tested.
Framework
FedAvg
FLOP

COVID-Net
91.33Â±0.10
91.52Â±0.13

MobileNet-v2
91.87Â±2.98
91.90Â±0.63

ResNet50
94.01Â±0.19
94.51Â±0.27

ResNeXt
94.03Â±0.71
94.15Â±1.06

Table 6: Global testing accuracy on COVIDx when the datasets of the clients are IID.
Framework
FedAvg
FLOP

COVID-Net
88.84Â±0.34
88.51Â±0.26

MobileNet-v2
89.11Â±3.95
89.65Â±3.63

1.0
FedAvg
FLOP

Local Test Loss

0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1

0

20

40
60
Epochs

80

100

Figure 7: Local testing loss for CIFAR-10. X-axis: epoch; Yaxis: averaged local testing loss.
Table 7: Local testing accuracy for Kvasir when the datasets
of the clients are IID.
Framework
FedAvg
FLOP

ResNet50
97.82Â±0.87
98Â±0.46

MobileNet-v2
98.38Â±0.31
98.45Â±0.17

Table 8: Global testing accuracy for Kvasir when the datasets
of the clients are IID. Note that while FedAvg slightly outperforms FLOP in this case, it is likely at the expense of model
privacy
Framework
FedAvg
FLOP

4.4

ResNet50
97.24Â±1.07
96.45Â±2.64

MobileNet-v2
98.28Â±0.19
98.04Â±0.29

Discussion

In this subsection, we discuss why our FLOP algorithm leads to the
improved results when compared with FedAvg.
The component of the networks we do not share is Mğ‘ğ‘¢ , which
in our experiments is the classifier. It is typically composed of
several linear layers. The classifier whose output is the predicted
labels, is the most related component to the data distribution of
the ground-truth labels. Hence, we believe this component carries
much more information of clientsâ€™ datasets. This motivates keeping
the classifier separate from server in the FLOP algorithm.

ResNet50
91.49Â±0.29
91.45Â±0.25

ResNeXt
90.98Â±0.59
91.20Â±0.81

Because this component will be affected by clientsâ€™ data, it is
highly personalized for each client. In the classical FedAvg framework, the clients share the full model and receive a new one after
each round from the sever. The new classier is less personalized for
the clientsâ€™ data than the former one. Thus, the local testing loss is
less stable than ours, and we similarly see that our framework can
achieve superior local testing accuracy.

4.5

Results of the IID cases

In this subsection, we report the training accuracy for the IID cases
on medical datasets. The hyperparameters and other experimental
settings of the IID cases are the same as the Non-IID cases.
COVIDx. While we initially expect that there may be a tradeoff
between model performance and privacy protection, we actually
find that our FLOP framework outperforms classical FedAvg on four
models (COVID-Net, MobileNet-v2, ResNet50, ResNeXt), though
the improvement is less for the IID case than for the non-IID case.
The local testing accuracy on the COVIDx dataset for the IID case
are shown in Table 5. We believe that the less dramatic improvement
than for the non-IID case is in line with our discussion in Section
4.4. Specifically, in the IID case, the non-shared classifier for each
client is slightly less personalized, since the data distributions on
the clients are more similar than in the non-IID case. Again, we see
that the performance of the ResNet50 model is the best among the
four models tested.
As for the global testing accuracy in Table 6, the accuracy of
our FLOP method is again comparable to the accuracy of FedAvg,
again strengthening our argument that FLOP is a strong method
of preserving privacy and training effective models in a federated
setting.
Kvasir. With respect to local testing accuracy, shown in Table 7,
FLOP again outperforms FedAvg on the Kvasir dataset in the IID
case, and again MobileNet-v2 outperforms the ResNet50, which is
consistent across the results for non-IID and IID cases.
With respect to global testing accuracy, shown in Table 8, our
FLOP method performs competitively, but does not outperform, the
FedAvg scheme. We expect that this slight underperformance is in
tradeoff to the additional privacy afforded to FLOP by sharing only
a partial model between server and clients.

5

CONCLUSION

We have proposed a Federated Learning method in which only a
partial model is shared between clients and server â€“ FLOP â€“ and
demonstrated its use particularly for applications with medical
data. Our proposed algorithm reduces privacy and security risks
by sequestering client data on their local devices. Experimental results on both real-world medical datasets and benchmark datasets
demonstrate the advantages of our algorithm. In future work, we
intend to accelerate the training of the models following the techniques in [13, 35] and apply our algorithm to other tasks. Overall,
we believe that our research makes an important step for improving
the performance of deep learning models on data-scarce healthcare
tasks, as our algorithm allows different hospitals to collaboratively
train models without sharing local patientsâ€™ data.

REFERENCES
[1] 2020. CORONAVIRUS. â€œhttps://coronavirus.jhu.edu/map.htmlâ€.
[2] Ossama Abdel-Hamid, Abdel-rahman Mohamed, Hui Jiang, Li Deng, Gerald
Penn, and Dong Yu. 2014. Convolutional neural networks for speech recognition.
IEEE/ACM Transactions on audio, speech, and language processing 22, 10 (2014),
1533â€“1545.
[3] Yoshinori Aono, Takuya Hayashi, Lihua Wang, Shiho Moriai, et al. 2017. Privacypreserving deep learning via additively homomorphic encryption. IEEE Transactions on Information Forensics and Security 13, 5 (2017), 1333â€“1345.
[4] Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H Brendan
McMahan, Sarvar Patel, Daniel Ramage, Aaron Segal, and Karn Seth. 2017. Practical secure aggregation for privacy-preserving machine learning. In Proceedings
of the 2017 ACM SIGSAC Conference on Computer and Communications Security.
1175â€“1191.
[5] Theodora S Brisimi, Ruidi Chen, Theofanie Mela, Alex Olshevsky, Ioannis Ch
Paschalidis, and Wei Shi. 2018. Federated learning of predictive models from
federated electronic health records. International journal of medical informatics
112 (2018), 59â€“67.
[6] Fei Chen, Zhenhua Dong, Zhenguo Li, and Xiuqiang He. 2018. Federated metalearning for recommendation. arXiv preprint arXiv:1802.07876 (2018).
[7] Nanshan Chen, Min Zhou, Xuan Dong, Jieming Qu, Fengyun Gong, Yang Han,
Yang Qiu, Jingli Wang, Ying Liu, Yuan Wei, et al. 2020. Epidemiological and clinical
characteristics of 99 cases of 2019 novel coronavirus pneumonia in Wuhan, China:
a descriptive study. The Lancet 395, 10223 (2020), 507â€“513.
[8] Joseph Paul Cohen, Paul Morrison, and Lan Dao. 2020. COVID-19 image data
collection. arXiv 2003.11597 (2020). https://github.com/ieee8023/covid-chestxraydataset
[9] Jonas Geiping, Hartmut Bauermeister, Hannah DrÃ¶ge, and Michael Moeller. 2020.
Inverting Gradientsâ€“How easy is it to break privacy in federated learning? arXiv
preprint arXiv:2003.14053 (2020).
[10] Robin C Geyer, Tassilo Klein, and Moin Nabi. 2017. Differentially private federated
learning: A client level perspective. arXiv preprint arXiv:1712.07557 (2017).
[11] Varun Gulshan, Lily Peng, Marc Coram, Martin C Stumpe, Derek Wu, Arunachalam Narayanaswamy, Subhashini Venugopalan, Kasumi Widner, Tom Madams,
Jorge Cuadros, et al. 2016. Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs. Jama
316, 22 (2016), 2402â€“2410.
[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual
learning for image recognition. In Proceedings of the IEEE conference on computer
vision and pattern recognition. 770â€“778.
[13] Zhouyuan Huo, Bin Gu, Heng Huang, et al. 2018. Decoupled parallel backpropagation with convergence guarantee. In International Conference on Machine
Learning. PMLR, 2098â€“2106.
[14] Peter Kairouz, H Brendan McMahan, Brendan Avent, AurÃ©lien Bellet, Mehdi
Bennis, Arjun Nitin Bhagoji, Keith Bonawitz, Zachary Charles, Graham Cormode,
Rachel Cummings, et al. 2019. Advances and open problems in federated learning.
arXiv preprint arXiv:1912.04977 (2019).
[15] Yoon Kim. 2014. Convolutional neural networks for sentence classification. arXiv
preprint arXiv:1408.5882 (2014).
[16] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014).
[17] Alex Krizhevsky, Geoffrey Hinton, et al. 2009. Learning multiple layers of features
from tiny images. (2009).
[18] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. Imagenet classification with deep convolutional neural networks. In Advances in neural information
processing systems. 1097â€“1105.

[19] Rajesh Kumar, Abdullah Aman Khan, Sinmin Zhang, WenYong Wang, Yousif
Abuidris, Waqas Amin, and Jay Kumar. 2020. Blockchain-federated-learning and
deep learning models for covid-19 detection using ct imaging. arXiv preprint
arXiv:2007.06537 (2020).
[20] Qun Li, Xuhua Guan, Peng Wu, Xiaoye Wang, Lei Zhou, Yeqing Tong, Ruiqi Ren,
Kathy SM Leung, Eric HY Lau, Jessica Y Wong, et al. 2020. Early transmission
dynamics in Wuhan, China, of novel coronavirusâ€“infected pneumonia. New
England Journal of Medicine (2020).
[21] Boyi Liu, Bingjie Yan, Yize Zhou, Yifan Yang, and Yixian Zhang. 2020. Experiments of federated learning for covid-19 chest x-ray images. arXiv preprint
arXiv:2007.05592 (2020).
[22] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and
Blaise Aguera y Arcas. 2017. Communication-Efficient Learning of Deep Networks from Decentralized Data. In Artificial Intelligence and Statistics. 1273â€“1282.
[23] Dimitri Palaz, Mathew Magimai Doss, and Ronan Collobert. 2015. Convolutional
neural networks-based continuous speech recognition using raw speech signal.
In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP). IEEE, 4295â€“4299.
[24] Konstantin Pogorelov, Kristin Ranheim Randel, Carsten Griwodz, Sigrun Losada
Eskeland, Thomas de Lange, Dag Johansen, Concetto Spampinato, Duc-Tien
Dang-Nguyen, Mathias Lux, Peter Thelin Schmidt, Michael Riegler, and PÃ¥l
Halvorsen. 2017. KVASIR: A Multi-Class Image Dataset for Computer Aided
Gastrointestinal Disease Detection. In Proceedings of the 8th ACM on Multimedia
Systems Conference (MMSysâ€™17). ACM, New York, NY, USA, 164â€“169. https:
//doi.org/10.1145/3083187.3083212
[25] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and LiangChieh Chen. 2018. Mobilenetv2: Inverted residuals and linear bottlenecks. In
Proceedings of the IEEE conference on computer vision and pattern recognition.
4510â€“4520.
[26] Reza Shokri and Vitaly Shmatikov. 2015. Privacy-preserving deep learning. In
Proceedings of the 22nd ACM SIGSAC conference on computer and communications
security. 1310â€“1321.
[27] Santiago Silva, Boris A Gutman, Eduardo Romero, Paul M Thompson, Andre
Altmann, and Marco Lorenzi. 2019. Federated learning in distributed medical
databases: Meta-analysis of large-scale subcortical brain data. In 2019 IEEE 16th
international symposium on biomedical imaging (ISBI 2019). IEEE, 270â€“274.
[28] Virginia Smith, Chao-Kai Chiang, Maziar Sanjabi, and Ameet S Talwalkar. 2017.
Federated multi-task learning. In Advances in Neural Information Processing
Systems. 4424â€“4434.
[29] Abdul Waheed, Muskan Goyal, Deepak Gupta, Ashish Khanna, Fadi Al-Turjman,
and PlÃ¡cido Rogerio Pinheiro. 2020. Covidgan: Data augmentation using auxiliary
classifier gan for improved covid-19 detection. IEEE Access 8 (2020), 91916â€“91923.
[30] Dawei Wang, Bo Hu, Chang Hu, Fangfang Zhu, Xing Liu, Jing Zhang, Binbin
Wang, Hui Xiang, Zhenshun Cheng, Yong Xiong, et al. 2020. Clinical characteristics of 138 hospitalized patients with 2019 novel coronavirusâ€“infected pneumonia
in Wuhan, China. Jama 323, 11 (2020), 1061â€“1069.
[31] Linda Wang and Alexander Wong. 2020. COVID-Net: A Tailored Deep Convolutional Neural Network Design for Detection of COVID-19 Cases from Chest
X-Ray Images. arXiv preprint arXiv:2003.09871 (2020).
[32] Wenlin Wang, Hongteng Xu, Zhe Gan, Bai Li, Guoyin Wang, Liqun Chen, Qian
Yang, Wenqi Wang, and Lawrence Carin. 2020. Graph-driven generative models
for heterogeneous multi-task learning. In Proceedings of the AAAI Conference on
Artificial Intelligence, Vol. 34. 979â€“988.
[33] Han Xiao, Kashif Rasul, and Roland Vollgraf. 2017. Fashion-mnist: a novel
image dataset for benchmarking machine learning algorithms. arXiv preprint
arXiv:1708.07747 (2017).
[34] Saining Xie, Ross Girshick, Piotr DollÃ¡r, Zhuowen Tu, and Kaiming He. 2017.
Aggregated residual transformations for deep neural networks. In Proceedings of
the IEEE conference on computer vision and pattern recognition. 1492â€“1500.
[35] Qian Yang, Zhouyuan Huo, Wenlin Wang, Heng Huang, and Lawrence Carin.
2019. Ouroboros: On Accelerating Training of Transformer-Based Language
Models. arXiv preprint arXiv:1909.06695 (2019).
[36] Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong. 2019. Federated machine
learning: Concept and applications. ACM Transactions on Intelligent Systems and
Technology (TIST) 10, 2 (2019), 1â€“19.
[37] Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015. Character-level convolutional
networks for text classification. In Advances in neural information processing
systems. 649â€“657.
[38] Bo Zhao, Konda Reddy Mopuri, and Hakan Bilen. 2020. iDLG: Improved Deep
Leakage from Gradients. arXiv preprint arXiv:2001.02610 (2020).
[39] Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas Chandra. 2018. Federated learning with non-iid data. arXiv preprint arXiv:1806.00582
(2018).
[40] Chuansheng Zheng, Xianbo Deng, Qing Fu, Qiang Zhou, Jiapei Feng, Hui Ma,
Wenyu Liu, and Xinggang Wang. 2020. Deep learning-based detection for COVID19 from chest CT using weak label. medRxiv (2020).
[41] Ligeng Zhu, Zhijian Liu, and Song Han. 2019. Deep leakage from gradients. In
Advances in Neural Information Processing Systems. 14747â€“14756.

