Deep learning of contagion dynamics on complex networks
Charles Murphy, Edward Laurence, and Antoine Allard

arXiv:2006.05410v3 [physics.soc-ph] 15 Jan 2021

DeÌpartement de Physique, de GeÌnie Physique, et dâ€™Optique,
UniversiteÌ Laval, QueÌbec (QueÌbec), Canada G1V 0A6 and
Centre interdisciplinaire en modeÌlisation matheÌmatique,
UniversiteÌ Laval, QueÌbec (QueÌbec), Canada G1V 0A6
(Dated: January 18, 2021)

Forecasting the evolution of contagion dynamics is still an open problem to which mechanistic models only
offer a partial answer. To remain mathematically or computationally tractable, these models must rely on simplifying assumptions, thereby limiting the quantitative accuracy of their predictions and the complexity of the
dynamics they can model. Here, we propose a complementary approach based on deep learning where the
effective local mechanisms governing a dynamic are learned from time series data. Our graph neural network
architecture makes very few assumptions about the dynamics, and we demonstrate its accuracy using different
contagion dynamics of increasing complexity. By allowing simulations on arbitrary network structures, our
approach makes it possible to explore the properties of the learned dynamics beyond the training data. Finally,
we illustrate the applicability of our approach using real data of the COVID-19 outbreak in Spain from January
to October 2020. Our results demonstrate how deep learning offers a new and complementary perspective to
build effective models of contagion dynamics on networks.
I.

INTRODUCTION

Our capacity to prevent or contain outbreaks of infectious
diseases is directly linked to our ability to accurately model
contagion dynamics. Since the seminal work of Kermack
and McKendrick almost a century ago [1], a variety of models incorporating ever more sophisticated contagion mechanisms has been proposed [2â€“5]. These mechanistic models
have provided invaluable insights about how infectious diseases spread, and have thereby contributed to the design of
better public health policies. However, several challenges remain unresolved, which call for contributions from new modeling approaches [6â€“8].
For instance, many complex contagion processes involve
the nontrivial interaction of several pathogens [9â€“12], and
some social contagion phenomena, like the spread of misinformation, require to go beyond pairwise interactions between
individuals [13â€“15]. Also, while qualitatively informative, the
forecasts of most mechanistic models lack quantitative accuracy [16]. Indeed, most models are constructed from a handful of mechanisms which can hardly reproduce the intricacies
of real complex contagion dynamics. One approach to these
challenges is to complexify the models by adding more detailed and sophisticated mechanisms. However, mechanistic
models become rapidly intractable as new mechanisms are
added. Moreover, models with higher complexity require the
specification of a large number of parameters whose values
can be difficult to infer from limited data.
There has been a recent gain of interest towards using machine learning to address the issue of the often limiting complexity of mechanistic models [12, 17â€“23]. This new kind
of approach aims at training predictive models directly from
observational time series data. These data-driven models are
then used for various tasks such as making accurate predictions [19, 21], gaining useful intuitions about complex phenomena [12] and discovering new patterns from which better mechanisms can be designed [17, 18]. While these approaches were originally designed for regularly structured

data, this new paradigm is now being applied to epidemics
spreading on networked systems [24, 25], and more generally to dynamical systems [26â€“28]. Meanwhile, the machine
learning community has dedicated a considerable amount of
attention on deep learning on graphs, structure learning and
graph neural networks (GNN) [29â€“31]. Recent works showed
great promise for GNN in the context of community detection
[32], link prediction [33], network inference [34], as well as
in the context of discovering new materials and drugs [35, 36].
These advances suggest that GNN could be prime candidates
for building effective data-driven dynamical models on networks.
In this paper, we show how GNN, usually used for structure learning, can also be used to model contagion dynamics
on complex networks. Our contribution is threefold. First, we
design a training procedure and an appropriate GNN architecture capable of representing a wide range of dynamics, with
very few assumptions. Second, we demonstrate the validity of
our approach using various contagion dynamics on networks
of different natures, with increasing complexity as well as on
real epidemiological data. Finally, we show how our approach
can provide predictions for previously unseen network structures, therefore allowing the exploration of the properties of
the learned dynamics beyond the training data. Our work
generalizes the idea of constructing dynamical models from
regularly structured data to arbitrary network structures, and
suggests that our approach could be accurately extended to
many other classes of dynamical processes.

II.

RESULTS

In our approach, we assume that an unknown dynamical process, denoted M, takes place on a known network structureâ€”or ensemble of networksâ€”, denoted G =
(V, E;Î¦, â„¦), where V = {v1 , Â· Â· Â· , vN } is the node set and
E = eij |vj is conntected to vi âˆ§ (vi , vj ) âˆˆ V 2 is the edge
set. We also assume that the network(s) can have some

2

S is the set of possible node states, and R is the set of possible node outcomes. This way of defining D allows us to
formally concatenate multiple realizations of the dynamics in
a single dataset. Additionally, the elements xi (t) â‰¡ (Xt )i
and yi (t) â‰¡ (Yt )i correspond to the state of node vi at time
t and its outcome, respectively. Typically, we consider that
the outcome yi (t) is simply the state of node vi after transitioning from state xi (t). In this case, we have S = R and
xi (t + âˆ†t) = yi (t) where âˆ†t is the length of the time steps.
However, if S is a discrete setâ€”i.e. finite and countableâ€”
yi (t) is a transition probability vector conditioned on xi (t)
from which the following
state, xi (t + âˆ†t), will be sampled.

The element yi (t) m corresponds to the probability that node
vi evolves to state m âˆˆ S given that it was previously in state
xi (t)â€”i.e. R = [0, 1]|S| . Since, in the case M is a stochastic dynamics, we do not typically have access to the transition probabilities yi (t) directly but to the observed outcome
stateâ€”e.g. xi (t + âˆ†t) in the event where X is temporally
orderedâ€”we define the observed outcome yÌƒi (t) as

(yÌƒi (t))m = Î´ xi (t + âˆ†t), m , âˆ€m âˆˆ S
(2)

where Î´(x, y) is the Kronecker delta. Finally, we assume that
M acts on Xt locally and identically at all times, according
to the structure of G. In other words, knowing the state xi as
well as the states of all the neighbors of vi , the outcome yi
is computed using a time independent function f identical for
all nodes:

yi â‰¡ f xi , Î¦i , xNi , Î¦Ni , â„¦iNi ,
(3)

where xNi = xj |vj âˆˆ Ni denotes the states of the neighbors, Ni := {vj |eij âˆˆ E} is the set of the neighbors, Î¦Ni :=
{Î¦j |vj âˆˆ Ni } and â„¦iNi := {â„¦ij |vj âˆˆ Ni }. As a result, we
impose a notion of locality where the underlying dynamics is
time invariant and invariant under the permutation of the node
labels in G, under the assumption that the node and edge attributes are left invariant.
Our objective is to build a model MÌ‚, parametrized by a
GNN with a set of tunable parameters Î˜, trained on the observed dataset D to mimic M given G, that is
MÌ‚(Xt0 , G0 ; Î˜)

â‰ˆ

M(Xt0 , G0 ) ,

(4)

for all states Xt0 and all networks G0 . The architecture of MÌ‚,
detailed in Sec. V A, is designed to act locally similarly to M.

Simple
1.0
Transition probability

metadata, taking the form of node and edge attributes denoted Î¦i = (Ï†1 (vi ), Â· Â· Â· , Ï†Q (vi )) for node vi and â„¦ij =
(Ï‰1 (eij ), Â· Â· Â· , Ï‰P (eij )) for edge eij , respectively, where Ï†q :
V â†’ R and Ï‰p : E â†’ R. These metadata can take various
forms like node characteristics or edge weights. We also denote the node and edge attribute matrices Î¦ = (Î¦i |vi âˆˆ V)
and â„¦ = (â„¦ij |eij âˆˆ E), respectively.
Next, we assume that M has generated a time series D on
G. This time series takes the form of a pair of consecutive

snapshots D = (X, Y ) with X = X1 , Â· Â· Â· , XT and Y =

Y1 , Â· Â· Â· , YT , where Xt âˆˆ S |V| is the state of the nodes at
time t, Yt âˆˆ R|V| is the outcome of M defined as

Yt = M Xt , G ,
(1)

Complex

(a)

(b)

0.8
0.6

GT
GNN
MLE
Infection
Recovery

0.4
0.2
0.0

0
25
50
75
Number of infected neighbors [`]

0
25
50
75
Number of infected neighbors [`]

FIG. 1. (color online) Predictions of GNN trained on a BarabaÌsiAlbert random network (BA) [38] for the (a) simple and (a) complex contagion dynamics. The solid and dashed lines correspond to
the transition probabilities of the dynamics used to generate the training data (labeled GT for â€œground truthâ€), and predicted by the GNN,
respectively. Markers correspond to the maximum likelihood estimation (MLE) of the transition probabilities computed from the dataset
D. The colors indicate the type of transition: infection (S â†’ I) in
blue and recovery (S â†’ I) in red. The standard deviations, as a
result of averaging the outcomes given `, are shown using a colored
area around the lines (typically narrower than the width of the lines)
and using vertical bars for the markers.

In this case, the locality is imposed by a modified attention
mechanism inspired by Ref. [37]. The advantage of imposing
locality allows our architecture to be inductive: If the GNN is
trained on a wide range of local structuresâ€”i.e. nodes with
different neighborhood sizes (or degrees) and statesâ€”it can
then be used on any other networks within that range. This
suggests that the topology of G will have a strong impact on
the quality of the trained models, an intuition that is confirmed
below. Simiarly to Eq. (3), we can write each individual node
outcome computed by the GNN using a function fË† such that

yÌ‚i â‰¡ fË† xi , Î¦i , xNi , Î¦Ni , â„¦iNi ; Î˜
(5)

where yÌ‚i is the outcome of node vi predicted by MÌ‚.
The objective described by Eq. (4) must be encoded into a
global loss function, denoted L(Î˜). Like the outcome functions, L(Î˜) can be decomposed locally, where the local losses
of each node L(yi , yÌ‚i ) are arithmetically averaged over all
possible node inputs (xi , Î¦i , xNi , Î¦Ni , â„¦iNi ), where yi and
yÌ‚i are given by Eqs (3) and (5), respectively. By using an
arithmetic mean to the evaluation of L(Î˜), we assume that
the node inputs are distributed uniformly. Consequently, the
model should be trained equally well on all of them. This is
important because in practice we only have access to a finite
number of inputs in D and G, for which the node input distribution is typically far from being uniform. Hence, in order
to train effective models, we recalibrate the inputs using the
following global loss
L(Î˜) =

X

X

tâˆˆT 0 vi âˆˆV 0 (t)


wi (t)
L yi (t), yÌ‚i (t)
0
Z

(6)

whereP
wi (t) is
Pa weight assigned to node vi at time t, and
Z 0 = tâˆˆT 0 vi âˆˆV 0 (t) wi (t) is a normalization factor. Here,

3
the training node set V 0 (t) âŠ† V and the training time set T 0 âŠ†
[1, T ] allow us to partition the training dataset for validation
and testing when required.
The choice of weights needs to reflect the importance of
each node at each time. Because we wish to lower the influence of overrepresented inputs and increase that of rare inputs,
a sound choice of weights is

âˆ’Î»
wi (t) âˆ Ï ki , xi , Î¦i , xNi , Î¦Ni , â„¦iNi

(7)

where ki is the degree of node vi in G, 0 â‰¤ Î» â‰¤ 1 is an hyperparameter. Equation (7) is an ideal choice, because it corresponds to a principled importance sampling approximation of
Eq. (6) [39], which is relaxed via the exponent Î». We obtain a
pure importance sampling scheme when Î» = 1. Note that the
weights can rarely be exactly computed using Eq. (7), because
the distribution Ï is typically computationally intensive to obtain from data, especially for continuous S with metadata. We
illustrate various ways to evaluate the weights in Sec. V B and
in Supplementary Material.
We now illustrate the accuracy of our approach by applying it to four types of synthetic dynamics of various natures
(see Sec. V C for details on the dynamics). We first consider
a simple contagion dynamics: The discrete-time susceptibleinfected-susceptible (SIS) dynamics. In this dynamics, nodes
are either susceptible (S) or infected (I) by some disease,
i.e. S = {S, I} = {0, 1}, and transition between each state
stochastically according to an infection probability function
Î±(`), where ` is the number of infected neighbors of a node,
and a constant recovery probability Î². A notable feature of
simple contagion dynamics is that susceptible nodes get infected by the disease through their infected neighbors independently. This reflects the assumption that disease transmission behaves identically whether a person has a large number
of infected neighbors or not.
Second, to relax this assumption we consider a complex
contagion dynamics with a nonmonotonic infection function
Î±(`) where the aforementioned transmission events are no
longer independent [14]. This contagion dynamics has an interesting interpretation in the context of the propagation of
a social behavior, where the local popularity of a behavior
(large `) hinders its adoption. The independent transmission
assumption can also be lifted when multiple diseases are interacting [10]. Thus, we also consider an asymmetric interacting contagion dynamics with two diseases. In this case,
S = {S1 S2 , I1 S2 , S1 I2 , I1 I2 } = {0, 1, 2, 3} where U1 V2 corresponds to a state where a node is in state U with respect to
the first disease and in state V with respect to the second disease. The interaction between the diseases occurs through a
coupling that is active only when a node is infected by at least
one disease, otherwise it behaves identically to the simple contagion dynamics. This coupling might increase or decrease the
virulence of the other disease.
Whereas the previously presented dynamics capture various features of contagion phenomena, real datasets containing
this level of detail about the interactions among individuals
are rare [40â€“42]. A class of dynamics for which dataset are
easier to find is that of mass-action metapopulation dynam-

ics [43â€“46], where the status of the individuals are gathered
by geographical regions. These dynamics typically evolve on
the weighted networks of the individualsâ€™ mobility between
regions and the state of a region consists in the number of people that are in each individual health state. As our final case,
we investigate a type of deterministic metapopulation dynamics where the population size is constant and where people
can either be susceptible (S), infected (I) or recovered from
the disease (R). As a result, we define the state of the node as
three-dimensional vectors specifying the fraction of people in
each stateâ€”i.e. S = R = [0, 1]3 .
Figure 1 shows the GNN predictions for the infection and
recovery probabilities of the simple and complex contagion
dynamics as a function of the number of infected neighbors `.
We then compare them with their ground truths, i.e. Eq. (20)
using Eqs. (18)â€“(22) for the infection functions. We also show
the maximum likelihood estimators (MLE) of the transition
probabilities computed from the fraction of nodes in state x
and with ` infected neighbors that transitioned to state y in the
complete dataset D. The MLE, which are typically used in
this kind of inference problem [47], stands as a reference to
benchmark the performance of our approach.
We find that the GNN learns remarkably well the transition
probabilities of the simple and complex contagion dynamics.
In fact, the predictions of the GNN seem to be systematically
smoother than the ones provided by the MLE. This is because
the MLE is computed for each individual pair (x, `) from disjoint subsets of the training dataset. This implies that a large
number of samples of each pair (x, `) is needed for the MLE
to be accurate; a condition rarely met in realistic settings, especially for high degree nodes. This also means that the MLE
cannot be used directly to interpolate beyond the pairs (x, `)
present in the training dataset, in sharp contrast with the GNN
which, by definition, can interpolate within the dataset D.
Furthermore, all of its parameters are hierarchically involved
during training, meaning that the GNN benefits from any sample to improve all of its predictions, which are smoother and
more consistent.
It is worth mentioning that the GNN is not specifically designed nor trained to compute the transition probabilities as a
function of a single variable, namely the number of infected `.
In reality, the GNN computes its outcome from the complete
multivariate state of the neighbors of a node. The interacting contagion and the metapopulation dynamics, unlike the
simple and complex contagions, are examples of such multivariate cases. Their outcome is thus harder to visualize in a
representation similar to Fig. 1. Figures 2(aâ€“h) addresses this
issue by comparing each of the GNN predictions yÌ‚i (t) with
its corresponding target yi (t) in the dataset D. We quantify
the global performance of the models in different scenarios,
for the different dynamics and underlying network structures,
using the Pearson correlation coefficient r between the predictions and targets (see Sec. V A). We also compute the error,
defined from the Pearson coefficient as 1 âˆ’ r, for each degree class k, i.e. between the predictions and targets of only
the nodes of degree k. This allows us to quantify the GNN
performance for every local structure.
Figures 2(iâ€“k) confirm that the GNN provides more accu-

4

FIG. 2. (color online) Comparison between the targets and the predictions of GNN trained on ErdoÌ‹s-ReÌnyi networks (ER, top row)
and on BarabaÌsi-Albert networks [38] (BA, middle row) for the (a, e, i) simple, (b, f, j) complex, (c, g, k) interacting and (d, h, l)
metapopulation dynamics. Each point shown on the panels (aâ€“h) corresponds to a different pair (yi (t), yÌ‚i (t)) in the complete dataset D.
We also indicate the Pearson coefficient r on each panel to measure the correlation between the predictions and the targets and use it as a
global performance measure. The panels (iâ€“l) show the errors (1 âˆ’ r) as a function of the number of neighbors for GNN trained on ER and
BA networks, and those of the corresponding MLE. These errors are obtained from the Pearson coefficients computed from subsets of the
prediction-target pairs where all nodes have degree k.

rate predictions than the MLE in general and across all degrees. This is especially true in the case of the interacting contagion, where the accuracy of the MLE seems to deteriorate
rapidly for large degree nodes. This is a consequence of how
scarce the inputs are for this dynamics compared to both the
simple and complex contagion dynamics for training datasets
of the same size, and of how fast the size of the set of possible inputs scales, thereby quickly rendering MLE completely
ineffective for small training datasets. The GNN, on the other
hand, is less affected by the scarcity of the data, since any
sample improves its global performance, as discussed above.
Figure 2 also exposes the crucial role of the network G
on which the dynamics evolves in the global performance of
the GNN. Namely, the heterogeneous degree distributions of
BarabaÌsi-Albert networks (BA)â€”or any scale-free network
for that matterâ€”offer, without surprise, a wider range of degrees than those of homogeneous ErdoÌ‹s-ReÌnyi networks (ER).
We can take advantage of this heterogeneity to train GNN
models that generalize well across a larger range of local
structures, as seen on Fig. 2(iâ€“l) (see also Supplementary Material). Although, for low degrees, the predictions on BA
networks are not systematically always better than those on
ER networks as shown in the interacting and metapopulation
cases. This nonetheless suggests a wide applicability of our

approach for real complex systems, whose underlying network structures recurrently exhibit a heterogeneous degree
distribution [48].
We test the trained GNN on unseen network structures
by recovering the bifurcation diagrams of the contagion and
metapopulation dynamics. In the infinite-size limit |V| â†’ âˆž,
these dynamics have two possible long-term outcomes: the
absorbing state where the diseases quickly die out, and the
endemic/epidemic state in which a macroscopic fraction of
nodes remains (endemic) or has been infected over time (epidemic) [4, 10, 49]. These possible long-term outcomes exchange stability during a phase transition which is continuous
for the simple contagion and metapopulation dynamics, and
discontinuous for the complex and interacting contagion dynamics. The position of the phase transition depends on the
parameters of the dynamics as well as on the topology of the
network. Note that for the interacting contagion dynamics, the
stability of absorbing and endemic states do not change at the
same point, giving rise to a bistable regime where both states
are stable.
Figure 3 shows the different bifurcation diagrams obtained
by performing numerical simulations with the trained GNN
models [using Eq. (5)] while varying the average degree of
networks on which the GNN has not been trained. Quantita-

5

FIG. 3. Bifurcation diagrams of the (a) simple, (b) complex, (c) interacting and (d) metapopulation dynamics on Poisson networks [38]
composed of |V| = 2000 nodes with different average degrees hki. The prevalence is defined as the average fraction of nodes that are
asymptotically infected by at least one disease and the outbreak size corresponds to the average fraction of nodes that have recovered. Those
are computed from numerical simulations using the â€œground truthâ€œ (GT) dynamics (circles in blue) and the GNN trained on BarabaÌsi-Albert
networks (triangles in orange). The error bars correspond to the standard deviations of these numerical simulations. The trained GNN used are
the same ones as those used for Fig. 2. As a reference, we also indicate with dashed lines the value(s) of average degree hki corresponding to
the network(s) on which the GNN were trained. On panel (d), more than one value of hki appear as multiple networks with different average
degrees were used to train the GNN.

tively, the predictions are also strikingly accurateâ€”essentially
perfect for the simple and complex contagion dynamicsâ€”
which is remarkable given that the bifurcation diagrams were
obtained on networks the GNN had never seen before. On the
one hand, it illustrates how insights can be gained about the
underlying process concerning the existence of phase transitions and their order, among other things. On the other hand,
it suggests how the GNN can be used for diverse applications,
such as predicting the dynamics under various network structures (e.g. designing intervention strategies that affect the way
individuals interact and are therefore connected).
Finally, to further substantiate the applicability of our approach, we illustrate the performance of a GNN model trained
on a dataset of the COVID-19 outbreak in Spain from March
to October 2020 (see Sec. V D for further details). As seen in
Fig. 4(câ€“k), the performance results indicate that the model
predicts correctly the number of susceptible and recovered
people. Additionally, the GNN predictions for the number
of infected people are highly correlated with the ground truth,
even though they are far from perfectly accurate, which may
be due to the low prevalence of the disease in the dataset. In
Fig. 4(b), we also compare the predictions of the GNN with
a metapopulation model with a simple contagion infection
mechanism (see Sec. V C 4). The parameters of the metapopulation model, which have been fixed to fit the specifics of
the COVID-19 disease (see Sec. V D), have also been used in
a previous work to evaluate the efficacy of the containment
measures in Spain [50]. Interestingly, the GNN seems to predict correctly the daily outbreak size, whereas the metapopulation model overestimate by far the outbreak size. This
could be due to the social distancing and confinement measures in place during that period that fortunately diminished
dramatically the observed number of cases. In fact, the interaction between the epidemic and the social dynamics (e.g.
confinements) generated a dynamical process that, unlike the
metapopulation model, is probably closer to a complex contagion phenomena than to a simple one [12]. In turn, our results
suggest that simple contagion models are not appropriate to

accurately predict the evolution of the COVID-19 pandemic
over a long period of time, and that complex contagion models could provide better effective models.

III.

DISCUSSION

We introduced a data-driven approach that learns effective
mechanisms governing the propagation of diverse dynamics
on complex networks. We proposed a reliable training protocol, and we validated the projections of our GNN architecture on simple, complex, interacting contagion and metapopulation dynamics using synthetic networks. Interestingly, we
found that our approach performs better when trained on data
whose underlying network structure is heterogeneous, which
could prove useful in real-world applications of our method
given the ubiquitousness of scale-free networks [51].
By recovering the bifurcation diagram of various dynamics, we illustrated how our approach can leverage time series
from an unknown dynamical process to gain insights about its
propertiesâ€”e.g. the existence of a phase transition and its order. We have also shown how to use this framework on real
datasets, which in turn could then be used to help build better
effective models. In a way, we see this approach as the equivalent of a numerical Petri dishâ€”offering a new way to experiment and gain insights about an unknown dynamicsâ€”that is
complementary to traditional mechanistic modeling to design
better intervention procedures, containment countermeasures
and to perform model selection.
Although we focused the presentation of our method on
contagion dynamics, its potential applicability reaches many
other realms of complex systems modeling where intricate
mechanisms are at play. We believe this work establishes solid
foundations for the use of deep learning in the design of realistic effective models of complex systems.

6

FIG. 4. (color online) GNN Training of the Spain COVID-19 outbreak dataset on the multiplex weighted network of people mobility
between Spain 52 provinces: (a) Spain mobility multiplex network, (b) daily predictions of the number of infected people, (câ€“k)
detailed performance scatter plots for the training (c-f-i), validation (d-g-j) and testing (e-h-k) datasets. In all panels, the ground truth
(GT) values correspond to the values as they appear in the dataset. On panel (a), the thickness of the edges are proportional to the average
number of people transitioning between all connected node pairs, and the size of the nodes are proportional to the size of the population
Ni within the corresponding province. On pabel (b), we show the evolution of the GT outbreak size (solid line) and compare it with the
outbreak size predicted by the GNN (dashed line) and an equivalent metapopulation model (dash-dotted line). The predictions, rather than
being trajectories, are computed using the GT states of the previous time step, which explains the seemingly inconsistent decrease of the
outbreak size over time predicted by the metapopulation model. This is due to a systematic overestimation of the metapopulation model with
respect to the GT. We use different scales for the GT/GNN predictions (blue axis on the left) and metapopulation model ones (red axis on the
right) to improve the visualization. On panels (câ€“k), we show the accuracy as obtained from comparing the target and prediction outcomes.
We use the number of people in the different states to represent these outcomes, that we compute from the product of the population sizes Ni
with the GT and the GNN outcomesâ€”yi (t) and yÌ‚i (t), respectively, which we recall correspond to probability vectors of the fraction of nodes
in each state. We also separated the target-prediction pairs in columns with respect the states susceptible (câ€“e), infected (fâ€“h) and recovered
(iâ€“k).
IV.

V.

ACKNOWLEDGEMENTS AND SUPPORT
A.

The authors are grateful to Emily M. Cyr and Guillaume
St-Onge for their comments and to Vincent Thibeault, Xavier
Roy-Pomerleau, FrancÌ§ois Thibault, Patrick Desrosiers, Louis
J. DubeÌ, Simon Hardy and Laurent HeÌbert-Dufresne for fruitful discussions. They also want to thank the anonymous reviewers for their insightful comments which led to substantial
improvements. This work was supported by the Sentinelle
Nord initiative from the Canada First Research Excellence
Fund (CM, EL, AA), the Conseil de recherches en sciences
naturelles et en geÌnie du Canada (CM, AA) and the Fonds de
recherche du QueÌbec-Nature et technologie (EL).

MATERIAL AND METHODS

Graph neural network and training details

In this section, we briefly present our GNN architecture, the
training settings, the synthetic data generation procedure and
the hyperparmeters used in our experiments.

1.

Architecture

We use the GNN architecture shown in Fig. 5 and detailed
in Tab. I. First, we transform the state xi of every node with
a shared multilayer perception (MLP), denoted fË†in : S â†’ Rd

7

...

...

FIG. 5. (color online) Visualization of the GNN architecture. The blocks of different colors represent mathematical operations. The red
blocks correspond to trainable affine transformation parametrized by weights and biases. The purple blocks represent activation functions
between each layer. The core of the model is the attention module [37], which is represented in blue. The orange block at the end is an
exponential Softmax activation that transforms the output into properly normalized outcomes.

where d is the resulting number of node features, such that
Î¾i = fË†in (xi ) .

(8)

We concatenate the node attributes Î¦i to xi , when these attributes are available, in which case fË†in : S Ã— RQ â†’ Rd .
At this point, Î¾i is a vector of features representing the state
(and attributes) of node vi . Then, we aggregate the features of
the first neighbors using a modified attention mechanism fË†att ,
inspired by Ref. [37] (see Sec. V A 2),
Î½i = fË†att (Î¾i , Î¾Ni ) ,

(9)

where we recall that Ni = {vj |eij âˆˆ E} is the set of nodes
connected to node vi . We also include the edge attributes â„¦ij
into the attention mechanism, when they are available. To do
so, we transform the edge attributes â„¦ij into abstract edge features, such that Ïˆij = fË†edge (â„¦ij ) where fË†edge : RP â†’ Rdedge
is also a MLP, before they are used in the aggregation. Finally,
we compute the outcome yÌ‚i of each node vi with another MLP
fË†out : Rd â†’ R such that
yÌ‚i = fË†out (Î½i ) .

2.

(10)

Attention Mechanism

We use an attention mechanism inspired by the graph attention network architecture (GAT) [37]. The attention mechanism consists of three trainable functions A : Rd â†’ R,
B : Rd â†’ R and C : Rdedge â†’ R, that combine the feature vectors Î¾i , Î¾j and Ïˆij of a connected pair of nodes vi and
vj , where we recall that d and dedge are the number of node
and edge features, respectively. Then, the attention coefficient
aij is computed as follows:
h


i
aij = Ïƒ A Î¾i + B Î¾j + C Ïˆij
(11)

where Ïƒ(x) = [1 + eâˆ’x ]âˆ’1 is the logistic function. Notice
that, by using this logistic function, the value of the attention

coefficients is constrained to the open interval (0, 1), where
aij = 0 implies that the state of vj has no effect on the outcome of vi and aij = 1 implies the effect is maximal. As a
result, aij quantifies the influence of the state of node vj over
the outcome of node vi , which also adds to the interpretability
of our model. We compute the aggregated feature vectors Î½i
of node vi such that

Î½i = fË†att (Î¾i , Î¾Ni ) = Î¾i +

X

aij Î¾j .

(12)

vj âˆˆNi

It is important to stress that, at this point, Î½i contains some information about vi and all of its neighbors in a pairwise manner. In all our experiments, we fix A, B, and C to be affine
transformations with trainable weight matrix and bias vector.
Also, we use multiple attention modules in parallel to increase
the expressive power of the GNN architecture, as suggested by
Ref. [37].
The attention mechanism described by Eq. (11) is slightly
different from the vanilla version of Ref. [37]. Similarly to
other well-known GNN architectures [33, 52, 53], the aggregation scheme of the vanilla GAT is designed as an average
of
P the feature vectors of the neighborsâ€”where, by definition,
vj âˆˆNi aij = 1 for all vi â€”rather than as a general weighted
sum like for Eq. (12). This is often reasonable in the context of
structure learning, where the node features represent some coordinates in a metric space where connected nodes are likely
to be close [33]. Yet, in the general case, this type of constraint
was shown to lessen dramatically the expressive power of the
GNN architecture [31]. We also reached the same conclusion
while using average-like GNN architectures (see the Supplementary Material). By contrast, the aggregation scheme described by Eq. (12) allows our architecture to represent various dynamic processes correctly on networks.

8
Dynamics

Simple

Complex

Interacting

Input layers

Linear(1, 32)
ReLU
Linear(32, 32)
ReLU

Linear(1, 32)
ReLU
Linear(32, 32)
ReLU

Linear(1, 32)
ReLU
Linear(32, 32)
ReLU
Linear(32, 32)
ReLU

Number of attention layers

2

2

4

Output layers

Linear(32, 32)
ReLU
Linear(32, 2)
Softmax

Linear(32, 32)
ReLU
Linear(32, 2)
Softmax

Linear(32, 32)
ReLU
Linear(32, 32)
ReLU
Linear(32, 4)
Softmax

Number of parameters

6 698

6 698

11 188

Metapopulation
Linear(4, 32)âˆ—
ReLU
Linear(32, 32)
ReLU
Linear(32, 32)
ReLU
Linear(32, 32)
ReLU
8**
Linear(32, 32)
ReLU
Linear(32, 32)
ReLU
Linear(32, 32)
ReLU
Linear(32, 3)
Softmax
99 883

TABLE I. Layer by layer description of the GNN models for each dynamics. For each sequence, the operations are applied from top
to bottom. The operations represented by Linear(m, n) correspond to linear (or affine) transformations of the form f (x) = W x + b,
where x âˆˆ Rm is the input, W âˆˆ RnÃ—m and b âˆˆ Rn are trainable parameters. The operations ReLU and Softmax are activation
. (*) Here, the dimension of the input is 4, because we confunctions given by ReLU(x) = max {x, 0} and Softmax(x) = Pexp(x)
i exp(xi )
catenated with three-dimensional vector state with the rescaled and centered population size Ni . (**) Because the networks are weighted
for the metapopulation dynamics, we initially
transform the edge weights into abstract feature representations using a sequence of layers,

i.e. Linear(1, 32), ReLU, Linear(1, 32) applied from left to right, before using them in the attention modules. These layers are trained
alongside all the other layers.

3.

4.

Training settings

In all experiments, we use the cross entropy loss as the local
loss function,
X

L yi , yÌ‚i = âˆ’
yi,m log yÌ‚i,m ,

(13)

m

where yi,m corresponds to the m-th element of the outcome
vector of node vi , which either is a transition probability for
the stochastic contagion dynamics or a fraction of people for
the metapopulation dynamics. For the simple, complex and
interacting contagion dynamics, we used the observed outcomes yÌƒi , corresponding to the stochastic state of node vi at
the next time step, as the target in the loss function. While we
noticed a diminished performance when using the observed
outcomes as opposed to the true transition probabilities (see
Supplementary Material), this setting is more realistic and
shows what happens when the targets are noisy. The effect of
noise can be tempered by increasing the size of the dataset (see
the Supplementary Material). For the metapopulation dynamics, since this model is deterministic, we used the true targets
without adding noise.

Performance measures

We use the Pearson correlation coefficient r as a global performance measure defined on a set of targets Y and predictions YÌ‚ as


E (Y âˆ’ E[Y ])(YÌ‚ âˆ’ E[YÌ‚ ])
r= q 
(14)
 

E (Y âˆ’ E[Y ])2 E (YÌ‚ âˆ’ E[YÌ‚ ])2

where E[W ] denotes the expectation of W . Also, because the
maximum correlation occurs at r = 1, we also define 1 âˆ’ r as
the global error on the set of target-prediction pairs.
5.

Synthetic data generation

We generate data from each dynamics using the following
algorithm:
1. Sample a graph G from a given generative model (e.g.
the ErdoÌ‹s-ReÌnyi G(N, M ) or the BarabaÌsi-Albert network models).

2. Initialize the state of the system X(0) = xi (0) i=1..N .
For the simple, complex and interacting contagion dynamics, sample uniformly the number of nodes in each

9
state. For the metapopulation dynamics, sample the
population size for each node from a Poisson distribution of average 104 and then sample the number of
infected people within each node from a binomial distribution of parameter 10âˆ’5 . For instance, a network
of |V| = 103 nodes will be initialized with a total of
100 infected people, on average, distributed among the
nodes.
3. At time t, compute the observed outcomeâ€”Yt for the
metapopulation dynamics, and YÌƒt for the three stochastic dynamics. Then, record the states Xt and Yt .
4. Repeat step 3 until (t mod ts ) = 0, where ts is a resampling time. At this moment, apply step 2 to reinitialize the states Xt and repeat step 3.
5. Stop when t = T , where T is the targeted number of
samples.
The resampling step parametrized by ts indirectly controls the
diversity of the training dataset. We allow ts to be small for
the contagion dynamics (ts = 2) and larger for the metapopulation dynamics (ts = 100) to emphasize on the performance
of the GNN rather than the quality of the training dataset,
while acknowledging that different values of ts could lead to
poor training (see Supplementary Material).
We trained the simple, complex and interacting contagion
GNN models on networks of size |V| = 103 nodes and on
time series of length T = 104 . To generate the networks, we
either used ErdoÌ‹s-ReÌnyi (ER) random networks G(N, M ) or
BarabaÌsi-Albert (BA) random networks. In both cases, the
parameter of the generative network models are chosen such
that the average degree is fixed to hki = 4.
To train our models on the metapopulation dynamics, we
generated 10 networks of |V| = 100 nodes and generated for
each of them time series of ts = 100 time steps. This number of time steps roughly corresponds to the moment where
the epidemic dies out. Simiarly to the previous experiments,
we used the ER and the BA models to generate the networks,
where the parameters were chosen such that hki = 4. However, because this dynamics is not stochastic, we varied the
average degree of the networks to increase the variability in
the time series. This was doneby randomly removing a fraction p = 1 âˆ’ ln 1 âˆ’ Âµ + eÂµ of their edges, where Âµ was
sampled for each network uniformly between 0 and 1. In this
scenario, the networks were directed and weighted, with each
edge weight eij being uniformly distributed between 0 and 1.
6.

Hyperparameters

The optimization of the parameters was performed using
the rectified Adam algorithm [54], which is hyperparameterized by b1 = 0.9 and b2 = 0.999, as suggested in Ref. [54].
To build a validation dataset, we selected a fraction of the
node states randomly for each time step. More specifically,
we chose node vi at time t proportionally to its importance
weight wi (t). For all experiments on synthetic dynamics, we

randomly selected 10 nodes to be part of the validation set, on
average. For all experiments, the learning rate  was reduced
by a factor 2 every 10 epochs with initial value 0 = 0.001. A
weight decay of 10âˆ’4 was used as well to help regularize the
training. We trained all models for 30 epochs, and selected the
GNN model with the lowest loss on validation datasets. We
fixed the importance sampling bias exponents for the training
to Î» = 0.5 in the simple, complex and interacting contagion
cases, and fixed it to Î» = 1 in the metapopulation case.
B.

Importance weights

In this section, we show how to implement the importance
weights in the different cases. Other versions of the importance weights are also available in the Supplementary Material.
1.

Discrete state stochastic dynamics

When S is a finite countable set, the importance weights
can be computed exactly using Eq. (7),
h
iâˆ’Î»
wi (t) âˆ Ï ki , xi (t), xNi (t)
(15)


where Ï k, x, xN is the probability to observe a node of degree k in state x with a neighborhood in state xN in the complete dataset D. When S is a discrete and countable set, inputs
can be simplified from (k, x, xN ) to (k, x, `) without loss of
generality, where ` is a vector whose entries are the number
of neighbors in each state. The distribution is then estimated
from the complete dataset D by computing the fraction of inputs that are in every configuration
|V|

1 X 
Ï(k, x, `) =
I ki = k
|V|T i=1

Ã—

T

 

X
I xi (t) = x I `i (t) = `
(16)
t=1

where I(Â·) is the indicator function.
2.

Continuous state deterministic dynamics

The case of continuous statesâ€”e.g. for metapopulation
dynamicsâ€”is more challenging than its discrete counterpart,
especially if the node and edge attributes, Î¦i and â„¦ij , need
to be accounted for. One of the challenges is that we cannot count the inputs like in the previous case. As a result,
the estimated distribution Ï cannot be estimated directly using
Eq. (16), and we use instead

âˆ’Î»
wi (t) = P (ki ) Î£(Î¦i , â„¦i |ki ) Î  xÌ„(t)
(17)

where P (ki ) is the fraction of nodes with degree ki ,
Î£(Î¦i , â„¦i |ki ) is the joint probability density function (pdf)

10
conditioned on the degree ki for theP
node attributes Î¦i and
the sum of the edge attributes â„¦i â‰¡ vj âˆˆNi â„¦ij , and where

Î  xÌ„(t) is the pdf for the average of node states at time t
P
1
xÌ„(t) = |V|
vi âˆˆV xi (t). The pdf are obtained using Gaussian
kernel density estimators (KDE) [55]. Provided that the density values of the KDE are unbounded above, we normalize
the pdf such that the density of each sample used to construct
the KDE sum to one. Further details on how we designed the
importance weights are provided in the Supplementary Material.

C.

Dynamics

In what follows, we describe in details the contagion dynamics used for our experiments. We specify the node outcome function f introduced in Eq. (3) and the parameters of
the dynamics.

1.

Simple contagion

We consider the simple contagion dynamics called the
susceptible-infected-susceptible (SIS) dynamics for which
S = {S, I} = {0, 1}â€”we use these two representations of
S interchangeably. Because this dynamics is stochastic, we
let R = [0, 1]2 . We define the infection function Î±(`) as the
probability that a susceptible node becomes infected given its
number of infected neighbors `
Pr(S â†’ I|`) = Î±(`) = 1 âˆ’ (1 âˆ’ Î³)` ,

(18)

where Î³ âˆˆ [0, 1] is the disease transmission probability. In
other words, a node can be infected by any of its infected
neighbors independently with probability Î³. We also define
the constant recovery probability as
(19)

Pr(I â†’ S) = Î² .

The node outcome function for the SIS dynamics is therefore
f (xi , xNi ) =

(

1 âˆ’ Î±(`i ), Î±(`i )

(Î², 1 âˆ’ Î²)



if xi = 0,
if xi = 1,

(20)

where
`i =

X

vj âˆˆNi

Î´(xj , 1)

(21)

is the number of infected neighbors of vi and Î´(x, y) is the
Kronecker delta. Note that for each case in Eq. (20), the
outcome is a two-dimensional probability vector, where the
first entry is the probability that node vi becomes/remains
susceptible at the following time step, and the second entry
is the probability that it becomes/remains infected. We used
(Î³, Î²) = (0.04, 0.08) in all experiments involving this simple
contagion dynamics.
2.

Complex contagion

To lift the independent transmission assumption of the
SIS dynamics, we consider a complex contagion dynamics
for which the node outcome function has a similar form as
Eq. (20), but where the infection function Î±(`) has the nonmonotonic form
Î±(`) =

1
`3
`/Î·
z(Î·) e âˆ’ 1

(22)

where z(Î·) normalizes the infection function such that
Î±(`âˆ— ) = 1 at its global maximum `âˆ— and Î· > 0 is a parameter controlling the position of `âˆ— . This function is inspired by
the Planck distribution for the black-body radiation, although
it was chosen for its general shape rather than for any physical meaning whatsoever. We used (Î·, Î²) = (8, 0.06) in all
experiments involving this complex contagion dynamics.
3.

Interacting contagion

We define the interacting contagion as two SIS dynamics that are interacting and denote it as the SIS-SIS dynamics. In this case, we have S = {S1 S2 , I1 S2 , S1 I2 , I1 I2 } =
{0, 1, 2, 3}. Simiarly to the SIS dynamics, we have R =
[0, 1]4 and we define the infection probability functions
Î±g (`g ) = 1 âˆ’ (1 âˆ’ Î³g )`g

Î±gâˆ— (`g )

= 1 âˆ’ (1 âˆ’ Î¶Î³g )

`g

if x = 0

(23a)

if x = 1, 2 ,

(23b)

where Î¶ â‰¥ 0 is a coupling constant and `g is the number of
neighbors infected by disease g, and also define the recovery probabilities Î²g for each disease (g = 1, 2). The case
where Î¶ > 1 corresponds to the situation in which the diseases are synergistic (i.e. being infected by one increases the
probability of getting infected by the other), whereas competition is introduced if Î¶ < 1 (being already infected by one
decreases the probability of getting infected by the other). The
case Î¶ = 1 falls back on two independent SIS dynamics that
evolve simultaneously on the network. The outcome function
is composed of 16 entries that are expressed as follows

11

ï£± 




 

ï£´
ï£´ 1 âˆ’ Î±1 (`i,1 ) 1 âˆ’ Î±2 (`i,2 ) , Î±1 (`i,1 ) 1 âˆ’ Î±2 (`i,2 ) , 1 âˆ’ Î±1 (`i,1 ) Î±2 (`i,2 ), Î±1 (`i,1 )Î±2 (`i,2 )
ï£´
ï£´
 

ï£´
 




ï£´
ï£²
Î²1 1 âˆ’ Î±2âˆ— (`i,2 ) , 1 âˆ’ Î²1 1 âˆ’ Î±2âˆ— (`i,2 ) , Î²1 Î±2âˆ— (`i,2 ), 1 âˆ’ Î²1 Î±2âˆ— (`i,2 )
f (xi , xNi ) = 






ï£´
1 âˆ’ Î±1âˆ— (`i,1 ) Î²2 , Î±1âˆ— (`i,1 )Î²2 , 1 âˆ’ Î±1âˆ— (`i,1 ) 1 âˆ’ Î²2 , Î±1âˆ— (`i,1 ) 1 âˆ’ Î²2
ï£´
ï£´
ï£´


ï£´
ï£´
ï£³ Î²1 Î²2 , [1 âˆ’ Î²1 ]Î²2 , Î²1 [1 âˆ’ Î²2 ], [1 âˆ’ Î²1 ][1 âˆ’ Î²2 ]
where we define `i,g as the number of neighbors of vi that
are infected by disease g. We used (Î³1 , Î³2 , Î²1 , Î²2 , Î¶) =
(0.01, 0.012, 0.19, 0.22, 50) in all experiments involving this
interacting contagion dynamics.
4.

Metapopulation

The metapopulation dynamics considered is a deterministic
version of the susceptible-infection-recovered (SIR) metapopulation model [43â€“46]. We consider that the nodes are populated by a fixed number of people Ni , which can be in
three statesâ€”susceptible (S), infected (I) or recovered (R).
We therefore track the number of people in every state at each
time. Furthermore, we let the network G be weighted, with
the weights describing the mobility flow of people between
regions. In this case, â„¦ij âˆˆ R is the average number of people that are traveling from node vj to node vi . Finally, because
we assume that the population size is on average steady, we let
Î¦i = Ni be a node attribute and work with the fraction of people in every epidemiological state. More precisely, we define
the state of node vj by xj = (sj , ij , rj ), where sj , ij and rj
are the fractions of susceptible, infected and recovered people, respectively. From these definitions, we define the node
outcome function of this dynamics as
ï£«
ï£¶
sj âˆ’ sj Î±Ìƒj
ï£¬
ï£·
i
f (xj , xNj , G) = ï£­ ij âˆ’ Ï„jr + sj Î±Ìƒj ï£¸
(25)
ij
rj + Ï„r
where

Î±Ìƒj = Î±(ij , Nj ) +

X kj â„¦jl Î±(il , Nl )
P
,
vn âˆˆNj â„¦jn

(26)

vl âˆˆNj

and kj is the degree of node vj . The function Î±(i, N ) corresponds to the infection rate, per day, at which an individual is
infected by someone visiting from a neighboring region with
iN infected people in it, and is equal to

iN
R0
R0
Î±(i, N ) = 1 âˆ’ 1 âˆ’
â‰ˆ 1 âˆ’ eâˆ’ Ï„ r i .
Ï„r N

(27)

where R0 corresponds to the reproduction number and, Ï„r is
the average recovery time in days. In all experiments with this
metapopulation dynamics, we used (R0 , Ï„r ) = (8.31, 7.5).

D.

if xi = 0,
if xi = 1,
if xi = 2,
if xi = 3.
(24)

COVID-19 outbreak in Spain
1.

Dataset

We use the time series of the COVID-19 outbreak in Spain.
The dataset is originally composed of the daily incidence
number of the 52 Spanish provinces monitored for 262 days
between February 1st 2020 and October 20th 2020 [56], and
coupled with the origin destination (OD) network of individual mobility [57]. The network is multiplex, directed and
weighted, where the weight of each edge eÎ½ij represents mobility flow from province vj and to province vj using transportation Î½. The metadata associated to each node is the population
of province vi [58], noted Î¦i = Ni . The metadata associated
to each edge, â„¦Î½ij , corresponds to the average number of people that moved from vj to vi using Î½ as the main means of
transportation.
Because the raw dataset only includes the incidence number, that is the number of new infected cases per day, we artificially include a recovery mechanism where the average recovery time Ï„r = 7.5 days, which was used in previous studies
involving COVID-19 in Spain [50].
2.

Models

We consider a GNN model that is essentially identical to
the one used for the metapopulation dynamics (see Tab. I),
with the exception that we used different attention modules for
each type of edges. Then, to combine the features of each multiplex layer, we simply average pooled the features together.
For the metapopulation model, we fixed R0 = 2.5 and
Ï„r = 7.5. These values were used in Ref. [50] to evaluate
the efficacy of the containment measures in Spain.
3.

Training

To train the model on the COVID-19 dataset, we use the
same hyperparameters values that we used for the metapopulation experiment, which are listed in Sec. V A 3. The only
differences were the value of the exponent bias, which we
fixed to Î» = 0.75. Similarly an earlier discussion in Sec. V A,
we constructed the validation dataset by randomly selecting a
partition of the nodes at each time step proportionally to their
importance weights wi (t). Analogously, the test dataset was

12
constructed by uniformly selecting the partitions of nodes at
each time step. We selected on average 10% of the complete

dataset to build the validation set and used 30% to build the
testing set, thus leaving 60% of the samples for training set.

[1] W. O. Kermack and A. G. McKendrick, â€œA Contribution to the
Mathematical Theory of Epidemics,â€ Proc. R. Soc. A 115, 700â€“
721 (1927).
[2] H. W. Hethcote, â€œThe Mathematics of Infectious Diseases,â€
SIAM Rev. 42, 599â€“653 (2000).
[3] C. I. Siettos and L. Russo, â€œMathematical modeling of infectious disease dynamics,â€ Virulence 4, 295â€“306 (2013).
[4] I. Z. Kiss, J. C. Miller, and P. L. Simon, Mathematics of Epidemics on Networks (Springer, 2017) p. 598.
[5] F. Brauer, C. Castillo-Chavez, and Z. Feng, Mathematical
Models in Epidemiology (Springer, 2019).
[6] N. C. Grassly and C. Fraser, â€œMathematical models of infectious disease transmission,â€ Nat. Rev. Microbiol. 6, 477â€“487
(2008).
[7] A. Pastore y Piontti, N. Perra, L. Rossi, N. Samay, and
A. Vespignani, Charting the Next Pandemic: Modeling Infectious Disease Spreading in the Data Science Age (Springer,
2019).
[8] C. Viboud and A. Vespignani, â€œThe future of influenza forecasts,â€ Proc. Natl. Acad. Sci. U.S.A. 116, 2802â€“2804 (2019).
[9] D. M. Morens, J. K. Taubenberger, and A. S. Fauci, â€œPredominant Role of Bacterial Pneumonia as a Cause of Death in
Pandemic Influenza: Implications for Pandemic Influenza Preparedness,â€ J. Infect. Dis. 198, 962â€“970 (2008).
[10] J. Sanz, C.-Y Xia, S. Meloni, and Y. Moreno, â€œDynamics of
Interacting Diseases,â€ Phys. Rev. X 4, 041005 (2014).
[11] S. Nickbakhsh, C. Mair, L. Matthews, R. Reeve, P. C. D. Johnson, F. Thorburn, B. von Wissmann, A. Reynolds, J. McMenamin, R. N. Gunson, and P. R. Murcia, â€œVirusâ€“virus interactions impact the population dynamics of influenza and the common cold,â€ Proc. Natl. Acad. Sci. U.S.A. 116, 27142â€“27150
(2019).
[12] L. HeÌbert-Dufresne, S. V. Scarpino, and J.-G. Young, â€œMacroscopic patterns of interacting contagions are indistinguishable
from social reinforcement,â€ Nat. Phys. 16, 426â€“431 (2020).
[13] D. Centola, â€œThe Spread of Behavior in an Online Social Network Experiment,â€ Science 329, 1194â€“1197 (2010).
[14] S. Lehmann and Y.-Y. Ahn, eds., Complex Spreading Phenomena in Social Systems, Computational Social Sciences
(Springer, 2018).
[15] I. Iacopini, G. Petri, A. Barrat, and V. Latora, â€œSimplicial models of social contagion,â€ Nat. Commun. 10, 2485 (2019).
[16] M. Biggerstaff, M. Johansson, D. Alper, L. C. Brooks,
P. Chakraborty, D. C. Farrow, S. Hyun, S. Kandula, C. McGowan, N. Ramakrishnan, R. Rosenfeld, J. Shaman, R. Tibshirani, R. J. Tibshirani, A. Vespignani, W. Yang, Q. Zhang, and
C. Reed, â€œResults from the second year of a collaborative effort
to forecast influenza seasons in the United States,â€ Epidemics
24, 26â€“33 (2018).
[17] S. L. Brunton, J. L. Proctor, and J. N. Kutz, â€œDiscovering governing equations from data by sparse identification of nonlinear
dynamical systems,â€ Proc. Natl. Acad. Sci. USA 113, 3932â€“
3937 (2016).
[18] J. N. Kutz, â€œDeep learning in fluid dynamics,â€ J. Fluid Mech.
814, 1â€“4 (2017).
[19] J. Pathak, Z. Lu, B. R. Hunt, M. Girvan, and E. Ott, â€œUsing
machine learning to replicate chaotic attractors and calculate

Lyapunov exponents from data,â€ Chaos 27, 121102 (2017).
[20] B. Lusch, J. N. Kutz, and S. L. Brunton, â€œDeep learning for
universal linear embeddings of nonlinear dynamics,â€ Nat. Commun. 9, 1â€“10 (2018).
[21] J. Pathak, B. Hunt, M. Girvan, Z. Lu, and E. Ott, â€œModel-Free
Prediction of Large Spatiotemporally Chaotic Systems from
Data: A Reservoir Computing Approach,â€ Phys. Rev. Lett. 120,
024102 (2018).
[22] B. M. de Silva, D. M. Higdon, S. L. Brunton, and J. N. Kutz,
â€œDiscovery of Physics From Data: Universal Laws and Discrepancies,â€ Front. Artif. Intell. 3, 25 (2020).
[23] X. Chen, T. Weng, H. Yang, C. Gu, J. Zhang, and M. Small,
â€œMapping topological characteristics of dynamical systems into
neural networks: A reservoir computing approach,â€ Phys. Rev.
E 102, 033314 (2020).
[24] R. Dutta, A. Mira, and J.-P. Onnela, â€œBayesian inference
of spreading processes on networks,â€ Proc. R. Soc. A 474,
20180129 (2018).
[25] C. Shah, N. Dehmamy, N. Perra, M. Chinazzi, A.-L. BarabaÌsi,
A. Vespignani, and R. Yu, â€œFinding Patient Zero: Learning Contagion Source with Graph Neural Networks,â€ (2020),
arXiv:2006.11913.
[26] F. A. Rodrigues, T. Peron, C. Connaughton, J. Kurths,
and Y. Moreno, â€œA machine learning approach to predicting dynamical observables from network structure,â€ (2019),
arXiv:1910.00544.
[27] A. Salova, J. Emenheiser, A. Rupe, J. P. Crutchfield, and R. M.
Dâ€™Souza, â€œKoopman operator and its approximations for systems with symmetries,â€ Chaos 29, 093128 (2019).
[28] E. Laurence, C. Murphy, G. St-Onge, X. Roy-Pomerleau, and
V. Thibeault, â€œDetecting structural perturbations from time series with deep learning,â€ (2020), arXiv:2006.05232.
[29] Z. Zhang, P. Cui, and W. Zhu, â€œDeep Learning on Graphs: A
Survey,â€ (2018), arXiv:1812.04202.
[30] J. Zhou, GeÌ Cui, Z. Zhang, C. Yang, Z. Liu, L. Wang, C. Li,
and M. Sun, â€œGraph Neural Networks: A Review of Methods
and Applications,â€ (2018), arXiv:1812.08434.
[31] K. Xu, W. Hu, J. Leskovec, and S. Jegelka, â€œHow Powerful are
Graph Neural Networks?â€ (2018), arXiv:1810.00826.
[32] B. Perozzi, R. Al-Rfou, and S. Skiena, â€œDeepWalk: Online Learning of Social Representations,â€ Proc. ACM SIGKDD
Int. Conf. Knowl. Discov. Data Min. , 701â€“710 (2014),
arXiv:1403.6652.
[33] W. L. Hamilton, R. Ying, and J. Leskovec, â€œRepresentation
Learning on Graphs: Methods and Applications,â€ (2017),
arXiv:1709.05584.
[34] Z. Zhang, Y. Zhao, J. Liu, S. Wang, R. Tao, R. Xin, and
J. Zhang, â€œA general deep learning framework for network reconstruction and dynamics learning,â€ Appl. Netw. Sci. 4, 110
(2019).
[35] A. Fout, J. Byrd, B. Shariat, and A. Ben-Hur, â€œProtein Interface Prediction using Graph Convolutional Networks,â€ in Adv.
Neural Inf. Process. Syst. 30 (2017) pp. 6530â€“6539.
[36] M. Zitnik, M. Agrawal, and J. Leskovec, â€œModeling polypharmacy side effects with graph convolutional networks,â€ in Bioinformatics, Vol. 34 (2018) pp. i457â€“i466.

13
[37] P. VelicÌŒkovicÌ, G. Cucurull, A. Casanova, A. Romero, P. LioÌ€,
and Y. Bengio, â€œGraph attention networks,â€ in International
Conference on Learning Representations (2018).
[38] A.-L. BarabaÌsi, Network Science (Cambridge University Press,
2016) p. 474.
[39] R. Y. Rubinstein and D. P. Kroese, Simulation and the Monte
Carlo Method, 3rd ed. (Wiley, 2016) p. 414.
[40] L. Isella, J. StehleÌ, A. Barrat, C. Cattuto, J.-F. Pinton, and
W. Van den Broeck, â€œWhatâ€™s in a crowd? Analysis of face-toface behavioral networks,â€ J. Theor. Biol. 271, 166â€“180 (2011).
[41] R. Mastrandrea, J. Fournet, and A. Barrat, â€œContact Patterns in
a High School: A Comparison between Data Collected Using
Wearable Sensors, Contact Diaries and Friendship Surveys,â€
PLOS ONE 10, e0136497 (2015).
[42] M. GeÌnois and A. Barrat, â€œCan co-location be used as a proxy
for face-to-face contacts?â€ EPJ Data Sci. 7, 11 (2018).
[43] V. Colizza, R. Pastor-Satorras, and A. Vespignani, â€œReactionâ€“diffusion processes and metapopulation models in heterogeneous networks,â€ Nat. Phys. 3, 276â€“282 (2007).
[44] D. Balcan, B. GoncÌ§alves, H. Hu, J. J. Ramasco, V. Colizza, and
A. Vespignani, â€œModeling the spatial spread of infectious diseases: The global epidemic and mobility computational model,â€
J. Comput. Sci. 1, 132â€“145 (2010).
[45] M. Ajelli, Q. Zhang, K. Sun, S. Merler, L. Fumanelli, G. Chowell, L. Simonsen, C. Viboud, and A. Vespignani, â€œThe RAPIDD
Ebola forecasting challenge: Model description and synthetic
data generation,â€ Epidemics 22, 3â€“12 (2018).
[46] D. Soriano-PanÌƒos, L. Lotero, A. Arenas, and J. GoÌmezGardenÌƒes, â€œSpreading Processes in Multiplex Metapopulations
Containing Different Mobility Networks,â€ Phys. Rev. X 8,
031039 (2018).
[47] P. Eichelsbacher and A. Ganesh, â€œBayesian Inference for
Markov Chains,â€ J. Appl. Probab. 39, 91â€“99 (2002).
[48] I. Voitalov, P. van der Hoorn, R. van der Hofstad, and D. Krioukov, â€œScale-free networks well done,â€ Phys. Rev. Research 1,
033034 (2019).
[49] R. Pastor-Satorras, C. Castellano, P. Van Mieghem, and
A. Vespignani, â€œEpidemic processes in complex networks,â€
Rev. Mod. Phys. 87, 925â€“979 (2015).
[50] A. Aleta and Y. Moreno, â€œEvaluation of the potential incidence
of COVID-19 and effectiveness of containment measures in
Spain: A data-driven approach,â€ BMC Med. 18, 157 (2020).
[51] I. Voitalov, P. van der Hoorn, R. van der Hofstad, and D. Krioukov, â€œScale-free networks well done,â€ Phys. Rev. Research 1,
033034 (2019).
[52] T. N. Kipf and M. Welling, â€œSemi-Supervised Classification with Graph Convolutional Networks,â€
(2016),
arXiv:1609.02907.
[53] C. Morris, M. Ritzert, M. Fey, W. L. Hamilton, J. E.
Lenssen, G. Rattan, and M. Grohe, â€œWeisfeiler and Leman
Go Neural: Higher-order Graph Neural Networks,â€ (2018),
arXiv:1810.02244.
[54] L. Liu, H. Jiang, P. He, W. Chen, X. Liu, J. Gao, and J. Han,
â€œOn the Variance of the Adaptive Learning Rate and Beyond,â€
(2019), arXiv:1908.03265.
[55] W. J. Conover, Practical nonparametric statistics (John Wiley
& Sons, 1998) p. 350.
[56] â€œCOVID-19 en EspanÌƒa,â€ https://cnecovid.isciii.
es (2020), [Accessed: 20-October-2020].
[57] â€œObservatorio del Transporte y la LogÄ±Ìstica en EspanÌƒa,â€
https://observatoriotransporte.mitma.gob.
es/estudio-experimental (2018), [Accessed: 11August-2020].

[58] â€œInstituto Nacional de EstadÄ±Ìstica,â€ https://www.ine.es
(2020), [Accessed: 11-August-2020].

Deep learning of contagion dynamics on complex networks
â€” Supplementary Material â€”
Charles Murphy, Edward Laurence, and Antoine Allard
DeÌpartement de Physique, de GeÌnie Physique, et dâ€™Optique,
UniversiteÌ Laval, QueÌbec (QueÌbec), Canada G1V 0A6 and
Centre interdisciplinaire en modeÌlisation matheÌmatique,
UniversiteÌ Laval, QueÌbec (QueÌbec), Canada G1V 0A6
(Dated: January 14, 2021)

CONTENTS
I. Derivation of the importance weights

2

A. Preliminaries

2

B. Generalizing the importance weights to continuous states

2

1. Neighbor-Dependent Weights

2

2. Reducing the Complexity

3

II. Loss descent patterns
III. Impact of some Hyperparameters

4
6

A. Performance measures and other metrics

6

B. Time Series Length

6

C. Network size

8

D. Resampling time

9

E. Importance Sampling Bias

9

F. Graph Neural Network Architecture

11

1. Models

11

2. Results

13

References

15

2
I. DERIVATION OF THE IMPORTANCE WEIGHTS
A.

Preliminaries

The importance weight wi (t) quantifies the extent to which the configuration of a node i at a time t
weighs in the loss function. In turn, this affects how the parameters are optimized by correcting further
for more important configurations. Yet, it is necessary to correctly define what â€importanceâ€ means in this
context, otherwise it can lead to badly trained models.
In the main paper, we considered the idea that the configurations should be weighted by an importance
sampling (IS) scheme [1] where the target distribution is assumed uniform over all possible configurations.
By doing so, we enforce the assumption that all configurations are equally important. Thus, the weights
must be inversely proportional to the distribution of these configurations as they are observed in the training
dataset. In the context of dynamical processes with a discrete and finite state set S on simple graphs, this
observed distribution is simply Ï(k, x, xN ), where k is the degree of a node, x âˆˆ S is its state and xN âˆˆ S k
is the vector state of its neighbors. The importance weight of a node i at time t is then
h
iâˆ’Î»
wi (t) âˆ Ï(ki , xi , xNi )

(1)

where we allow Î» to vary between 0 and 1, corresponding to no IS and pure IS, respectively.

B. Generalizing the importance weights to continuous states

For the metapopulation dynamics, we need to generalize Eq. (1) because the probability distribution Ï
in this form can only be evaluated where S is a finite and countable set: Ï can be computed by counting.

When S is a subset of R, counting cannot really be done directly and efficiently, which in turn prevents us
from evaluating Ï. Instead, we must rely on some assumptions in order to evaluate Ï efficiently.

1. Neighbor-Dependent Weights

We consider the direct generalization of Ï(ki , xi , xNi ) to real numbers. First, we factor Ï(ki , xi , xNi ) =
P (ki )Q(xi , xNi |ki ). By doing so, the dependence of the importance weight with the degree is more conspicuous. Then, we break apart Q(xi , xNi |ki ), because Q(x, xN |k) must be permutation-invariant under
the neighbors states. This can be done in various ways, but the simplest one is probably
Q(xi , xNi |ki ) =

Y

jâˆˆNi

[q(xi , xj |ki )]1/ki ,

(2)

3
where q(x, x0 |k) is the pairwise state probability conditioned on the degree k. Here, the geometric mean

ensures that Q(x, xN |k) does not have an artificially small value for nodes of high degree, as the values of
Q(x, xN |k) should roughly be of the size magnitude, for any degrees. In this context, we interpret q(x, x0 |k)
as being the probability to observe in the training dataset a node of degree k that is in state x and connected
to node in state x0 . Therefore, its values must be normalized and bounded by the interval [0, 1].
We make use of kernel density estimators (KDE) [2] with a Gaussian kernel to represent q(x, x0 |k).

For each value of k, we simply build a different KDE, denoted qÌ‚(x, x0 |k). The function qÌ‚(x, x0 |k) returns

density values, which can have any positive value. Thereby, we normalize it to obtain a probability value
such that
q(x, x0 |k) =

qÌ‚(x, x0 |k)
zk

(3)

where
zk =

T X X
X

I(ki = k)qÌ‚ xi (t), xj (t) ki

t=1 iâˆˆV jâˆˆNi



(4)

where I(Â·) is the indicator function.
Furthermore, the configurations must also be weighted with all additional information, that is with the
node and edge attributes, i.e. Î¦i and â„¦ij , respectively. This is easily achieved with KDE, where we simply
concatenate these attributes to the state pairs, i.e. q(x, x0 , Ï†, Ï‰|k).

2.

Reducing the Complexity

For continuous state dynamics such as the metapopulaton one, one would like to compute the importance
weights using Eq. (3). Now, the problem with using Eq. (3) is that, for a given KDE function q(x, x0 , Ï†, Ï‰|k),
a lot of samples are used to build it. The evaluation of standard KDE is known to scale like O(nm), where

n is the number of samples used to build the KDE and m is the number of samples on which we wish to


evaluate the KDE. Consequently, we need O (N kmax T )2 steps in order to evaluate all the normalization
constant zk and all the importance weights. For reasonably lengthy time series with not too large networks,

it renders the evaluation of the importance weights very inefficient. This is especially intensive for scale1

free networks whose maximum degree is kmax = O(N Î½âˆ’1 ) [3], where Î½ is the exponent of the degree
distribution.

To reduce the computational burden of evaluating the importance weights, we consider including additional assumptions. First, we assume that the node and edge attributes are conditionally independent
from the state pair. This is equivalent to assuming that the degree encodes all the information needed

4
to describe the state pairs. This allows us to factor them out of the pairwise state probability such that
q(x, x0 , Î¦, â„¦|k) = Î£(Î¦, â„¦|k)q(x, x0 |k). We additionally assume that the edge attributes are correctly de-

scribed by their respective mean taken over the neighbors of the node. This is equivalent to using the
P
strength of the node â„¦i = jâˆˆNi â„¦ij . Finally, we assume that, at a given time t, the state of the nodes
P
are also correctly described by the average taken over all the nodes, i.e. xÌ„(t) = N1 iâˆˆV xi (t). Thus, we

obtained the form we used in the main paper,

h

iâˆ’Î»
wi (t) = P (ki )Î£ Î¦i , â„¦i |ki Î  xÌ„(t)

(5)

where Î£ and Î  are represented by KDE using a similar strategy to Eq. (3). Those simplifications reduce
the complexity of evaluating of the importance weights to O(N 2 + T 2 ), a considerable improvement to


O (N kmax T )2 .
II. LOSS DESCENT PATTERNS

In the case of the simple, complex and interacting contagion dynamics, we address a problem similar to
a classification problem: For a given input, the model learns to assign it the correct label, i.e. the discrete
state to which the node transition to. However, contrary to more standard classification problems, the label
that the graph neural network (GNN) model learns to assign is not deterministic. Instead, it is assigned
stochastically with a transition probability distribution provided by the dynamical process. This dramatically changes how the cross entropy loss decreases as the training goes on, because it is no longer expected
to descend to zero (see Fig. 1(aâ€“c)). What is expected to descend to zero is hopefully the difference between the ground truth transition probabilities and those of the GNN. Hence, choosing an objective function
such as the Kullback-Liebler divergence (KLD), denoted D, which is intimately related to the cross entropy

loss and that quantifies the difference between two probability distributions, should shed some light as to
why the cross entropy loss drops to a non-zero constant value. Consider the KLD between two discrete
probability distributions p and q,
(6)

D(p||q) = H(p, q) âˆ’ H(p)
where H(p, q) = âˆ’

P

i pi log qi

is the cross entropy of p and q, and H(p) = âˆ’

P

i pi log pi

is the entropy

of p. It is well known that minimizing D(p||q) with respect to q yields D(p||q) = 0 when q = p, which in

turn leads to

min [D(p||q)] = min [H(p, q) âˆ’ H(p)] = min [H(p, q)] âˆ’ H(p) = 0.
q

q

q

5

FIG. 1. Loss optimization patterns during training. (aâ€“c) Loss as expressed by Eq. (3) in the main text, (dâ€“
f) average entropy of the GNN model predictions, (gâ€“i) average Jensen-Shannon distance (JSD) between the GNN
predicted LTPs and the ones given by the MLE. We show the results obtained when using BarabaÌsi-Albert networks
to generate the data; similar conclusions are obtained when using data generated with ErdoÌ‹s-ReÌnyi networks. All
measures shown by these plots are approximated using the importance sampling scheme used to compute the loss.
The vertical dotted lines show the minimum value of the validation loss, corresponding to our criterion for the model
selection.

From this expression, we readily obtain the minimum expected value of the cross entropy loss,
min [H(p, q)] = H(p) .
q

(7)

In a realistic scenario, where the ground truth probabilities are not accessible directly, the entropy H(p) is

also not accessible directly, which prevents the use of the KLD as the objective function altogether. For
this reason, we use the cross entropy loss in our experiments involving stochastic dynamics. It is also worth
monitoring the average entropy of the GNN outcome, which should, in the event that it is close to the ground
truth, be of the same magnitude as the minimum of cross entropy loss.
In Fig. 1, we show an example of loss descent for each stochastic dynamics case. We also show the
entropy of the GNN outcome averaged over the training dataset, and show the average Jensen-Shannon

6
distance [4] (JSD), a symmetric version of the KLD, between the ground truth transition probabilities and
the GNN predictions.

III.

IMPACT OF SOME HYPERPARAMETERS

A. Performance measures and other metrics

In this section, we investigate the impact of several hyperparameters. To quantify the performance of our
models, we used two kinds of metrics. The first one is similar to the one we used in the main paper, which
is the Pearson error 1 âˆ’ r computed from the Pearson correlation coefficient r between all target-prediction

pairs in the dataset. For completeness, the exact definition of r is provided in the Material and Methods

section of the main paper. The second one corresponds to the log Jensen-Shannon distance [4] averaged
over all target-prediction pairs. The two metrics provide a similar picture of the global performance of the
GNN model. Also, because we use discrete state dynamics in this context, we are allowed to evaluate the
effective sample size (ESS) in the following way:
P
2
P
xâˆˆS
` n(x, `)
neff = P
2
P 
xâˆˆS
` n(x, `)

(8)

where

T


XX
n(x, `) =
I xi (t) = x âˆ§ `i (t) = `

(9)

iâˆˆV t=1

is the number of nodes at any times in the dataset that were in state x and that had a neighborhood state
vector `. To better appreciate the relationship between the performance metrics and the ESS, we center and
rescale the ESSs with the mean and standard deviation ESS across the experiments which varies the same
hyperparameter.

B.

Time Series Length

The time series length, denoted by T , corresponds to the number of time steps in the training dataset. It
also affects the length of an epoch. We investigate the values T = {100, 500, 1000, 5000, 10000}.

Figure 2 shows the accuracy diagrams of GNN models trained using different time series lengths. As

we can expect, longer time series tend to yield better models. This is unsurprising for two reasons. First,
because the targets with which the models are trained are noisy, it generally helps to have larger a training
dataset. Using noisy targets yields a noisy objective function as well, for which the noise can be reduced by
increasing the number of samples. Second, using larger datasets means that we train the model for a longer

7

FIG. 2. Accuracy diagrams for different time series lengths T : We show the accuracy diagrams, that is the error
as a function of the degree of the nodes, of GNNs trained on the simple (left column), complex (middle column)
and interacting contagion dynamics evolving on ErdoÌ‹s-ReÌnyi (ER, top row) and BarabaÌsi-Albert (BA, bottom row)
networks. In every panel, we indicate the value of the changing hyperparameter, namely the time series length, with
the symbols and the colors according to the legend. The maximum likelihood estimators (MLE), computed from the
procedure specified in the main paper, is indicated as a reference. Panel (g) shows the normalized effective sample size
(ESS) as a function of the hyperparameter. Finally, panel (h) shows the relationship between the errorâ€”the average
log-JSD error to be more preciseâ€”as a function of the ESS. In panels (g, h), the symbols and line style encode the
type of networks used to generate the training dataset and the colors indicate the dynamics.

period of time. We also note that, because the gradient descent is performed using a stochastic technique,
the results can be a bit inconsistent with our previous observations. This is likely to also affect our next
results, hence we need to keep it in mind. A time-consuming way of addressing this issue would be to train
multiple GNNs in the same configurations, and to then average their errors together.

8

FIG. 3. Accuracy diagrams for different network sizes N : We refer to Fig. 2 for the organization of the panels.

C.

Network size

The network size, denoted by N , is the number of nodes in the networks on which the dynamics evolved
to generate the training dataset. We investigate the values N = {100, 500, 1000, 5000}.
Similarly, Fig. 3 shows the accuracy diagrams when changing the network size. At first, increasing
N seems to affect the performance of the models differently depending on the type of networks used.
First, for ErdoÌ‹s-ReÌnyi (ER) networks, increasing N does not tend to increase the ESS. This is expected
because the maximum degree only slighting increase when the number of nodes is increased, for fixed the
average degree hki. Hence, we do not observe additional degree classes when N is marginally increased

and the training dataset variety remains similar. For BarabaÌsi-Albert (BA) networks, we observe something

different: While the increase in N leads to higher ESS, there is still no substantial gain in performance.
This can be explained by looking at the degree distribution. As more nodes are added to the network, the
degree classes get more populated, resulting in increased ESS. However, because the degree distribution is
scale-free (with exponent âˆ’3), these are not populated evenly and more degree classes are created as N
increases.

9

FIG. 4. Accuracy diagrams for different resampling times ts : We refer to Fig. 2 for the organization of the panels.
D.

Resampling time

The resampling time, denoted by ts , corresponds to the number of time steps before the states of the
nodes are reinitialized when generating the training dataset. Recall that this hyperparameter was introduced
in the main paper to improve the variability in the dataset, where small values of ts is expected to increase
the ESS. We investigate the values ts = {1, 10, 100, 1000, 10000}.

In Fig. 4, we show the accuracy diagrams when the resampling time is changed. It is clear from Fig. 4

that decreasing the resampling time increases the ESS, thus we tend to train better models. However, in
most cases, the gain seems to be marginal.

E.

Importance Sampling Bias

The role of the importance sampling bias, denoted Î», is to modulate the influence of the importance
weights, where Î» = 1 corresponds to the ideal case, which is a standard IS scheme, and Î» = 0 correspond
to a uniform sampling scheme, that is without IS. There are multiple reasons why it would be preferable to
use an exponent Î» < 1. First, it is possible to poorly define the importance weights wi (t) by a bad choice

10

FIG. 5. Accuracy diagrams for different important sampling bias exponents Î»: Similarly to Fig. 2, we show the
accuracy diagrams of GNN models trained on (left column) simple, (middle column) complex and (right column)
interacting contagion dynamics propagating on (aâ€“f) ErdoÌ‹s-ReÌnyi (ER) and (gâ€“l) BarabaÌsi-Albert (BA) networks.
We also show the maximum likelihood estimators (MLE) for comparison. Additionally, the panels (aâ€“c) and (gâ€“i)
correspond to GNN models trained using the observed outcome, denoted yÌƒi (t) in the main paper, which corresponds
to the state of the node at the next time step: the labels are noisy in this case. Conversely, the GNNs corresponding to
panels (dâ€“f) and (jâ€“l) used the true transition probabilities, denoted yi (t) in the main paper: the labels are deterministic
in this case. On all panels, the symbols and colors indicate the value of Î» as specified by the legend.

11
of assumptions (see Sec. I). This is in part due to the fact that we rely on some statistics to represent the
training dataset, which can either contain false assumptions or be poorly estimated due to a small sample
size. Second, in the case of stochastic dynamics, letting Î» = 1 is analogous to putting a strong emphasis
on rare configurations, which in turn are likely to suffer from a small sample size. This will lead to a poor
estimation of the objective function, which is likely to reduce the overall performance of the model. That
being said, we investigate the values Î» âˆˆ {0, 0.25, 0.5, 0.75, 1}.

In Fig. 5, we show the accuracy diagrams when the IS bias exponent Î» is changed. To better appreciate

the comparison between these different training settings, we used the same dataset, networks and training
settings. We also trained our models in two different scenarios: we considered using the observed outcome
yÌƒi (t), defined as the state of node i at the next time step, to evaluate the objective function. We also used
the true transition probabilities, that is the outcome yi (t), to evaluate the loss. The difference between these
two scenarios is that, in the first case, the targets yÌƒi (t) are noisy, and in the other, the targets yi (t) are
deterministic. As it was mentioned earlier, we argue that the choice of Î» will be dependent on the stochastic
nature of the underlying dynamics.
First, from Fig. 5, we can see that choosing Î» = 1 rarely leads to the most proficient models, for models
trained on both the ER and the BA networks. Only the case of the interacting contagion dynamics does it
seem to improve the performance, but as we discussed before, this is conditioned on the fact that either the
sample size is large enough or the dynamics is deterministic. Interestingly, the case Î» = 0 often has similar
performance as the case Î» = 1 in the noisy scenarios. In general, the best models seem to be obtained when
Î» is somewhat in the middle, as if the pure IS and the no IS cases are both too strong assumptions.

F. Graph Neural Network Architecture

We investigate the accuracy diagrams of the models when we use different GNN architectures. To be
more specific, we consider six additional GNN architectures that has been shown to perform well in the
context of structure learning [5â€“8].

1.

Models

We label the 6 models as follows: We call our architecture the Att-GNN, which is described in detail in
the main paper. We also consider the architecture from Ref. [8] with multiple aggregation scheme. This
class of architectures aggregate the neighborsâ€™ features as follows:
Î½i = A(Î¾i ) + fAGG ({{B(Î¾j )|j âˆˆ Ni }})

(10)

12
where {{Â·}} denoted a multiset and we recall that A and B are linear transformations with a trainable weight

matrices and bias vectors. Also, we need to specify the fAGG function, which is a differentiable and
permutation-invariant function that aggregates the neighborsâ€™ features. We consider three cases for the
fAGG function: the mean pooling case, denoted Mean-GNN, where
k
X
xi

fAGG ({{x1 , Â· Â· Â· , xk }}) =

i=1

k

,

(11)

the max pooling case, denoted Max-GNN, where the Âµth feature is aggregated such that
[fAGG ({{x1 , Â· Â· Â· , xk }})]Âµ = max{xÂµ,1 , Â· Â· Â· , xÂµ,k } ,

(12)

and the sum pooling case, denoted Sum-GNN, where similar to the mean pooling case,
fAGG ({{x1 , Â· Â· Â· , xk }}) =

k
X

xi ,

(13)

i=1

Then, we consider three additional standard architectures: the GraphSage architecture from Ref. [9], the
graph convolution network (denoted GCN) from Ref. [5] and the original graph attention network (denoted
GAT) from Ref. [7]. The GraphSage aggregates the neighborsâ€™ features similarly to the Mean-GNN:
Î½i = W 1 Î¾i + W 2

X

Î¾j ,

(14)

jâˆˆNi

which, in turn is similar to the GCN,
Î½i = W

X

jâˆˆNi âˆ¨{i}

Î¾j
.
(ki + 1)(kj + 1)

(15)

Here, W and W i are a trainable weight matrix. Finally, the GAT architecture aggregates the neighborsâ€™
features as follows:
X

Î½i = W

aij Î¾j

(16)

jâˆˆNi âˆ¨{i}

where

and

aij = P

eÎ¸ij

jâˆˆNi âˆ¨{i} e

Î¸ij



Î¸ij = LeakyReLUÎ± aT W Î¾i + bT W Î¾j .

In this case, a and b are weight vectors and LeakyReLUÎ± is an activation function such that
ï£±
ï£´
ï£²x
if x > 0
LeakyReLUÎ± (x) =
,
ï£´
ï£³Î±x otherwise
and Î±, the negative slope, is generally fixed to 0.2.

(17)

(18)

(19)

13

Transition probabilities

Transition probabilities

Att-GNN

Sum-GNN

(a)

1.00

Max-GNN

Mean-GNN

(b)

(c)

(d)

(i)

(j)

(k)

0.75
0.50

GT
GNN

0.25
0.00

(h)

1.00
0.75
0.50
0.25
0.00

0

20
40
60
Number of infected neighbors [`]

0

20
40
60
Number of infected neighbors [`]

Transition probabilities Transition probabilities

GraphSAGE
1.00

0

20
40
60
Number of infected neighbors [`]

GAT

0

20
40
60
Number of infected neighbors [`]

GCN

(e)

(f )

(g)

(l)

(m)

(n)

0.75
0.50
0.25
0.00
1.00
0.75
0.50
0.25
0.00

0

20

40

60

Number of infected neighbors [`]

0

20

40

60

Number of infected neighbors [`]

0

20

40

60

Number of infected neighbors [`]

FIG. 6. Prediction of different GNN architectures on (aâ€“g) simple and (hâ€“n) complex contagion dynamics on
BarabaÌsi-Albert Networks: We show the infection and recovery probabilities as predicted by the trained GNNs
(dashed lines), and given by the ground truth (GT, solid lines). Each column corresponds to a different architecture.
In the top and bottom rows, all models have been trained on the same training dataset and networks. The training
settings and parameters of the dynamics are the same as described in the main paper. Also, we used the same training
dataset and networks to train each GNN architecture.

2.

Results

In Fig. 6, we show the predicted transition probabilities for the simple and complex contagion dynamics.
We see that, in general, the standard GNN architectures yield poor performance in predicting the infection
probabilities, even though they have been trained using the same dataset, networks and hyperparameters.
We believe this is due to the fact that they internally use a non-extensive aggregation operatorâ€”for instance
mean-pooling and max-pooling, whose output does not scale with the size of the input. To be clearer, let us
assume a node of degree k of which we wish to aggregate the features of its k neighbors. By using a nonextensive aggregator, the output is expected to be of a similar scale as that of any other node of degree k 0 .

14

FIG. 7. Accuracy diagrams for different GNN architectures: On each panel, the different GNN architectures were
trained on the same dataset with the same training settings and hyperparameters. For further details, we refer to Fig. 2.

Hence, the GNN model is likely to have a hard time distinguishing the vector features of nodes of different
degreesâ€”a structural feature that we know has a huge impact on most of the dynamical processes on
networks. This is in part why almost all GNN architectures described above fail at learning and representing
contagion dynamics. The only ones that perform similarly are the Att-GNN and Sum-GNN, which both use
extensive aggregators.
From Figs. 6 and 7, we can also appreciate how some GNN architectures are better than others at predicting the recovery probability, this is independent from the neighborsâ€™ states unlike the infection probability.
Specifically, the GraphSage and GCN architectures have a hard time predicting the recovery probabilities.
They may be due to the fact that their aggregator does not distinguish the different values of neighbor features and accept all contributions equally, as opposed to for instance the GAT which is expected to weigh
the neighborsâ€™ features before aggregating them. The same principle applies to the other architectures that
predict correctly the recovery probability.
In summary, not all GNN architectures are capable of learning a dynamical process on networks, which
also supports the idea, presented in Ref. [10], that most GNN architectures in fact do not have a highly
expressive power. Then, we can ask if having an extensive aggregator will always be sufficient in the
context of dynamical process learning. From our work, it seems to be the case, but the few examples we
provide in this paper are far from conclusive in that regard. However, in the case where extensive aggregator
would be insufficient, one could consider new strategies such that which is presented in Ref. [11], where

15
multiple aggregators are used in parallel.

[1] R. Y. Rubinstein and D. P. Kroese, Simulation and the Monte Carlo Method, 3rd ed. (Wiley, 2016) p. 414.
[2] W. J. Conover, Practical nonparametric statistics (John Wiley & Sons, 1998) p. 350.
[3] M. BogunÌƒaÌ, R. Pastor-Satorras, and A. Vespignani, â€œCut-offs and finite size effects in scale-free networks,â€ Eur.
Phys. J. B 38, 205â€“209 (2004).
[4] T. M. Cover and J. A. Thomas, Elements of Information Theory, 2nd ed. (Wiley-Interscience, 2005) p. 776.
[5] T. N. Kipf and M. Welling, â€œSemi-Supervised Classification with Graph Convolutional Networks,â€ (2016),
arXiv:1609.02907.
[6] W. L. Hamilton, R. Ying, and J. Leskovec, â€œRepresentation Learning on Graphs: Methods and Applications,â€
(2017), arXiv:1709.05584.
[7] P. VelicÌŒkovicÌ, G. Cucurull, A. Casanova, A. Romero, P. LioÌ€, and Y. Bengio, â€œGraph attention networks,â€ in
International Conference on Learning Representations (2018).
[8] C. Morris, M. Ritzert, M. Fey, W. L. Hamilton, J. E. Lenssen, G. Rattan, and M. Grohe, â€œWeisfeiler and Leman
Go Neural: Higher-order Graph Neural Networks,â€ (2018), arXiv:1810.02244.
[9] W. L. Hamilton, R. Ying, and J. Leskovec, â€œInductive representation learning on large graphs,â€ in Proceedings
of the 31st International Conference on Neural Information Processing Systems, NIPSâ€™17 (2017) p. 1025â€“1035.
[10] K. Xu, W. Hu, J. Leskovec,

and S. Jegelka, â€œHow Powerful are Graph Neural Networks?â€

(2018),

arXiv:1810.00826.
[11] G. Corso, L. Cavalleri, D. Beaini, P. LioÌ€, and P. VelicÌŒkovicÌ, â€œPrincipal Neighbourhood Aggregation for Graph
Nets,â€ (2020), arXiv:2004.05718.

