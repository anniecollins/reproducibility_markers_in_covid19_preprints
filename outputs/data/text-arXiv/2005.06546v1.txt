Triaging moderate COVID-19 and other viral pneumonias
from routine blood tests
Forrest Sheng Bao, PhDâ€‹1,â€‹*, Youbiao Heâ€‹1,â€‹*, Jie Liu, MD, PhDâ€‹2,â€‹*, Yuanfang Chenâ€‹3,4â€‹, Qian Li, MDâ€‹5â€‹, Christina R. Zhang, MDâ€‹6â€‹,
Lei Han, MD, PhDâ€‹4,7â€‹, Baoli Zhuâ€‹4,7â€‹, Yaorong Ge, PhDâ€‹8â€‹, Shi Chen, PhDâ€‹9,11#â€‹, Ming Xu, MD, PhD,â€‹4, 7, 9,#â€‹, Liu Ouyang, MD, PhDâ€‹10,#
1. Department of Computer Science, Iowa State University, Ames, IA, 50011, USA
2. Department of Radiology, Union Hospital, Tongji Medical College, Huazhong University of Science and
Technology, Wuhan 430022, China
3. Institute of HIV/AIDS/STI Prevention and Control, Jiangsu Provincial Center for Disease Control and Prevention,
Nanjing 210009, China
4. Public Health Research Institute of Jiangsu Province, Nanjing 210009, China
5. Department of Pediatrics, Kunshan People's Hospital Affiliated To Jiangsu University, Kunshan 215300, China
6. Division of Neurology, Department of Medicine, University of British Columbia, Vancouver, BC V6T 1Z3, Canada
7. Department of Occupational Disease Prevention, Jiangsu Provincial Center for Disease Control and Prevention,
Nanjing 210009, China
8. Department of Software and Information Systems, University of North Carolina at Charlotte, Charlotte, NC 28223,
USA
9. Department of Public Health Sciences, College of Health and Human Services, University of North Carolina at
Charlotte, Charlotte, NC 28262, USA
10. Department of Orthopaedics, Union Hospital, Tongji Medical College, Huazhong University of Science and
Technology, Wuhan 430022, China
11. School of Data Science, College of Health and Human Services, University of North Carolina at Charlotte,
Charlotte, NC 28262, USA
Emails: fsb@iastate.edu, yh54@iastate.edu, liu_jie0823@163.com, yf_chen2010@163.com, qianyingbingfeng@163.com,
christina.zhang@alumni.ubc.ca, hanlei@jscdc.cn, zhubl@jscdc.cn, yge@uncc.edu, schen56@uncc.edu,
sosolou@126.com, ouyangliu211@hust.edu.cn

Abstractâ€‹: The COVID-19 is sweeping the world with deadly consequences. Its contagious nature and clinical
similarity to other pneumonias make separating subjects contracted with COVID-19 and non-COVID-19 viral
pneumonia a priority and a challenge. However, COVID-19 testing has been greatly limited by the availability
and cost of existing methods, even in developed countries like the US. Intrigued by the wide availability of
routine blood tests, we propose to leverage them for COVID-19 testing using the power of machine learning.
Two proven-robust machine learning model families, random forests (RFs) and support vector machines
(SVMs), are employed to tackle the challenge. Trained on blood data from 208 moderate COVID-19 subjects
and 86 subjects with non-COVID-19 moderate viral pneumonia, the best result is obtained in an SVM-based
classifier with an accuracy of 84%, a sensitivity of 88%, a specificity of 80%, and a precision of 92%. The results
are found explainable from both machine learning and medical perspectives. A privacy-protected web portal is
set up to help medical personnel in their practice and the trained models are released for developers to further
build other applications. We hope our results can help the world fight this pandemic and welcome clinical
verification of our approach on larger populations.

1. Introduction
COVID-19 is a pandemic that has devastated the lives of billions of people around the world in recent months
[1]â€‹, causing hundreds of thousands of deaths worldwide and overwhelming hospitals in hot spots like Lombardy,
Italy or New York City, USA. COVID-19, at its early stage, shares many symptoms with other illnesses,
especially other virus-induced pneumonias, such as coughing and fevering â€‹[2]â€‹[3]â€‹. Because of its extremely
contagious nature, it is important to quickly distinguish COVID-19 from other viral pneumonias, in order to
facilitate safe isolation of COVID-19 patients.
Page 1

The gold standard to diagnose COVID-19 is genome testing. But it is limited due to accessibility and cost. No
other ways can better diagnose or rule out COVID-19. For example, COVID-19 has very nonspecific lung CT
imaging findings â€‹[4]â€‹, which are similar to findings seen in other viral pneumonias, including those secondary to
influenza â€‹[5]â€‹. Moreover, especially in developing countries ravaged by the virulent COVID-19, CT imaging,
genomic, and even antibody/serology (e.g., IgM/IgG) testing are all likely to be poorly accessible. Even in the
United States, there is still a huge gap between testing capacity and testing needs â€‹[6]â€‹. By late April, many
Americans are still denied from testing due to issues like the shortage of swabs â€‹[7]â€‹.
Therefore, in this paper, we turn our attention to a commonly accessible modality: routine blood tests â€‹[8]â€‹, hoping
to distinguish COVID-19 from other viral pneumonias based on widely-available, common blood chemistry
profiles. Compared with other testing methods, routine blood tests have much more available supplies,
equipment, and personnel. Massively and pervasively performed at hospitals and labs daily, they can be very
affordable. But as we will show in this paper, this task is not trivial due to complicated underline correlation
between blood biomarkers. Hence, a powerful tool to mine complex patterns from data is needed.
As a subarea of Artificial Intelligence (AI), machine learning (ML) studies giving computers a skill (usually from
data) automatically without being manually programmed to do so. Hence, we hope to leverage ML methods to
make use of routine blood tests for differentiating moderate COVID-19 cases from other moderate viral
pneumonia cases automatically. Thus, given blood test values of a moderate viral pneumonia patient, the
computer is expected to tell whether s/he is contracted with COVID-19 or other viruses. An ML method acquires
such a skill through learning a mathematical model from blood test values of many human subjects and whether
they are contracted with COVID-19. The reason we focus on moderate cases is because a great majority of
COVID-19 patients show moderate symptoms, and they are contagious although most of them eventually
recover on their own. Once detected early, such patients can be isolated to cut the transmission chain.
The ML problem in this paper is a typical classification task. To solve, we employ random forests (RFs) â€‹[9]â€‹ and
support vector machines (SVMs) â€‹[10]â€‹, two families of classifiers that have been widely and empirically
considered robust and powerful â€‹[11]â€“[13]â€‹. Our experimental results show that the SVM-based approach is
superior to the RF-based in general. An RBF-kernel SVM trained on 294 subjects, 208 COVID-19 vs. 86
non-COVID-19, achieves an accuracy of 84%, a sensitivity of 88%, a specificity of 80%, and a precision of 92%
in differentiating COVID-19 from non-COVID-19 viral pneumonia at moderate stage. Thorough medical
explanations can justify the effectiveness of using ML to tackle the problem.
We completely open our results to help the medical practice and research to fight this pandemic. Our model is
released at â€‹http://github.com/forrestbao/covid-19â€‹. We further release a privacy-protected web portal based on
the trained model to assist medical workers to triage patients at â€‹http://forrestbao.github.io/covid-19â€‹ All source
code will be released upon the acceptance of this manuscript at a peer-reviewed journal. Although the results
are promising, we look forward to verifying its effectiveness on larger populations with researchers and doctors
around the world.

2. Data and problem formulation
2.1 Subjects
We treat a blood â€‹testâ€‹ in the medical context, such as white blood cell count or the concentration of a specific
protein in serum, as a â€‹featureâ€‹ in the machine learning context. Some blood tests are often ordered together,
Page 2

forming a blood test panel or blood test work. This study is based on routine blood tests from 3 groups (called
classesâ€‹ in machine learning) of subjects:
1. Patients with â€‹moderateâ€‹ â€‹non-COVID-19 viralâ€‹ pneumonia (N = 86, age: 50.65 â¤Â± 17.38, mean 52; gender:
30 males and 56 females), short as "â€‹viral"
2. Patients with â€‹moderateâ€‹ COVID-19 (N = 208, age: 47.96 Â± 15.05, mean 47, gender: 85 males and 123
females), short as "â€‹moderate"
3. Patients with â€‹severeâ€‹ COVID-19 (N = 118, age: 67.57 Â± 11.94, mean 68, gender: 62 males and 56
females), short as "â€‹severe"
The lab tests were ordered independently by individual doctors in two hospitals in China: Wuhan Union Hospital,
in Wuhan where the outbreak started, and Kunshan People's Hospital, in Kunshan, which is 1 hour west of
Shanghai. All COVID-19 data from Wuhan were collected between Jan. 18 and Feb. 22, 2020 when patients
were initially admitted into the hospital, while all non-COVID-19 blood tests from Kunshan were performed
between Jan. 1, 2016 and Dec. 1, 2019. The temporal separation minimizes the chance of mixing data of
COVID-19 subjects and non-COVID-19 subjects. All COVID-19 subjects were confirmed with RT-PCR testing of
SARS-COV-2 (the virus causing COVID-19) genome. All non-COVID-19 subjects were confirmed with radiology
findings and had no bacterial infections. All COVID-19 subjects had no bacterial infections.
Note that the distinction between moderate and severe conditions is a clinical diagnosis and hence subjective. A
moderately trained doctor can differentiate between the two easily. Usually a patient with severe pneumonia will
be overall sicker, e.g., with a lower SpO2, or unstable/ abnormal vital signs. According to the COVID-19
guideline by National Health Commission of China â€‹[14]â€‹, a severe adult subject is who meets any one of the
following:
1. Shortness of breath, RR > 30 breaths/minute;
2. Oxygen saturation < 93% at rest
3. Arterial oxygen partial pressure (PaO2)/ fraction of inspired oxygen (FiO2) < 300mmHg
(1mmHg=0.133kPa).
Any subject less severe than severe but with radiological findings of pneumonia is considered moderate. Note
that in the Chinese guideline, there is one more level above severe called critical, for anyone needing
mechanical ventilation or ICU, or having shock. They are excluded from this study. All subjects in this study are
adults.

2.2 Features
Because the blood tests were ordered by different doctors over a long span of time, not all subjects had identical
features (again, a feature is a blood test), and nearly no feature has values on all subjects. To deal with the
sparsity of the raw data, we first excluded any feature missing on half of the initial subject pool in any group, and
then removed any subject missing more than 20% of features. The initial pools of 99 viral, 213 moderate, and
122 severe subjects were thus narrowed down to 86 viral, 208 moderate, and 118 severe subjects, respectively.
The demographics of final subjects were given in â€‹Section 2.1â€‹ above. The features were narrowed down to those
in â€‹Table 1â€‹. There were some additional common features between severe COVID-19 and moderate COVID-19
detailed in â€‹Table 2â€‹.
Table 1â€‹: Features shared among the 3 classes
Feature/test name (abbreviations)
Count of White blood cell (WBC)

unit
Million cells per liter (10â€‹9â€‹/L) or thousand cells per microliter (10â€‹3â€‹/mL)

Page 3

Hemoglobin (HGB)

grams per liter (g/L)

Platelet count

Million cells per liter (10â€‹9â€‹/L) or thousand cells per microliter (10â€‹3â€‹/mL)

Neutrophil percent

%

Neutrophil count

Million cells per liter (10â€‹9â€‹/L) or thousand cells per microliter (10â€‹3â€‹/mL)

Lymphocyte percent

%

Lymphocyte count

Million cells per liter (10â€‹9â€‹/L) or thousand cells per microliter (10â€‹3â€‹/mL)

C-reaction protein (CRP)

Milligrams per liter (mg/L; to convert to mg/dL, divide by 10)

Total bilirubin (TBL)

Micromole per liter (umol/L; to convert to mg/dL, divide by 17.1036)

Blood urea nitrogen (BUN)

millimole per liter (mmol/L; to convert to mg/dL, divide by 0.3571)

Creatinine

Micromole per liter (umol/L; to convert to mg/dL, divide by 88.417)

Lactate dehydrogenase (LDH)

Units per liter (U/L)

D-dimer

micrograms per liter (mg/L; to convert to ng/mL, divide by 0.001)

Table 2â€‹: Additional features for severe vs. moderate
Feature name (abbreviations)

Unit

IL-6

pg/ml

Erythrocyte sedimentation rate (ESR)

Millimeters per hour (mm/h)

Procalcitonin

Nanograms per microliter (ng/mL)

Alanine transaminase (ALT)

Units per liter (U/L)

Aspartate transaminase (AST)

Units per liter (U/L)

Creatine kinase (CRK)

Units per liter (U/L)

Several preprocessing steps were performed on the remaining data. A missing value on a feature was replaced
with the mean of that feature. On each feature, all values were standardized to zero mean and unit variance. For
c-reaction protein (CRP), when a high-sensitivity CRP (hsCRP) value was available, we used the hsCRP value
as the CRP value.
Curious about how demographics play a role, we will report the results with and without consideration of gender
and age separately. In the former case, age and gender become two additional features.

2.3 Tasks
The 3 groups of subjects thus form 3 binary classification tasks:
1. [primary] moderate vs viral (N=208 vs. 86)
2. [secondary] Severe vs. viral (N=118 vs. 86)
3. [bonus] Severe vs. moderate (N=117 vs. 2041)
Medical workers need help the most from the primary task of differentiating moderate COVID-19 cases from

1

The numbers are not 118 vs 208 because 5 samples have too many missing values in additional features in Table 2.

Page 4

cases of non-COVID-19 moderate pneumonia. The secondary task is not a very fair apple-to-apple comparison
as the difference in blood test values could be a result of different pneumonia severities rather than different
pathogens. The bonus task is introduced simply out of curiosity, because severe subjects can be easily
distinguished from moderate subjects based on blood oxygen saturation and/or other symptoms such as
difficulty to breath.
Mathematically, the goal of each of the classification problems above is finding a function f : â„d â†’ â„¤
representing a complex relationship between â€‹dâ€‹ blood test values of a subject and the diagnosis of the subject.
For example, for the primary task,
â— f (x1, x2, Â· Â· Â· , xd ) = 1 if the subject is contracted with moderate COVID-19, and
â—

f (x1, x2, Â· Â· Â· , xd ) =âˆ’ 1 if with moderate non-COVID-19 viral pneumonia.

Of course, for all human subjects, the order of features must be identical. Per â€‹Table 1â€‹, in this paper, x1 is

always how many white blood cells (WBCs) per microliter, x2 is always how many milligrams of CRP per liter,
x4 is always the percentage of neutrophil among all white blood cells, etc. In machine learning, the vector
composed of all feature values X = [x1 , x2 , Â· Â· Â· , xd ] is called a â€‹feature vectorâ€‹, the function fâ€‹ â€‹ is called a
classifierâ€‹, and the output of â€‹fâ€‹ is called a â€‹labelâ€‹ which is one of the known classes.

The function â€‹fâ€‹ is usually obtained empirically from â€‹samplesâ€‹, which are pairs of feature vectors and expected
labels, through a process called â€‹trainingâ€‹. Each sample corresponds to a human subject in this paper. The
process of plugging in an (unseen) feature vector X
â€‹ 'â€‹ into the classifier to yield an output fâ€‹ (X')â€‹ is called the
predictingâ€‹. A bad or poorly trained classifier would simply memorize all samples. When given an unseen
sample, the classifier very likely will not output the expected label. A good classifier instead captures the pattern
of the data across features holistically to maximize its ability to deal with radical (or extreme, outlying) unseen
samples.
As we will see later in this paper, a classifier often involves many parameters that will not be changed by the
training samples in the training process. Such parameters are called â€‹hyperparametersâ€‹ and need to be
customized on a problem-by-problem basis to maximize the performance of the classifier.

3. Methods
Machine learning is needed for the tasks above because blood test features have rather complicated
relationships or correlations which will be missed and not made use of by simple thresholding on raw features,
e.g., claiming that CRP>2.5 means COVID-19, otherwise non-COVID-19 viral pneumonia. For example, in two
studies of hospitalized COVID-19 patients, one â€‹[2]â€‹ of which has more severe subjects than the other â€‹[15]â€‹, while
LDH elevation above its normal range is observed on similar ratios of patients (73% and 76%, respectively),
WBC and lymphocyte decreases below their normal ranges are found on very different ratios of patients: 25%
[2]â€‹ vs. 9% â€‹[15]â€‹ for WBC and 63% â€‹[2]â€‹ vs. 35% â€‹[15]â€‹ for lymphocytes. This shows that LDH elevation is more
severity-invariant than WBC or lymphocyte count, and WBC and lymphocyte changes seem to be correlated.
While thresholding on LDH might capture the characteristics of COVID-19, fixed or limited discrete thresholds on
WBC and lymphocytes will be less effective. A rule that captures the joint change in WBC and lymphocytes
might be more desired. Experimental results later also suggest that thresholding-based approaches, including
decision trees and random forests, are outperformed by SVM-based which can build complicated functions
involving multiple features through kernel methods.
We pick two empirically effective and robust families of classifiers, random forests (RFs) and support vector
machines (SVMs), as representatives to study the general feasibility and effectiveness of using ML to make use
Page 5

of routine blood tests for COVID-19 triage. As the state of the art before deep learning â€‹[11], [12]â€‹, another popular
branch of ML, RFs and SVMs are considered suitable for problems with small amounts of samples. Our problem
is not suitable for deep learning at this point due to the limited number of samples. Once much more data
becomes available, it will definitely be worth trying deep learning-based approaches. In this section we will
briefly go over RFs and SVMs, and graphically explain how they work to our audience who are not familiar with
ML.

3.1 Decision trees and Random forests
Instead of developing an analytical form (e.g., an equation) of the function fâ€‹ â€‹ , decision trees (DTs) [â€‹ 16]â€‹ and
random forests (RFs) â€‹[9]â€‹ achieve classification by using binary comparison rules. A random forest is a collection
of DTs. A DT, standalone or as part of an RF, is a set of hierarchical binary thresholding rules like a nested
sequence of if-then-else statements. â€‹Figure 1â€‹ is an example DT in a graphical representation. Each rectangular
box is called a â€‹nodeâ€‹ on the tree and the tree starts with the â€‹rootâ€‹ node at the top. Each node compares the value
of a feature with a threshold, e.g., CRP <= 2.23 in the root node. Arrows represent the order of applying rules,
e.g., if CRP>2.23 then we check whether creatinine is <= 48.65. A node that has no arrow pointing out of is
called a â€‹leafâ€‹ node, which represents a final classification decision. Given an unseen sample, starting from the
root node, we compare the feature value with the threshold, and then go to the next node based on the result of
comparison, until reaching a leaf node. The sequence of nodes visited from the root to the leaf node is called a
pathâ€‹. The number of nodes on a path minus 1 is called the â€‹depthâ€‹ of the path. So the leftmost path in â€‹Figure 1
means that if a subject's CRP <=2.23, and D-dimer<=1.1, and platelet count <= 359.0, then this subject is
classified contracted with non-COVID-19 viral pneumonia. A feature may be checked more than once with
different thresholds, e.g., D-dimer is checked twice on the leftmost path with moderate as the leaf node (CRP
<=2.23, D-dimer<=1.1, platelet count> 35.9.0, and D-dimer<=0.97). The feature to be compared at each node,
the order of comparison rules, and the thresholds are all obtained through training. The maximum depth of a
path is a parameter to be searched using grid search in â€‹Section 4.1â€‹.

Figure 1â€‹: An example decision tree for classifying moderate COVID-19 vs. moderate non-COVID-19 viral from
blood tests. For each non-leaf node block, the threshold is at the top, with the portion of samples falling into this
node in the middle, and the dominant class at the bottom. For leaf nodes, the thresholds are gone. The most
Page 6

important feature is CRP, at the root (top) of the decision tree. By thresholding CRP at 2.23, samples below the
threshold are dominantly moderate non-COVID-19 pneumonia while those above or equal to are dominantly
moderate COVID-19.
A decision tree is constructed recursively from an empty tree. Starting from the root node, a feature and a
threshold are chosen to branch or fork (technically â€‹splitâ€‹) a node into two. The recursive process to construct the
tree stops when all paths end up at leaf nodes. Among many ways to determine the feature and the threshold,
this paper employs one of the classics, the CART algorithm â€‹[16]â€‹ whose core is the Gini impurity or Gini index.
For a Boolean comparison condition S, its Gini impurity is defined as: g (S) = 1 âˆ’ âˆ‘ P 2 (class = j | S) , where P is
j=Â±1

the probability density function. Gini impurity measures how likely a randomly chosen sample satisfying a
condition would be incorrectly labeled if it was randomly labeled according to the label distribution among all
samples satisfying that condition â€‹[17], [18]â€‹. Given a feature F and a threshold T, the expectation of Gini impurity
over two mutually exclusive conditions, F>T and F<=T, is computed as
E g (F , T ) = P (F > T ) g(F > T ) + P (F â‰¤ T ) g(F â‰¤ T ) . The CART algorithm exhaustively searches over all
features and all their values to find the combination of F and T that maximizes E g (F , T ) , i.e., arg max E g (F , T ) ,
F, T

and split a node accordingly.
The sample pool over which the probabilities above are calculated changes in the iterative process. It begins
with all samples when splitting the root node. Then for non-root nodes, the sample pool includes only those
already satisfying the conditions along the path. For example, in â€‹Figure 1â€‹, to split the left node under the root
(D-dimer<=1.1), the sample pool includes only those already satisfying the condition CRP<=2.23. Hence the
probability calculation should reflect the population reduction as we go down the tree. When to stop the split and
to create a leaf node depends on 3 hyperparameters: the maximum depth of a path, the minimum Gini impurity,
and the minimum sample pool.
A random forest is just a collection (technically an â€‹ensembleâ€‹) of DTs that share the same set of
hyperparameters. Those member DTs are trained from different sets of samples randomly taken with
replacement from the same set of original samples. And when constructing each node, not all features are
considered but only a random subset. The member DTs make predictions independently and the final
classification of the RF is the majority vote of all DTs. The number of member DTs and the size of the random
subset of features are two hyperparameters to be searched using grid search in â€‹Section 4.1â€‹.

3.2 To threshold or not to threshold, this is a question
Now allow us to use a toy problem to graphically illustrate how a DT and an RF work, and explain when SVMs
might be better. â€‹Figure 2â€‹ visualizes samples, each of which has two features (i.e., d=2), using a 2D Cartesian
coordinate system such that each axis represents a feature. The blue squares and red circles represent two
classes (+1 and -1) of samples. Obviously the two classes have quite separate means along the X (horizontal)
axis and only the tails of their distributions (along X axis) slightly overlap. Thresholding on the X axis, as
depicted by the vertical black line, would perfectly separate the two classes, without using the information from
Feature 2. This can be easily implemented using a DT of only one node that thresholds on Feature 1 and also
splits the plane into the left and right halves.

Page 7

Figure 2â€‹: A toy classification problem. The distribution of samples here do not reflect that of our data.
An RF trained on the same dataset using both features will further split the plane into many tiles as shown in
Figure 3â€‹. Recall that an RF uses majority votes of its composing DTs to classify. The darkness of a tile in
Figure 3â€‹ represents the extent of majority, e.g., the RF is most sure that samples distributed to the top right
belong to class -1 (red) and those to the bottom left belong to +1 (blue).

Figure 3â€‹: An example showing the decision boundary of a random forest-based classifier
But if we look at the overall picture, the two classes both exhibit a "diagonal" distribution. Unseen samples of
class 1 (the big blue question mark) might appear to the top right side of the decision line while unseen samples
of class -1 (the big red question mark) to the bottom left. So a sloped decision line, as illustrated by the black
dash line in â€‹Figure 4â€‹, is more preferred for being more future-proof. The goal of a linear classifier, such as an
SVM, is to find such a sloped decision line (called a â€‹decision hyperplaneâ€‹ when multi-dimensional) that would
not have been easily obtained by thresholding on single features. In our example, an RF can only achieve such
a sloped decision hyperplane limitedly, e.g., the "staircase" pattern of red tiles in â€‹Figure 3â€‹.
Page 8

Figure 4â€‹: A sloped decision line more robust for potentially unseen samples depicted by big question marks.
To decide which sloped decision hyperplane is the best, SVMs introduce a concept called the â€‹marginâ€‹, which is
a strip- or slice-shaped zone (bounded by the blue and red dot-dash lines in Figure 1) centered at and parallel to
the decision hyperplane and without any samples within. An SVM maximizes the width of the margin, or in other
words, finds the widest "safe zone" between outliers of the two classes. A soft-margin SVM moves one step
further by allowing a few samples (that make the envelope bumpy or zigzaggy) fall into the margin in exchange
for a better slope to widen the margin. â€‹Figure 5â€‹ illustrates a flatter decision hyperlane and its wider margin which
a handful of samples fall into. SVMs use a constant C to control the tradeoff between margin width and samples
fall into the margin (including misclassifications). A larger C means less tolerance to samples in the margin,
while a smaller C means a wider margin. As we will see, SVMs can be easily extended further to achieve
nonlinear decision hyperplanes.

Figure 5â€‹: A soft-margin SVM with a wider margin tolerating a few samples falling into the margin.
Page 9

3.3 Support Vector Machines (SVMs)
After the intuitive explanations above about SVMs, let us formally introduce it. Given samples in two classes, an
SVM â€‹[10]â€‹ (strictly speaking, a hard-margin linear SVM) finds a multi-dimensional direction along which the
closest samples of both classes are apart the most. In other words, the difference among most alike samples of
two classes are maximized. This idea gives SVMs great generalizability, making it superior to RFs on popular
benchmarks â€‹[12]â€‹. SVMs have been theoretically and empirically proven to be adept and robust with a small
amount of samples.
Without losing generality, an SVM-based classifier is defined as f (X) = sign (W Â· X + b) where sâ€‹ ignâ€‹ is the sign

function, X is the weight vector, the operation Â· is dot product between two vectors, and b a scalar is the bias.
Given a sample X â€² , the prediction is f (X â€²) = sign (W Â· X â€² + b) . The loss function is defined as
J (W ) =

||W ||2
2

+ C âˆ‘ Î¾ 2 i where the operator || Â· || means L2-norm, the hinge loss Î¾ i = max (1 âˆ’ y i (W Â· X i + b), 0)
i

represents the prediction error, y i is the ground truth label for the â€‹i-â€‹ th sample X i , and C is the hyperparameter
mentioned above that controls the tradeoff between margin width and samples falling into the margin. An SVM
can be derived into a dual formulation where the goal is to maximize the new loss function

J (Î±1 , Î±2 , Â· Â· Â·) = âˆ‘ Î±i âˆ’ 21 âˆ‘ Î±i Î±j y i y j X i Â· X j , subject to âˆ€i, Î±i â‰¥ 0 and âˆ‘ Î±i y i = 0 . In the dual formulation, given a new
i

i, j

i

sample X â€² , the prediction is f (X â€²) = sign (âˆ‘ Î±i y i X i Â· X â€² + b) . The SVMs discussed so far are linear SVMs, where
i

the classification relies on linear relationships (a simple weighted sum) of features.
In order to exploit the nonlinear inter-feature relationship (e.g., BMI is a feature built on top of two features,
weight and height) for a better performance, samples can be first (usually nonlinearly) mapped into a feature
space via a mapping Î¦ before being fed into an SVM â€‹[19]â€‹. This results in a nonlinear SVM-based classifier
f (X â€²) = sign (âˆ‘ Î±i y i Î¦(X i ) Â· Î¦(X â€²) + b) where the dot product part is usually rewritten as a kernel function
i

K(X i , X â€²) = Î¦(X i ) Â· Î¦(X â€²) . â€‹Figure 6â€‹ illustrates such a transformation. One of the commonly used nonlinear

kernels is the radial basis function (RBF) kernel K (X i , X â€²) = exp(âˆ’ Î³ || X i âˆ’ X â€²||2 ) where Î³ is another

hyperparameter. A nonlinear SVM can also be soft-margin, with samples falling into the margin. In â€‹Section 4â€‹,
we will report results obtained in linear SVMs and RBF-kernel SVMs separately.

Figure 6â€‹: An example showing how to achieve nonlinearity in (hard-margin) SVMs.
Source: â€‹https://commons.wikimedia.org/wiki/File:Kernel_Machine.svg
Page 10

4. Results and Discussions
4.1 Setups
Cross validationâ€‹: Because the purpose of training a classifier is to use it to make predictions on unseen
samples (e.g., given blood test values of a new subject, tell whether s/he is contracted with COVID-19 or
non-COVID-19 viral pneumonia), the way to evaluate its performance is to train it with some samples and then
to check its predictions on others. The two groups of samples are called the â€‹training setâ€‹ and the â€‹test setâ€‹,
respectively. To ensure that the classifier does not "see" test samples in the training process, the two sets
should be disjoint, i.e., their intersection is empty.
If we use a fixed pair of training and test sets to evaluate the classifier, it would be unclear how the same
approach to train a classifier would remain effective on a different pair of training and test sets. To assess the
robustness of a classifier configuration for a task, including the choice of the classifier family and the
hyperparameters, cross validation (also called rotation estimation) â€‹[20]â€‹ partitions all data into many
non-overlapping sets, from which it forms different pairs (called â€‹foldsâ€‹) of training and test sets. In each fold, a
one set is used for testing after all others are used for training. The test set rotates among all sets sequentially
until all sets have been the test set exactly once. Finally, cross validation averages the performances across all
folds.
All folds are independent but share the same classifier configuration. In each fold, the classifier is trained from
scratch -- hence the classifier never sees test samples during the training stage. Because the training sets for all
folds differ, they result in different classifiers. Note that the purpose of cross validation is to assess the
robustness of using a classifier configuration to solve the problem, not to obtain the final classifier. Once a
classifier configuration is found effective and robust, the final classifier is trained using all samples. The last step
is usually called â€‹refitâ€‹ in many ML toolboxes.
To maximize the size of the training set, we use leave-one-out cross-validation (LOOCV) where each test set
contains only one sample. One may imagine each fold of the LOOCV as a doctor using data from N-1 subjects
to train the classifier and then applying the model on a new patient, while the entire process of LOOCV as N
doctors doing this independently and we counting how many of them give thumbs up and thumbs down to the
respective models. We can safely say that we test the approach on N subjects, and no prior knowledge of the
test subject is "leaked" to the classifier.
Grid search for hyperparametersâ€‹: As mentioned earlier, many classifier families contain hyperparameters,
which are constants preset before training. Hyperparamters usually affect the performance of a classifier
significantly and need to be customized for a problem. Grid search â€‹[21]â€‹ is a technique to find the best
hyperparameter, or hyperparameter combinations if multiple hyperparameters. For each hyperparameter, grid
search first generates a set of candidates. Then it produces the Cartesian product for candidate sets of all
hyperparameters, and exhaustively runs cross validation over all points in the Cartesian product â€‹[22]â€‹. The best
cross validation performance is to be reported, and the point yielding it is used to produce the final model.
For SVMs, the hyperparameters to be searched include C for all soft-margin SVMs, and ð›¾ additionally if using
the RBF kernel. Two rounds of grid searches, first coarse and then fine, are performed. The grid search begins
with a 2-based exponential range from 2âˆ’10 to 210 and an initial step of 2 for C (and ð›¾ when using the RBF
kernel). In the second and the fine round, we linearly sample 20 points for C (and ð›¾, respectivelyï¼‰in the range

Page 11

where the best C (and ð›¾) is (are) found.
For DTs and RFs, the maximum depth (max_d in â€‹Table 3â€‹) is searched from 1 to 10 levels with a step of 1 level.
For RFs, the number of trees (n_tree in â€‹Table 3â€‹) are searched for 10, 20, 50, and 100, and the number of
features to consider when splitting nodes takes 3 values: total number of features, square root of it, and base-2
logarithm of it. All other hyperparameters for DTs and RFs use their respective default values in the scikit-learn
toolbox â€‹[23]â€‹.
Evaluation metricsâ€‹: We use balanced accuracy, thus, average recall of both classes, as the primary metric.
Other metrics to be reported include sensitivity (recall on positive class), specificity (recall on negative class),
and precision â€‹[24]â€‹. In statistics, sensitivity (or specificity) means the ratio of true positive (or negative) to actual
P
TN
positive (or negative), i.e., sensitivity = T PT+F
N (specificity = T N +F P ). Precision means the ratio of true positive to
predicted positive, i.e., precision =

TP
T P +F P

.

In the primary and secondary tasks, COVID-19 samples are defined as positive. In the bonus task, severe
COVID-19 is defined as positive. In the primary task, false positives are better than false negatives, because it is
safer to isolate non-COVID-19 subjects seriously as COVID-19 subjects to cut the transmission chain than the
other way around which spreads COVID-19 in the community. Hence, a classifier of a high sensitivity (and
optionally a high precision) and low specificity is preferred over a classifier of low sensitivity and high specificity.
Class weightsâ€‹: Because the dataset is unbalanced, i.e., unequal amounts of samples in two classes, to get a
fair evaluation, class weights are set inversely proportional to the number of samples in each class.
Implementation detailsâ€‹: All experiments were carried out in scikit-learn â€‹[23]â€‹. In particular, the linear SVM and
the RBF-kernel SVM are accessed through its wrapper for LIBLINEAR â€‹[25]â€‹ and LIBSVM â€‹[26]â€‹, respectively.

4.2 Quantitative results
Our initial effort shows very promising results. The performances of various classifiers on different classification
tasks are given in â€‹Table 3â€‹.
For the primary taskâ€‹, moderate vs. viral (moderate COVID-19 vs. moderate non-COVID-19 viral pneumonia),
we separately reporte the demographics-aware (first 4 rows) and demographics-free (rows 5 to 8) results.
Overall, RBF-SVM and RF models alternate to take the first spot in both cases. When demographics are not
considered, the RF-based classifier slightly outperforms RBF-SVM classifier with an accuracy of 85.68% vs
85.15%. When demographics are considered, the RF-based classifier loses edge slightly with an accuracy of
82.92% vs. 83.87%.
Thresholding-based classifiers (DTs and RFs) have quite different performances on the positive (moderate
COVID-19) and negative (moderate non-COVID-19) classes. Take RFs as examples. The differences between
sensitivity and specificity are 21.66% and 13.24%, respectively, with and without demographics. In contrast,
SVM-based classifiers perform more balancedly between classes. For RBF-kernel SVMs, the
sensitivity-specificity gaps are 7.27% and 8.77%, respectively, with and without demographics. The different
sensitivity-specificity gaps of RFs and SVMs might be because that negative-class samples have a long-tail
"diagonal" pattern that thresholding-based classifiers cannot effectively cope with as discussed in â€‹Figure 3â€‹. This
is in line with our discussion in â€‹Section 3.2â€‹. In this sense, the RBF-kernel SVMs are superior to the RFs despite
similar accuracies.

Page 12

Between the two RBF-kernel SVMs, the degraphics-aware one is superior because it has both a high sensitivity
(87.50%) and a high precision (91.46%), i.e., 87.50% moderate COVID-19 subjects are "caught" by the classifier
and 91.46% of those caught by the classifier are actually contracted with COVID-19. The one without
considering age and gender has a slightly higher precision (94.92%) but a much lower sensitivity (80.76%
instead of 87.5%), meaning that nearly 1 out of 5 COVID-19 subjects are misclassified as non-COVID-19.
For all types of classifiers, the demographics-aware results are no worse than the demographics-free results in
terms of sensitivity: 75% in both cases for linear SVMs, 87.5% vs. 81% for RBF-kernel SVMs, 93.75% vs.
92.31% for RFs, and 89.42% vs. 87.98% for DTs. This â€‹aligns well with reports that different age groups respond to
COVID-19 differently and usually older groups are more vulnerable to COVID-19. The feature ranking analysis later in
Section 4.3â€‹ shows that gender plays a smaller role than age.
Hence, the conclusion for the primary task is that the RBF-kernel SVM considering age and gender is the best.
Our web portal and released model is based on it. For the next two tasks, we focus on reporting the results of
the two best classifier families, the RFs and the RBF-SVMs.
Table 3â€‹: Performances of different classifiers on different tasks.
task

Classifier type
SVM, linear kernel

Best hyperparameters2

sensitivity (%) specificity (%) precision (%)

78.78

75.00

82.56

91.23

83.87

87.50

80.23

91.46

2

93.75

72.09

89.41

81.92

89.42

74.42

89.42

78.2

75.00

81.40

90.70

C=1.52, ð›¾=0.00918

85.15

80.76

89.53

94.92

max_d=5, n_tree=20

85.68

92.31

79.07

91.43

Decision tree

max_d = 5

81.78

87.98

75.58

89.71

SVM, RBF kernel

C=6.675, ð›¾=0.0011

92.16

88.98

95.35

96.33

Random Forest

max_d=5, n_tree=50â€‹2

86.35

88.98

93.72

88.24

Secondaryâ€‹: severe vs.
viral
(without age and gender)

SVM, RBF kernel

C=24, ð›¾=0.003397

89.04

83.89

94.18

95.19

Random Forest

max_d=3, n_tree=50â€‹2

85.08

86.44

83.72

87.93

Bonusâ€‹: severe vs.
moderate
(with age and gender)

SVM, RBF kernel

C=3.67, ð›¾=0.00335

76.39

75.21

77.56

65.67

Random Forest

max_d=5, n_tree = 20â€‹2

76.44

67.52

85.37

72.48

Primaryâ€‹: moderate vs. viral SVM, RBF kernel
(with age and gender)
Random Forest

C=45, ð›¾=0.0047
3

max_d = 9, n_tree = 50

Decision tree

max_d = 6

SVM, linear kernel

C=3.01

Primaryâ€‹: moderate vs. viral SVM, RBF kernel
(without age and gender)
Random Forest

Secondaryâ€‹: severe vs.
viral
(with age and gender)

C=1

balanced
accuracy (%)

82.92â€‹

In the secondary taskâ€‹: severe vs. viral (i.e., severe COVID-19 vs. moderate non-COVID-19 pneumonia), the
RBF-SVMs outperform the RFs a lot, with balanced accuracies 92.16% vs. 86.35% when considering age and
gender, and 89.04% vs. 85.08% when not. Unlike in the primary task where the accuracy differences between
SVMs and RFs are within 0.5 percent point, in this task SVMs lead the RFs by 4 to 6 percent points. We
hypothesize that this is due to the wide range of test values for severe subjects, causing extreme samples at
"diagonal" long tails. This aligns with our explanation in â€‹Section 3.2â€‹ that capturing the multidimensional
2
3

For all RFs, the optimal number of features is always the base-2 logarithm of the total number of features.
depending on the random state

Page 13

distribution trend of data for a sloped decision hyperplane is better than thresholding on features. Like the case
in the primary task, both types of classifiers do better when considering age and gender. Not only in terms of
sensitivity but also in terms of accuracy.
For the onus taskâ€‹: severe vs. moderate (i.e., severe COVID-19 vs. moderate COVID-19), we report the
demographics-aware results only for space sake. It turns out to be the most difficult task, with both classifiers
showing very low accuracies, 76.39% for the RBF-SVM and 76.44% for the RF. Hence classifying severe and
moderate COVID-19 subjects from blood tests alone is not effective. Additional modalities, such as radiological
findings or epidemiology information, need to be taken into consideration. However, remember that the bonus
task was introduced out of curiosity rather than clinical needs.

4.3 Medical interpretation of the result
Science is not only about making things work but also why they work. Beyond the promising performances of
machine learning models, we further want to know whether they capture the characteristics of COVID-19 that
can be explained medically. One way to do so is to compute the feature importance, which measures how and
to what extent a feature influences the classifiers. In the example in â€‹Figure 2â€‹, Feature 1 has a heavier influence
on the decision hyperplane's slope than Feature 2, and we may say that Feature 1 is more important than
Feature 2 for that toy task. If the importances of features can agree with medical explanations, then we are more
sure on the effectiveness of using ML to solve our problem. For simplicity, we focus on two simple model
families: the linear SVMs and the DTs here because there is a one-to-one correspondence between raw
features and the elements of the classifiers (weights for linear SVMs or nodes for DTs). It is much harder to
explain an RBF-kernel SVM or an RF. In this part of the study, we focus on the primary task: classifying
moderate COVID-19 and moderate non-COVID-19 viral pneumonia from routine blood tests.
For a linear SVM, the more important a feature is, the (sloped) decision hyperplane will be more perpendicular to
the axis of the feature, like Feature 1 in â€‹Figure 2â€‹, and its corresponding weight in the weight vector W will have
a higher magnitude. When the normalized weight on a feature is 1, the hyperplane is perpendicular to the axis of
the feature, and the linear SVM becomes a comparator or a 1-level DT, like the vertical decision line in â€‹Figure 2â€‹.
A normalized weight of a feature being 0 means that the feature has no influence on the classifier, e.g., Feature
2 has no influence on the vertical decision line.
For the DTs, we used Gini impurity to decide the splits of nodes. Hence, the importance score of a feature to a
DT is its Gini impurity, which characterizes how well two classes can be separated by thresholding on this
feature. Using the example in â€‹Figure 1â€‹, Feature 1 has a higher Gini impurity than Feature 2, because nearly all
samples to the left of the decision line belong to one class and all to the right belong to the other. But a
(non-sloped) decision line on Feature 2 will always result in a fair mixture of both classes. Hence, Feature 1 is
more important than Feature 2.
The importance scores and ranks of features are listed in â€‹Table 4â€‹. Importance scores are the higher the better
while ranks are the lower the better. For the SVM and the DT, CRP ranks at the top. CRP is a nonspecific
marker of systemic inflammation, so it is not surprising to see it elevated in subjects with COVID (which is a
systemic infection) than subjects without. The 0.509 normalized weight on CRP from the linear SVM shows that
the slope of the hyperplane to the CRP axis is nearly 45 degrees, capturing a "diagonal" pattern illustrated in
Figure 2â€‹.
The next few important features capture a clear impact of COVID-19 to our immune system. For the SVM, the
next 3 features (ranks 2 to 4) are WBC count, neutrophil count, and lymphocyte count. For the DT, the 4th and
Page 14

5th most important features are neutrophil percentage, and neutrophil count. WBCs, neutrophils, and
lymphocytes are all cell categories in our immune system. Lâ€‹ymphocytes and neutrophils are two subclasses of
WBCs â€‹fighting against viral and bacterial/fungal infections, respectively. Because all 4 features here are lower in
the COVID-19 class, we hypothesize that our immune system is compromised more by COVID-19 than by other
viral pneumonias. This is in line with reports that low WBC is found in 70% of COVID-19 subjects and
lymphocytopenia in 63% â€‹[2]â€‹.
Not surprisingly, the first organs attacked by COVID-19 are our lungs. The SVM and DT rank D-dimer at the 6th
and 3rd spots, respectively, while the DT ranks platelet count at the 7th. D-dimer is a small protein fragment in
the blood after a blood clot is degraded by fibrinolysis. The increase of D-dimer in blood for COVID-19 is
probably due to more blood clots in lungs than non-COVID-19 viral infections. Platelets are also related to blood
clots. The lower amount of platelets for COVID-19 is probably a result of consuming too many of them on
forming clots. HGB is the protein responsible for carrying oxygen in red blood cells. It is the 6th most important
feature per the DT. Its increase for COVID-19 could be a possible reaction to boost up the oxygen carrying
function in our blood as clots in the lungs narrows the interface to air.
Table 4â€‹: Feature ranking according to two ML models, a linear SVM and a DT, and how their means change in
COVID-19 with respect to non-COVID-19 pneumonia. Importance scores are the higher the better while ranks
are the lower the better.
feature
ID

feature name

importance (rank)
by SVM

by DT

change in COVID-19
w.r.t. non-COVID-19

1

WBC

0.082 (2)

0.008 (8)

lower

2

HGB

0.015 (12)

0.024 (6)

higher

3

platelet

0.000 (15)

0.023 (7)

lower

4

neutrophil %

0.019 (11)

0.050 (4)

lower

5

neutrophil #

0.052 (4)

0.027 (5)

lower

6

lymphocyte %

0.047 (7)

0.000 (9)

N/A

7

lymphocyte #

0.076 (3)

0.000 (10)

lower

8

CRP

0.509 (1)

0.638 (1)

higher

9

TBil

0.051 (5)

0.000 (11)

higher

10

BUN

0.030 (9)

0.000 (12)

lower

11

creatinine

0.039 (8)

0.143 (2)

higher

12

LDH

0.027 (10)

0.000 (13)

higher

13

D-dimer

0.049 (6)

0.087 (3)

higher

14

age

0.003 (13)

0.000 (14)

N/A

15

gender

0.002 (14)

0.000 (15)

N/A

As we go down the ranks, the damage of COVID-19 spreads to the kidneys and the liver. SVM ranks TBL as the
6th most important feature. Increased TBL indicates damage to the kidney. In our data, COVID-19 subjects have
higher TBL than non-COVID-19 subjects, showing possible more damage to the kidney caused by COVID-19
than by non-COVID-19 viral pneumonia. Although there is little report in literature about kidney damage to mild
Page 15

or moderate COVID-19 patients, it was found prevalent in patients who died from COVID-19 â€‹[27]â€‹, with possible
renal cell infections by the virus â€‹[28]â€‹. The 9th most important feature by SVM is BUN, which indicates how well
both the kidneys and liver function. We observed decreased BUN in moderate COVID-19 patients than
moderate non-COVID-19 patients. Because liver damage lowers the BUN level while kidney damage raises the
BUN level, we hypothesize that COVID-19 causes severe damage to the liver, given that we already know that
the kidneys are damaged badly from the rank (5th) of TBL per the SVM. Liver damage has been found prevalent
in COVID-19 patients â€‹[29]â€‹ without pre-existing liver conditions. Go back to the 8th most important feature by
SVM, creatinine, which is also the 2nd most important feature by the DT. Creatinine is synthesized by the liver
and removed from the blood by the kidneys. Because the analysis above hypothesizes liver damage which
reduces its synthesis, we infer that creatinine increase for COVID-19 is due to the kidney damage.
The 10th most important feature per the SVM is â€‹LDH. Like CRP, it is also a nonspecific laboratory finding that is
found in many different diseases. It is highly expressed in tissues throughout the body, and its elevation is often
a marker of tissue damage. So in patients who are more septic (more severe infections) it would be higher. It is
also in other contexts, such as liver failure, hemolysis (destruction of red blood cell), etc. Liver damage was
discussed above. We hypothesize that the hemolysis is related to the high HGB levels in our data and the low
blood oxygen symptom commonly seen on COVID-19 patients. Other study shows a very high level of LDH in
>70% of COVID-19 subjects â€‹[2]â€‹.
Lastly, both age and gender are ranked at or near the bottom for both classifiers, showing that age and gender
play a little role in blood test changes due to COVID-19.

4.4 The web portal, source code and models
We believe openness as a core value of science. Hence we will release our results in 3 ways. First, we release a
web portal for medical workers around the world to directly use our results in their clinical practices. The web
portal is basically a web form with 15 boxes, corresponding to the 13 features plus age and gender. It can be
opened on any computer or smartphone through a web browser. Entering the values for them and then clicking
the submit button will generate a prediction, COVID-19 or not. To protect the privacy of the patients, no user
data will leave the browser including being uploaded to our server. Our web portal serves only as a distributor to
deliver the trained SVM model and parameters to the user's browser, and then all computation is done locally in
the browser. The computer can even be disconnected from the Internet after the portal finishes loading. Or, the
web portal can also be saved as a web page, copied to any computer or smartphones, and opened in any web
browser. The web portal is developed in libsvm-js4, which is compiled from LIBSVM using Emscripten, a
transpiler from C++ to JavaScript. The web portal is at â€‹http://forrestbao.github.io/covid-19
The trained models, in both libsvm and python/sklearn format, are also released in the Github repository
https://github.com/forrestbao/covid-19â€‹ Our source code is to be released upon the acceptance of this paper.

7. Conclusion and future work
In this paper, we proposed an approach to automatically separate COVID-19 and non-COVID-19 subjects both
showing moderate pneumonia symptoms based on routine blood tests using machine learning. This approach is
developed in response to the cost, availability, and throughout limits of other COVID-19 screening methods. We
tested this approach on a dataset of 86 moderate non-COVID-19 viral pneumonia patients and 208 moderate
COVID-19 patients. The initial result is promising with an RBF-kernel SVM achieving an 84% accuracy (class
balanced), an 88% sensitivity, and a 92% precision. We further studied whether this approach can be used to
4

â€‹https://github.com/mljs/libsvm

Page 16

separate severe COVID-19 against non-COVID-19 viral and to determine the stage of COVID-19. Our results
can be explained from both machine learning and medical backgrounds. If validated on a larger population, our
approach can provide another rapid COVID-19 testing solution that can be done using existing lab facilities
capable of routine blood tests.

Acknowledgments
Bao and He's work is partially supported by (U.S.) National Science Foundation (NSF) under grant numbers
MCB-1821828 and CNS-1817089. Xu's work is partially supported by the National Science Foundation for
Young Scientists of China (81703201), the Natural Science Foundation for Young Scientists of Jiangsu Province
(BK20171076), the Jiangsu Provincial Medical Innovation Team (CXTDA2017029), the Jiangsu Provincial
Medical Youth Talent program (QNRC2016548), the Jiangsu Preventive Medicine Association program
(Y2018086), the Lifting Program of Jiangsu Provincial Scientific and Technological Association, and the Jiangsu
Government Scholarship for Overseas Studies. Xinran Wang and Zeyu Yang helped build the web portal. Drs.
Cen Chen and Minghui Qiu of Alibaba Group, China, also provided feedback on the experimental design. The
authors would like to thank Tianyu Yan of Pennsylvania State University for her help on preparing the
manuscript.

References
[1] The Lancet Infectious Diseases, â€œCOVID-19: endgames,â€ â€‹Lancet Infect. Dis.,â€‹ Apr. 2020, doi:
10.1016/S1473-3099(20)30298-Xâ€‹.
[2] C. Huang â€‹et al.,â€‹ â€œClinical features of patients infected with 2019 novel coronavirus in Wuhan, China,â€
Lancetâ€‹, vol. 395, no. 10223, pp. 497â€“506, Feb. 2020.
[3] F. He, Y. Deng, and W. Li, â€œCoronavirus disease 2019: What we know?,â€ â€‹J. Med. Virol.â€‹, Mar. 2020, doi:
10.1002/jmv.25766â€‹.
[4] H. X. Bai â€‹et al.â€‹, â€œPerformance of radiologists in differentiating COVID-19 from viral pneumonia on chest CT,â€
Radiologyâ€‹, p. 200823, Mar. 2020.
[5] L. L. Maragakis, â€œCoronavirus Disease 2019 vs. the Flu,â€ â€‹Johns Hopkins Medicineâ€‹.
https://www.hopkinsmedicine.org/health/conditions-and-diseases/coronavirus/coronavirus-disease-2019-vsthe-fluâ€‹ (accessed Apr. 30, 2020).
[6] J. M. Sharfstein, S. J. Becker, and M. M. Mello, â€œDiagnostic Testing for the Novel Coronavirus,â€ â€‹JAMA,â€‹ Mar.
2020, doi: â€‹10.1001/jama.2020.3864â€‹.
[7] A. Harmon, â€œWhy We Donâ€™t Know the True Death Rate for Covid-19,â€ â€‹New York TImesâ€‹, Apr. 17, 2020.
[8] National Heart, Lung, and Blood Institute, â€œBlood tests.â€ â€‹https://www.nhlbi.nih.gov/health-topics/blood-tests
(accessed Apr. 30, 2020).
[9] L. Breiman, â€œRandom Forests,â€ â€‹Mach. Learn.â€‹, vol. 45, no. 1, pp. 5â€“32, Oct. 2001.
[10] C. Cortes and V. Vapnik, â€œSupport-vector networks,â€ â€‹Mach. Learn.â€‹, vol. 20, no. 3, pp. 273â€“297, Sep. 1995.
[11] M. FernÃ¡ndez-Delgado, E. Cernadas, S. Barro, and D. Amorim, â€œDo we Need Hundreds of Classifiers to
Solve Real World Classification Problems?,â€ â€‹J. Mach. Learn. Res.â€‹, vol. 15, no. 90, pp. 3133â€“3181, 2014,
Accessed: May 03, 2020. [Online].
[12] M. Wainberg, B. Alipanahi, and B. J. Frey, â€œAre Random Forests Truly the Best Classifiers?,â€ â€‹J. Mach.
Learn. Res.,â€‹ vol. 17, no. 110, pp. 1â€“5, 2016, Accessed: May 03, 2020. [Online].
[13] S. Sathe and C. C. Aggarwal, â€œNearest Neighbor Classifiers Versus Random Forests and Support Vector
Machines,â€ in â€‹2019 IEEE International Conference on Data Mining (ICDM)â€‹, Nov. 2019, pp. 1300â€“1305.
[14] National Health Commission of China, English translation by WHO China office, â€œDiagnosis and Treatment
Protocol for Novel Coronavirus Pneumonia (Trial version 7),â€ Mar. 2020. [Online]. Available:
https://www.chinadaily.com.cn/pdf/2020/1.Clinical.Protocols.for.the.Diagnosis.and.Treatment.of.COVID-19.
V7.pdfâ€‹.
[15] N. Chen â€‹et al.,â€‹ â€œEpidemiological and clinical characteristics of 99 cases of 2019 novel coronavirus
Page 17

pneumonia in Wuhan, China: a descriptive study,â€ â€‹Lancetâ€‹, vol. 395, no. 10223, pp. 507â€“513, Feb. 2020.
[16] L. Breiman, J. Friedman, C. J. Stone, and R. A. Olshen, â€‹Classification and regression trees.â€‹ CRC press,
1984.
[17] L. E. Raileanu and K. Stoffel, â€œTheoretical Comparison between the Gini Index and Information Gain
Criteria,â€ â€‹Ann. Math. Artif. Intell.â€‹, vol. 41, no. 1, pp. 77â€“93, May 2004.
[18] U. M. Fayyad and K. B. Irani, â€œThe Attribute Selection Problem in Decision Tree Generation,â€ in â€‹Proceedings
of the Tenth National Conference on Artificial Intelligence (AAAIâ€™92),â€‹ 1992, pp. 104â€“110.
[19] B. E. Boser, I. M. Guyon, and V. N. Vapnik, â€œA training algorithm for optimal margin classifiers,â€ in â€‹COLT â€™92:
Proceedings of the fifth annual workshop on Computational Learning Theoryâ€‹, 1992, pp. 144â€“152.
[20] R. Kohavi, â€œA study of cross-validation and bootstrap for accuracy estimation and model selection,â€ in
IJCAIâ€™95: Proceedings of the 14th international joint conference on Artificial intelligence,â€‹ 1995, vol. 14, pp.
1137â€“1145.
[21] C. wei Hsu, C. chung Chang, and C. jen Lin, â€œA practical guide to support vector classification,â€ â€‹National
Taiwan University, Taiwan, Tech. Repâ€‹, 2010.
[22] J. Bergstra and Y. Bengio, â€œRandom Search for Hyper-Parameter Optimization,â€ â€‹J. Mach. Learn. Res.â€‹, vol.
13, no. 10, pp. 281â€“305, 2012, Accessed: May 09, 2020. [Online].
[23] F. Pedregosa â€‹et al.,â€‹ â€œScikit-learn: Machine Learning in Python ,â€ â€‹J. Mach. Learn. Res.â€‹, vol. 12, pp.
2825â€“2830, 2011.
[24] J. Davis and M. Goadrich, â€œThe relationship between Precision-Recall and ROC curves,â€ in â€‹Proceedings of
the 23rd international conference on Machine learningâ€‹, Pittsburgh, Pennsylvania, USA, Jun. 2006, pp.
233â€“240, Accessed: Apr. 30, 2020. [Online].
[25] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin, â€œLIBLINEAR: A Library for Large Linear
Classification,â€ â€‹J. Mach. Learn. Res.,â€‹ vol. 9, no. Aug, pp. 1871â€“1874, 2008, Accessed: Apr. 30, 2020.
[Online].
[26] C.-C. Chang and C.-J. Lin, â€œLIBSVM: A library for support vector machines,â€ â€‹ACM Transactions on
Intelligent Systems and Technologyâ€‹, vol. 2, pp. 27:1â€“27:27, 2011.
[27] Y. Cheng â€‹et al.,â€‹ â€œKidney disease is associated with in-hospital death of patients with COVID-19,â€ â€‹Kidney Int.,â€‹
vol. 97, no. 5, pp. 829â€“838, May 2020.
[28] H. Su â€‹et al.â€‹, â€œRenal histopathological analysis of 26 postmortem findings of patients with COVID-19 in
China,â€ â€‹Kidney Int.â€‹, Apr. 2020, doi: â€‹10.1016/j.kint.2020.04.003â€‹.
[29] C. Zhang, L. Shi, and F.-S. Wang, â€œLiver injury in COVID-19: management and challenges,â€ â€‹Lancet
Gastroenterol Hepatol,â€‹ vol. 5, no. 5, pp. 428â€“430, May 2020.

Page 18

