Learning to Match Distributions for Domain
Adaptation

arXiv:2007.10791v3 [cs.LG] 27 Jul 2020

Chaohui Yu1 , Jindong Wang2âˆ—, Chang Liu2 , Tao Qin2 ,
Renjun Xu3 , Wenjie Feng1 , Yiqiang Chen1 , Tie-Yan Liu2
1
ICT, CAS 2 Microsoft Research 3 Zhejiang University

Abstract
When the training and test data are from different distributions, domain adaptation
is needed to reduce dataset bias to improve the modelâ€™s generalization ability.
Since it is difficult to directly match the cross-domain joint distributions, existing
methods tend to reduce the marginal or conditional distribution divergence using
predefined distances such as MMD and adversarial-based discrepancies. However,
it remains challenging to determine which method is suitable for a given application
since they are built with certain priors or bias. Thus they may fail to uncover the
underlying relationship between transferable features and joint distributions. This
paper proposes Learning to Match (L2M) to automatically learn the cross-domain
distribution matching without relying on hand-crafted priors on the matching loss.
Instead, L2M reduces the inductive bias by using a meta-network to learn the
distribution matching loss in a data-driven way. L2M is a general framework that
unifies task-independent and human-designed matching features. We design a
novel optimization algorithm for this challenging objective with self-supervised
label propagation. Experiments on public datasets substantiate the superiority of
L2M over SOTA methods. Moreover, we apply L2M to transfer from pneumonia
to COVID-19 chest X-ray images with remarkable performance. L2M can also
be extended in other distribution matching applications where we show in a trial
experiment that L2M generates more realistic and sharper MNIST samples.

1

Introduction

Traditional machine learning generally assumes that training and test data are from the same distribution. In reality, this i.i.d. assumption barely holds. When an algorithm is trained on one domain
and then tested on another domain, the performance is likely to drop due to the different data distribution [1]. Since the collection of massive labeled data is expensive and time-consuming, a more
promising approach is to perform domain adaptation (DA) to enable the consistent performance of a
predictive function on different domains.
The core challenge of DA is to match the cross-domain joint distributions [2]. However, the labels on
the target domain are often unavailable in unsupervised DA. Therefore, a trend is to approximately
match the joint distributions by matching the marginal and conditional distributions as theoretically
verified in [2, 3]. Existing approaches achieve this goal via learning a domain-invariant representation
by minimizing predefined distribution distances such as MMD [4â€“7], or an implicit discrepancy by
an adversarial min-max game [8â€“11]. Recent works suggest that in addition to jointly matching these
two distributions with equal weights [12], an adaptive weighting scheme is necessary to achieve
better distribution matching performance [7, 13â€“15].
âˆ—

The first two authors contributed equally. Correspondence to: jindong.wang@microsoft.com

Preprint. Under review.

Unfortunately, it remains challenging to apply DA to new applications. Existing methods are built
with their own priors and inductive bias in approximating the joint distribution matching, which may
fail to uncover the underlying relationship between transferable features and joint distributions [16].
For instance, MMD [17] may not be discriminative enough for high-dimensional data, and JensenShannon divergence is not sensitive to mode collapse [18â€“21]. A recent work Learning to Transfer
(L2T) [22] aims to reduce such bias by learning the â€œtransfer experienceâ€ from thousands of precomputed tasks before applying to new problems. However, L2T needs to build historical tasks from
large auxiliary datasets, which is expensive and burdensome. Since deep learning makes it possible
to learn features directly from the original datasets, can we design an automatic distribution matching
strategy in a data-driven way?
In this work, we propose a Learning to Match (L2M) framework to automatically match the crossdomain distributions while reducing the inductive bias on matching functions. Stepping back from the
hand-crafted and predefined distances, we construct a meta-network to learn the distribution matching
functions directly from the source and target domains. The meta-network is an MLP network which
is theoretically a universal approximator for almost any continuous function [23]. We design a novel
matching feature generator to L2M, where both task-independent and human-designed matching
features can be taken as inputs to the meta-network for better distribution matching. Therefore,
L2M can be seen as a general framework that unifies the deep features and human-crafted features
(pre-defined distances) from the view of traditional vs. deep learning. Since it is challenging to
optimize L2M with the unavailability of target domain labels, we propose to construct and update
meta-data in a self-supervised manner [42] for updating the distribution matching loss. On the basis
of matching features and meta-data, we propose an online optimization algorithm for L2M which can
achieve accurate and steady performance.
Experiments show that L2M outperforms several state-of-the-art methods on public DA datasets.
L2M is a general and flexible framework that can be used in other cross-domain tasks. We apply
L2M to COVID-19 X-ray image classification by transferring knowledge from normal pneumonia
to COVID-19, where L2M outperforms other methods in this data-hungry and imbalanced task. As
an extension, L2M can be used for generating more realistic and sharper hand-written digits. The
code of L2M will be released soon at https://github.com/jindongwang/transferlearning/
tree/master/code/deep/Learning-to-Match.

2

Related Work

Transfer learning and domain adaptation. Domain adaptation (DA) is a specific area of transfer
learning [24]. Existing works tend to explicitly or implicitly reduce the distribution divergence. The
explicit distances are predefined divergence, such as Maximum Mean Discrepancy (MMD) [17], KL
or JS divergence, cosine similarity, mutual information, and higher-order moments [25], which are
well investigated in recent DA works [4â€“7, 26]. Optimal transport (OT) is another popular measure for
distribution matching [11, 27â€“30]. There are other geometrical distances or transforms such GFK [31]
and subspace learning [32, 33]. The implicit distance indirectly bridges the distribution gap through
adversarial nets [34], or learnable metrics [35]. GAN-based DA methods learn domain-invariant
features by confusing the feature extractor and discriminator [8â€“10], while metric learning [35]
focuses on the sample-wise distance. Recent research implies performance improvement by adding
more prior to the matching strategy such as adaptive weights between marginal and conditional
distributions [7, 14, 15] with weights generated by the A-distance [2]. Learning to transfer (L2T) [22]
is similar to our idea in spirit. However, L2T has to manually construct thousands of transfer tasks
to learn a linear transformation matrix using MMD, while L2M does not rely on historical tasks
and learns non-linear feature maps, which is more efficient and general. There are several works
aiming at bridging two domains by normalization such as BN [36], AutoDIAL [37], AdaBN [38],
and TransNorm [39], which did not focus on direct learning the cross-domain joint distributions.
Distribution matching. Generative adversarial nets (GANs) [34] matches distributions between
training and generated samples by iteratively training a domain discriminator and generator to
confuse the discriminator. Our L2M is model-agnostic that can be applied in an adversarial manner
by adopting GAN-based schemes such as DANN [8] or can also work without GAN. The pixel-level
DA [40] learns the distribution matching in pixel-space.
2

ğ’™ğ‘ 
ğ’™ğ‘¡

ğºğ‘¦

â„’cls

Main model update: ğœ™ ğ‘¡ + 1 â† ğœ™ ğ‘¡ âˆ’

ğœ™(ğ‘¡)

(train)

ğ›¼âˆ‡ğœ™ [â„’cls

ğœ™(ğ‘¡ + 1)

(train)

ğœ™ +ğœ†â„’ match ğœƒ ğ‘¡ ] á‰š

ğœ™(ğ‘¡)

ğ‘“ğœ™
ğ·

âˆª

Meta-network update:

ğ‘”ğœƒ â„’match

ğœƒ(ğ‘¡)

(meta)

ğœƒ(ğ‘¡ + 1) â† ğœƒ ğ‘¡ âˆ’ ğ›½âˆ‡ğœƒ â„’val

ğœƒ á‰š

Matching feature generator

(a) The architecture

ğœƒ(ğ‘¡ + 1)

ğœƒ(ğ‘¡)

(b) The learning procedure

Figure 1: The framework and computing flow of the proposed L2M approach.

3
3.1

Methodology
Learning to Match

We can decompose hÏ† into a feature extractor fÏ† and a classification layer Gy , where fÏ† is explicitly
parameterized by Ï† since it is more important for domain-invariant representation learning. Under the
principle of structural risk minimization (SRM) [41], the optimal model parameter can be learned as:
Ï†? = arg min Lcls (Gy â—¦ fÏ† ; Ds ) + Î»Lmatch (fÏ† (Ds ), fÏ† (Dt )),
Ï†

(1)

where Lcls is the classification loss on the source domain, Lmatch is the distribution matching loss,
and Î» is a trade-off parameter.
It is challenging to directly match the cross-domain joint distributions since the labels for the target
domain are not available. Therefore, existing methods tend to approximate Lmatch using different
priors. For instance, if we let Lmatch = d(Ds , Dt ) where d is a predefined distance such as MMD [4],
then we can get the explicit distribution matching. If Lmatch = D(Ds , Dt ) where D is the adversarial
discriminator [8], then we get the implicit distribution matching. In a nutshell, the main difference
among existing works is the design of explicit or implicit Lmatch .
In this paper, we postulate L2M to automatically match the distributions across domains. The core
of L2M is a meta-network that learns the distribution matching in a data-driven manner. To be
specific, the meta-network is a Multi-Layer Perceptron (MLP) that has the ability to approximate any
continuous functions [23]. Therefore, L2M can learn the distribution matching loss directly from the
source and target domains:
Lmatch (Ds , Dt ) = gÎ¸ (fÏ† (Ds ), fÏ† (Dt )),

(2)

where g(Â·) is the distribution matching function (network) parameterized by Î¸. It is clear that this
formulation is a general form that can theoretically include existing pre-defined distances.
With the meta-network gÎ¸ , we build a general framework as shown in Fig. 1(a). The architecture
consists of four parts: feature extractor fÏ† , label classifier Gy , meta-network gÎ¸ , and matching feature
generator. Specifically, fÏ† is a CNN network to extract the features of the input domains, Gy is
trained to minimize the prediction loss on the labeled (source) domain, and gÎ¸ is an MLP network,
which is used to match the cross-domain distributions (learn Lmatch ). The most important part is
the matching feature generator, which generates useful inputs to the meta-network gÎ¸ . For a general
framework that allows both deep and human-designed features, we concatenate the deep features
(direct link) with the human-designed distances (the green module D) via the concatenation module
âˆª. Then, the matching features can be taken as inputs by the meta-network gÎ¸ to learn the distribution
matching functions.
Our learning objective is well in accordance with the DA theory proposed in [2] that directly learns
to reduce the distribution divergence between domains, such that the risk on the target domain can be
bounded. The learning objective of L2M can be obtained as:
min = arg min Lcls (Ï†) + Î»Lmatch (Ï†; Î¸).
Î¸,Ï†

(3)

However, it remains challenging to optimize the above equation due to three reasons. Firstly, what
kind of matching features should we take as inputs to the meta-network gÎ¸ for better distribution
matching? Secondly, we only have the labeled source domain and the unlabeled target domain,
how to compute the distribution matching loss Lmatch without the target domain labels? Thirdly,
3

even if we have the matching features and optimization data, we cannot use a simple EM algorithm
for optimization since updating Lcls and Lmatch on the same training data will definitely lead to
overfitting and local optimum. Therefore, it is still non-trivial to optimize L2M.
In the next sections, we introduce how to tackle the above three challenges.
3.2

Matching feature generator

The matching feature generator generates useful representations as inputs to the meta-network g. We
use F to denote the matching features. Technically, F can be any useful representations. In this paper,
we propose two different kinds of matching features as shown in Table 1: (1) Task-independent
features, which are general and can be automatically computed by the main network fÏ† as shown
in the direct link of Fig. 1(a); (2) Human-designed distances (features) as indicated in the green
module D in Fig. 1(a), which are the pre-defined distances such as MMD or adversarial game.
These features can either be used alone, or be concatenated by the concatenation module âˆª. In later
experiments, it is surprising to find that the combination of these two features can be seen as the
combination of deep and human-designed features, which generally leads to better performance.
More details of the matching features are in the supplementary file.
Table 1: Description of the matching features. q(Â·) is the function of last layer before softmax. dm
and dc are marginal and conditional explicit distances using MMD. Dm and Dc are marginal and
conditional implicit distances using adversarial min-max game. [a, b] is the concatenation of a and b.
Feature Type
Task-independent
Human-designed
(pre-defined dist.)

3.3

Notation
Femb
Flogit
Fmmd
Fadv

Description
Feature embedding
logit
Explicit distribution distance
Implicit distribution distance

Calculation
xi ), fÏ† (x
xj )]
Femb = [fÏ† (x
xi )), q(fÏ† (x
xj ))]
Flogit = [q(fÏ† (x
xi ), fÏ† (x
xj )), dc (fÏ† (x
xi ), fÏ† (x
xj ))]
Fmmd = [dm (fÏ† (x
xi ), fÏ† (x
xj )), Dc (fÏ† (x
xi ), fÏ† (x
xj ))]
Fadv = [Dm (fÏ† (x

The construction of meta-data

We introduce the idea of â€œmeta-dataâ€. Since direct computation of the distribution matching loss
Lmatch is hard due to the unavailability of target labels, we turn to using the meta-data Dmeta instead.
xtj }mÃ—C
x, yÌ‚) where yÌ‚ is the predicted (pseudo) label on
To be more specific, Dmeta = {x
j=1 âˆ¼ Pt (x
the target domain. In each iteration, we randomly sample m instances for each class with high
prediction scores calculated by the main network as the ground truth of the meta-data. This selection
is iterated in the whole learning process for better performance. The pseudo labels of the meta-data
can get more confident since the meta-data are chosen from the target domain data with the highest
prediction probabilities. This assumption is validated in early works [7, 10] and can also be seen
as a self-supervised technique [42]. Therefore, the matching loss is calculated on the training data
(train)
(Lmatch = Lmatch ) when updating the main network fÏ† , and the meta-network gÎ¸ is updated on the
meta-data.
3.4

Learning algorithm

In this paper, we propose an online updating algorithm for L2M. Fig. 1(b) illustrates the key learning
steps. It should be noted that the data for updating Ï† and Î¸ are different: when updating Ï†, we use the
normal training data from the source domain to calculate the cross-entropy loss; when updating Î¸, we
use the source domain and the pseudo-labeled target domain meta-data. The learning procedure of
L2M consists of two main steps: main network update and meta-network update. In the following,
we use t to denote learning steps.
Main network update. This step is mainly for updating Ï† for the main network. To enforce the
update of Î¸ in the next step, we construct an assist model which is a copy of the main model by
inheriting the same architecture and parameters from the main model (fÏ† , Gy , gÎ¸ ) and use it for
calculating the loss. We employ SGD for optimizing the classification loss Lcls and distribution
matching loss Lmatch . Lcls can be formulated as:
(train)

Lcls

x)), y),
= E(xx,y)âˆ¼Bs `(CE) (Gy (fÏ† (x
4

(4)

where `(CE) is the cross-entropy loss and Bs denotes a mini-batch data sampled from Ds . The
distribution matching loss Lmatch is calculated by the meta-network gÎ¸ :
(train)

xi ), fÏ† (x
xj ); Ï†),
Lmatch = Ex i âˆ¼Bs ,xxj âˆ¼Bt gÎ¸ (fÏ† (x

(5)

where Bt is a mini-batch data sampled from Dt . Note that this step does not need the meta-data from
x) and do not need
the target domain since we only sample a batch of source and target domain data (x
the target domain label y. Therefore, we do not update the matching loss.
After getting the training loss, the updating equation of the copied main model can be obtained by
moving the current Ï†(t) towards the descent direction of objective in Eq. (3):
(train)

Ï†(t + 1) = Ï†(t) âˆ’ Î±âˆ‡Ï† [Lcls

(train)

(Ï†) + Î»Lmatch (Ï†; Î¸(t))]|Ï†(t) ,
(train)

where Î± is the learning rate of the assist model. Lcls
Eq. (4) and (5).

(6)

(train)

(Ï†) and Lmatch (Ï†; Î¸(t)) are computed by

Meta-network update. This step is for updating Î¸ for the meta-network gÎ¸ on the meta-data Dmeta .
Similar to updating Ï†, it is natural that updating Î¸ requires â€œground-truthâ€ available for the distribution
matching loss Lmatch . However, this is not available in UDA problems. To solve this challenge,
we employ a self-supervised strategy with the assumption that after one epoch of updating Ï†(t) to
Ï†(t + 1), the distribution matching loss can get smaller with the increasing confidence of the target
pseudo labels. This pseudo-label assumption is widely adopted in previous DA works [6, 7, 10].
Therefore, this validation loss can be updated by computing the discrepancy between the distribution
matching loss on Ï†(t) and Ï†(t + 1):
(meta)

Lval

x; Ï†(t))) âˆ’ gÎ¸ (f (x
x; Ï†(t + 1)))),
= Ex âˆ¼Dmeta tanh(gÎ¸ (f (x

(7)

where tanh(Â·) is an activation function. Note that we fix Ï† in this step and minimize Eq. (7) w.r.t.
Î¸ can gradually update the meta-network gÎ¸ . The pseudo labels can be easily obtained by a single
forward-pass and then selected according to the confidence (softmax probability). To ensure their
confidence, we choose the samples with probabilities â‰¥ 0.8 in our experiments.
Denote Î² the learning rate of meta-network gÎ¸ , then Î¸ can be updated as:
(meta)

Î¸(t + 1) = Î¸(t) âˆ’ Î²âˆ‡Î¸ Lval

(Î¸; Ï†(t), Ï†(t + 1))|Î¸(t) .

(8)

The above two steps are used iteratively as the pseudo labels of the meta-data can be more confident
and all the losses can be iteratively minimized. In our experiments, we observe that the network will
converge in dozens of epochs. The complete algorithm and convergence analysis are presented in the
supplementary file.
As for inference, L2M is the same as existing DA methods [6, 7, 10, 43]. We simply fix Ï† and Î¸ and
use the main model to perform a single forward-pass to get the results for the test data.

4
4.1

Experiments on Public Datasets
Experimental setup

Datasets. We adopt four public datasets: ImageCLEF-DA [44], Office-Home [45], VisDA2017 [46] and Office-31 [47]. They are widely used by most UDA approaches [7, 9, 10, 43].
The detailed dataset descriptions are presented in the supplementary file.
Baselines and implementations. We comapre L2M with several recent DA methods: ResNet [48],
DDC [6], DAN [5], DANN [8], JAN [49], MADA [50], CAN [51], MEDA [7], DAAN [15],
CDAN [43], DeepJDOT [29], MDD [10], and TransNorm [39]. The main network of all methods
including L2M are based on ImageNet-pretrained ResNet50. The hyperparameter setting for L2M
are presented in the supplementary file. We follow the standard protocols for UDA and take classification accuracy on the target domain as the evaluation metric and target labels are only used for
evaluation. The best parameters are tuned according to [52]. The results are the average accuracy of
10 experiments by following the same protocol [6, 7, 10, 43].
5

4.2

Analysis of matching features

Before using L2M, a natural question is which matching feature should be used for better performance. Moreover, how is the performance of MMD and adversarial discrepancy in L2M
compared to existing MMD or adversarial-based DA methods? To answer these questions, we
randomly choose two pairs of DA tasks from Office-Home dataset (R â†’ A, R â†’ P, and vice
versa) to compare the performance of existing distance-based methods (DAN [5] and MEDA [7]
use MMD while DANN [8], CDAN [43], and DAAN [15] use adversarial-based discrepancy)
with L2M. Technically, all matching features can be combined, which will result in 24 = 16
different matching features. For computational issue, we construct eight matching features:
{Femb , Flogit , Fmmd , Fadv , [Femb , Fmmd ], [Femb , Fadv ], [Flogit , Fmmd ], [Flogit , Fadv ]}. It should
be noted that both Femb and Flogit can be applied to both explicit (deep) and implicit (adversarial)
matching networks, leading to ten features in total. In addition, we do not combine three or four
features since their performance can naturally be better but with more computations.
Table 2: Matching features of L2M.

Explicit

Implicit

Method
DAN (marginal) [5]
MEDA (joint) [7]
L2M (emb)
L2M (logit)
L2M (mmd)
L2M (emb+mmd)
L2M (logit+mmd)
DANN (marginal) [8]
DAAN (joint) [15]
CDAN (conditional) [43]
L2M (emb)
L2M (logit)
L2M (adv)
L2M (emb+adv)
L2M (logit+adv)

Râ†’A
63.1
61.2
71.1
70.3
69.3
71.1
71.5
63.2
66.3
70.9
70.8
71.6
72.7
71.8
71.4

Aâ†’R
67.9
68.8
76.1
76.6
73.4
76.9
76.7
70.1
73.7
76.0
77.8
71.7
78.5
79.3
76.6

Pâ†’R
67.7
72.9
79.1
79.4
75.2
78.6
78.5
76.8
74.0
77.3
79.3
79.4
80.3
80.6
78.6

Râ†’P
74.3
76.0
83.7
83.6
83.2
83.1
82.8
68.5
78.8
81.6
83.2
83.6
83.1
83.5
82.8

The feature dimensions of each matching feature are presented in the supplementary file. The
comparison results are in Table 2. For better clarification, we compare the performance of best
MMD- and adversarial-based methods in Fig. 2(a), along with the average performance of L2M
using these features. More experiments can be found at the supplementary. Firstly, we see that in both
explicit and implicit distribution matching, L2M can generally achieve competitive performance with
different matching features. This verifies that L2M is effective for distribution matching. Secondly, in
some cases, the performance of L2M with MMD distances are better than previous adversarial-based
methods. Since adversarial-based methods require much more training time, this makes L2M+MMD
suitable solutions for resource-constrained applications. Thirdly, the performance of L2M with both
task-independent features and pre-defined distances are generally better than using each feature
solely, indicating the common practice is useful that deep learning performance can be boosted by
combining deep features (Femb or Flogit ) with human-designed features (Fmmd or Fadv ). We also
observe that L2M with embeddings are generally better than logits, which is probably because those
embeddings contain richer information than logits. Therefore, in the next experiments, we adopt
[Femb , Fadv ] for a balance between computation and better performance. In real applications, more
domain-dependent matching features can be added according to domain knowledge.
4.3

Performance against SOTA methods

The results on Office-Home dataset are shown in Table 3, while the results on ImageCLEF-DA
and VisDA-17 datasets are in Table 4. The results on Office-31 are provided in supplementary file
due to space limits. From the results, we see that the L2M outperforms all comparison methods.
Specifically, on ImageCLEF-DA dataset, although the baseline for this dataset is very high, L2M
still achieves an average accuracy of 89.1% with a 0.6% improvement over the second-best baseline.
On Office-Home dataset, L2M achieves an average accuracy of 69.6% with a 1.5% improvement
compared to the second-best. Office-Home dataset is rather complicated and involves more samples
and categories, which indicates the effectiveness of L2M. On Office-31 dataset, L2M achieves an
average accuracy of 89.5%, which is also highly competitive. Last, on the VisDA-17 dataset, which
6

Table 3: Accuracy (%) on Office-Home for UDA (ResNet-50).
Method
ResNet [48]
DAN [5]
DANN [8]
JAN [49]
MEDA [7]
DAAN [15]
CDAN [43]
ALDA [64]
CDAN+TransNorm [39]
MDD [10]
L2M

Aâ†’C
34.9
43.6
45.6
45.9
46.6
50.5
50.7
53.7
50.2
54.9
57.5

Aâ†’P
50.0
57.0
59.3
61.2
68.9
65.0
70.6
70.1
71.4
73.7
74.0

Aâ†’R
58.0
67.9
70.1
68.9
68.8
73.7
76.0
76.4
77.4
77.8
78.5

Câ†’A
37.4
45.8
47.0
50.4
49.0
53.7
57.6
60.2
59.3
60.0
63.0

Câ†’P
41.9
56.5
58.5
59.7
66.4
62.7
70.0
72.6
72.7
71.4
73.1

Câ†’R
46.2
60.4
60.9
61.0
66.1
64.6
70.0
71.5
73.1
71.8
72.5

Pâ†’A
38.5
44.0
46.1
45.8
51.8
53.5
57.4
56.8
61.0
61.2
63.5

Pâ†’C
31.2
43.6
43.7
43.4
45.0
45.2
50.9
51.9
53.1
53.6
56.6

Pâ†’R
60.4
67.7
68.5
70.3
72.9
74.0
77.3
77.1
79.5
78.1
80.5

Râ†’A
53.9
63.1
63.2
63.9
61.2
66.3
70.9
70.2
71.9
72.5
72.0

Râ†’C
41.2
51.5
51.8
52.4
50.3
54.0
56.7
56.3
59.0
60.2
60.2

Râ†’P
59.9
74.3
76.8
76.8
76.0
78.8
81.6
82.1
82.9
82.3
83.6

AVG
46.1
56.3
57.6
58.3
60.2
61.8
65.8
66.6
67.6
68.1
69.6

Table 4: Accuracy (%) on ImageCLEF-DA and VisDA-2017 for UDA (ResNet-50).

Task

(a) Matching features

100
90
80
70
60
50 1 510

I
C
P
C

Câ†’P
65.5
69.8
74.3
73.2
75.8
74.2
75.2
74.2
78.0
79.7

Pâ†’C
91.2
91.3
91.5
91.7
89.2
91.7
92.2
94.3
94.8
96.0
3.5
3.0
2.5
2.0
1.5
1.0

P
I
C
P

m

50

nt/C

(b) Meta-data

VisDA-2017
Method
Synâ†’Real
DAN [5]
49.8
JAN [49]
61.6
DANN+TransNorm [39]
66.3
DeepJDOT [29]
66.9
MCD [53]
69.2
GTA [54]
69.5
CDAN [43]
70.0
CDAN+TransNorm [39]
71.4
MDD [10]
74.6
L2M
77.5

AVG
80.7
83.3
85.0
85.5
85.7
85.8
85.8
87.7
88.5
89.1

ResNet-50
JAN
MEDA
L2M

A C

Task

P R

(c) Distribution distance

0.6
0.5
0.4
0.3
0.2
0.1
0.00

classification loss
matching loss

Loss

Accuracy(%)

85
MMD
MMD+L2M
80
Adv
Adv+L2M
75
70
65
60 R A A R P R R P

Câ†’I
78.0
84.1
87.0
86.4
89.5
89.5
88.8
91.3
92.3
92.0

-distance

ImageCLEF-DA
Iâ†’P Pâ†’I Iâ†’C
74.8 83.9 91.5
75.0 86.2 93.3
75.0 86.0 96.2
78.1 90.4 93.1
78.2 87.5 94.2
76.8 88.0 94.7
75.0 87.9 96.0
77.7 90.7 97.7
78.3 90.8 96.7
78.7 91.0 97.0

Accuracy(%)

Method
ResNet [48]
DAN [5]
DANN [8]
MEDA [7]
CAN [51]
JAN [49]
MADA [50]
CDAN [43]
CDAN+TransNorm [39]
L2M

50

100

Epoch

150

200

(d) Convergence

Figure 2: (a) Comparison between the best existing methods with predefined distance and the average
of L2M. (b) Analysis of the number of meta-data m. (c) Distribution discrepancy between two
domains. (d) Convergence of L2M.
is rather larger compared to the other datasets (280,000+ images), L2M achieves an accuracy of
77.5% with a significant improvement of 2.9%. All these results demonstrate that L2M can achieve
competitive performance on DA tasks.
4.4

Detailed analysis

Analysis of meta-data. We empirically analyze the batch size m of the meta-data Dmeta . It is
obvious that a larger m will bring more uncertainty, and a smaller m is likely to make the metanetwork unstable. We record the performance of L2M using different values of m on several randomly
selected tasks in Fig. 2(b). The results indicate that L2M is robust to m and a small m can lead to
competitive performance. Therefore, we set m = 5 in our experiments for computational efficiency.
Distribution discrepancy. The A-distance [2] measures the distribution discrepancy that is defined
as dA = 2(1 âˆ’ 2), where  is the classifier loss to discriminate the source and target domains.
Smaller A-distance indicates better domain-invariant features. Fig. 2(c) shows that L2M can achieve
a lower dA , implying a lower generalization error of L2M.
Convergence analysis. L2M introduces a meta-network, which may make the training process
harder. In this section, we empirically evaluate the convergence of L2M. As shown in Fig. 2(d), the
results on a randomly-chosen task show that L2M can reach a quick and steady convergence in a
limited number of iterations. Therefore, L2M can be easily trained.
7

Table 5: Results on COVID-19 X-ray adaptation (normal pneumonia â†’ COVID-19, ResNet-18).
Here we use the 95% confidence interval, where the corresponding value of z is 1.96. The computed
confidence interval r is around 1.3%.
Method
Train on source
Train on target (ideal state)
Fine-tuning
DLAD [55]
DANN [8]
MCD [53]
CDAN+TransNorm [39]
L2M

(a) GAN

Precision (%)
63.5
91.7
56.3
62.0
61.4
63.2
85.0
70.1

(b) GMMN

Recall (%)
66.7
55.0
75.0
73.3
71.7
60.0
39.2
78.3

F1 (%)
65.0
68.8
64.3
67.2
66.2
61.5
63.7
74.0

(c) L2M

Figure 3: Generated samples from GAN, GMMN, and L2M.

5

Application to COVID-19 Chest X-ray Image Classification

Other than public datasets, we apply L2M to a COVID-19 chest X-ray image classification dataset [56],
where the source domain is normal or pneumonia, and the target domain is normal or COVID-19
pneumonia. Note that this is a class-imbalanced task which is more challenging and realistic. We use
F1, Precision, and Recall as the evaluation metrics for this highly-imbalanced binary classification
task. As shown in Table 7, L2M achieves better results compared to finetune and other DA methods.
Here we use the 95% confidence interval, where the corresponding value of z is 1.96. The computed
confidence interval r is around 1.3%. More details about the dataset, comparison methods, and results
are in the supplementary file.

6

Discussions

Extending L2M for image generation. We show the potential of L2M in generating MNIST
hand-written digits. We use GAN [34] (adversarial distance) and GMMN [57] (MMD distance) as
the baselines. We replace the MMD module in GMMN with L2M. Hyperparameter settings and
training details are in supplementary. The generated samples are shown in Fig. 3. L2M can generate
more realistic samples compared to GAN, and sharper samples compared to GMMN. This indicates
the potential of L2M in image generation. It should be noted that this is only a trial experiment and
more efforts are needed for achieving SOTA performance on image generation.
Limitations and solutions. L2M can be roughly regarded as that requires updating two networks
iteratively. Therefore, compared with regular DA methods (e.g., DANN, CDAN, MDD), L2M needs
more than more training time. It is suggested to use a smaller batch size of meta-data compared
to training data to reduce GPU memory increment and speed up training. However, the inference
time is the same as other methods for using the same backbone. L2M can be more efficient by
adopting knowledge distillation as suggested in meta-pseudo-labels (MPL) [58], which is left for
future research. Additionally, a pre-trained L2M model can be deployed to the edge devices which
can achieve accurate and fast inference.

7

Conclusions

In this paper, for the first time, we step back from focusing on designing distribution matching features
according to human knowledge, and instead, propose L2M to automatically match the cross-domain
joint distributions for domain adaptation. Our work shows that by taking diverse matching features
including task-independent and human-designed distances, L2M can directly learn the distribution
matching in a data-driven way. L2M can be seen as a general framework that unifies deep feature
learning and human-designed feature learning for better distribution matching. Experiments on
public datasets substantiate the superiority of L2M over state-of-the-art approaches on DA and
image generation tasks. We apply L2M to COVID-19 X-ray image adaptation experiment, where it
8

significantly outperforms existing methods in such a highly imbalanced task. We believe that L2M
can be helpful in other problems such as domain generalization, open-set DA, and partial transfer
learning, which will be the focus of future research.

References
[1] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in
deep neural networks? In NIPS, pages 3320â€“3328, 2014.
[2] Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of representations
for domain adaptation. In NIPS, pages 137â€“144, 2007.
[3] Han Zhao, Remi Tachet des Combes, Kun Zhang, and Geoffrey J Gordon. On learning invariant
representation for domain adaptation. In ICML, 2019.
[4] Sinno Jialin Pan, Ivor W Tsang, James T Kwok, and Qiang Yang. Domain adaptation via
transfer component analysis. IEEE TNN, 22(2):199â€“210, 2011.
[5] Mingsheng Long, Yue Cao, Jianmin Wang, and Michael I Jordan. Learning transferable features
with deep adaptation networks. In ICML, 2015.
[6] Eric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko, and Trevor Darrell. Deep domain
confusion: Maximizing for domain invariance. arXiv preprint:1412.3474, 2014.
[7] Jindong Wang, Wenjie Feng, Yiqiang Chen, Han Yu, Meiyu Huang, and Philip S Yu. Visual
domain adaptation with manifold embedded distribution alignment. In MM, pages 402â€“410,
2018.
[8] Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation.
In ICML, 2015.
[9] Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain
adaptation. In CVPR, pages 7167â€“7176, 2017.
[10] Yuchen Zhang, Tianle Liu, Mingsheng Long, and Michael I Jordan. Bridging theory and
algorithm for domain adaptation. In ICML, 2019.
[11] Nicolas Courty, RÃ©mi Flamary, Devis Tuia, and Alain Rakotomamonjy. Optimal transport for
domain adaptation. IEEE TPAMI, 39(9):1853â€“1865, 2016.
[12] Mingsheng Long, Jianmin Wang, Guiguang Ding, Jiaguang Sun, and Philip S Yu. Transfer
feature learning with joint distribution adaptation. In ICCV, 2013.
[13] Jindong Wang, Yiqiang Chen, Shuji Hao, Wenjie Feng, and Zhiqi Shen. Balanced distribution
adaptation for transfer learning. In ICDM, pages 1129â€“1134, 2017.
[14] Zhen Fang, Jie Lu, Feng Liu, Junyu Xuan, and Guangquan Zhang. Open set domain adaptation:
Theoretical bound and algorithm. IEEE TNNLS, 2019.
[15] Chaohui Yu, Jindong Wang, Yiqiang Chen, and Meiyu Huang. Transfer learning with dynamic
adversarial adaptation network. In ICDM, 2019.
[16] Fredrik D Johansson, David Sontag, and Rajesh Ranganath. Support and invertibility in
domain-invariant representations. arXiv preprint arXiv:1903.03448, 2019.
[17] Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard SchÃ¶lkopf, and Alexander
Smola. A kernel two-sample test. JMLR, 13(Mar):723â€“773, 2012.
[18] Aaditya Ramdas, Sashank Jakkam Reddi, BarnabÃ¡s PÃ³czos, Aarti Singh, and Larry Wasserman.
On the decreasing power of kernel and distance based nonparametric hypothesis tests in high
dimensions. In AAAI, 2015.
[19] Zhiting Hu, Zichao Yang, Ruslan Salakhutdinov, and Eric P Xing. On unifying deep generative
models. arXiv preprint arXiv:1706.00550, 2017.
9

[20] Ferenc HuszÃ¡r. How (not) to train your generative model: Scheduled sampling, likelihood,
adversary? arXiv preprint arXiv:1511.05101, 2015.
[21] Lucas Theis, AÃ¤ron van den Oord, and Matthias Bethge. A note on the evaluation of generative
models. arXiv preprint arXiv:1511.01844, 2015.
[22] Ying Wei, Yu Zhang, Junzhou Huang, and Qiang Yang. Transfer learning via learning to transfer.
In ICML, pages 5085â€“5094, 2018.
[23] BalÃ¡zs CsanÃ¡d CsÃ¡ji. Approximation with artificial neural networks. Faculty of Sciences, Etvs
Lornd University, Hungary, 24:48, 2001.
[24] Sinno Jialin Pan, Qiang Yang, et al. A survey on transfer learning. IEEE TKDE, 22(10):1345â€“
1359, 2010.
[25] Werner Zellinger, Thomas Grubinger, Edwin Lughofer, Thomas NatschlÃ¤ger, and Susanne
Saminger-Platz. Central moment discrepancy (cmd) for domain-invariant representation learning. In ICLR, 2017.
[26] Baochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation.
In ECCV, pages 443â€“450, 2016.
[27] Nicolas Courty, RÃ©mi Flamary, and Devis Tuia. Domain adaptation with regularized optimal
transport. In ECML PKDD, pages 274â€“289. Springer, 2014.
[28] Nicolas Courty, RÃ©mi Flamary, Amaury Habrard, and Alain Rakotomamonjy. Joint distribution
optimal transportation for domain adaptation. In NIPS, pages 3730â€“3739, 2017.
[29] Bharath Bhushan Damodaran, Benjamin Kellenberger, RÃ©mi Flamary, Devis Tuia, and Nicolas
Courty. Deepjdot: Deep joint distribution optimal transport for unsupervised domain adaptation.
In ECCV, pages 447â€“463, 2018.
[30] Zhen Zhang, Mianzhi Wang, and Arye Nehorai. Optimal transport in reproducing kernel hilbert
spaces: Theory and applications. IEEE TPAMI, 2019.
[31] Boqing Gong, Yuan Shi, Fei Sha, and Kristen Grauman. Geodesic flow kernel for unsupervised
domain adaptation. In CVPR, pages 2066â€“2073. IEEE, 2012.
[32] Baochen Sun and Kate Saenko. Subspace distribution alignment for unsupervised domain
adaptation. In BMVC, pages 24â€“1, 2015.
[33] Baochen Sun, Jiashi Feng, and Kate Saenko. Return of frustratingly easy domain adaptation. In
AAAI, volume 6, page 8, 2016.
[34] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, pages 2672â€“2680,
2014.
[35] Yong Luo, Yonggang Wen, Ling-Yu Duan, and Dacheng Tao. Transfer metric learning: Algorithms, applications and outlooks. arXiv preprint arXiv:1810.03944, 2018.
[36] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training
by reducing internal covariate shift. In ICML, 2015.
[37] Fabio Maria Cariucci, Lorenzo Porzi, Barbara Caputo, Elisa Ricci, and Samuel Rota Bulo.
Autodial: Automatic domain alignment layers. In ICCV, pages 5077â€“5085. IEEE, 2017.
[38] Yanghao Li, Naiyan Wang, Jianping Shi, Xiaodi Hou, and Jiaying Liu. Adaptive batch normalization for practical domain adaptation. Pattern Recognition, 80:109â€“117, 2018.
[39] Ximei Wang, Ying Jin, Mingsheng Long, Jianmin Wang, and Michael I Jordan. Transferable
normalization: Towards improving transferability of deep neural networks. In NeurIPS, pages
1951â€“1961, 2019.
10

[40] Konstantinos Bousmalis, Nathan Silberman, David Dohan, Dumitru Erhan, and Dilip Krishnan.
Unsupervised pixel-level domain adaptation with generative adversarial networks. In CVPR,
pages 3722â€“3731, 2017.
[41] Masashi Sugiyama. Introduction to statistical machine learning. Morgan Kaufmann, 2015.
[42] Longlong Jing and Yingli Tian. Self-supervised visual feature learning with deep neural
networks: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020.
[43] Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Michael I Jordan. Conditional adversarial
domain adaptation. In NeurIPS, pages 1640â€“1650, 2018.
[44] The imageclef-da challenge 2014. https://www.imageclef.org/2014.
[45] Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan.
Deep hashing network for unsupervised domain adaptation. In CVPR, pages 5018â€“5027, 2017.
[46] Xingchao Peng, Ben Usman, Neela Kaushik, Judy Hoffman, Dequan Wang, and Kate Saenko.
Visda: The visual domain adaptation challenge, 2017.
[47] Kate Saenko, Brian Kulis, Mario Fritz, and Trevor Darrell. Adapting visual category models to
new domains. In ECCV, pages 213â€“226. Springer, 2010.
[48] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In CVPR, pages 770â€“778, 2016.
[49] Mingsheng Long, Han Zhu, Jianmin Wang, and Michael I Jordan. Deep transfer learning with
joint adaptation networks. In ICML, 2017.
[50] Zhongyi Pei, Zhangjie Cao, Mingsheng Long, and Jianmin Wang. Multi-adversarial domain
adaptation. In AAAI, 2018.
[51] Weichen Zhang, Wanli Ouyang, Wen Li, and Dong Xu. Collaborative and adversarial network
for unsupervised domain adaptation. In CVPR, pages 3801â€“3809, 2018.
[52] Kaichao You, Ximei Wang, Mingsheng Long, and Michael Jordan. Towards accurate model
selection in deep unsupervised domain adaptation. In ICML, pages 7124â€“7133, 2019.
[53] Kuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku, and Tatsuya Harada. Maximum classifier
discrepancy for unsupervised domain adaptation. In CVPR, pages 3723â€“3732, 2018.
[54] Swami Sankaranarayanan, Yogesh Balaji, Carlos D Castillo, and Rama Chellappa. Generate to
adapt: Aligning domains using generative adversarial networks. In CVPR, pages 8503â€“8512,
2018.
[55] Jianpeng Zhang, Yutong Xie, Yi Li, Chunhua Shen, and Yong Xia. Covid-19 screening on chest
x-ray images using deep learning based anomaly detection. arXiv preprint arXiv:2003.12338,
2020.
[56] Yifan Zhang, Shuaicheng Niu, Zhen Qiu, Ying Wei, Peilin Zhao, Jianhua Yao, Junzhou Huang,
Qingyao Wu, and Mingkui Tan. Covid-da: Deep domain adaptation from typical pneumonia to
covid-19. arXiv preprint arXiv:2005.01577, 2020.
[57] Yujia Li, Kevin Swersky, and Rich Zemel. Generative moment matching networks. In ICML,
2015.
[58] Hieu Pham, Qizhe Xie, Zihang Dai, and Quoc V Le. Meta pseudo labels. arXiv preprint
arXiv:2003.10580, 2020.
[59] Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li Fei-Fei. Mentornet: Learning
data-driven curriculum for very deep neural networks on corrupted labels. In ICML, 2018.
[60] Jun Shu, Qi Xie, Lixuan Yi, Qian Zhao, Sanping Zhou, Zongben Xu, and Deyu Meng. Metaweight-net: Learning an explicit mapping for sample weighting. In NeurIPS, 2019.
11

[61] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, FranÃ§ois
Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural
networks. JMLR, 17(1):2096â€“2030, 2016.
[62] Mingsheng Long, Han Zhu, Jianmin Wang, and Michael I Jordan. Unsupervised domain
adaptation with residual transfer networks. In NIPS, pages 136â€“144, 2016.
[63] Ruichu Cai, Zijian Li, Pengfei Wei, Jie Qiao, Kun Zhang, and Zhifeng Hao. Learning disentangled semantic representation for domain adaptation. In IJCAI, 2019.
[64] Minghao Chen, Shuai Zhao, Haifeng Liu, and Deng Cai. Adversarial-learned loss for domain
adaptation. In AAAI, 2020.
[65] Yann LeCun, LÃ©on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning
applied to document recognition. Proceedings of the IEEE, 86(11):2278â€“2324, 1998.
[66] Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical bayesian optimization of machine
learning algorithms. In NIPS, pages 2951â€“2959, 2012.

12

Supplementary: Learning to Match Distributions for Domain Adaptation
A

Learning algorithm for L2M

We also put the framework and key learning steps of L2M here for better illustration. The complete
learning procedure of L2M is listed in Algorithm 1.
ğ’™ğ‘ 
ğ’™ğ‘¡

ğºğ‘¦

â„’cls

Main model update: ğœ™ ğ‘¡ + 1 â† ğœ™ ğ‘¡ âˆ’

ğœ™(ğ‘¡)

(train)

ğ›¼âˆ‡ğœ™ [â„’cls

ğœ™(ğ‘¡ + 1)

(train)

ğœ™ +ğœ†â„’ match ğœƒ ğ‘¡ ] á‰š

ğœ™(ğ‘¡)

ğ‘“ğœ™
ğ·

âˆª

Meta-network update:

ğ‘”ğœƒ â„’match

ğœƒ(ğ‘¡)

(meta)

ğœƒ(ğ‘¡ + 1) â† ğœƒ ğ‘¡ âˆ’ ğ›½âˆ‡ğœƒ â„’val

ğœƒ á‰š

Matching feature generator

(a) The architecture

ğœƒ(ğ‘¡ + 1)

ğœƒ(ğ‘¡)

(b) The learning procedure

Figure 1: The framework and computing flow of the proposed L2M approach.
Algorithm 1 Learning algorithm of L2M
s
t
Input: Source domain Ds = {(xsi , yis )}ni=1
, target domain Dt = {xtj }nj=1
, learning rate Î±, Î², max
epochs T .
Output: {Ï†? , Î¸? }.
1: Initialize Ï†(0) and Î¸(0).
2: while epoch t < T do
3:
Build an assist model with its parameter inherited from the main model Ï†(t).
4:
Sample a mini-batch data Bs , Bt from both the source and target domain.
5:
Update Ï† by step i in Fig. 1(b). The loss consists of Lcls and Lmatch , we only update the assist
model Ï† and the meta-network Î¸ only be updated in step ii.
6:
Select the data with the highest prediction confidence from Dt to construct meta-data Dmeta .
7:
Update the meta-network Î¸ by step ii in Fig. 1(b).
8: end while
9: return {Ï†? , Î¸ ? }
It is worth noting that this optimization is general and can be naturally used in image generation
tasks. Hence, we also use the same optimization step in the MNIST digits generation experiments by
injecting this process directly to the GMMN [57] models. Therefore, L2M is a general and flexible
framework that can work for most cross-domain distribution matching tasks.
A.1

Matching features

Task-independent matching features. It is natural to use the extracted feature embedding by fÏ†
as one kind of task-independent features, which is denoted as Femb âˆˆ Rd , where d is the number
of neurons in this layer. For classification tasks, another kind of features is the network logit:
Flogit âˆˆ RC , which is the activation of the last FC layer before softmax. Note that in fact, Flogit
should be computed by Gy . For symbolic brievity we also draw it in the same way as Femb in
Fig. 1(a). Denote q the function of last FC layer, then they can be computed as:
Femb = [fÏ† (xi ).fÏ† (xj )].
(1)
Flogit = [q(fÏ† (xi )).q(fÏ† (xj ))].

(2)

Human-designed matching features. We adopt two popular distances as human-designed matching features: explicit distribution matching distance using MMD (Fmmd âˆˆ R), and implicit distribution matching distance using adversarial nets (Fadv âˆˆ R). Their basic idea is to approximate the
joint distributions using marginal or conditional distributions. A recent work MEDA [7] showed that
matching both conditional and marginal distributions can be useful. Therefore, we denote dm , dc the
marginal and conditional distances (losses) respectively. Then, these features can be computed as:
Fmmd = [dm (fÏ† (xi ), fÏ† (xj )), dc (fÏ† (xi ), fÏ† (xj ))].
(3)
13

Fadv = [Dm (fÏ† (xi ), fÏ† (xj )), Dc (fÏ† (xi ), fÏ† (xj ))].

(4)

For explicit distribution matching using MMD [17], the marginal and conditional distances can be
computed as:
2

dm = kExâˆ¼Bs Ï†(x) âˆ’ Exâˆ¼Bt Ï†(x)kHk ,
dc = Ecâˆ¼C Exâˆ¼Bs(c) Ï†(x) âˆ’ Exâˆ¼Bt(c) Ï†(x)

(5)

2
Hk

,

where Hk is the Reproducing Kernel Hilbert Space (RKHS) induced by kernel k, B (c) denotes
samples belonging to class c, and Ï†(Â·) some feature mapping function.
For implicit distribution matching using GAN [34], the main idea is to design a domain discriminator
Gd to identify which domain the samples belong to. We train fÏ† and Gy to confuse Gd , and eventually
Gd gets confused and fails to discriminate the domains. In this situation, the marginal and conditional
adversarial distances can be respectively computed as:
Dm = Exâˆ¼Bs âˆªBt `d (Gd (fÏ† (x)), d),
(c)

(c)

Dc = Ecâˆ¼C Exâˆ¼Bs âˆªBt `d (Gd (yÌ‚ (c) fÏ† (x)), d),

(6)

where `d is the cross-entropy loss for domain classification, and d is the domain label (0 or 1) of the
(c)
(c)
input sample x. Gd and `d are the conditional domain discriminator and its cross-entropy loss
associated with class c, respectively. yÌ‚ (c) is the predicted label over the class c of the input sample x.
Note that the target domain Dt has no labels, making it difficult to compute the conditional distance
dc . We apply prediction to Dt using the classifier Gy trained on Ds to obtain soft labels, which will
be iteratively refined. Clearly, MMD and adversarial distance are only two options for predefined
distance and others can be used. In specific problems, more task-dependent features can be used.
This makes L2M a general and flexible framework.
A.2

Convergence analysis and theoretical insights

In addition to empirically analyzing the convergence of L2M, we provide some theoretical analysis.
The convergence of L2M depends on two items: the classification loss Lcls on the training data, and
the distribution matching loss Lmatch on the meta-data. The convergence of Lcls is well ensured since
it is a standard cross-entropy loss in deep neural networks. The convergence of Lmatch depends on
two factors: the construction of meta-data and the loss itself. We adopt an iterative way to construct
the meta-data by using the pseudo labels provided by the trained network. According to several
recent works [7, 10, 43], the convergence of such an iterative pseudo-label can be ensured, i.e., the
pseudo labels will be more accurate, providing a strong support to the construction of the meta-data.
On the other hand, the convergence of Lmatch can also be ensured as long as the meta-network gÎ¸
is differential (in our work, it is differential) by following [59, 60]. Therefore, the convergence of
Lmatch can be ensured.
In the view of domain adaptation theory, L2M is designed by following the DA theory according
to [2] that the risk on the target domain is bounded by the following theorem:
Theorem 1 Let h âˆˆ H be a hypothesis, s (h) and t (h) be the expected risks on the source and
target domain, respectively, then
t (h) 6 s (h) + dH (p, q) + C0 ,

(7)

where C0 is a constant for the complexity of hypothesis and plus the risk of an ideal hypothesis
for both domains. dH (p, q) refers to the distribution divergence between domains. As can be seen,
L2M is directly minimizing the distribution distance (distribution matching loss) Lmatch , which is in
consistence with the above theorem.
A.3

Remarks

L2M vs. AutoML. L2M shares the same goal with AutoML: both are trying to reduce the human
intervention in a machine learning process. However, AutoML focuses more on â€œautoâ€ while L2M
14

Table 1:
Dataset
Office-31
ImageCLEF-DA
Office-Home
VisDA-2017

Public datasets description.
#Sample #Class
#Domain
4,110
31
A, W, D
1,800
12
C, I, P
15,588
65
A, C, P, R
280,000
12
Synthetic, Real

can be seen as a combination of deep features and human-designed features. Moreover, AutoML
focuses on architecture design, hyperparameter search, and channel pruning, which are different from
L2M. The main goal of L2M is to learn a good and automatic distribution matching between domains.
From this point of view, L2M can also be seen as an â€œautomatedâ€ DA method. Future works may lay
emphasis on domain adaptation architecture design, which is more like automl.
L2M is not yet another â€œSOTAâ€ and is not intending replacing other methods. The results in
this paper demonstrates that the performance of L2M outperforms several SOTA methods. However,
our goal is not to develop yet another SOTA to the community, but to introduce another kind of DA
algorithm that can be easily applied to real applications without specific concentration on the loss
function design and distribution matching module. Therefore, for a new application, both L2M and
other existing SOTA methods are applicable. The advantage of using L2M is that it requires less
human intervention of algorithm selection, while a simple embedding matching feature can achieve a
competitive performance. If you need better results, you still need to have a deep domain knowledge
and integrate it in L2M with the embeddings or logits features. Therefore, L2M can be used to
enhance other methods.

B
B.1

Experimental Details
Datasets

ImageCLEF-DA. ImageCLEF-DA [44] is a benchmark dataset for ImageCLEF 2014 domain
adaptation challenge, and it is collected by selecting the 12 common categories shared by the
following public datasets and each of them is considered as a domain: Caltech âˆ’ 256 (C),
ImageN et ILSV RC 2012 (I), P ascal V OC 2012 (P). There are 50 images in each category
and 600 images in each domain. We use all domain combinations and build 6 transfer tasks.
Office-Home. Office-Home [45] consists of images from 4 different domains: Artistic images
(A), Clip Art (C), P roduct images (P) and Real âˆ’ W orld images (R). For each domain, the
dataset contains images of 65 object categories collected in office and home settings. Similarly, we
use all domain combinations and construct 12 transfer tasks.
VisDA-2017. VisDA-2017 [46] is a simulation-to-real dataset with two extremely distinct domains:
Synthetic renderings of 3D models and Real collected from photo-realistic or real-image datasets.
With 280K images in 12 classes, the scale of VisDA-2017 brings challenges to domain adaptation.
Office-31. Office-31 dataset [47] is a standard and maybe the most popular benchmark for unsupervised domain adaptation. It consists of 4,110 images within 31 categories collected from everyday
objects in an office environment. It consists of three domains: Amazon (A), which contains images
downloaded from amazon.com, W ebcam (W) and DSLR (D), which contain images respectively
taken by web camera and digital SLR camera under different settings. We evaluate all our methods
across six transfer tasks on all three domains.
The statistics of these datasets are shown in Table 1.
B.2

Implementation Details

For different variants of L2M using different matching features, we report the dimension information
of eight matching features of each dataset in Table 2.
All methods use the ImageNet-pretrained ResNet-50 as the backbone network. Results of the
comparison methods are obtained from original papers. For L2M, we set max iterations to be
15

Table 2: Dimension of matching features of the datasets.
Dataset
Office-31
ImageCLEF-DA
Office-Home
VisDA-2017

Femb
2,048
2,048
2,048
2,048

Flogit
31
12
65
12

Fmmd
2
2
2
2

Fadv
2
2
2
2

[Femb , Fmmd ]
2,050
2,050
2,050
2,050

[Femb , Fadv ]
2,050
2,050
2,050
2,050

[Flogit , Fmmd ]
33
14
67
14

[Flogit , Fadv ]
33
14
67
14

Table 3: Accuracy (%) on Office-31 for UDA (ResNet-50).
Method
ResNet [48]
DDC [6]
DAN [5]
D-CORAL [26]
RTN [62]
DANN [8]
ADDA [9]
JAN [49]
MADA [50]
MEDA [7]
CAN [51]
DSR [63]
CDAN [43]
ALDA [64]
MDD [10]
L2M

Aâ†’W
68.4
75.6
80.5
77.0
84.5
82.0
86.2
85.4
90.0
86.0
92.5
93.1
94.1
95.6
94.5
93.2

Aâ†’D
68.9
76.5
78.6
81.5
77.5
79.7
77.8
84.7
87.8
86.3
90.1
92.4
92.9
94.0
93.5
94.1

Dâ†’W
96.7
96.0
97.1
97.1
96.8
96.9
96.2
97.4
97.4
97.1
98.8
98.7
98.6
97.7
98.7
98.8

Dâ†’A
62.5
62.2
63.6
65.9
66.2
68.2
69.5
68.6
70.3
72.1
72.1
73.5
71.0
72.2
74.6
75.9

Wâ†’D
99.3
98.2
99.6
99.6
99.4
99.1
98.4
99.8
99.6
99.2
100.0
99.8
100.0
100.0
100.0
100.0

Wâ†’A
60.7
61.5
62.8
64.3
64.8
67.4
68.9
70.0
66.4
73.2
69.9
73.9
69.3
72.5
72.2
74.7

AVG
76.1
78.3
80.4
80.9
81.6
82.2
82.9
84.3
85.2
85.7
87.2
88.6
87.7
88.7
88.9
89.5

200000. The mini-batch SGD with nesterov momentum of 0.9 and batchsize 32 is used as the
optimization strategy. The learning rate Î± of the meta-model and the overall model changes by
Î±
following [8]: Î±k = (1+Î³k)
âˆ’Ï… , where k is the training iteration linearly changing from 1 to max
iterations, Î³ = 0.001, Î± = 0.004, and decay rate Ï… = 0.75. The initial learning rate Î² of the
meta-network is 0.01 and will gradually decrease to 0.0001 during training. Meta-network gÎ¸ uses
a d âˆ’ 1024 âˆ’ 1024 âˆ’ 1 structure where d is the dimension of input matching features, and more
information of different matching features can be seen in Table 2. We follow the standard protocols
for unsupervised domain adaptation [61], we use classification accuracy on the target domain as the
evaluation metric and target labels are only used for evaluation. The results are the average accuracy
of 10 experiments by following the same protocol [6, 7, 10, 43]. We use Pytorch to implement L2M
and it is trained on a Linux machine with a 16GB P100 GPU.
B.3

Results on Office-31 dataset

Table 3 reports the results on Office-31, which indicates that L2M outperforms all the recent DA
methods in classification accuracy.
B.4

More ablation experiments of L2M

We show more ablation experiments of L2M on Office-Home and ImageCLEF-DA in Table 4 and
Table 5, respectively. We did not run ablation experiments on VisDA-17 since this dataset is rather
larger and needs more computations. The ablation results on other datasets are enough for observing
the patterns of L2M variants. Combining these results with that from the main paper, more insightful
conclusions can be made. (1) L2M achieves the best performance on multiple datasets, which
indicates the efficiency of L2M. (2) All the 4 variants of L2M can achieve competitive performance,
implying the effectiveness of the meta-network on matching functions and L2M is able to fit a wide
range of matching features. (3) L2M (emb+adv) outperforms the other 3 variants of L2M, which
indicates L2M can learn more representative and transferable features by taking as input deep features
and human-designed features.
Despite the performance on these public datasets, we want to emphasis that in real applications,
L2M (emb+adv) is perhaps not always the best matching features. Therefore, in order to achieve
the best performance, users can try several combinations of matching features along with their own
domain experience before finding the suitable features. Since the performance of most matching
features are with a low variance, any matching feature can achieve competitive performance compared
to existing methods.
16

Table 4: Accuracy (%) on Office-Home for UDA (ResNet-50).
Method
Aâ†’C Aâ†’P Aâ†’R Câ†’A Câ†’P Câ†’R Pâ†’A Pâ†’C Pâ†’R Râ†’A Râ†’C Râ†’P AVG
L2M (emb)
55.8 73.9 77.8 61.4 72.9 73.1 62.5 54.5 79.3 71.1 60.2 83.2 69.1
L2M (logit)
56.2 70.2 71.7 57.7 68.1 71.1 60.7 57.1 79.4 71.6 61.5 83.6 67.4
L2M (logit+adv) 55.4 70.6 76.6 58.1 69.2 70.1 61.1 55.5 78.6 71.4 60.7 82.8 67.5
L2M (emb+adv) 56.1 75.0 79.3 62.8 73.3 73.7 63.8 54.1 80.6 71.8 60.2 83.5 69.5

Table 5: Accuracy (%) on ImageCLEF-DA for UDA (ResNet-50).
Method
L2M (emb)
L2M (logit)
L2M (logit+adv)
L2M (emb+adv)

B.5

Iâ†’P
78.0
76.2
77.0
78.7

Pâ†’I
90.5
88.0
89.2
91.0

Iâ†’C
96.2
96.8
95.7
97.0

Câ†’I
91.4
89.5
89.8
92.0

Câ†’P
78.3
76.8
77.3
79.7

Pâ†’C
94.1
94.5
93.3
96.0

AVG
88.1
87.0
87.1
89.1

Feature visualization.

We visualize the network activation (before FC layer) on task Pâ†’R using t-SNE in Fig. 2. ResNet-50
does not align the distributions. JAN aligns both marginal and conditional distributions with equal
weights, while MEDA adaptively aligns these two distributions whose results are better. However,
the source and target domains are not fully matched by MEDA. For L2M, both the cross-domain
distributions and categories are aligned well, implying that L2M learns more discriminative features.

(a) ResNet50

(b) JAN

(c) MEDA

(d) L2M

Figure 2: (Best viewed in color) The t-SNE visualization of network activation on task Pâ†’R. Red
circles are the source samples and blue circles are the target samples.

C

Application to COVID-19 Chest X-ray Image Adaptation

Other than benchmarking L2M on popular public datasets including Office-31, Office-Home,
ImageCLEF-DA, and VisDA-17, we compare the performance of several DA methods including
L2M in a real application. Different from public datasets, this application will prove the effectiveness
of L2M and other DA methods in a real-world task, which is more appealing and inspiring.
We present more details for applying L2M to COVID-19 chest X-ray image adaptation tasks. COVID19 is a specific type of pneumonia compared to the normal kind of pneumonia, and there is not
too much COVID-19 data available, it becomes necessary and feasible to use the sufficient labeled
pneumonia data to help classify the COVID-19 symptom. Therefore, this task is a binary classification
task, where the source domain is the well-labeled pneumonia data to classify whether this patient
is having pneumonia or not, and the target domain is the unlabeled COVID-19 data. Out task is to
classify whether each of the the target domain samples is having a COVID-19 symptom or not.
This is a binary classification task, i.e., the normal category vs. pneumonia on the source domain,
and the normal category vs. COVID-19 on the target domain. We also notice that this dataset is
highly-imbalanced (as shown in the next section). Therefore, for better illustrate the results, we adopt
F1 score, Recall, and Precision as the evaluation metrics rather than classification accuracy. These
metrics are better for imbalanced classification tasks. It also demonstrates our contribution that L2M
can achieve robust preformance in imbalanced tasks compared to other DA methods.
17

C.1

Dataset

Table 6 shows the description of the dataset2 . Note that in this task, we use some COVID-19 data
as the validation set to better tune the hyperparameters. In the source domain, there are two classes:
normal and pneumonia, while there are normal and COVID-19 classes in the target and validation
dataset. Fig. 3 shows some examples from the source and target domain. It is clear that data from two
domains are very similar especially for pneumonia and COVID-19 classes. Therefore, it is feasible to
perform domain adaptation or transfer learning between these two domains.
Table 6: Dataset description of pneumonia and COVID-19 dataset
Domain
Source
Target
Validation

Symptom
Pneumonia
COVID-19
COVID-19

#Normal
5,613
885
254

#Pneumonia
2,306
0
0

#COVID-19
0
60
25

Source domain

Pneumonia

#Total
7,919
945
279

Target domain

Normal

COVID-19

Normal

Figure 3: Samples from the source and target domain.

C.2

Baselines and experimental details

We mainly compare the performance of L2M with three categories of methods: (1) Deep learning
baselines, (2) Deep diagnostic methods, and (3) unsupervised domain adaptation methods. The deep
learning baselines include three baselines:
â€¢ Train on source: train a network on the source domain, and then apply the pretrained model
on the target domain.
â€¢ Train on target: this is an ideal state since there are no labels for the target domain in our
task. Therefore, we directly use several extra labeled COVID-19 data from the dataset (they
are 30% of the target domain data) and train a network on these data. Then, we can apply
prediction on the target data.
â€¢ Fine-tuning: This is a combination of the above two baselines. Firstly, we train a network
on the source domain. Then, we fine-tune the pretrained model on the extra labeled target
domain data. Finally, we apply prediction on the target data.
The deep diagnostic method is DLAD [55].
The unsupervised DA methods are DANN [8], MCD [53], and CDAN+TransNorm [39]. All methods
are using ResNet-18 as the backbone network following [56]. The results of these methods are
obtained from COVID-DA [56] to ensure a fair comparison. Note that we did not compare COVIDDA since this method is a semi-supervised method that explicitly uses the labeled data on the target
domain. Therefore, we only use the report of unsupervised methods and train L2M with the same
experimental settings.
2

The dataset is available at https://github.com/qiuzhen8484/COVID-DA

18

C.3

Analysis of the results

The results are shown in Table 7. Here we use the 95% confidence interval, where the corresponding
value of z is 1.96. The computed confidence interval r is around 1.3%.
This table is the same as the main paper but with more analysis of the results. From the results, we
see that L2M outperforms all comparison methods in terms of F1 score and Recall. In Precision, the
performance of Training on labeled target data achieves the best results, which is reasonable since
this approach trains on the labeled target domain data and is expected to achieve the best precision.
The UDA methods, namely DANN, MCD, and CDAN+TransNorm can sometimes achieve worse
results than baselines, indicating that the different distribution distance of pneumonia and COVID-19
data are not that easy to compute by adversarial distance (DANN and MCD are using adversarial
distances) or statistical alignment (TransNorm uses a source-target normalization technique) since
these methods are built with their own priors and biases. In this situation, it is necessary to perform
domain adaptation in a data-driven way stepping back from these predefined distances. Therefore,
L2M can be useful in real-world applications. It also demonstrates our contribution that L2M can
achieve robust preformance in imbalanced tasks compared to other DA methods.
Table 7: Results on COVID-19 X-ray adaptation (normal pneumonia â†’ COVID-19, ResNet-18).
Here we use the 95% confidence interval, where the corresponding value of z is 1.96. The computed
confidence interval r is around 1.3%.
Method
Train on source
Train on target (ideal state)
Fine-tuning
DLAD [55]
DANN [8]
MCD [53]
CDAN+TransNorm [39]
L2M

Precision (%)
63.5
91.7
56.3
62.0
61.4
63.2
85.0
70.1

Recall (%)
66.7
55.0
75.0
73.3
71.7
60.0
39.2
78.3

F1 (%)
65.0
68.8
64.3
67.2
66.2
61.5
63.7
74.0

On the other hand, we also notice that the performance of fine-tuning is worse than training on source
and target, which is probably due to the distribution gap between the source and target domains. In
a nutshell, among baselines and DA methods other than L2M, training on target achieves the best
performance, indicating the importance of labeled data. We can also see that in COVID-DA [56],
authors used semi-supervised settings to improve the F1, Precision, and Recall score to over 90%,
which clearly shows optimistic performance. Therefore, L2M and other methods can also be applied
to semi-supervised DA tasks by adopting several labeled target samples. This is left for future work
since this work mainly focuses on unsupervised DA.
C.4

Ablation study

We show the ablation study of L2M on this COVID-19 data in Table 8. It is shown that by combining
different matching features, L2M can generally achieve better performance than the comparison
methods.
Table 8: Ablation experiments of L2M on COVID-19 experiment. Here we use the 95% confidence
interval, where the corresponding value of z is 1.96. The computed confidence interval r is around
1.3%.
L2M variant
L2M (emb)
L2M (logit)
L2M (emb+mmd)
L2M (logit+mmd)
L2M (emb+adv)
L2M (logit+adv)
L2M (mmd)
L2M (adv)

F1
73.2
68.3
69.1
74.0
68.9
69.4
71.2
65.5

19

Recall
75.0
70.0
78.3
78.3
65.4
71.7
70.0
65.0

Precision
71.4
66.7
61.8
72.3
78.5
67.2
72.4
66.1

D

Details for Image Generation

We train GMMNs on the benchmark datasets MNIST [65]. We use the standard test set of 10,000
images, and randomly select 5000 from the standard 60,000 training images for validation. The
remaining 55,000 are used for training. We train the GMMN network in both the input data space
and code space of an auto-encoder. For all the networks, a uniform distribution in [-1, 1]H is used as
the prior for the H-dimensional stochastic hidden layer at the top of the GMMN, which is followed
by 4 ReLU layers. The output layer is a logistic sigmoid function, which guarantees that the code
space dimensions lay in [0, 1]. The auto-encoder has 4 layers, 2 for the encoder and 2 for the decoder.
For more details about the architecture of GMMN and auto-encoder, please refer to the original
paper [57].
We train the GMMNs with mini-batch of size 1000, for each mini-batch, a set of 1000 samples will be
generated from the network. The loss and gradient are computed from these 2000 samples. We replace
the original square root loss function LMMD with Lmatch of L2M to get the result GMMN+L2M.
We set max epochs to be 500 and use Adam as the optimization strategy. The learning rate and
momentum for both GMMN and auto-encoder, dropout rate for the auto-encoder are tuned using
Bayesian optimization [66].
Fig. 4 shows more MNIST samples generated by GMMN with MMD and L2M. It is clear that L2M
generates sharper samples than MMD. We believe that L2M has more potential in image generation
and this is only a test experiment. We are well aware that there are lots of existing works for image
generation these years and adhere to hope that L2M could be significantly extended for this task in
the future.

(a) GMMN
GMMN

(b) L2M
JointJoint
MMD

GMMN+AE

Figure 4: More MNIST samples generated by GMMN and L2M.

20

