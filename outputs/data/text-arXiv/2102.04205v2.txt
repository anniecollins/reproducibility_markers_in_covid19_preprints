How Pandemic Spread in News: Text Analysis Using Topic Model

WANG Minghao, Dr. Paolo MENGONI, IEEE Member

How Pandemic Spread in News: Text Analysis Using Topic Mode

1

Abstract

Researches about COVID-19 has increased largely, no matter in the biology field or the
others. This research conducted a text analysis using LDA topic model. We firstly scraped
totally 1127 articles and 5563 comments on SCMP covering COVID-19 from Jan 20 to May
19, then we trained the LDA model and tuned parameters based on the ğ¶ğ¶

ğ‘‰ğ‘‰

coherence as

the model evaluation method. With the optimal model, dominant topics, representative
documents of each topic and the inconsistence between articles and comments are analyzed. 3

possible improvements are discussed at last.
Keywords: text analysis, topic modeling, web scraping

How Pandemic Spread in News: Text Analysis Using Topic Mode

2

Introduction

Background
The World Health Organization (WHO) defined the SARS-CoV-2 virus outbreak as a
severe global threat. The COVID-19 pandemic (hereinafter referred to as pandemic) has made
tremendous impact on the whole world, including medical system, livelihood, employment and
political issues. These issues have been projected to the media atmosphere significantly as well,
which can be regarded as a media infodemic [1]. It is a precious chance to observe the topics
that people are most interested in among the media during the pandemic using text analysis and
topic modeling techniques.
Related works
Topic model
Topic model is a key technique to discover the latent semantic structure from text contents.
This technique can be traced back to the latent semantic analysis (LSA) [2], which used
singular-value decomposition to index and retrieve the semantic structure automatically.
However, due to some basic statistical problems, a new approach called Latent Semantic
Indexing (LSI) was proposed by Thomas Hofmann, which used statistical latent class model
[3]. Latent Dirichlet Allocation was proposed then by Blei et al then as an extension of PLSA,
which added Dirichlet priors for the document-specific topic mixtures [5] that has improved
the modelâ€™s generalization ability on unseen documents.

How Pandemic Spread in News: Text Analysis Using Topic Mode

3

In the LDA, a topic was defined as a distribution over a fixed vocabulary [6]. For example,
the pandemic topic has words about pandemic (e.g. mask, vaccine) with high probability, and
the politics topic has words (e.g. election, regime, lie) about politics with high probability.
These topics are determined before the document has been formed. The basic idea behind LDA
was that documents usually bear multiple topics in different proportions. Each topic consists
of multiple words in different proportions as well. The LDA model can capture documentsâ€™
hidden structures (i.e. the topics, per-document topic distributions, per-document per-word
topic assignments).
Text analysis using topic model
In the field of text analysis, more and more researches are using topic model to mine data
from media contents [7]. Compared with traditional text analysis methods, topic model is
effective on discover the latent message from especially short texts, which may convey rich
meaning via self-defined hashtags or slangs.
Research questions
Inspired by previous research, we are curious about what are the media contents that most
people are interested in during the pandemic. Rather than purely analyze user generated
contents (UGC) on platforms like Twitter and Weibo, we are more interested in professional
news websites (e.g. SCMP, BBC News, CNN) for mainly 2 reasons: first, long news articles
offer richer corpus and that will help to train a more precise topic model and get more precise
analysis results; second, comparing news articles together with comments may reveal the effect

How Pandemic Spread in News: Text Analysis Using Topic Mode

4

of information diffusion and agenda setting.
In this way, we should train and fine-tune an LDA model that suits the corpus the best.
RQ1: What is the optimal LDA model like based on the corpus of news articles and
comments?
Specifically, we are curious that what contents appear the most frequently among the
articles and comments? What are people exactly saying when they talk about the pandemic?
RQ2: What is the distribution of different topics among both the news articles and
comments covering the pandemic?
Moreover, during surfing on these news websites, we noticed that some people are talking
about B when the article is about A. An example is shown in Figure 1 and Figure 2. In this case,
the article reported some recent situation about the pandemic, including where did the
coronavirus come from, what is the riskiest way of contagion, and who are the most susceptible
among the crowd. However, among the 4 comments (since there is one duplicate), 2 of them
are accusing Wuhan for the pandemic and laughing at Chinese people, and 1 of them is
suspecting the credibility of media diffusing pandemic messages. This inconsistence between
news articles and comments can reflect the effects of information diffusion and the interaction
between information sender and receiver.
RQ3: How inconsistent are the news articles and their comments? And what factors
contribute to the inconsistence?

How Pandemic Spread in News: Text Analysis Using Topic Mode

7

Method

Data collection
South China Morning Post (hereinafter referred to as SCMP) was selected as the data
source of this research for mainly 3 reasons compared with other news websites accessible:
first, it has comments posted, which takes a half of the data we need; second, the news on
SCMP is inclusive and international enough, which will give us a relatively comprehensive
global view of the pandemic information; third, SCMP is considered credible according to the
research by the Centre for Communication and Public Opinion Survey, CUHK.
We chose the Python library BeautifulSoup to scrape articles and comments from SCMPâ€™s
website. Some difficulties occurred. First, the index pages, which contained URLs of articles,
were infinite rolling while no obvious page number existed, which made it difficult to parse
the page in a static manner. However, by monitoring the XHR (XMLHttpRequest), there found
a code grows (from 0 to 60, 20 a step) in the requested URL. Some other pages contained 13digit timestamps as their indicator. So, with this pattern recognized, this problem was handled
by requesting from those URLs with an apikey.
The second problem was that, at each article page, texts were hidden somewhere and
parsing by tags didnâ€™t work. After observing the json data, texts were found hidden in
â€œAPOLLO_STATEâ€ where sentences of the article were broken into fragments stored in
dictionary-like blocks. Then there came the third problem that after parsing the json
preliminarily, fragments from other articles blended together. The solution for these two

How Pandemic Spread in News: Text Analysis Using Topic Mode

8

problems was that, regular expression was used firstly to recognize those fragmented sentences,
and an indicator for the end of an article was detected then. Finally, those sentences were glued
together as an article.
The whole data set consisted of 1127 news articles and 5563 pieces of comments from Jan
20 to May 19, after dropping null values (e.g. unparseable pages). 6 attributes of the articles
were collected, namely the news id, the title, the article text, the release time of the article, the
collecting date and the URL it came from. 7 attributes of the comments were collected, namely
the username, the raw comment, the cleaned comment, the date of that comment, the news id,
the status that if this comment was a reply and the collecting date.
Experiment
Data preprocessing
Proper data preprocessing is of great significance to train a precise topic model. With the
help of Python library nltk, we first tokenized the documents (i.e. both news articles and
comments) by any space or punctuation. Then, all tokens regarded as stop words would be
blocked out from the token list, which would be made into dictionary and corpus later. Besides
all default stop words from nltk, 1042 new stop words (including integer numbers from 1 to
999) were added according to preliminary topic modeling experiments.
Stemmer was not utilized in this research due to mainly 2 reasons: first, after trying both
porter stemmer and snowball stemmer in nltk, words would become hard to read after being
stemmed (e.g. â€œchinaâ€ is stemmed as â€œchinâ€, which is the same word means â€œjawâ€), which

How Pandemic Spread in News: Text Analysis Using Topic Mode

9

would lead to difficulties and misunderstandings in interpreting topic modeling results; second,
another stemmer lancaster seemed not available at this moment. No proper stemmer did we
find so far.
After building preprocessed articles and comments into dictionary then corpus using
Python library gensim, two data frames were formed for different purposes. The first one was
following the order of original data, consisting of news id, document (both news articles and
comments), release time, token and type (news articles are marked as type 0 and comments are
marked as type 1). This data frame was made for future indexing and text analysis. The other
one was shuffled in order to train topic model. Training set took 90% of the shuffled data
because the amount of our data was relatively small and more data was needed for training.
The other 10% was remained as test set.
LDA model training, fine-tuning and evaluation
There are 4 parameters in the gensim LDA model we would adjust, namely the number of
topics, iteration, chunk size and passes. The number of topics is the most important parameter
for a topic model, since too few topics will make the model imprecise (underfitting) while too
many will lead to overfitting. The iterations refer to the maximum number to iterate through
each document in the corpus to calculate the probability of each topic. Since the model is
trained on one data chunk at a time, the chunk size refers to the number of documents in every
training chunk. The passes refers to how many times the whole corpus is trained on. The model
will be more precise with bigger iterations and passes, while it takes longer time to train the

How Pandemic Spread in News: Text Analysis Using Topic Mode

10

model as well. We would trade that off by choosing the minimum of them meeting enough
precision.
Several commonly used evaluation methods were considered. As addressed in the original
paper of LDA [4], perplexity was applied to evaluate the model performance. Perplexity
reflects how uncertain that a word belongs to a specific topic category. The smaller the
perplexity, the more precise is the model. However, some up-to-date experiments and research
have revealed that this perplexity is confused to some extent.[8] Even Dr. Radim Å˜ehÅ¯Å™ek, the
author of gensim, also thought there were some problems to measure LDA with perplexity.
Coherence measures like NPMI or UMass have been widely deployed in many cases, while
the ğ¶ğ¶ğ‘‰ğ‘‰ coherence was proved to be the best performing one according the research by Michael

RÃ¶der et al, which showed results of evaluating topic models the closest to human evaluation.[8]
ğ¶ğ¶ğ‘‰ğ‘‰ coherence is essentially an index measures the co-occurrence of the words extracted by the

topic model. If those words from the same topic co-occurs very often (i.e. the ğ¶ğ¶ğ‘‰ğ‘‰ coherence
is high), the model is well performed.

In order to control variable, we assume these four parameters are decoupled, which means
adjusting them separately will not affect other parameters. We will prove this assumption later.
To save training time, we firstly tried topic number from 2 to 50, 10 a step (iterations =
10, chunk size = 100, passes = 10). As shown in Figure 3, the ğ¶ğ¶ğ‘‰ğ‘‰ coherence grows up mainly

within 12 topics. Then, we tried topic number from 2 to 17, 1 a step (iterations = 10, chunk size
= 100, passes = 10). As shown in Figure 4, the ğ¶ğ¶ğ‘‰ğ‘‰ coherence grows up mainly within 7 topics,

then it keeps relatively stable (there is a drop when topics = 8). The ğ¶ğ¶ğ‘‰ğ‘‰ coherence of 7 topics

How Pandemic Spread in News: Text Analysis Using Topic Mode

11

is 0.477.

Figure 3
The ğ¶ğ¶ğ‘‰ğ‘‰ coherence on different number of topics (2 to 50, 10 a step).

Figure 4
The ğ¶ğ¶ğ‘‰ğ‘‰ coherence on different number of topics (2 to 17, 1 a step).
Different iterations were tested (i.e. 10, 100, 500, 1000) together with 7 topics. While the

How Pandemic Spread in News: Text Analysis Using Topic Mode

12

ğ¶ğ¶ğ‘‰ğ‘‰ coherence didnâ€™t grow up as the iterations added. As for the chunk size and passes, no

apparent grow as the parameters added. Results are shown in Table 1.

With these tuned parameters, we tested the model on the test set and got satisfactory results
(ğ¶ğ¶ğ‘‰ğ‘‰ coherence = 0.410).

About the decoupling assumption, we have also tested the same chunk size and passes

with 12 topics, and then we calculated the Pearson correlation between the results, and they
behaved strong correlations ( ğ‘Ÿğ‘Ÿğ‘ğ‘â„ğ‘¢ğ‘¢ğ‘¢ğ‘¢ğ‘¢ğ‘¢ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘  = 0.857, ğ‘Ÿğ‘Ÿğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ = 0.735 ), which proved those
parameters can affect the model independently.

Table 1
The ğ¶ğ¶ğ‘‰ğ‘‰ coherence of different iterations, chunk size and passes.
parameter parameter value
iterations 10
100
500
1000
chunksize 10
100
500
1000
passes 5
10
50
100

c_v coherence
other parameters
0.439
num_topics=7, chunksize=100, passes=10
0.426
0.403
0.366
0.381
num_topics=7, iterations=100, passes=10
0.460
0.422
0.425
0.426
num_topics=7, iterations=100, chunksize=100
0.410
0.432
0.425

Topic analysis method
With the well-tuned model, we firstly delivered a visualization work to give an overview
of the whole data set using Python library pyLDAvis. Then, we calculated the dominant topic
(i.e. the topic with the highest probability) of each document. We gathered those dominant
topics and calculated the proportion of them. We also extracted the most representative

How Pandemic Spread in News: Text Analysis Using Topic Mode

13

documents of each topic by gathering the dominant topics separately and then picked up the
document with the highest probability within that topic.
Topic inconsistence evaluation method
As addressed in the Introduction, we are going to compare the difference between articles
and comments of the same news id, so an effective method to measure the inconsistence
between them is needed. Considering every document had a list of topics (usually 7 digits) after
applying that document with the optimal model, we delivered a small experiment on 3 measures
(i.e. Spearmanâ€™s rank correlation coefficient, Kendall rank correlation coefficient and cosine
similarity) to decide which measure to apply. We considered 2 pairs of topic lists, which is
shown as Table 2. Each pair has only the first two digits different, and they should be considered
very similar in the topic distribution. However, the results show that both Spearmanâ€™s and
Kendall rank correlation coefficient are sensitive to the digits, while the cosine similarity is
more stable that different digits share the similar ranks. In this way, cosine similarity is selected
as the measure to evaluate the topic differences. The measure scheme is shown in Figure 5.

How Pandemic Spread in News: Text Analysis Using Topic Mode

14

Table 2
Comparison of different correlation measures on 2 pairs of similar topic lists.
topic_list1 [1, 2, 0, 6, 3, 4, 5]
topic_list2 [2, 1, 0, 6, 3, 4, 5]
topic_list3 [1, 6, 0, 2, 3, 4, 5]
topic_list4 [6, 1, 0, 2, 3, 4, 5]

Spearman_cor = 0.964,
Kendall_tau = 0.905,
cosine_sim = 0.989
Spearman_cor = 0.107,
Kendall_tau = 0.143,
cosine_sim = 0.725

Figure 5
A schematic diagram of the method to calculate the inconsistence between news articles and
comments of the same news id.
Results
The optimal model has following parameters: number of topics = 7, iterations = 10, chunk
size = 100, passes = 5.
Overview
Table 3 shows the top 7 topic words within 7 topics. Words under each topic is described
manually. As we can see from those topic words, the difference between topics are relatively
obvious. Most of the words have actual meanings. It reflected the main agendas in SCMP
covering COVID-19 from Jan to May. Interestingly, although this is an issue about health,

How Pandemic Spread in News: Text Analysis Using Topic Mode

15

while topics about health only take less than a half of all topics.
Table 3
The top 7 words together with their probability within 7 topics. The descriptions are made
manually.
topic number
0
1
2

top 7 topic words
0.016*"patients" 0.015*"covid"
0.011*"china"

0.009*"$"

0.016*"workers" 0.016*"hong"

description

0.013*"disease"

0.012*"wuhan"

0.010*"vaccine"

0.010*"coronavirus"

0.009*"found"

Treatment

0.007*"economy"

0.007*"economic"

0.006*"us"

0.006*"companies"

0.005*"million"

Commercial

0.015*"kong"

0.012*"home"

0.010*"india"

0.009*"singapore"

0.008*"family"

International livelihood

3

0.043*"us"

0.015*"china"

0.014*"taiwan"

0.012*"war"

0.010*"scmp"

0.009*"comment"

0.008*"military"

Conflict

4

0.012*"cases"

0.012*"new"

0.010*"coronavirus"

0.009*"000"

0.008*"health"

0.007*"city"

0.007*"government"

Spread

5

0.029*"china"

0.026*"\'"

0.013*"world"

0.009*"trump"

0.008*"chinese"

0.007*"even"

0.006*"email"

Politics

6

0.037*"china"

0.023*"us"

0.017*"chinese"

0.011*"beijing"

0.011*"pandemic"

0.010*"world"

0.009*"coronavirus"

China/US

An overview graph of the topic distribution is shown in Figure 6. Notice that the numbers
in the circles are different from the topic numbers. The overlapping of circles shows the
distance (or closeness) between topics. The square of a circle refers to the total amount of that
topic. We can find that circle 2, 3, 4, correspond with topic 1 (Commercial), 5 (Politics), 6
(China/US) respectively, are closely tied, which is consistent with our common sense. One
word usually belongs to several topics. Some frequently applied words during the pandemic
and their topic list (sorted by topic probability) are shown in Table 4.
Table 4
Frequently applied words during the pandemic together with their belonged topics.
keyword
coronavirus
vaccine
infections
symptoms
respiratory
government
lockdown
outbreak
wuhan
migrant
huawei
trade

topic list (sorted)
4 0 6 1
0 6
4 0
0 4
0
4 1 5 6 2
4 2 1
6 4 1 0
0 5 4 6
2
3
6 1 3

How Pandemic Spread in News: Text Analysis Using Topic Mode

16

Figure 6
An overview graph of the topic distribution.

Dominant topic analysis
As shown in Figure 7, topic 5 (Politics) becomes the most popular of the pandemic report
in SCMP from Jan to May, which takes more than a half of the agendas. Topic1 (Commercial)
takes the second place among 7 topics with nearly 1/5 of the agendas. Topic 6 (China/US) takes
nearly 10% of the agendas which is in the third place. Interestingly, articles and comments
about treatment and virus itself merely take 0.5%. At the same time, very polarized conflict
contents only appear 0.5% as well.

How Pandemic Spread in News: Text Analysis Using Topic Mode

17

Figure 7
The proportion of each topic as the dominant (i.e. the topic with the highest probability of a
document).
Table 5 shows the most representative contents of each topic. All the contents are
happened to be comments.
Table 5
The most representative contents of each topic. All the contents are happened to be
comments.
TOPIC0 - 0.378, 2020-04-25
Guess their bank account is deep enough to cover the cost of their extended trip, as they
seem not to be too concerned with a little pandemic-related hiccup in their RTW itinerary.
TOPIC1 - 0.573, 2020-05-03
@Werner Ziegler
Actually no.... International scientists
determined "Type A" were mostly found in the US and Australia. China is mainly Type B.
TOPIC2 â€“ 0.523, 2020-04-29
The CCP is laughable. The rest of the world can see through its lies. and responsibility.
avoidance immediately. What a joke the CCP is.
TOPIC3 â€“ 0.379, 2020-04-20
how can you be cutting trips even primary sch children know that we are fighting dangerous
contengious virus yet they cut trips.....?
TOPIC4 â€“ 0.491, 2020-05-06
Dr. Fauci is the only credible person on that podium. Americans trust Fauci more than
they trust Trump or Pompeo.
TOPIC5 â€“ 0.701, 2020-05-18
"'They told us it wasn't contagious': Chinese blogger Fang Fang's forbidden diary reveals
how Wuhan authorities told people coronavirus could NOT be passed between people"
Fang FangSEARCH "Fang Fang Daily Mail"
TOPIC6 - 0.506, 2020-03-23
Trump is using China as a target to shift the public's attention because the government, under
his leadership is failing the American people big time!

How Pandemic Spread in News: Text Analysis Using Topic Mode

18

Topic inconsistence analysis
As shown in Figure 8, articles and comments shared a relatively good inconsistence in
general. News ids with similarity lower than 60% only took 34.3%. Within the news ids of low
similarity (i.e. of high inconsistence), the distribution of their dominant topics is shown in
Figure 9. In order to examine if there was any specific topic contribute to the inconsistence
more, we calculated Pearson correlation coefficient between this distribution and the dominant
topic distribution among all documents. High correlation (ğ‘Ÿğ‘Ÿ = 0.957) exists, which means no
specific topic leads to the inconsistence. We selected several articles and their corresponding
comments and tried to get some clues about the reason for inconsistence. Figure 10 and Figure
11 shows the article and comment with the lowest similarity (0.418). The article was talking
about the â€œslumpâ€ occurred due to the pandemic, and trading volume and trend were discussed
as well. While the comment was concerned with the home office fashion. Essentially, this
concern was stemmed from the economic â€œslumpâ€: besides the lockdown, many people work
at home during this period to save office rental. In this case, the article and the comment were
potentially related.

Figure 8
The distribution of topic similarity within different ranges.

How Pandemic Spread in News: Text Analysis Using Topic Mode

19

Figure 9
The proportion of inconsistent documents grouped by their dominant topics.

Figure 10
A case of topic inconsistence (article). Essentially, this comment is stemmed from this article.

How Pandemic Spread in News: Text Analysis Using Topic Mode

20

Figure 11
A case of topic inconsistence (comment). Essentially, this comment is stemmed from this
article.

Figure 12 shows a relative obvious inconsistence case. This article was purely about the
characteristics of the virus and some possible treatments. However, political accusation took a
half part of the comments. Within all 4 comments, there was also one comment criticized
mediaâ€™s spreading panic emotion. Only one comment was concerned with the articleâ€™s words.

How Pandemic Spread in News: Text Analysis Using Topic Mode

Figure 12
Another case of topic inconsistence. Political issues are the most concerned with the
comments, while the article itself is merely about medical issues.

21

How Pandemic Spread in News: Text Analysis Using Topic Mode

22

Discussion

As we can see from the results, the articles and comments on SCMP covering COVID-19
from Jan to May are mainly concerned with topics like â€œpoliticsâ€ and â€œcommercialâ€ stemmed
from the pandemic rather than the pandemic itself, and â€œpoliticsâ€ has attracted the most
attention. Among topics related to the pandemic itself, people have paid much attention on the
spreading situation while little on the treatment-related messages like â€œvaccineâ€ or â€œventilatorâ€.
We can describe these two features as people tend to â€œcomplainâ€ and â€œpanicâ€. Some people are
passionate with accusing China as the â€œsource of evilâ€ and laughing at Chinaâ€™s telling lies.
Topic inconsistence between articles and comments existed to a normal extent, and no
obvious topic was proved to lead the inconsistence. Some inconsistence was due to the very
deep relationships between the article and comment, that means people may be associative in
some cases. Meanwhile, we also found some inconsistence due to the emotional expression.
When the news was talking about fact, people were often caring about political issues.
Our research still remains some work to be finished further in the future. For example, the
SCMP provided news categorized by regions like â€œGreater Chinaâ€, â€œNorth Americaâ€ and
â€œEuropeâ€. These news articles have been already categorized by the editor, and different
distribution among different regions could be discovered. Another one is about the data
preprocessing. We were using unigram as our language model, and precision may be sacrificed
for the speed. Some words like â€œHong Kongâ€ were broken down into â€œHongâ€ â€œKongâ€, and this
may lead to some kind of misunderstanding when generating the topics. Different from the
unigram, bigram takes 2 words next to the keyword when calculating the conditional

How Pandemic Spread in News: Text Analysis Using Topic Mode

23

probability, thus it would be more precise to understand the contents. With enough computing
power, bigram or even trigram should be tried in data preprocessing. Cosine similarity was
applied in this research to measure the inconsistence between articles and comments. After
some experiments, this method was the optimal among the options available now. But it is still
sensitive to the topic number to some extent. In the future, a better measure remains to be
discovered.

How Pandemic Spread in News: Text Analysis Using Topic Mode

24

Conclusion

Inspired by previous researches, this research applied LDA topic model to analyze the
news articles and comments on SCMP covering the COVID-19 pandemic from Jan 20 to May
19. Model parameters were tuned on the training set after a series of experiments and tested on
the test set. ğ¶ğ¶ğ‘‰ğ‘‰ coherence, which was used as the measure of model performance, was good
with the set of optimal parameters. Using the optimal model, the dominant topics from Jan to
May was counted, the most representative contents were also analyzed, while several
visualization works were conducted. Topic inconsistence phenomenon was measured with
cosine similarity and the factors lead to this phenomenon were discussed. At last, 3 possible
improvements were proposed.

How Pandemic Spread in News: Text Analysis Using Topic Mode

25

References

[1] Cinelli, M., Quattrociocchi, W., Galeazzi, A., Valensise, C. M., Brugnoli, E., Schmidt, A.
L., ... & Scala, A. (2020). The covid-19 social media infodemic. arXiv preprint
arXiv:2003.05004.
[2] Deerwester, S., Dumais, S. T., Furnas, G. W., Landauer, T. K., & Harshman, R. (1990).
Indexing by latent semantic analysis. Journal of the American society for information
science, 41(6), 391-407.
[3] Hofmann, T. (1999, August). Probabilistic latent semantic indexing. In Proceedings of the
22nd annual international ACM SIGIR conference on Research and development in
information retrieval (pp. 50-57).
[4] Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent dirichlet allocation. Journal of
machine Learning research, 3(Jan), 993-1022.
[5] Cheng, X., Yan, X., Lan, Y., & Guo, J. (2014). Btm: Topic modeling over short texts. IEEE
Transactions on Knowledge and Data Engineering, 26(12), 2928-2941.
[6] Blei, D. M. (2012). Probabilistic topic models. Communications of the ACM, 55(4), 77-84.
[7] Hong, L., & Davison, B. D. (2010, July). Empirical study of topic modeling in twitter. In
Proceedings of the first workshop on social media analytics (pp. 80-88).
[8] RÃ¶der, M., Both, A., & Hinneburg, A. (2015, February). Exploring the space of topic
coherence measures. In Proceedings of the eighth ACM international conference on Web
search and data mining (pp. 399-408).

