MedDG: A Large-scale Medical Consultation Dataset for Building Medical
Dialogue System
Wenge Liu1 * , Jianheng Tang1âˆ— , Jinghui Qin2 , Lin Xu3 , Zhen Li4 , Xiaodan Liang1,2â€ 

arXiv:2010.07497v1 [cs.CL] 15 Oct 2020

1

Sun Yat-Sen University, 2 Dark Matter AI Inc., 3 National University of Singapore, 4 Chinese University of Hong Kong, Shenzhen
{kzllwg, sqrt3tjh, cathyxl2016, xdliang328}@gmail.com,
qinjingh@mail2.sysu.edu.cn,
lizhen@cuhk.edu.cn

Abstract
Developing conversational agents to interact with patients
and provide primary clinical advice has attracted increasing attention due to its huge application potential, especially
in the time of COVID-19 Pandemic. However, the training
of end-to-end neural-based medical dialogue system is restricted by an insufficient quantity of medical dialogue corpus. In this work, we make the first attempt to build and release a large-scale high-quality Medical Dialogue dataset related to 12 types of common Gastrointestinal diseases named
MedDG, with more than 17K conversations collected from
the online health consultation community. Five different categories of entities, including diseases, symptoms, attributes,
tests, and medicines, are annotated in each conversation of
MedDG as additional labels. To push forward the future research on building expert-sensitive medical dialogue system,
we proposes two kinds of medical dialogue tasks based on
MedDG dataset. One is the next entity prediction and the
other is the doctor response generation. To acquire a clear
comprehension on these two medical dialogue tasks, we implement several state-of-the-art benchmarks, as well as design
two dialogue models with a further consideration on the predicted entities. Experimental results show that the pre-train
language models and other baselines struggle on both tasks
with poor performance in our dataset, and the response quality can be enhanced with the help of auxiliary entity information. From human evaluation, the simple retrieval model
outperforms several state-of-the-art generative models, indicating that there still remains a large room for improvement
on generating medically meaningful responses1 .

Introduction
During the COVID-19 pandemic, millions of patients worldwide have been facing delays in diagnosis and treatment
due to the diversion of medical resources. As a result,
telemedicine is increasingly expected to play a role in
relieving therapeutic stress. According to McCall (2020),
telemedicine has substantially risen from around 10% of
general medicine consultations before COVID-19 to approximately 75% in the UK during the peak of the pandemic.
* equal

contribution
corresponding author
1
Data and code in this paper are publicly available at https:
//github.com/lwgkzl/MedDG
â€ 

Medical Conversation
I have a bloated stomach sometimes with
acid reflux and frequent ringing (Female, 17
years old)
èƒƒéƒ¨èƒ€æ°”æœ‰æ—¶å€™ä¼šåé…¸ç»å¸¸æ€§ä¼šå‘å‡ºé¸£å“
ï¼ˆå¥³ï¼Œ17å²ï¼‰
How long has this been going on exactly?
è¿™ç§æƒ…å†µå…·ä½“å¤šé•¿æ—¶é—´äº†ï¼Ÿ

Annotated entities
Symptom: bloating,
acid reflux
ç—‡çŠ¶: èƒƒèƒ€, åé…¸

Attribute: Duration
å±æ€§: æ—¶é•¿

One year.
ä¸€å¹´
Any stomachaches, hiccups, etc.?
æœ‰æ²¡æœ‰è…¹ç–¼ã€æ‰“å—ç­‰ï¼Ÿ

Symptom: bellyache,
hiccup
ç—‡çŠ¶: è…¹ç—›, æ‰“å—

Sometimes my stomach hurts but only a few
times I burp.
æœ‰æ—¶å€™è…¹ç–¼ä½†å°±å‡ æ¬¡ä¼šæ‰“å—ã€‚

Symptom: bellyache,
hiccup
ç—‡çŠ¶: è…¹ç—›, æ‰“å—

How's the digestion these days?
è¿™æ®µæ—¶é—´æ¶ˆåŒ–æ€ä¹ˆæ ·ã€‚
Sometimes I feel indigestible.
æœ‰æ—¶å€™è§‰å¾—ä¸æ¶ˆåŒ–ã€‚
Well, this condition is caused by indigestion,
and oral omeprazole plus morphine is
recommended for treatment.
å—¯ï¼Œè¿™ç§æƒ…å†µå±äºæ¶ˆåŒ–ä¸è‰¯é€ æˆçš„ï¼Œå»ºè®®
å£æœå¥¥ç¾æ‹‰å”‘åŠ å—ä¸å•‰æ²»ç–—ã€‚

Symptom: Indigestion
ç—‡çŠ¶: æ¶ˆåŒ–ä¸è‰¯
Symptom: Indigestion
Medicine: Omeprazole,
Domperidone
ç—‡çŠ¶: æ¶ˆåŒ–ä¸è‰¯
è¯ç‰©: å¥¥ç¾, å—ä¸å•‰

Figure 1: Part of a typical medical consultation dialogue in
the MedDG dataset between a patient (orange) and a doctor
(blue) with the corresponding annotated entities.

Figure 1 demonstrates a typical medical consultation dialogue. The patient reported some health issues in the beginning, and then the doctor kept asking to obtain more specific
information about the patient. Finally, the doctor made a disease diagnosis and provided medical advice based on both
the gathered information and clinic experience.
However, online medical consultations are much less efficient than offline consultations since most patients are unable to describe their symptoms clearly and communicate
with doctors efficiently. In such situation, doctors need to
spend more time to understand patientsâ€™ conditions and give
a correct diagnosis. In a study of COVID-19 (Gong et al.
2020), among 8,913 online medical consultations, nearly
half (48.08%) of counselees had no related symptoms and
were asked irrelevant questions, which wastes doctorsâ€™ time

dramatically.
Medical conversational agents pose a great tool to assist doctors in responding to easily-diagnosed common disease and pre-collecting patientsâ€™ medical information for the
hard disease. In order to advance the development of medical conversational agents, several medical dialogue corpora
have been released to facilitate the research of information
extraction (Lin et al. 2019; Zhang et al. 2020a; Shi et al.
2020) and automatic diagnosis (Wei et al. 2018; Xu et al.
2019). However, these corpora fail to address the problem of
dialogue generation due to the lack of full natural languagebased dialogues and the limited dataset scale. Specifically,
the quantity of dialogues in these corpora is around 500
to 2k, which is apparently insufficient to train the learningbased dialogue generation model.
To mitigate the dilemma of data scarcity, we collect
MedDG, a large-scale Medical Dialogue dataset related to
12 Gastrointestinal diseases. As shown in Table 1, compared
with previous datasets, our dataset has the following advantages. First, our MedDG contains more than 17K dialogues
and 385k utterances, which is 10 times larger than previous datasets, thus more suitable for the training of generative
models. Second, MedDG is informative and diverse, including 12 types of diseases and 160 types of entities, which is
much closer to the realistic medical consultation scenario.
Finally, the average and median number of the entity occurrence times are significantly higher than other corpora, indicating that the annotation labels are abundant and relatively
balanced.
In addition to constructing MedDG, we propose two tasks
based on the dataset. the first task is to predict next entities according to dialogue contexts, while the other aims
to generate doctorsâ€™ responses. To get a clear understanding of these two tasks, we compare several state of the art
models on MedDG, including both classic neural baselines
and popular pre-train language models, such as BERT (Devlin et al. 2019) and GPT (Radford et al. 2019). Besides,
we also design two entity-aware dialogue systems including
both retrieval-based and generation-based models to utilize
entity-level information. Experimental results demonstrate
that the response quality can be enhanced by combining
entity information with dialogue history in the generation
phase, while there still remains a large room for improvement on designing specific dialogue modules to utilize entity
annotation and generate medically meaningful responses.
Our contribution can be summarized as follows:
â€¢ We collect MedDG, a large-scale medical dialogue
dataset related to 12 types of common gastrointestinal diseases. MedDG contains more than 17K conversations and
385K utterances with the annotation of 5 types of medical
entities, making it as a convincing benchmark to evaluate
the medical consultation capability of dialogue systems.
â€¢ We provide various baselines for both the entity prediction task and the response generation task on the MedDG
dataset, and propose two methods to make use of dialogue
entity prediction results in the medical dialogue system.
â€¢ We conduct extensive experiments, including quantitative
and human evaluations, to compare many state-of-the-art

neural-based and pre-train based models, giving a comprehensive understanding of the new proposed medical
dialogue generation task. Results show that utilizing predicted entities as auxiliary information could effectively
improve the response quality by the input concatenation
method.

Related Work
Medical Dialogue Methods
Natural language understanding of medical dialogue has
been investigated a lot recently, such as information extraction (Zhang et al. 2020a), relation prediction (Du et al. 2019;
Lin et al. 2019) and slot filling (Shi et al. 2020). The adoption of reinforcement learning framework in dialogue systems (Dhingra et al. 2017; Li et al. 2017; Peng et al. 2018)
has inspired dialogue management strategy learning in the
medical domain. Wei et al. (2018) took the first step to address the issue of medical dialogue for automatic diagnosis
using the Deep Q-Network. Furthermore, Xu et al. (2019)
improved the rationality of decision-making for medical dialogue, which incorporates external probabilistic symptoms
related to the framework of reinforcement learning. Liao
et al. (2020) assigned different workers to conduct symptom
acquisition and disease diagnosis hierarchically. However,
these reinforcement learning approaches merely learn from
tabular data containing the presence of symptoms, while ignoring the significance of other important information such
as the attributes of the symptom, tests, and medicine.
Most of the previous work merely focuses on a single
module of the medical dialogue system, e.g., natural language understanding or dialogue management, while the
study of constructing a complete system are relatively few.
The early attempts include Ferguson et al. (2009); Wong,
Thangarajah, and Padgham (2011); Gatius and Namsrai
(2012); Liu et al. (2016). A more recent work is Xu et al.
(2019), which introduces a knowledge-routed dialogue system for automatic diagnosis. It uses pre-defined templates to
simulate a doctorâ€™s response based on the action predicted
by the dialogue management module. However, the action
pool is limited to symptoms and diseases, unable to handle
other everyday situations like check requirements and medical recommendations. Beyond that, the ignorance of a more
general response generation module leads to a huge gap in
practical applications.

Medical Dialogue Datasets
Wei et al. (2018) first launched a dataset for medical diagnosis, but it only contains the structured user goal data
instead of natural language dialogue. The DX dataset (Xu
et al. 2019) contains 527 natural language dialogues, but the
sentence patterns are simple. Lin et al. (2019) collected the
CMDD dataset with 2067 dialogues on four pediatric diseases and Zhang et al. (2020a) releases MIE with 1120 dialogues on six cardiovascular diseases. However, CMDD and
MIE are all proposed only for the purpose of NLU training,
and not large enough to build a complete generative dialog
system. Besides, Shi et al. (2020) is another medical dialogue corpus in the general domain with 2k labeled data and

Dataset

Domain

# Diseases

# Dialogues

# Utterances

# Entities

Avg.

Med.

Pediatrics
Pediatrics
Pediatrics
Cardiology

4
5
4
6

710
527
2,067
1,120

2,816
87,005
18,129

70
46
161
71

67.06
65.95
194.39
93.70

33
12
18
64

Gastroenterology

12

17,864

385,951

160

1357.64

428

MZ (Wei et al. 2018)
DX (Xu et al. 2019)
CMDD (Lin et al. 2019)
MIE (Zhang et al. 2020a)
MedDG (ours)

Table 1: Comparison between our corpus and other human-labeled medical dialogue corpora. Statistics include the quantity of
dialogues, disease types and entity types, the average (Avg.) and medium (Med.) occurrence times of entities, and the corpus
diversity (Div.), respectively.

(1)

Collect the source dialogue corpus
Anonymization and filtering

(2)

Make annotation guideline
Label part of data manually

(3) Design or fine-tune the annotation program
Label the rest of data automatically

N
Sampling

(4) Binomial test
Y
Obtain the final dataset

Figure 2: The semi-automated construction and annotation
pipeline of the MedDG dataset.

100k unlabeled data, but in the form of separate utterance
instead of the whole dialogue. Table 1 summarizes the public human-labeled Chinese medical dialogue corpora. Compared with these corpora, our proposed MedDG involves
more diseases, entities, dialogues and utterances to alleviate
the issue of data scarcity.

MedDG Dataset
To further advance the research in medical dialogue system, we constructed a natural language dialogue dataset that
includes the conversation between doctors and patients in
the medical consultation scenario with the annotation of
important entities. In this section, we introduce our proposed dataset from the perspective of construction details
and dataset statistics.

Construction Details
In the following, we describe our pipeline to construct and
annotate the dataset. As shown in Figure 2, to control costs
within a reasonable range, we design a semi-automated annotation method. First, a part of the dataset is manually an-

notated, and then an automatic program is designed to label
the rest of the dataset.
The source doctor-patient dialogues are collected from
the Gastroenterology department of a Chinese online health
community, Chunyu-Doctor2 , where patients can submit
posts about their relative gastrointestinal problems and then
consult with qualified doctors for professional diagnosis advice. We select consultations related to Gastroenterology because it is common, diverse, and has more inquiries, while
other departments may depend more on tests. Dialogues
containing personal information, images, or audios are all
filtered before annotation. Dialogues without enough turns
are also ignored. After discussing with domain experts, we
choose five main categories of entity for annotation: diseases, symptoms, attributes, tests, and medicine. We further
define items of each category based on the terminology lists
in the Chinese medical knowledge graph CMeKG3 and the
frequency in our corpus. Figure 3 lists the common items of
each entity category.
Similar to Zhang et al. (2020a); Shi et al. (2020), we
use an utterance-to-information annotation method instead
of sequential labeling because entities are not always explicitly or consecutively expressed. As shown in Figure 1, each
utterance of the conversation is labeled separately. Eight
annotators with the relevant medical background were involved in the annotation process. They first discuss to determine a primer annotation guide. Then each one annotates
a small part of data and reports the confusing utterance according to the guide. We summarize the found issues, improve the guide, and launch the formal annotation process.
Each conversation is randomly assigned to three annotators,
and a total of 1,000 conversations are manually annotated.
The Cohenâ€™s kappa coefficient among annotators is between
94.81% and 97.59%, indicating a strong agreement between
annotators.
We then develop a program to label the rest conversation automatically. The program consists of regex rules for
each item to cover all human-labeled data, and apply these
rules to label other dialogues. We randomly pick 400 autolabeled utterances and show them to three experts to evaluate the annotation quality of the program. Experts are required to find errors and omissions from these utterances.
We modify the program to correct these errors, select 400
2
3

https://www.chunyuyisheng.com/
https://zstp.pcl.ac.cn:8002/

# Dialogues
# Utterances
# Char. per U
# Char. per D
# Entities per U
# Entities per D

Train

Dev

Test

Total

14,864
321,666
17.70
382.99
0.56
12.09

2,000
42,850
17.77
380.67
0.57
12.24

1,000
21,435
17.64
378.06
0.61
13.02

17,864
385,951
17.70
382.45
0.56
12.16

Table 2: Data statistics of MedDG. The last four rows are
the average number of characters in each utterance and dialogue, and the average number of entities in each utterance
and dialogue, respectively.
utterances again, and repeat this process until the accuracy
rate is higher than a given threshold. Eventually, the accuracy rate achieves 96.75%, which means the utterance-level
annotation accuracy is higher than 95% by the binomial test
with a significance value p < 0.01, validating the effectiveness of the annotation program. Finally, we put all the manually annotated data in the test set and randomly divide the
automatically labeled data into the training set and the development set.

Dataset Statistics

Entity Constitution

Models
Task Definition
Medical dialogue system aims to generate context consistent and medically meaningful responses conditioned on the
conversation histories. In this paper, we mainly focus on
two tasks, entity prediction and response generation. Formally, given the conversation history between doctors and
patients X = {X1 , X2 , ..., Xi , .., XK }, K is the current turn
of the dialogue history, Xi is a sentence from doctors or patients. The next doctorâ€™s response Y = {y1 , y2 , ..., yT } corresponds to an entity set ey = {ey1 , ..., eys }. The entity prediction task is to predict the entity set ey and we only consider the samples with a non-empty set, while the response
generation task is to generate Y directly.

Entity Prediction Model
The entity prediction model is made up of a neural dialogue
encoder in conjunction with a single-layer multi-label classifier. Concretely, the encoder input is the concatenation of

Duration

Disease

6%

7%

Test

14%

inducement

11%

Nature of pain

Medicine
8%

Location

Symptom

55%

23%

59%

17%

Medicine

Test
â€¦

â€¦
Transaminase
Colonoscope
Barium meal â€¦
Diabetes
Doppler ultrasound
CT
B-mode ultra â€¦
Routine blood test
Routine defecation
Colonoscopy
Gastroscope

Antibiotic
Trimebutine
Amoxicillin
Bacillus
Smecta
Talcid
Probiotics
Rabeprazole
Mosapride
Motilium
Omeprazole

194
235
277
318
319
376
731
839
998
4598
7808
0

2000

4000

6000

8000

798
844
1071
1418
1525
1561
1686
1745
2055
2196
0

10000

Disease

5574

2000

4000

6000

Symptom
â€¦

â€¦
Pneumonia
Pancreatitis
Irritable bowel
Cholecystitis
Gastric ulcer
Appendicitis
Esophagitis
Influenza
Constipation
Enteritis
Gastritis

203
317
652
652
684
760
832
2405
6192
7122
9977
0

Table 2 summarizes the data statistics of MedDG. The training/development/test set is divided into 14864/2000/1000
conversations, respectively. The average length of utterances
and dialogues are approximately the same in three sets, as
well as the average number of entities in each utterance and
dialogue, which means that the distribution of the data in
three sets is relatively consistent among three sets. There
are 160 entity items in total, consisting of 12 disease items,
62 symptom items, 4 attribute items, 20 test items, and 62
medicine items. The complete name list of 160 entity items
and the regex table are presented in the supplementary material. As shown in Figure 3, the frequency of symptoms is
the highest, accounting for 55% of the total.

Attribute

Attribute

5000

10000

Indigestion
Fever
Belch
Stomachache
Discomfort
Countercurrent
Nausea
Vomit
Abdominal disâ€¦
Abdominal pain
Diarrhea
15000

3491
4266
4637
5507
6028
6837
7121
7738
9356
12413
13024
0

5000

10000

15000

Figure 3: Entity distribution in the MedDG dataset. The pie
chart in the upper left corner shows the proportion of entities
in the five categories, and other five charts demonstrate the
entities statistics of each category.

the past k utterance, and the output is a single-dimensional
context vector hdial represents a summary of the dialogue
history. Then hdial is fed to a final single-unit dense layer
with sigmoid to get the prediction probability of each entity.
We attempt various text encoders including LSTM (Hochreiter and Schmidhuber 1997), TextCNN (Kim 2014), BERTwwm (Cui et al. 2019) and PCL-MedBERT4 . Among these
baselines, LSTM and TextCNN are two classic text encoder
based on RNN and CNN, respectively. BERT-wwm is a 12layer Chinese BERT pre-trained with whole word masking
on Wikipedia and fine-tuned on our dataset. PCL-MedBERT
is another 12-layer Chinese BERT additionally pre-trained
on 2.7G high-quality medical text and QA data. The hidden
state of [CLS] in the BERT model is used as the dialogue
state.
For all variants of the entity prediction model, during
training, network parameters are updated by minimizing the
binary cross entropy loss between the ground-truth entities
and the prediction probability, and during testing entities
with probabilities larger than 0.5 will be chosen as next entities.

4

https://code.ihub.org.cn/projects/1775

Response Generation Model
We adopt multiple widely used text generation models as
follows:
Seq2Seq (Sutskever, Vinyals, and Le 2014) is a classical attention-based sequence to sequence model with vanilla
RNN encoder and decoder.
HRED (Serban et al. 2016) extends the traditional RNN
encoder by stacking two RNNs in a hierarchical way: one at
word level and one at the utterance level, which is frequently
used as a dialogue encoder.
GPT2 (Radford et al. 2019) is a language model based on
Transformer. We use the parameter pre-trained on chinese
chitchat dialogues5 .
DialoGPT (Zhang et al. 2020b) is a variant of GPT-2
model using a maximum mutual information scoring function to penalize bland responses.
For each model, the generation loss is the average of negative log likelihood of the target sequence {ytâˆ— }(1 â‰¤ t â‰¤ T ):
Lg =

T
1X
log P (ytâˆ— ).
T t=1

(1)

Entity-aware Dialogue Model
Since the generative models mentioned above are irrelevant
to the annotated entity labels in our dataset, we further propose two entity-aware dialogue models to make use of the
auxiliary entity information.
Entity concatenation This method is to directly concatenate the entity information after the dialogue history as new
input text. We first train a BERT classifier to predict possible
medical entities in the next utterance based on the dialogue
history. These predicted entities are then concatenated to the
end of dialogue history as additional learning signals, encouraging models to generative response relevant to these
entities. We use the -Entity suffix to distinguish generative
models trained on new entity-concatenated data.
Entity retrieval We also built a retrieval-based dialogue
system that uses predicted entities as key information to
retrieve the most relevant response. Firstly, we collect an
entity-utterance dict from the training set, where dict key is
a combination of entities and value is a set of utterance containing such entity combination. Then we also use the BERT
classifier to predict possible entities from dialogue history.
Finally, we retrieve a key from the dict which has smallest
entity combination while covering all predicted entities. The
response is randomly picked from the utterance set corresponding to such key. This baseline is named as Retrieval.

Experiments
Implementation Details
For RNN-based models, the single-layer LSTM (Hochreiter
and Schmidhuber 1997) is used as RNN encoders and decoders. Both the word embedding and hidden dimensions of
LSTM are set to 300. We use Adam optimizer with a minibatch size of 16 and set the initial learning rate to 0.001. For
5

https://github.com/yangjianxin1/GPT2-chitchat

BERT-based and GPT-based models, we follow the configurations in the origin paper. Each model is trained up to 30
epochs. The overall training time is less than one day on a
machine with four NVIDIA RTX2080 Ti graphic cards. The
patience argument is set to 5 for early stopping. The models are saved according to the best F 1e score for the entity
prediction task and the best BLEU-4 score for the response
generation task in the validation set of MedDG. All experiments are implemented by Pytorch.

Automatic Evaluation
Metrics For the entity prediction task, the metrics Pe , Re
and F 1e refer to the precision, recall and F1 measures of
all categories of predicted entities. We also calculate the F1
score of each entity category and donate them as F 1D (Diseases), F 1S (Symptoms), F 1A (Attributes), F 1T (Test)
and F 1M (Medicine), respectively. Pf is the future prediction precision by considering all entities occurred in the
following conversation as target entities with the ignorance
of order.
For the response generation task, the smoothed sentencelevel BLEU-1 and BLEU-4 (Chen and Cherry 2014) are
used as the uni-gram and four-gram lexical similarity metric. Entity-P/R/F1 is the precision/recall/F1 score between
predicted entities in generated response and gold entities to
measure the correctness of entity usage. Distinct-1/2 (Li
et al. 2016) is also provided to evaluates the uni-gram and
bi-gram diversity of generated responses.
Results The results of the entity prediction task are shown
in Table 3. Compare with classic RNN and CNN encoder,
the BERT-based encoders reach better performance in terms
of all metrics, which validates the effectiveness of pretraining. Moreover, PCL-MedBERT outperforms BERT in
most of the metrics, proving that conducting further pretraining on domain-specific data can improve the model performance. Thus PCL-MedBERT is chosen as the backbone
of all entity-aware dialogue models. However, the predicted
entity F1 metric F 1e is still lower than 30%, which remains
a large room for improvement.
The performance of both retrieval-based and generationbased dialogue models are summarized in Table 4. We analyze the results from the following perspectives:
The influence of entity concatenation All the generative models obtain more than 4% absolute improvement in
terms of the Entity-F1 metric after adding predicted entities
to enhance the generation, and the BLEU scores are also
improved, proving that the entity concatenation method can
effectively improve the generation quality. Meanwhile, the
diversity metrics (Distinct-1/2) decrease in most cases. This
is because the guidance of the entity enforces the model to
generate more fixed patterns, such as asking symptoms or
informing disease.
Retrieval vs. Generation Since we only implement a relatively simple retrieval model based on merely the predicted
entities without contextual information, the BLEU score is
much lower than generative modes. In contrast, the diversity
and entity correctness metrics are relatively high. The entity
recall is higher than all generation models because our re-

Model

F 1D

F 1S

F 1A

F 1T

F 1M

Pe

Re

F 1e

Pf

LSTM
TextCNN
BERT
PCL-MedBERT

31.18
29.54
31.66
33.72

21.72
20.55
24.27
25.62

48.95
50.33
52.44
46.85

25.05
23.58
26.03
27.49

15.66
19.01
19.82
20.78

25.34
22.37
26.05
26.46

27.75
30.12
31.09
33.07

26.49
25.67
28.35
29.40

45.74
42.75
47.33
46.82

Table 3: Results of the entity prediction task on the MedDG dataset. Note that all metrics are normalized to [0, 100] and the best
results are in bold.
BLEU-1

BLEU-4

Distinct-1

Distinct-2

Entity-P

Entity-R

Entity-F1

Retrieval

23.08

12.58

0.62

9.98

11.44

33.11

17.00

Seq2Seq
Seq2Seq-Entity
HRED
HRED-Entity

26.12
35.24
31.56
38.66

14.21
19.20
17.28
21.19

0.88
0.75
1.07
0.75

4.77
5.32
8.43
7.06

14.07
12.41
13.29
12.01

11.45
25.65
11.25
26.78

12.63
16.73
12.18
16.58

GPT2
GPT2-Entity
DialoGPT
DialoGPT-Entity

29.35
30.87
34.57
34.90

14.47
16.56
18.09
18.61

1.26
0.87
0.50
0.77

13.53
11.20
9.92
9.87

7.33
20.76
11.30
21.16

12.22
14.51
9.99
13.53

9.17
17.08
10.61
16.51

Model

Table 4: Performance of all comparison dialogue systems on the MedDG dataset in terms of response quality, diversity and
entity correctness. Note that all metrics are normalized to [0, 100] and the best results are in bold.

Sentence
Smoothness

Knowledge
Correctness

Entire
Quality

Retrieval

4.23

4.29

4.18

HRED
HRED-Entity
GPT2
GPT2-Entity

3.46
3.93
2.84
3.27

3.12
4.05
2.70
2.69

3.16
4.05
2.73
2.83

Îº

0.41

0.59

0.52

Model

Table 5: Results of human rating on the MedDG datasets. Îº
is the average pairwise Cohenâ€™s kappa score between annotators.

trieval strategy ensures that all predicted entities are covered
in the response.
RNN-based Models vs. GPT-based Models Although
the GPT2-Entity model generates more correct entities, the
BLEU scores of HRED-Entity are higher than all GPT-based
methods. The possible reason is that the parameters of GPT2
and DialoGPT are pre-trained by the chit-chat corpus, so
they tend to generate responses irrelevant to the medical domain. We also observe this phenomenon in human evaluation. Besides, the Entity-F1 scores of all dialogue models are
all lower than 20%, meaning that there still remains a large
room for improvement on generating meaningful responses
with correct entities.

Manual Evaluation
We also perform human evaluation for a more thorough comparison of five selected dialogue models
(Retrieval/HRED/HRED-Entity/GPT2/GPT2-Entity).
We randomly picked 100 test cases in each dataset. Three
annotators with a medical background are invited to rate the
response of these models independently in three aspects:
sentence smoothness, knowledge correctness, and the
entire quality. The scores range from 1 (strongly bad) to 5
(strongly good).
The evaluation result is summarized in Table 5. To our
surprise, the Retrieval model is significantly higher than the
other generative models on all metrics, especially in terms of
sentence fluency, which is inconsistent with the automatic
evaluation. A possible explanation is that the BLEU metric only measures the similarity between predicted response
and the gold sentence, while neglecting other reasonable responses in the corpus. From the results of four generation
models, we can observe that HRED-based models outperform GPT2-based models by a large margin. Corresponding to the automatic evaluation, incorporating entities helps
to further improve the generation quality. In addition, the
average pairwise Cohenâ€™s kappa scores between annotators
exceed 0.4 for all metrics, which indicates a moderate annotator agreement.

Case Study
We take a close look at the model performance by studying
the examples of the response from five different models in
the MedDG dataset. As shown in Table 6, the responses of
retrieval model included all the predicted entities, but they

Utterance & Gold Response

Model Response

ğ‘·ğ‘·1: I have a dull pain and grunting in the upper right
corner of my belly button. (Female, 44 years old)
æˆ‘è‚šè„æ–œå³ä¸Šæ–¹éšç—›è¿˜å’•å™œå™œå«ã€‚ï¼ˆå¥³ï¼Œ44å²ï¼‰

Retrieval

Hello, how long has this situation been? How many times a day do you poop?
æ‚¨å¥½ï¼Œè¿™ç§æƒ…å†µå¤šé•¿æ—¶é—´äº†ï¼Ÿå¤§ä¾¿ä¸€å¤©æ‹‰å‡ æ¬¡ã€‚

GPT2

Hello, how long has this been happening?
ä½ å¥½ï¼Œè¿™ç§æƒ…å†µæœ‰å¤šä¹…äº†ï¼Ÿ

GPT2-Entity

Hello, how long has this been happening?
ä½ å¥½ï¼Œè¿™ç§æƒ…å†µå¤šä¹…äº†ï¼Ÿ

HRED

Hello, how long has this been happening?
ä½ å¥½ï¼Œè¿™ç§æƒ…å†µå¤šä¹…äº†ï¼Ÿ

Golden Label: Duration (æ—¶é•¿)

HRED-Entity

Hello, how long has this been happening?
ä½ å¥½ï¼Œè¿™ç§æƒ…å†µå¤šä¹…äº†ï¼Ÿ

ğ‘·ğ‘·2 : For more than a week, I took a B-ultrasound yesterday and said nothing was found, but I felt full after eating.
ä¸€ä¸ªå¤šæ˜ŸæœŸï¼Œæ˜¨å¤©ç…§äº†Bè¶…è¯´æ²¡å‘ç°ä»€ä¹ˆ,ä½†æ˜¯åƒ
å®Œå°±è§‰å¾—é¥±èƒ€æ„Ÿã€‚

Retrieval

Are there any other discomforts: nausea, heartburn, acid reflux, belching,
abdominal distension, abnormal stool, etc?
è¿˜æœ‰å…¶ä»–ä¸é€‚å—ï¼šæ¶å¿ƒã€çƒ§å¿ƒã€åé…¸ã€å—³æ°”ï¼Œè…¹èƒ€ï¼Œå¤§ä¾¿å¼‚å¸¸ç­‰ï¼Ÿ

GPT2

Is this normal?.
è¿™ç§æ˜¯æ­£å¸¸çš„å—ã€‚

GPT2-Entity

Any nausea, acid reflux and belching?
æœ‰æ²¡æœ‰æ¶å¿ƒï¼Œåé…¸æ‰“å—ï¼Ÿ

Predicted Label: Reflux, Hiccups, Nausea, Heartburn
(åæµ, æ‰“å—, æ¶å¿ƒ, çƒ§å¿ƒ)

HRED

Do you have cholecystitis?
æœ‰èƒ†å›Šç‚å—ï¼Ÿ

Golden Label: Diarrhea, Abdominal pain, Nausea,
Vomiting, Reflux (è…¹æ³», è…¹ç—›, æ¶å¿ƒ, å‘•å, åæµ)

HRED-Entity

Is there any heartburn, acid reflux, bloating, greasy heartburn?
æœ‰æ²¡æœ‰çƒ§å¿ƒåé…¸è…¹èƒ€åŒæ²¹è…»çš„çƒ§å¿ƒï¼Ÿ

ğ‘·ğ‘·6 : May I ask what kind of disease I have?
è¯·é—®æ‚¨æˆ‘è¿™ä¸ªå¤§æ¦‚æ˜¯å“ªé‡Œçš„é—®é¢˜ã€‚

Retrieval

This may be caused by gastritis. It is recommended that you have a gastroscope
to rule out digestive ulcers.
è¿™ä¸ªå¯èƒ½æ˜¯æœ‰èƒƒç‚ï¼Œå»ºè®®æ‚¨æœ€å¥½åšä¸ªèƒƒé•œï¼Œå¯ä»¥æ’é™¤æ¶ˆåŒ–é“æºƒç–¡ã€‚

ğ‘«ğ‘«6 : Consider you this may be caused by enteritis, gastritis
or gastrointestinal dysfunction.
è€ƒè™‘ä½ è¿™æœ‰å¯èƒ½æ˜¯èƒƒè‚ ç‚ï¼Œèƒƒè‚ åŠŸèƒ½ç´Šä¹±å¼•èµ·çš„ã€‚

GPT2

Sorry, I was a bit busy just now.
ä¸å¥½æ„æ€ï¼Œåˆšæ‰æœ‰ç‚¹å¿™ã€‚

GPT2-Entity

This situation is considered to be caused by gastritis with gastric mucosa. You
can consider taking medicine for a period of time to adjust.
è¿™ç§æƒ…å†µè€ƒè™‘æ˜¯èƒƒç‚ä¼´èƒƒé»è†œå¯¼è‡´çš„ï¼Œå¯ä»¥è€ƒè™‘æœè¯ä¸€æ®µæ—¶é—´è°ƒç†ä¸€ä¸‹ã€‚

HRED

Consider gastrointestinal?
è€ƒè™‘èƒƒè‚ ï¼Ÿ

HRED-Entity

Considering the possibility of gastritis, it is recommended to have a gastroscope
to see.
è€ƒè™‘èƒƒç‚å¯èƒ½æ€§å¤§ï¼Œå»ºè®®åšä¸ªèƒƒé•œçœ‹çœ‹ã€‚

ğ‘«ğ‘«1 : Hello, how long has this been happening?
ä½ å¥½ï¼Œè¿™ç§æƒ…å†µå¤šä¹…äº†ï¼Ÿ
Predicted Label: Duration (æ—¶é•¿)

ğ‘«ğ‘«2 : Are there nausea, vomiting, acid reflux, abdominal
pain and diarrhea?
æœ‰æ¶å¿ƒå‘•ååé…¸ï¼Œè…¹ç—›è…¹æ³»å—ï¼Ÿ

â€¦â€¦

Predicted Label: Gastritis, Gastroscope (èƒƒç‚, èƒƒé•œ)
Golden Label: Gastrointestinal dysfunction, Gastritis,
Enteritis (èƒƒè‚ åŠŸèƒ½ç´Šä¹±, èƒƒç‚, è‚ ç‚)

Table 6: Examples of the generated response. Pi represents the ith utterance of the patient and Di donates the doctorâ€™s ith
gold response. Due to the space limitation, we only demonstrates the retrieval baseline and four generative models (GPT/GPTEntity/HRED/HRED-Entity) in the first two turns (D1 , D2 ) and the last turn (D6 ). All correct entities in the predicted response
are underlined.
involved other redundant entities and information. For instance, there are four unnecessary entities in the second turn.
For the generation-based dialogue models, after adding the
guidance of auxiliary entities, the predicted sentences are
more informative and contain more correct entities. For instance, in D6 , the original GPT model produced the dull
response â€œSorry, I was a bit busy just nowâ€ without any
meaningful entities, while GPT-Entity generated sentences
with rich entity information, such as â€gastritisâ€. The HREDbased models have the same results. In the second turn, the
origin HRED model generates the wrong entity â€cholecystitisâ€, while HRED-Entity generates the correct entity â€acid
refluxâ€ with the help of predicted entities.

Conclusion and Future Work
In this paper, we proposed MedDG, a large-scale Chinese
medical dialogue consultation dataset with the annotation of
rich medical entities. We implement several state-of-the-art
models for entity prediction and dialogue generation tasks
and design two entity-aware dialogue models. Experimental
results show that response quality can be enhanced with the
help of predicted entities. Since the entity list in MedDG is
obtain from a medical knowledge graph, in the future, we
plan to introduce the domain knowledge into the medical
dialogue to model the relationship between different medical
entities.

References
Chen, B.; and Cherry, C. 2014. A systematic comparison of
smoothing techniques for sentence-level bleu. In Proceedings of the Ninth Workshop on Statistical Machine Translation, 362â€“367.
Cui, Y.; Che, W.; Liu, T.; Qin, B.; Yang, Z.; Wang, S.; and
Hu, G. 2019. Pre-Training with Whole Word Masking for
Chinese BERT. arXiv preprint arXiv:1906.08101 .
Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.
BERT: Pre-training of Deep Bidirectional Transformers for
Language Understanding. In NAACL-HLT.
Dhingra, B.; Li, L.; Li, X.; Gao, J.; Chen, Y.-N.; Ahmed,
F.; and Deng, L. 2017. Towards End-to-End Reinforcement Learning of Dialogue Agents for Information Access.
In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),
484â€“495.
Du, N.; Wang, M.; Tran, L.; Lee, G.; and Shafran, I. 2019.
Learning to Infer Entities, Properties and their Relations
from Clinical Conversations. In EMNLP-IJCNLP, 4978â€“
4989.
Ferguson, G.; Allen, J.; Galescu, L.; Quinn, J.; and Swift,
M. 2009. Cardiac: An intelligent conversational assistant
for chronic heart failure patient heath monitoring. In 2009
AAAI Fall Symposium Series.
Gatius, M.; and Namsrai, T. 2012. A conversational system
to assist the user when accessing web sources in the medical
domain. In The Fifth International Conference on advances
in computer-human interactions, 160â€“164. Citeseer.
Gong, K.; Xu, Z.; Cai, Z.; Chen, Y.; and Wang, Z. 2020.
Internet hospitals help prevent and control the epidemic of
COVID-19 in China: Multicenter user profiling study. Journal of medical Internet research 22(4): e18908.
Hochreiter, S.; and Schmidhuber, J. 1997. Long short-term
memory. Neural computation 9(8): 1735â€“1780.
Kim, Y. 2014. Convolutional Neural Networks for Sentence Classification. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing
(EMNLP), 1746â€“1751. Doha, Qatar: Association for Computational Linguistics. doi:10.3115/v1/D14-1181. URL
https://www.aclweb.org/anthology/D14-1181.
Li, J.; Galley, M.; Brockett, C.; Gao, J.; and Dolan, B. 2016.
A Diversity-Promoting Objective Function for Neural Conversation Models. In Proceedings of NAACL-HLT, 110â€“119.
Li, X.; Chen, Y.-N.; Li, L.; Gao, J.; and Celikyilmaz, A.
2017. End-to-End Task-Completion Neural Dialogue Systems. In Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long
Papers), 733â€“743.
Liao, K.; Liu, Q.; Wei, Z.; Peng, B.; Chen, Q.; Sun, W.; and
Huang, X. 2020. Task-oriented Dialogue System for Automatic Disease Diagnosis via Hierarchical Reinforcement
Learning. CoRR abs/2004.14254. URL https://arxiv.
org/abs/2004.14254.

Lin, X.; He, X.; Chen, Q.; Tou, H.; Wei, Z.; and Chen, T.
2019. Enhancing Dialogue Symptom Diagnosis with Global
Attention and Symptom Graph. In EMNLP-IJCNLP, 5032â€“
5041.
Liu, C.; Sun, H.; Du, N.; Tan, S.; Fei, H.; Fan, W.; Yang,
T.; Wu, H.; Li, Y.; and Zhang, C. 2016. Augmented LSTM
framework to construct medical self-diagnosis android. In
2016 IEEE 16th International Conference on Data Mining
(ICDM), 251â€“260. IEEE.
McCall, B. 2020. Could telemedicine solve the cancer backlog? The Lancet Digital Health .
Peng, B.; Li, X.; Gao, J.; Liu, J.; and Wong, K.-F. 2018.
Deep Dyna-Q: Integrating Planning for Task-Completion
Dialogue Policy Learning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2182â€“2192. Melbourne, Australia: Association for Computational Linguistics. doi:10.
18653/v1/P18-1203. URL https://www.aclweb.org/
anthology/P18-1203.
Radford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; and
Sutskever, I. 2019. Language models are unsupervised multitask learners. OpenAI Blog 1(8): 9.
Serban, I. V.; Sordoni, A.; Bengio, Y.; Courville, A.; and
Pineau, J. 2016. Building end-to-end dialogue systems using
generative hierarchical neural network models. In AAAI.
Shi, X.; Hu, H.; Che, W.; Sun, Z.; Liu, T.; and Huang, J.
2020. Understanding Medical Conversations with Scattered
Keyword Attention and Weak Supervision from Responses.
national conference on artificial intelligence .
Sutskever, I.; Vinyals, O.; and Le, Q. V. 2014. Sequence
to sequence learning with neural networks. In Advances in
neural information processing systems, 3104â€“3112.
Wei, Z.; Liu, Q.; Peng, B.; Tou, H.; Chen, T.; Huang, X.;
Wong, K.-F.; and Dai, X. 2018. Task-oriented Dialogue System for Automatic Diagnosis. In Proceedings of the 56th
Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), volume 2, 201â€“207.
Wong, W.; Thangarajah, J.; and Padgham, L. 2011. Health
conversational system based on contextual matching of
community-driven question-answer pairs. In Proceedings of
the 20th ACM international conference on Information and
knowledge management, 2577â€“2580.
Xu, L.; Zhou, Q.; Gong, K.; Liang, X.; Tang, J.; and Lin, L.
2019. End-to-End Knowledge-Routed Relational Dialogue
System for Automatic Diagnosis. In AAAI.
Zhang, Y.; Jiang, Z.; Zhang, T.; Liu, S.; Cao, J.; Liu, K.;
Liu, S.; and Zhao, J. 2020a. MIE: A Medical Information
Extractor towards Medical Dialogues. In Jurafsky, D.; Chai,
J.; Schluter, N.; and Tetreault, J. R., eds., Proceedings of the
58th Annual Meeting of the Association for Computational
Linguistics, ACL 2020, Online, July 5-10, 2020, 6460â€“6469.
Association for Computational Linguistics. URL https://
www.aclweb.org/anthology/2020.acl-main.576/.
Zhang, Y.; Sun, S.; Galley, M.; Chen, Y.-C.; Brockett, C.;
Gao, X.; Gao, J.; Liu, J.; and Dolan, W. 2020b. DialoGPT:

Large-Scale Generative Pre-training for Conversational Response Generation. ArXiv abs/1911.00536.

