arXiv:2011.01207v3 [physics.comp-ph] 12 Feb 2021

Tinker-HP : Accelerating Molecular Dynamics
Simulations of Large Complex Systems with
Advanced Point Dipole Polarizable Force
Fields using GPUs and Multi-GPUs systems
Olivier Adjoua,‚Ä† Louis Lagard√®re*,‚Ä†,‚Ä° Luc-Henri Jolly,‚Ä° Arnaud Durocher,¬∂
Thibaut Very,¬ß Isabelle Dupays,¬ß Zhi Wang,k Th√©o Jaffrelot Inizan,‚Ä† Fr√©d√©ric
C√©lerse,‚Ä†,‚ä• Pengyu Ren,# Jay W. Ponder,k and Jean-Philip Piquemal‚àó,‚Ä†,#
‚Ä†Sorbonne Universit√©, LCT, UMR 7616 CNRS, F-75005, Paris, France
‚Ä°Sorbonne Universit√©, IP2CT, FR2622 CNRS, F-75005, Paris, France
¬∂Eolen, 37-39 Rue Boissi√®re, 75116 Paris, France
¬ßIDRIS, CNRS, Orsay, France
kDepartment of Chemistry, Washington University in Saint Louis, USA
‚ä•Sorbonne Universit√©, CNRS, IPCM, Paris, France.
#Department of Biomedical Engineering, The University of Texas at Austin, USA
E-mail: louis.lagardere@sorbonne-universite.fr,jean-philip.piquemal@sorbonne-universite.fr

Abstract
We present the extension of the Tinker-HP package (Lagard√®re et al., Chem. Sci.,
2018,9, 956-972) to the use of Graphics Processing Unit (GPU) cards to accelerate
molecular dynamics simulations using polarizable many-body force fields. The new
high-performance module allows for an efficient use of single- and multi-GPUs archi-

1

tectures ranging from research laboratories to modern supercomputer centers. After detailing an analysis of our general scalable strategy that relies on OpenACC and CUDA
, we discuss the various capabilities of the package. Among them, the multi-precision
possibilities of the code are discussed. If an efficient double precision implementation is
provided to preserve the possibility of fast reference computations, we show that a lower
precision arithmetic is preferred providing a similar accuracy for molecular dynamics
while exhibiting superior performances. As Tinker-HP is mainly dedicated to accelerate
simulations using new generation point dipole polarizable force field, we focus our study
on the implementation of the AMOEBA model. Testing various NVIDIA platforms including 2080Ti, 3090, V100 and A100 cards, we provide illustrative benchmarks of the
code for single- and multi-cards simulations on large biosystems encompassing up to
millions of atoms. The new code strongly reduces time to solution and offers the best
performances to date obtained using the AMOEBA polarizable force field. Perspectives
toward the strong-scaling performance of our multi-node massive parallelization strategy, unsupervised adaptive sampling and large scale applicability of the Tinker-HP code
in biophysics are discussed. The present software has been released in phase advance on
GitHub in link with the High Performance Computing community COVID-19 research
efforts and is free for Academics (see https://github.com/TinkerTools/tinker-hp).

Introduction
Molecular dynamics (MD) is a very active research field that is continuously progressing. 1,2
Among various evolutions, the definition of force fields themselves grows more complex. Indeed, beyond the popular pairwise additive models 3‚Äì7 that remain extensively used, polarizable force field (PFF) approaches are becoming increasingly mainstream and start to be more
widely adopted, 8‚Äì11 mainly because accounting for polarizability is often crucial for complex
applications and adding new physics to the model through the use of many-body potentials
can lead to significant accuracy enhancements. 10 Numerous approaches are currently under

2

development but a few methodologies such as the Drude 12‚Äì14 or the AMOEBA 15‚Äì17 models
emerge. These models are more and more employed because of the alleviation of their main
bottleneck: their larger computational cost compared to classical pairwise models. Indeed,
the availability of High Performance Computing (HPC) implementations of such models
within popular packages such as NAMD 18 or GROMACS 19 for Drude or Tinker-HP

20

for

AMOEBA fosters the diffusion of these new generation techniques within the research community. This paper is dedicated to the evolution of the Tinker-HP package. 20 The software,
which is part of the Tinker distribution, 21 was initially introduced as a double precision
massively parallel MPI addition to Tinker dedicated to the acceleration of the various PFFs
and non-polarizable force fields (n‚ÄìPFFs) present within the Tinker package. The code was
shown to be really efficient, being able to scale on up to tens of thousand cores on modern
petascale supercomputers. 20,22 Recently, it has been optimized on various platforms taking
advantage of vectorization and of the evolution of the recent CPUs (Central Processing
Units). 22 However, in the last 15 years, the field has been increasingly using GPUs (Graphic
Processor Unit) 23‚Äì25 taking advantage of low precision arithmetic. Indeed, such platforms offer important computing capabilities at both low cost and high energy efficiency allowing for
reaching routine microsecond simulations on standard GPUs cards with pair potentials. 24,26
Regarding the AMOEBA polarizable force field, the OpenMM package 27 was the first to
propose an AMOEBA-GPU library that was extensively used within Tinker through the
Tinker-OpenMM GPU interface. 28 The present contribution aims to address two goals: i)
the design of an efficient native Tinker-HP GPU implementation; ii) the HPC optimization
in a massively parallel context to address both the use of research laboratories clusters and
modern multi-GPUs pre-exascale supercomputer systems. The paper is organized as follows. First, we will describe our OpenACC port and its efficiency in double precision. After
observing the limitations of this implementation regarding the use of single precision, we
introduce a new CUDA approach and detail the various parts of the code it concerns after
a careful study of the precision. In both cases, we present benchmarks of the new code
3

on illustrative large biosystems of increasing size on various NVIDIA platforms (including
RTX 2080Ti, 3090, Tesla V100 and A100 cards). Then, we explore how to run on even
larger systems and optimize memory management by making use of latest tools such as
NVSHMEM. 29

OpenACC Approach
Global overview and definitions
Tinker-HP is a molecular dynamics application with a MPI layer allowing a significant acceleration on CPUs . The core of the application is based on the resolution of the classical
newton equations 30,31 given an interaction potential (force field) between atoms. In practice,
a molecular dynamic simulation consists into the repetition of the call to an integrator routine defining the changes of the positions and the velocities of all the atoms of the simulated
system between two consecutive timesteps. The same process is repeated as many times as
needed until the simulation duration is reached (see Figure 3). To distribute computations
over the processes, a traditional three dimensional domain decomposition is performed on
the simulation box (‚Ñ¶) which means that it is divided in subdomains (œà), each of which
being associated to a MPI process. Then, within each timestep, positions of the atoms
and forces are exchanged between processes before and after the computation of the forces.
Additionally, small communications are required after the update of the positions to deal
with the fact that an atom can change of subdomain during a timestep. This workflow is
described in detail in reference. 31
In recent years a new paradigm has emerged to facilitate computation and programming
on GPU devices. In the rest of the text, we will denote as kernel the smallest piece of code
made of instructions designed for a unique purpose. Thus, a succession of kernels might
constitute a routine and a program can be seen as a collection of routines designed for a
specific purpose. There are two types of kernels
4

‚Ä¢ Serial kernels, mostly used for variable configuration
‚Ä¢ Loops kernels, operating on multiple data sets
This programming style, named OpenACC , 32,33 is a directive-based language similar to
the multi-threading OpenMP paradigm with an additional complexity level. Since a target
kernel is destined to be executed on GPUs , it becomes crucial to manage data between both
GPU and CPU platforms. At the most elementary level, OpenACC compiler interacts
on a standard host (CPU ) kernel and generates a device (GPU ) kernel using directives
implemented to describe its parallelism along with clauses to manage global data behaviour
at both entry and exit point and/or kernel launch configuration [Figure 1]. This method offers
two majors benefits. Unlike the low-level CUDA programming language, 34 it takes only a
few directives to generate a device kernel. Secondly, the same kernel is compatible with both
platforms - CPUs and GPUs -. The portability along with all the associated benefits such
as host debug is therefore ensured. However, there are some immediate drawbacks mainly
because CPUs and GPUs do not share the same architecture, specifications and features.
Individual CPU cores benefit from a significant optimization for serial tasks, a high clock
frequency and integrate vectorization instructions to increase processing speed. GPUs on the
other hand were developed and optimized from the beginning for parallel tasks with numerous
aggregations of low clock cores holding multiple threads. This means that it may be necessary
to reshape kernels to fit device architecture in order to get appropriate acceleration. Once
we clearly exhibit a kernel parallelism and associate OpenACC directives to offload it on
device, it should perform almost as well as if it had been directly written in native CUDA .
Still, in addition to kernel launch instruction (performed by both OpenACC and CUDA )
before the appropriate execution, there is a global data checking operation overhead which
might slow down execution [Figure 1]. However, it is possible to overlap this operation using
asynchronous device streams in the kernel configuration [Figure 2]. Under proper conditions
and with directly parallel kernels, OpenACC can already lead to an efficient acceleration
close to the one reachable with CUDA .
5

Figure 1: OpenACC synchronous execution model on test kernel <fill>

Figure 2: OpenACC asynchronous execution on both kernels <test> & <test 1>.

6

In the following, we will say that a kernel is semi-parallel if one can find a partition
inside the instructions sequence which does not share any dependency at all. A semi-parallel
kernel is consequently defined parallel if all instructions in the partition do not induce a race
condition within its throughput.
Once a kernel is device compiled, its execution requires a configuration defining the associated resources provided by the device. With OpenACC , these resources are respectively
the total number of threads and the assignment stream. We can access the first one through
the gang and vector clauses attached to a device compute region directive. A gang is a
collection of vectors inside of which every thread can share cache memory. All gangs run
separately on device streaming multi-processors (SM) to process kernel instructions inside a
stream where many other kernels are sequentially queued. OpenACC offers an intermediate
parallelism level between gang and vector called worker . This level can be seen as a gang
subdivision.
It is commonly known that GPUs are inefficient for sequential execution due to their
latency. To cover up latency, each SM comes with a huge register file and cache memory
in order to hold and run as many vectors as possible at the same time. Instructions from
different gangs are therefore pipe-lined and injected in the compute unit. 34,35 From this
emerges the kernel occupancy‚Äôs concept which is defined as the ratio between the gang‚Äôs
number concurrently running on one SM and the maximum gang number that can actually
be held by this SM.

Figure 3: illustration of a MD timestep

7

Global scheme
The parallel computing power of GPUs is in constant evolution and the number of streaming
multiprocessors (SM) is almost doubling with every generation. Considering their impressive
compute potential in comparison to CPUs , one can assume that the only way to entirely
benefit from this power is to offload the entire application on device. Any substantial part of
the workflow of Tinker-HP should not be performed on the CPU platform. It will otherwise
represent a bottleneck to performance in addition to requiring several data transfers. As for
all MD applications, most of the computation lies in the evaluation of the forces. For the
AMOEBA polarizable model, it takes around 97% of a timestep to evaluate those forces when
running sequentially on CPU platform. Of these 97%, around 10% concern bonded forces
and 90% the non bonded ones, namely: polarization, (multipolar) permanent electrostatics
and van der Waals. The polarization which includes the iterative resolution of induced
dipoles largely dominates this part (see [Figure 3]). Non-bonded forces and polarization
in particular will thus be our main focus regarding the porting and optimization. We will
then benefit from the already present Tinker-HP MPI layer 20,22 to operate on several GPUs
. The communications can then be made directly between GPUs by using a CUDA aware
MPI implementation. 36 The Smooth Particle Mesh Ewald method 31,37,38 is at the heart of
both the permanent electrostatics and polarization non-bonded forces used in Tinker-HP ,
first through the iterative solution of the induced dipoles and then through the final force
evaluation. It consists in separating the electrostatic energy in two independent pieces: real
space and reciprocal space contributions. Let us describe our OpenACC strategy regarding
those two terms.
Real space scheme
Because the real space part of the total PME energy and forces has the same structure as
the van der Waals one, the associated OpenACC strategy is the same. Evaluating real space
energy and forces is made through the computation of pairwise interactions. Considering n
8

atoms, a total of n(n‚àí1) pairwise interactions need to be computed. This number is reduced
by half because of the symmetry of the interactions. Besides, because we use a cutoff distance
after which we neglect these interactions, we can reduce their number to being proportional
to n in homogeneous systems by using neighbor lists. The up-bound constant is naturally
reduced to a maximum neighbors for every atoms noted as Neigmax .
The number of interactions is up-bounded by n ‚àó Neigmax . In terms of implementation, we
have written the compute algorithm into a single loop kernel. As all the interactions are
independent, the kernel is semi-parallel regarding each iteration. By making sure that energy
and forces are added one at a time, the kernel becomes parallel. To do that, we can use
atomic operations on GPUs which allow to make this operation in parallel and solve any
race condition issue without substantially impacting parallel performance. By doing so, real
space kernels looks like [Listing 1]
c$acc parallel loop gang default ( present ) async
c$acc &

private ( scaling_data )

do i = 1 , numLocalAtoms
iglob = glob ( i )

! Get Atom i global id

! Get Atom iglob parameter and positions
...
! Gather Atoms iglob scaling interactions in ‚Äô scaling_data ‚Äô
...
c$acc loop vector
do k = 1 , numNeig ( i )
kglob = glob ( list (k , i ) )
! Get Atom kglob parameter and positions
! Compute distance ( d ) between iglob & kglob
if ( d < dcut ) then
call r e s o l v e _ s c a l i n g _ f a c t o r ( scaling_data )
...
call C o m p u t e _ i n t e r a c t i o n ! inlined
...
call Update_ ( energy , forces , virial )
end if
end do

9

end do

Listing 1: OpenACC real space offload scheme. The kernel is offloaded on device using
two of the three parallelism levels offered by OpenACC . The first loop is broken down
over gangs and gathers all data related to atom iglob using gang‚Äôs shared memory through
the private clause. OpenACC vectors are responsible of the evaluation and the addition of
forces and energy after resolving scaling factor if necessary. Regarding data management we
make sure with the present clause that everything is available on device before the execution
of the kernel.
At first, our approach was designed to directly offload the CPU vectorized real space
compute kernels which use small arrays to compute pairwise interactions in hope to align
memory access pattern at the vector level and therefore accelerate the code. 22 This requires
each gang to privatize every temporary array and results in a significant overhead with memory reservation associated to a superior bound on the gang‚Äôs number. Making interactions
computation scalar helps us remove those constraints and double the kernel performance.
The explanation behind this increase arises from the use of GPU scalar registers. Still, one
has to resolve the scaling factors of every interactions. As it happens inside gang shared
memory, the performance is slightly affected. However, we would benefit from a complete
removal of this inside search. There are two potential drawbacks to this approach:
> Scaling interactions between neighboring atoms of the same molecule can become very
complex. This is particularly true with large proteins. Storage workspace can potentially
affect shared memory and also kernel‚Äôs occupancy.
> Depending on the interactions, there is more than one kind of scaling factor. For example,
every AMOEBA polarization interactions need three different scaling factors.
The best approach is then to compute scaling interactions separately in a second kernel.
Because they only involve connected atoms, their number is small compared to the total
number of non-bonded interactions. We first compute unscaled non-bonded interactions and
10

then apply scaling correction in a second part. An additional issue is to make this approach
compatible with the 3d domain decomposition. Our previous kernel then reads: [Listing 2].
c$acc parallel loop gang default ( present ) async
do i = 1 , numLocalAtoms
iglob = glob ( i )

! Get Atom i global id

! Get Atom iglob parameter and positions
...
c$acc loop vector
do k = 1 , numNeig ( i )
kglob = glob ( list (k , i ) )
! Get Atom kglob parameter and positions
! Compute distance ( d ) between iglob & kglob
if ( d < dcut ) then
call C o m p u t e _ i n t e r a c t i o n ! inlined
...
call Update_ ( energy , forces , virial )
end if
end do
end do

call c or re ct_ sc a li n g

Listing 2: final OpenACC real space offload scheme. This kernel is more balanced and
exposes much more computational load over vectors. ‚Äôcorrect_scaling‚Äô routine applies the
correction of the scaling factors. This procedure appears to be much more suitable to device
execution.

Reciprocal space scheme
The calculation of Reciprocal space PME interactions essentially consists in five steps:
1. Interpolating the (multipolar) density of charge at stake on a 3D grid with flexible
b-spline order (still, the implementation is optimized to use an order of 5 as it is the
default and the one typically used with AMOEBA).
2. Switching to Fourier space by using a forward fast Fourier transform (FFt)
11

3. Performing a trivial scalar product in reciprocal space
4. Performing a backward FFt to switch back to real space
5. Performing a final multiplication by b-splines to interpolate the reciprocal forces
Regarding parallelism, Tinker-HP uses a two dimensional decomposition of the associated
3d grid based on successive 1D FFt‚Äôs . Here, we use the cuFFt library. 39 The OpenACC
offload scheme for reciprocal space is described in [Figure 4]

Figure 4: Reciprocal space offload scheme. Charge interpolation and Force interpolation are
both written in a single kernel. They are naturally parallel except for the atomic contributions to the grid in the first one. The approach remains the same for data management
between host and device as for real space: all data are by default device resident to prevent
any sort of useless transfer. Regarding MPI communications, exchanges take place directly
between GPUs through interconnection.
We just reviewed our offload strategy of the non-bonded forces kernels with OpenACC
, but the bonded ones remain to be treated. Also, the MPI layer has to be dealt with.
The way bonded forces are computed is very similar to the real space ones, albeit simpler,
which makes their offloading relatively straightforward. MPI layer kernels require, on the
other hand, a slight rewriting as communications are made after a packing pre-treatment. In
parallel, one does not control the throughput order of this packing operation. This is why it
becomes necessary to also communicate the atom list of each process to their neighbors. Now
that we presented the main offload strategies, we can focus on some global optimizations
regarding the implementation and execution for single and multi-GPUs . Some of them lead
to very different results depending on the device architecture.

Optimizations opportunities
> A first optimization is to impose an optimal bound on the vector size when computing
pair interactions. In a typical setup, for symmetry reasons, the number of neighbors for
12

real space interactions vary between zero and a few hundreds. Because of that second
loop in [Listing 2], the smallest vector length (32) is appropriate to balance computation
among the threads it contains. Another optimization concerns the construction of the
neighbor lists. Let us remind that it consists in storing, for every atom, the neighbors
that are closer than a cut distance (dcut ) plus a buffer dbuff . This buffer is related to the
frequency at which the list has to be updated. To balance computation at the vector
level and at the same time reduce warp discrepancy (as illustrated in [Figure 5]) we have
implemented a reordering kernel: we reorder the neighbor list for each atom so that the
firsts are the ones under dcut distance.

Figure 5: Illustration of compute balance due to the list reordering. Unbalanced computation
in the first image induces an issue called warp discrepancy: a situation where all threads
belonging to the same vector do not follow the same instructions. Minimizing that can
increase kernel performance significantly since we ensure load balancing among each thread
inside the vector.
> A second optimization concerns the iterative resolution of the induced dipoles. Among
the algorithms presented in, 38,40 the first method we offloaded is the preconditioned conjugated gradient (PCG). It involves a (polarization) matrix-vector product at each iteration.
13

Here, the idea is to reduce the computation and favor coalesce memory access by precomputing and storing the elements of (the real space part) of the matrix before the iterations.
As the matrix-vector product is being repeated, we see a performance gain starting from
the second iteration. This improves performance but implies a memory cost that could
be an issue on large systems or on GPUs with small memory capabilities. This overhead
will be reduced at a high level of multi-device parallelism.
> An additional improvement concerns the two dimensional domain decomposition of the
reciprocal space 3D grid involved with FFt. The parallel scheme for FFt used in TinkerHP is the following for a forward transform:
FFT(s) 1d dim(x) + x Transpose y + FFt(s) 1d dim(y) + y Transpose z + FFT(s) 1d
dim(z) .
Each Transposition represents an all-to-all MPI communication which is the major bottleneck preventing most MD Applications using PME to scale across nodes. 22,31,41 Given
the GPUs huge compute power, this communication is even more problematic in that
context. On device, we use the cuFFt 39 library. Using many cuFFt 1d batches is not
as efficient as using less batches in a higher dimension. Indeed, devices are known to
under perform with low saturation kernels. In order to reduce MPI exchanges and increase load on device, we adopted a simple 3d dimensional cuFFt batch when running on
a single device. On multiple GPUs , we use the following scheme based on a 1d domain
decomposition along the z axis :

cuF F t(s) 2d dim(x, y) + y T ranspose z + cuF F t(s) 1d dim(z)

which gives a 25% improvement compared to the initial approach.
> Profiling the application on a single device, we observed that real space computation is
on average 5 times slower than reciprocal space computation. This trend reverses using
14

multiple GPUs because of the communications mentioned above. This motivated the
assignment of these two parts in two different priority streams. Reciprocal space kernels
along with MPI communications are queued inside the higher priority stream, and real
space kernels - devoid of communications - can operate at the same time on the lower
priority stream to recover communications. This is illustrated in [Figure 6].

Figure 6: Representation of cuFFt‚Äôs communication/computation overlap using different
streams for direct and reciprocal space. Real space computation kernels are assigned to
asynchronous stream 18. reciprocal ones goes into high priority asynchronous stream 17.
real space kernel therefore recovers FFt grid exchanges. This profile was retrieved on 2GPUs
.

Simulation Validation and Benchmarks
Here, we use the same bench systems as in reference: 20,22 the solvated DHFR protein, the
solvated COX protein and the STMV virus, all with the AMOEBA force field, respectively
made of 23 558, 171 219 and 1 066 600 atoms. The molecular dynamics simulations were
run in the NVT ensemble at 300K for 5 pico-seconds simulation using a (bonded/nonbonded) RESPA integrator with a 2fs outer timestep (and a 1fs inner timestep) 42 and the
Bussi thermostat. 43 The performance was averaged over the complete runs. For validation
purposes, we compared the potential energy, temperature and pressure of the systems during
15

the first 50 timesteps with values obtained with Tinker-HP v1.2. Furthermore, these results
were compared to Tinker-OpenMM in the same exact setup. 28
We can directly observe the technical superiority of the Quadro architecture compared
to the Geforce one. Double precision (DP ) compute units of the V100 allow to vastly
outperform the Geforce. In addition, by comparing the performance of the Geforce RTX
to the one of the Quadro GV100, we see that Quadro devices are much less sensitive to
warp discrepancy and non-coalesce data accessing pattern. It is almost as if the architecture
of the V100 card overcomes traditional optimizations techniques related to parallel device
implementation. However, we see that our pure OpenACC implementation manages to
deliver more performance than usual device MD application with PFF in DP. The V100
results were obtained on the Jean-Zay HPE SGI 8600 cluster of the IDRIS supercomputer
Center (GENCI-CNRS, Orsay, France) whose converged partitions are respectively made of
261 and 351 nodes. Each one is made of 2 Intel Cascade Lake 6248 processors (20 cores at 2,5
GHz) accelerated with 4 NVIDIA Tesla V100 SXM2 GPUs , interconnected through NVIDIA
NVLink and coming respectively with 32 GB of memory on the first partition and 16 GB on
the second. Here as in all the tests presented in this paper, all the MPI communications
were made with a CUDA aware MPI implementation. 36 This results is very satisfactory as a
single V100 card is at least ten times faster than an entire node of this supercomputer using
only CPUs .
Multi-device benchmark results compared with host-platform execution are presented
in [Figure 7]. In practice, the DHFR protein is too small to scale out. MPI communications overcomes the computations even with optimizations. On the other hand, COX and
STMV systems show good multi-GPUs performances. Adding our latest MPI optimizations
- FFt reshaping and asynchronous compute between direct and reciprocal part of PME allows a substantial gain in performances. We see that on a Jean-Zay node we can only benefit from the maximum communication bandwidth when running on the entire node, hence
the relative inflexion point on the STMV performances on 2 GPUs setup. Indeed, all devices
16

are interconnected inside one node in such a way that they all share interconnection bandwidth. More precisely, running on 2 GPUs reduces bandwidth by four and therefore affects
the scalabity. It is almost certain that results would get better on an interconnected node
made exclusively of 2 GPUs . Those results are more than encouraging considering the fact
that we manage to achieve them with a full OpenACC implementation of Tinker-HP (direct
portage of the reference CPU code) in addition to some adjustments.
Table 1: Single device benchmark : MD production per day (ns/day). All simulations were
run using a RESPA/2 fs setup.
systems
/devices
(ns/day)

CPUs one node

RTX
2080Ti

RTX
2080Ti +
optim

V100

V100 +
optim

TinkerOpenMM
V100

DHFR
COX
STMV

0.754
0.103
0.013

2.364
0.341
n/a

3.903
0.563
n/a

8.900
1.051
0.111

9.260
1.120
0.126

6.300
0.957
0.130

Figure 7: Performance ratio between single node GPU and single node CPU performance.
Reference values can be found in [Table 1].
In summary, our DP implementation is already satisfactory compared to other applications such as Tinker-OpenMM. Our next section concerns the porting of Tinker-HP in a
17

downgraded precision.

CUDA Approach
Even though we already have a robust OpenACC implementation of Tinker-HP in double
precision, the gain in terms of computational speed when switching directly to single precision
(SP) is modest, as shown in [Table 2], which is inconsistent with the GPUs computational
capabilities.
This is more obvious for Geforce architecture devices since those cards do not possess
DP physical compute units and therefore emulate DP Instructions. According to [Table
3], theoretical ratios of 2 and 31 are respectively expected from V100 and RTX-2080 Ti
performances when switching from DP to SP which makes an efficient SP implementation
mandatory.
Table 2: Single precision MD production (ns/day) within the OpenACC implementation
V100
RTX-2080 Ti

DHFR
11.69
11.72

COX
1.72
1.51

STMV
0.15
n/a

Table 3: device hardware specifications
GPUs

Quadro GV100
Tesla V100 SXM2
Geforce RTX-2080 Ti
Geforce RTX-3090

Performances (Tflop/s)
DP
SP
7.40
7.80
0.42
0.556

14.80
15.70
13.45
35.58

memory
bandwidth
(GB/s)
870.0
900.0
616.0
936.2

Compute/Access
DP
SP
(Ops/8B)
(Ops/4B)
68.04
68.04
69.33
69.77
5.45
87.33
4.75
152.01

In practice, instead of doubling the speed on V100 cards, we ended up noticing a 1.25
increase factor on V100 and 3 on RTX on DHFR in SP compared to DP with the same
setup. All tests where done under the assumption that our simulations are valid in this
precision mode. More results are shown in [Table 2]. Furthermore, a deep profile conducted
18

on the kernels representing Tinker-HP ‚Äôs bottleneck - real space non-bonded interactions
- in the current state reveals an insufficient exploitation of the GPU SP compute power.
[Figure 8] suggests that there is still room for improvements in order to take full advantage
of the card‚Äôs computing power and memory bandwidth both in SP and DP . In order to
exploit device SP computational power and get rid of the bottleneck exposed by [Figure 8],
it becomes necessary to reshape our implementation method and consider some technical
aspects beyond OpenACC‚Äôs scope.

Figure 8: Profile of the matrix-vector compute kernel on the DHFR system. The left picure
is obtained with the double precision and the right one with simple precision. In both
modes, results indicate an obvious latency issue coming from memory accessing pattern
which prevents the device to reach its peak performance.

Global overview and definitions
As mentioned in the previous section, GPUs are most efficient with parallel computations
and coalesce memory access patterns. The execution model combines and handles effectively two nested levels of parallelism. The high level concerns multithreading and the low
level the SIMD execution model for vectorization. 22,44 This model stands for single instruction multiple threads (SIMT). 45 When it comes to GPU programming, SIMT also includes
control-flow instructions along with subroutine calls within SIMD level. This provides additional freedom of approach during implementation. To improve the results presented in the
last paragraph [Table 2] and increase peak performance on computation and throughput, it
19

is crucial to expose more computations in real space kernels and to minimize global memory accesses in order to benefit from cache & shared memory accesses as well as registers.
Considering OpenACC paradigm limitations in terms of kernel description as well as the
required low-level features, we decided to rewrite those specific kernels using the standard
approach of low-level device programming in addition to CUDA built-in intrinsics. In a
following section, we will describe our corresponding strategy after a thorough review on
precision.

Precision study and Validation
Definition i . We shall call p the machine precision (in SP or DP ), the smallest floating
point value such that 1 + p > 1. They are respectively 1.2 √ó 10‚àí7 and 2.2 √ó 10‚àí16 in SP
and DP .
Definition ii .Considering a positive floating point variable a, the machine precision a attached to a is
1 +  > 1 ‚áê‚áí a + p ‚àó a > a ‚áê‚áí a = p ‚àó a
Therefore an error made for a floating point operation between a and b can be expressed as
Àú = (a ‚äï b) (1 + p )
a‚äïb

(1)

Àú designates the numerical operation between a and b.
where ‚äï
Property i . Numerical error resulting from sequential reduction operations are linear while
those resulting from parallel reduction are logarithmic. Thus, parallel reductions are entirely
suitable to GPUs implementation as they benefit from both parallelism and accuracy.
Before looking further into the matter of downgrading precision, we have to make sure
that Tinker-HP is able to work in this mode. Although it has been proven in the literature 25,27,46 that many MD applications are able to provide correct results with simple
precision, extensive precision studies with polarizable force fields are lacking.
20

Figure 9: Illustration of the reduction operation on a 16 variables set. Each arithmetic
operation generates an error a that is accumulated during the sequential operation. On the
other hand, parallel reduction uses intermediate variables to significantly reduce the error.
When it comes to standard IEEE floating point arithmetic, regular 32 bits storage offers
no more than 7 significant digits due to the mantissa. In comparison, we benefit from
16 significant digits with DP 64 storage bits. Without any consideration on the floating
number‚Äôs sign, it is safe to assume that any application working with absolute values outside
[10‚àí7 , 107 ] scope will fail to deliver sufficient accuracy when operating in complete SP mode.
This is called floating point overflow. To overcome this, the common solution is to use a
mixed precision mode (MP) which encompasses both standard SP and a superior precision
container to store variables subject to SP overflowing. In practice, most MD applications
adopt SP for computation and a higher precision for accumulation. Moreover, applications
like Amber or OpenMM propose another accumulation method which rely on a different
type of variable. 46
The description made in the previous section shows that energy and the virial evaluation
are linear-dependent with the system‚Äôs size. Depending on the complexity of the interaction
in addition to the number of operations it requires, we can associate a constant error value
i to it. Thus, we can bound the error made on the computation of a potential energy with
Nint i < nNeigmax i where Nint represents the number of interactions contributing to this
21

energy and Neigmax the maximum number of neighbor it involves per atom. As it is linear
with respect to the system size we have to evaluate this entity with a DP storage container.
Furthermore, to reduce even more the accumulation of error due to large summation, we
shall employ buffered parallel reduction instead of sequential one [Figure 9]. On the other
hand, we have to deal with the forces which remain the principal quantities which drive
a MD simulation. The error made for each atom on the non bonded forces is bound by
Neigmax ‚àó i depending on the cutoff. However, each potential comes with a different i . In
practice, the corresponding highest values are the one of both van der Waals and bonded
potentials. The large number of pairwise interactions induced by the larger van der Waals
cutoff in addition to the functional form which includes power of 14 (for AMOEBA) causes
SP overflowing for distances greater than 3√Ö. By reshaping the variable encompassing the
pairwise distance, we get a result much closer to DP since intermediate calculations do not
overflow. Regarding the bonded potentials, bond
depends more on the conformation of the
i
system.
Parameters involved in bond pairwise evaluation (spring stiffness,...) causes a SP numer) standing between 1 √ó 10‚àí3 and 1 √ó 10‚àí2 which frequently reach 1 √ó 10‚àí1
ical error (bond
i
(following (1)) during summation process and this affects forces more than total energy. In
we evaluate the distances in DP before casting the result to SP . In
order to minimize bond
i
the end, bond
is reduced on the scope of [1 √ó 10‚àí4 ,1 √ó 10‚àí3 ] which represents the smallest
i
error we can expect from SP .
Furthermore, unlike the energy, a sequential reduction using atomic operations is applied
to the forces. The resulting numerical error is therefore linear with the total number of
summation operations. This is why we adopt a 64 bits container for those variables despite
the fact they can be held in a 32 bits container.
Regarding the type of the 64 bits container, we analyze two different choices. First we
have the immediate choice of a floating point. The classical mixed precision uses FP64
for accumulation and integration. Every MD applications running on GPU integrates this
22

mode. It presents the advantage of being straightforward to implement. Second, we can use
an integer container for accumulation: this is the concept of fixed point arithmetic introduced
by Yates. 47 To be able to hold a floating point inside an integer requires to define a certain
number of bits to hold the decimal part. It is called the fixed point fractional bits. The
left bits are dedicated to the integer part. Unlike the floating point, freezing the decimal
position constrains the approximation precision but offers a correct accuracy in addition to
deterministic operations. Considering a floating point value x and an integer one a and a
fractional bits value (fB), the relations establishing the transition back and forth between
them - as a = f (x) and x = f ‚àí1 (a) - is defined as follow :
a = int round x √ó 2fB
x=



real(a)
2fB

(2)
(3)

with int and real the converting functions and round the truncation function which extracts
the integer part. When it comes to MD, fixed point arithmetic are an excellent tool: each SP
pairwise contribution is small enough to be efficiently captured by 64 bits fixed point. For
instance, it takes only 27 bits to capture 8 digits after the decimal point with large place left
for the integer part. For typical values observed with different systems size, we are far from
the limit imposed by the integer part of the container. Inspired by the work of Walker, G√∂tz
et al., 46 we have implemented this feature inside Tinker-HP with the following configuration:
a 34 fractional bits has been selected for forces accumulation, which leaves 30 bits for the
integer part, thus setting the absolute limit value to 229 Kcal/mol¬∑√Ö. For the energy, we
only allocated 30 fractional bits given the fact that it grows linearly with the system size.
Besides, using integer container for accumulation avoids dealing with DP instructions which
significantly affects performance on Geforce cards unlike Tesla ones. In summary, we should
expect at least a performance or precision improvement from FP .
A practical verification is shown in [Figure 10]. In all cases, both MP and FP behave

23

similarly. Forces being the driving components of MD, the trajectories generated by our
mixed precision implementation are accurate. However, as one can see, if errors remain very
low for forces even for large systems, a larger error exists for energies, a phenomenon observed
in all previous MD GPUs implementations. Some specific post-treatment computations, like
in a BAR free energy computation or NPT Simulations with a Monte-Carlo barostat, require
accurate energies. In such a situation, one could use the DP capabilities of the code for
this post-processing step as Tinker-HP remains exceptionally efficient in DP even for large
systems. A further validation simulation in the NVE ensemble can be found in [Figure 15]
confirming the overall excellent stability of the code.

Neighbor list
We want to expose the maximum of computation inside a kernel using the device shared
memory. To do so, we consider the approach where a specific group of atoms interacts with
another one in a block-matrix pattern (see [Figure 11]). We need to load the parameters of
the group of atoms and the output structures needed for computation directly inside cache
memory and/or registers. On top of that, CUDA built-in intrinsics can be used to read
data from neighbor threads and if possible compute cross term interactions. Ideally, we can
expose Bcomp = B2size computations without a single access to global memory, with Bsize
representing the number of atoms within the group. With this approach, the kernel should
reach its peak in terms of computational load.
A new approach of the neighbor list algorithm is necessary to follow the logic presented
above. This method will be close to standard blocking techniques used in many MD applications. 25,27 Let us present the structure of the algorithm in a sequential and parallel - MPI
- context.

24

Figure 10: Absolute error between DP implementation and both FP and MP implementations on total potential energy and Forces. Forces root mean square deviation between
DP and MP for systems from 648 up to 2 592 000 atoms. As expected, both absolute errors
in SP and FP are almost identical on the energy and they grow linearly with the system
size. Logarithmic regression gives 0.99 value for the curve slope and up to 5 kcal/mol for
the largest system. However, the relative error for all systems is located under 7 √ó 10‚àí7 in
comparison to DP . One can also see that the error on the forces is independent of the system
size the theory according to which, numerical errors generated from the computation do not
depends of the system size.

25

Figure 11: Representation of interactions between two groups of atoms within Tinker-HP.
Bsize = 8 for the illustration
Box partitioning
Lets us recall that given a simulation box ‚Ñ¶, a set of œâ c with c ‚àà [0..Nc] forms a ‚Ñ¶ partition
if and only if

Ô£±
Ô£¥
Ô£≤ œâ 1 ‚à™ . . . ‚à™ œâ Nc = ‚Ñ¶
Ô£¥
Ô£≥ œâ 1 ‚à© . . . ‚à© œâ Nc = ‚àÖ

We consider in the following that each group deals with interactions involving atoms
within a region of space. In order to maximize Bcomp between every pair of groups, we must
then ensure their spatial compactness. Moreover, all these regions need to define a partition
of ‚Ñ¶ to make sure we do not end up with duplicate interactions. Following this reasoning,
we might be tempted to group them into small spheres but it is impossible to partition a
polygon with only spheres; not to mention the difficulties arising from the implementation
point of view.
The MPI layer of Tinker-HP induces a first partition of ‚Ñ¶ in P subdomains œà p , p ‚àà [0..P ]
where P is the number of MPI processes. Tinker-HP uses the midpoint image convention 48
so that the interactions computed by the process assigned to œà p are the ones whose midpoint
falls into œà p . The approach used in Tinker-HP for the non bonded neighbor list uses a cubic
partition œâ c , c ‚àà [1..Nc] of œà p and then collects the neighboring atoms among the neighboring
26

cells of œâ c . Here, we proceed exactly in the same way with two additional conditions to the
partitioning. First, the number of atoms inside each cell œâ c must be less or equal than Bsize .
Second, we must preserve a common global numbering of the cells across all domains œà p to
benefit from a unique partitioning of ‚Ñ¶.
Once the first partitioning in cells is done, an additional sorting operation is initiated to
define groups so that each of them contains exactly Bsize spatially aligned atoms following
the cell numbering (note that because of the first constrain mention earlier, one cell can
contain atoms belonging to a maximum of two groups). More precisely, the numbering of
the cells follows a one dimensional representation of the three dimension of the simulation
box.
Now, we want to find the best partitioning of œà p in groups that will ensure enough
proximity between atoms inside a group, minimizing the number of neighboring groups and
consequently maximizing Bcomp .
When the partitioning generates too flat domains, each group might end up having too
many neighboring groups. The optimal cell shape (close to a sphere) is the cube but we
must not forget the first constrain and end up with a very thin partition either. However,
atom groups are not affected by a partition along the inner most contiguous dimension in
the cell numbering. We can exploit this to get better partitioning. [Figure 12] illustrates
and explains the scheme on a two dimensional box. Partitioning is done in an iterative
manner by cycling on every dimension. We progressively increase the number of cells along
each dimension starting on the contiguous one until the first condition is fulfilled. During a
parallel run, we keep track of the cell with the smallest number of atoms with a reduction
operation. This allows to have a global partitioning of ‚Ñ¶ and not just œà p .
Now that we do dispose of a spatial rearrangement of the atoms into groups, we need to
construct pair-lists of all interacting groups according to the cutoff distance plus an additional
buffer to avoid reconstructing it at each timestep.
Groups are built in such a way that it is straightforward to jump from groups indexing to
27

Figure 12: Illustration of a two dimensional partition along with groups for a box of water.
The left Figure shows a 64 cells partition of ‚Ñ¶ while the right one refines this partitioning
into 88 cells. The groups are defined by re-indexing the atoms following the cells numbering
and their maximum size. Here Bsize = 16. A unique color is associated to every atom
belonging to the same group. No cell contains more than Bsize atoms or 2 groups. Once a
group is selected -23-, searching for its neighboring groups is made through the set of cells
(with respect to the periodic boundary conditions) near to the cells it contains - 43 & 44
-. Once this set is acquired, all the group indexes greater or equal than the selected group
constitute the actual list of neighbors to take the symmetry of the interactions into account.
We see that the group‚Äôs shape modulates the group neighborhood as illustrated with the
right illustration and a spatially flat group 23.

28

cells indexing. We chose to use an adjacency matrix which is GPUs suitable and compatible
with MPI parallelism.
Once it is built, the adjacency matrix directly gives the pair-list. Regarding the storage
size involved with this approach, note that we only require single bit to tag pair-group
n
l
e2 bits occupation which equals to d Bsize
e2 81 bytes. nl
interactions. This results in an d Bnsize

represents the number of atoms which participates to real space evaluation on a process
domain (œà p ). Of course, in terms of memory we cannot afford a quadratic reservation.
1
However the scaling factor d Bsize
e2 18 is small enough even for the smallest value of Bsize set

to 32 corresponding to device warp size. Not to mention that, in the context of multi-device
simulation, the memory distribution is also quadratic. The pseudo-kernel is presented in
[Listing 3]
c$acc parallel loop default ( present )
do i = 1 , numCells
celli = i
! get blocks_i inside celli
...
c$acc loop vector
do j = 1 , numCellsNeigh
! Get cellj with number
...
! Get blocks_j inside cellk
...
! Apply symmetrical condition
if ( cellj > celli ) cycle
c$acc loop seq
do bi in blocks_i
do bj in blocks_j
c$acc atomic
set matrix ( bj , bi ) to 1
end do
end do
end do

29

end do

Listing 3: Adjacency matrix construction pseudo-kernel. We browse through all the cells and
for each one we loop on their neighbors. It is easy to compute their ids since we know their
length as well as their arrangement. Given the fact that all cells form a partition of the box,
we can apply the symmetrical condition on pair-cells and retrieve the groups inside thanks
to the partitioning condition, which ensures that each cell contains at most two groups.
Once the adjacency matrix is built, a simple post-processing gives us the adjacency list
with optimal memory size and we can use the new list on real space computation kernels
following the process described in the introduction of this subsection and illustrated in [Figure
11]. In addition, we benefit from a coalesced memory access pattern while loading blocks
data and parameters when they are spatially reordered.
List filtering
It is possible to improve the performance of the group-group pairing with a similar approach
to the list reordering method mentioned in the OpenACC optimizations section above. By
filtering every neighboring group, we can get a list of atoms which really belong to a group‚Äôs
neighborhood. The process is achieved by following the rule:

Œ± ‚àà BI

if ‚àÉŒ±i ‚àà Œ≤I

such that dist(Œ±i , Œ±) ‚â§ dcut + dbuff .

Œ± and Œ±i are atoms, Œ≤ represents a group of Bsize atoms, B is the neighborhood of a group
and dist : (R3 √ó R3 ) ‚Üí R is the euclidean distance.
An illustration of the results using the filtering process is depicted in [Figure 13].
When the number of neighbor atoms is not a multiple of Bsize , we create phantom atoms
to complete the actual neighbor lists. A drawback of the filtering process is a loss of coalesced memory access pattern. As it has been entirely constructed in parallel, we don‚Äôt
have control of the output order. Nonetheless, this is compensated by an increase of Bcomp
30

Figure 13: Starting from the situation illustrated by Figure 12, we represent the geometry
resulting from the filtering process. We significantly reduce group 23 neighborhood with the
list filtering- it decreases from 144 atoms with the first list to 77 with the filtered one -.
Bcomp increases which corresponds to more interactions computed within each group pair.

31

for each interaction between groups, as represented by [Figure 13]. In practice, we measure a 75% performance gain between the original list and the filtered one for the van der
Waals interaction kernel. Moreover, [Figure 14] (deep profile of the previous bottleneck kernel: matrix-vector product ) shows a much better utilisation of the device computational
capability. We apply the same strategy for the other real space kernels (electrostatics and
polarization).

Figure 14: Real space kernel profiling results in mixed precision using our new group-Atoms
list.

PME separation
As mentioned above, the Particle Mesh Ewald method separates electrostatics computation
in two - real and reciprocal space -. A new profiling of Tinker-HP in single-device mixed
precision mode with the latest developments shows that the reciprocal part is the new bottleneck. More precisely, real space performs 20% faster than reciprocal space within a standard
PME setup. Moreover, reciprocal space is even more a bottleneck in parallel because of the
additional MPI communications induced by the cuFFt Transformations. This significantly
narrows our chances of benefiting from the optimizations mentioned in the previous optimization subsection. However, as both parts are independent, we can distribute them on
different MPI processes in order to reduce or even suppress communications inside FFt‚Äôs .
During this operation, a subset of GPUs are assigned to reciprocal space computation only.
32

Depending on the system size and the load balancing between real and reciprocal spaces, we
can break through the scalability limit and gain additional performance on a multi-device
configuration.

Mixed precision validation
To validate the precision study made above, we compare a 1 nanosecond long simulation in
DP on CPU (Tinker-HP 1.2) in a constant energy setup (NVE) with the exact same run
using both GPU MP & FP implementations.
We used the solvated DHFR protein and the standard velocity verlet integrator with a
0.5fs timestep, 12√Ö and 7√Ö cutoff distances respectively for van der Waals and real space
electrostatics and a convergence criteria of 1 √ó 10‚àí6 for the polarization solver. A grid of
64 √ó 64 √ó 64 was used for reciprocal space with fifth order splines. We also compare our
results with a trajectory obtained with Tinker-OpenMM in MP in the exact same setup, see
[Figure 15].
The energy is remarkably conserved along the trajectories obtained with Tinker-HP in
all cases: using DP, MP or FP with less oscillations than with Tinker-OpenMM with MP .

Available Features
The main features of Tinker-HP have been offloaded to GPU such as its various integrators
like the multi-timestep integrators: RESPA1 and BAOAB-RESPA1 49 which allow up to a
10 fs timestep with PFF (this required to create new neighbor lists to perform short-range
non-bonded interactions computations for both van der Waals and electrostatics). Aside
from Langevin integrators, we ported the Bussi 43 (which is the default) and the Berendsen
thermostats, as well as the Monte-Carlo and the Berendsen barostats. We also ported
free energy methods such as the Steered Molecular Dynamics 50 and van der Waals soft
cores for alchemical transformations, as well as the enhanced sampling method Gaussian
Accelerated Molecular Dynamics. 51 Even if it is not the main goal of our implementation as
33

Figure 15: Variation of the total energy during a NVE molecular dynamics simulation of the DHFR protein in DP and MP and FP . Energy fluctuations are respectively
within 1.45 Kcal/mol 1.82 Kcal/mol 1.75 Kcal/mol 1.69 Kcal/mol 3.45 Kcal/mol from TinkerHP DP SP FP and Tinker-OpenMM DP MP .
well optimized software suited to such simulations exist, we also ported the routines necessary
to use standard non-polarizable force fields such as CHARMM, 4 Amber 5 or OPLS. 52 Still,
we obtained already satisfactory performances with these models despite a simple portage,
the associated numbers can be found in supporting information and further optimization
is ongoing. On top of all these features that concern a molecular dynamics simulation, we
ported the "analyze" and "minimize" program of Tinker-HP, allowing to run single point
calculations as well as geometry optimizations. All these capabilities are summed up in
[Table 4]

Performance and Scalability Results
We ran benchmarks with various systems on a set of different GPUs in addition to Tesla
V100 nodes of the Jean-Zay supercomputer. We also ran the whole set of tests on the Ir√®ne
Joliot Curie ATOS Sequana supercomputer V100 partition to ensure for the portability of

34

Table 4: Available features in the initial Tinker-HP GPU release
Programs
Integrator
Force fields
Miscellaneous
Thermostat
Barostat

dynamic; analyze; minimize
VERLET(default); RESPA; RESPA1; BAOAB-RESPA;
BAOAB-RESPA1
AMOEBA; CHARMM/AMBER/OPLS
Steered MD (SMD); Gaussian Accelerated MD; Restrains Groups; Soft
Cores; Plumed
Bussi(default); Berendsen
Berendsen(default); Monte Carlo

the code. We used two different integrators: ( 2 fs RESPA along with 10 fs BAOAB-RESPA1
with heavy hydrogens). For each system, we performed 2.5 ps and 25 ps MD simulations with
RESPA and BAOAB-RESPA1 respectively and average the performance on the complete
runs. Van der Waals and real space electrostatics cutoffs were respectively set to 9 and
7√Ö plus 0.7√Ö neighbor list buffer for RESPA, 1√Ö for BAOAB-RESPA1. We use the Bussi
thermostat with the RESPA integrator. Induced dipoles were converged up to a 1 √ó 10‚àí5
convergence threshold with the conjugate gradient solver and a diagonal preconditioner. 38
The test cases are: water boxes within the range of 96 000 atoms (i.e. Puddle ) up to 2 592 000
atoms (i.e Bay ), the DHFR , COX and the Main Protease of Sars-Cov2 proteins (Mpro ) 53 as
well as the STMV virus. Table 5 gathers all single devices performances and [Figure 16]
illustrates the multi-device performance.
On a single GPU, the BAOAB-RESPA1 integrator performs almost twice as fast as
RESPA in all cases - 22.53 to 42.83 ns/day on DHFR , 0.57 to 1.11 ns/day for the STMV virus.
Regarding the RESPA integrator, results compared with those obtained in DP [Table 1] are
now consistent with the Quadro V100 theoretical performance [Table 1]. Moreover, we
observe a significant improvement on single V100 cards with DP in comparison to the OpenACC implementation which shows that the algorithm is better suited to the architecture.
However, this new algorithm considerably underperforms on Geforce architecture. For instance, for the cox system the speed goes from 0.65 ns/j with the OpenACC implementation
to 0.19 ns/j with the adapted CUDA implementation on Geforce RTX-2080 Ti. This is ob-

35

viously related to architecture constrains (lack of DP Compute units, sensitivity to SIMD
divergence branch, instruction latency) and shows that there is still room for optimization.
Tinker-HP is tuned to select the quickest algorithm depending on the target device. Concerning MP performance on Geforce Cards, we finally get the expected ratio compared with
DP : increasing computation per access improves the use of the device [Table 3]. Geforce
RTX-2080 Ti and GV100 results are close until the COX test case which is consistent with
their computing power, but GV100 performs better for larger systems. It is certainly due
to the difference in memory bandwidth which allows GV100 to perform better on memory
bound kernels and to reach peak performance more easily. For example, most of PME reciprocal space kernels are memory bounded due to numerous accesses to the three dimensional
grid during the building and extracting process.
A further comparison between architectures is given in supporting information.
For FP simulations, as expected, we don‚Äôt see any performance difference with MP on
V100 cards unlike Geforce ones which exhibit an 8% acceleration in average as the DP
accumulation is being replaced by an integer one ( an instruction natively handled by compute cores ). [Table 6] shows the performance of Tinker-OpenMM: with the same RESPA
framework, Tinker-HP performs 12 to 30% better on GV100 when the system size grows.
With Geforce RTX-2080 Ti the difference is slightly more steady except for the Lake test
case: around 18% and 25% better performance with Tinker-HP respectively with MP and
FP compared to Tinker-OpenMM.
The parallel scalability starts to be effective above 100 000 atoms. This is partly because of the mandatory host synchronisations needed by MPI and because of the difference
in performance between synchronous and asynchronous computation under that scale (for
example DHFR production drops to 12 ns/day when running synchronous with the host).
Kernel launching times are almost equivalent to their execution time and they do not overlap. Each GPU on the Jean Zay Supercomputer comes with a 300 GB/s interconnection
NVlink bandwidth. 4GPUs per node, all of them being interconnected, represents then a
36

100 Gb/s interconnection for each GPU pairs. The third generation PCI-Express bridge to
the host memory only delivers 16 Gb/s. With the RESPA integrator operating on a full
node made of 4 Tesla V100, the speed ratio grows from 1.14 to 1.95 respectively from Puddle to Bay test cases in comparison to a single device execution. The relatively balanced load
between pme real and reciprocal space allows to break through the scalability limit on almost every run with 2 GPUs with PME separation enabled. Performance is always worse
on 4GPUs with 1GPU dedicated to the reciprocal space and the others to the direct space
for the same reason mentioned earlier (direct/reciprocal space load balancing). We also
diminished the communication overhead by overlapping communication and computation.
Note that on a complete node of Jean-Zay with 4 GPUs , the bandwidth is statically shared
between all of them, which means that the performance showed here on two GPUs is less
that what can be expected on a node that would only consist in two GPUs interconnected
through NVlink. With the BAOAB-RESPA1 integrator, ratios between a full node and a
single device vary from 1.07 to a maximum of 1.58. Because of the additional short range
real space interactions, it is unsuited for pme separation, yet the reduced amount of FFt
offers a potential for scalability higher than RESPA. Such a delay in the strong scalability is
understandable given the device computational speed, the size of the messages size imposed
by the parallel distribution and the configuration run. The overhead of the MPI layer for
STMV with BAOAB-RESPA1 and 4GPUs bench is on average 41% of a timestep. It consists
mostly in FFt grid exchange in addition to the communication of dipoles in the polarization
solver. This is an indication of the theoretical gain we can obtain with an improvement of
the interconnect technology or the MPI layer. Ideally, we can expect to produce 2.63 ns/day
on a single node instead of 1.55 ns/day. It is already satisfactory to be able to scale on such
huge systems and further efforts will be made to improve multi-GPUs results in the future.
[ht]

37

38

Figure 16: Single node mixed precision scalability on the Jean Zay Cluster (V100) using the
AMOEBA polarizable force field
Table 5: Tinker-HP performances in (ns/day) on different devices and precision modes
Systems

DHFR

Mpro

RESPA 2fs

11.24
22.03

2.91
6.09

21.75
40.73

5.98
12.80

21.46
40.65

5.82
12.65

22.52
43.81

5.35
11.85

24.95
47.31

5.73
12.78

29.14
52.80

7.79
15.79

32.00
57.67

8.37
17.20

BAOAB-RESPA1 10fs

RESPA 2fs
BAOAB-RESPA1 10fs

RESPA 2fs
BAOAB-RESPA1 10fs

RESPA 2fs
BAOAB-RESPA1 10fs

RESPA 2fs
BAOAB-RESPA1 10fs

RESPA 2fs
BAOAB-RESPA1 10fs

RESPA 2fs
BAOAB-RESPA1 10fs

COX
Pond
Lake
DP Quadro GV100
1.76
1.08
0.36
3.61
2.25
0.76
MP
3.69
2.20
0.70
3.61
4.58
1.49
FP
3.57
2.12
0.67
7.77
4.52
1.47
MP Geforce RTX-2080 Ti
3.21
1.82
0.54
7.06
4.06
1.24
FP
3.45
1.95
0.57
7.63
4.35
1.32
MP Geforce RTX-3090
4.76
2.81
0.91
9.61
5.52
1.81
FP
5.10
3.02
0.96
10.46
5.96
1.90

STMV

Bay

0.24
0.53

0.11
0.24

0.44
1.01

0.20
0.46

0.43
1.00

0.20
0.45

0.33
0.82

0.15
n/a

0.35
0.87

0.16
n/a

0.60
1.23

0.28
0.59

0.64
1.32

0.30
0.63

Table 6: Tinker-OpenMM Mixed precision performances assessed with RESPA framework
Systems
Quadro GV100
Geforce
RTX-2080 Ti

DHFR
17.53

Mpro
4.50

COX
2.56

Pond
1.68

Lake
0.56

STMV
0.34

Bay
n/a

18.97

4.37

2.63

1.66

0.55

0.28

n/a

Towards larger systems
As one of the goals of the development of Tinker-HP is to be able to treat (very) large
biological systems such as protein complexes or entire viruses encompassing up to several
39

millions of atoms (as it is already the case with the CPUs implementation 20,22 by using
thousands of CPUs cores), we
review in the following section the scalability limit of the GPU implementation in terms
of system size knowing that GPUs don‚Äôt have the same memory capabilities
: where classical CPU nodes routinely benefit from more than 128 GB of memory, the
most advanced Ampere GPU architecture holds up to 40 GB of memory.

Tinker-HP Memory management model
MD with 3D spatial decomposition has its own pattern when it comes to memory distribution
among MPI processes. We use the midpoint rule to compute real space interactions as it is
done in the CPU implementation.
In practice, it means that each process holds information about its neighbors (to be able
to compute the proper forces). More precisely, a domain œà q belongs to the neighborhood of
œà p if the minimum distance between them is under some cutoff distance plus a buffer. To
simplify data exchange between processes, we transfer all positions in a single message, the
same thing is done with the forces.
An additional filtering is then performed to list the atoms actually involved in the interactions computed by a domain œà p . An atom - Œ± ‚àà ‚Ñ¶ - belong to domain œà p ‚Äôs interaction
area (Œªp ) if the distance between this atom and the domain is below

dcut +dbuff
.
2

Lets us call np the number of atoms belonging to œà p , nb the number of atoms belonging to
a process domain and its neighbors and nl the number of atoms inside Œªp . This is illustrated
in [Figure 17].
One can see that all data reserved with a size proportional to np are equally distributed
among processes. Those with size proportional to nb are only partially distributed. This
means that these data structures are not distributed if all domains œà p are neighbors. This is
why in practice the distribution only takes place at that level with a relatively high number
of process - more than 26 at least on a large box with 3d domain decomposition -. On the
40

Figure 17: Two dimensional spatial decomposition of a simulation box with MPI distribution
across 16 processes. œâ6 collects all the neighboring domains of œà 6 . Here nb < n
other hand, data allocated with a size proportional to nl (like the neighbor list) are always
more distributed when the number of processes increases.
On top of that, some data remain undistributed (proportional to n) like the atomic
parameters of each potential energy term. Splitting those among MPI processes would
severely increase the communication cost which we can not afford. As we cannot predict
how one atom will interact and move inside ‚Ñ¶, the best strategy regarding such data is
to make it available to each process. Reference Tinker-HP reduces the associated memory
footprint by using MPI shared memory space: only one parameter data instance is shared
among all processes within the same node.
No physical shared memory exist between GPUs of a node and
the only way to deal with undistributed data is by replicating them on each device which
is quickly impractical for large systems.
In the next section, we detail a strategy allowing to circumvent this limitation.

41

NVSHMEM feature implementation
As explained above, distribution of parameter data would necessarily result in additional
communications. Regarding data exchange optimizations between GPU devices, NVIDIA
develops a new library based on the OpenSHMEM 54 programming pattern which is called
NVSHMEM. 29 This library provides thread communication routines which operate on a
symmetric memory on each device meaning that it is possible to initiate device communication inside kernels and not outside with an API like MPI. The immediate benefit of such
approach resides in the fact that communications are automatically recovered by kernels instructions and can thereby participate to recover device internal latency. This library allows
to distribute n scale data over devices within one node.
Our implementation follows this scheme: divide a data structure (an array for instance)
across devices belonging to the same node following the global numbering of the atoms and
access this data inside a kernel with the help of NVSHMEM library. To do that, we rely
on a NVHSMEM feature which consists in storing a symmetric memory allocation address
in a pointer on which arithmetic operations can be done. Then, depending on the address
returned by the pointer, either a global memory access (GBA) or a remote memory access
(RMA) is instructed to fetch the data. The implementation requires a Fortran interface to
be operational since NVSHMEM source code is written in the C language. Moreover, an
additional operation is required for every allocation performed by the NVSHMEM specific
allocator to make the data allocated accessible through OpenACC kernels.
Such a singular approach affects performances since additional communications have to
be made inside kernels. Furthermore, all communications do not follow a special pattern
that would leave room for optimizations, meaning that each device accesses data randomly
from the others depending on the atoms involved in the interactions it needs to compute. In
order to limit performance loss, we can decide which data is going to be split across devices
and which kernels are going to be involved with this approach. In practice, we use this
scheme for the parameters of the bonded potential.
42

Figure 18: NVSHMEM memory distribution pattern across a four interconnected devices
node. A symmetric reserved space is allocated by the NVSHMEM library at initialization.
Thus, data is equally split across all devices in order. Every time a device needs to access
data allocated with NVSHMEM, either a GMA or RMA is issued.
Doing so, we distribute most of the parameter data - (torsions, angles, bonds,...) - and
therefore reduce the duplicated memory footprint.

Perspectives and additional results
During our NVSHMEM implementation, we were able to detect and optimize several memory
wells. For instance, the adjacency matrix described in section 1. has a quadratic memory
requirement following the groups of atoms. This means that this represents a potential risk
of memory saturation on a single device. To prevent this, we implemented a buffer limit on
this matrix to construct the pair-group list piece by piece. We also implemented algorithms
that prioritize computing and searching over storing where ever needed - essentially scaling
factor reconstruction -. In the end, Tinker-HP is able to reach a performance of 0.15 ns/day
for a 7 776 000 atoms water box with the AMOEBA force field and the BAOAB-RESPA1
integrator on a single V100 and scale-out to 0.25 ns/day on a complete node of Jean Zay on
the same system.
We also had the opportunity to test our implementation on the latest generation NVIDIA
GPU Ampere architecture: the Selene supercomputer which is made of nodes consisting in
43

DGX-A100 servers. A DGX-A100 server contains eight A100 graphic cards with 40 GB of
memory each and with latest generation inter-connection NVIDIA Switches. The results we
obtained on such a node with the same systems as above in the same RESPA and BAOABRESPA1 framework are listed in [Table 8] and [Figure 19].
We observe an average of 50% of performance gain for systems larger than 100 000 atoms
on a single A100 compared to a single V100 card. Also, the more efficient interconnection
between cards (NV-switch compared to NV-link) allows to scale better on several GPUs
with the best performances ever obtained with our code on all the benchmark systems, the
larger ones making use of all the 8 cards of the node. Although the code is designed to
do so, the latency and the speed of the inter-node interconnection on the present Jean-Zay
and Selene supercomputers did not allow us to scale efficiently across nodes, even on the
largest systems. Jean Zay provides 32 GB/s of network interconnection between nodes so
that each GPU pair has access to a 16 GB/s bandwidth. Unlike the 100 GB/s shared between
each GPU inside a node, we expect inter-node transit times to be 6.25 to 12.5 time slower
without taking the latency into account. This is illustrated by the experiment summarized
in [Table 7] as we observe the sudden increase of the overhead of the MPI layer relative to
the total duration of a timestep when running on two nodes. In this case, changing the
domain decomposition dimension to limit the number of neighbouring process quadruples
the production and exposes the latency issue (expressed here by the difference between the
fastest and the slowest MPI process). In a multi-node context the bottleneck clearly lies
in the inter-node communications. The very fast evolution of the compilers, as well as the
incoming availability of new classes of large pre-exascale supercomputers may improve this
situation in the future. Presently, the use of multiple nodes for a single trajectory is the
subject of active work within our group and results will be shared in due course. Still, one
can already make use of several nodes with the present implementation by using methods
such as unsupervised adaptive sampling as we recently proposed. 53 Such pleasingly parallel
approach already offers the possibility to use hundreds (if not thousands!) of GPU cards
44

simultaneously.
Table 7: multi-node performance on Jean Zay with Sea system and BAOAB-RESPA1. Here
the latency designates the time difference between the fastest and the slowest process.
# GPU
Domain Decomposition
Production speed (ns/day)
MPI Layer (%)
MPI Latency (%)

4

3d
0.252
24
1

1d
0.265
22
1

3d
0.02
97
4

8

1d
0.08
91
11

Table 8: Performance synthesis and scalability results on the Jean Zay (V100) and Selene
(A100) machines. MD production in ns/day with the AMOEBA polarizable force field
Systems

DHFR
Puddle
Mpro
COX
Pond
Lake
STMV
SARS-Cov2
Spike-ACE2
Bay
Sea

Size
(NumAtoms)

Jean Zay (V100)

Selene (A100)

Perf
(ns/day)
- 1GPU

Best Perf
(ns/day)
- #GPU

Perf
(ns/day)
- 1GPU

Best Perf
(ns/day)
- #GPU

23 558
96 000
98 694
174 219
288 000
864 000
1 066 624

43.83
14.63
14.03
8.64
4.90
1.62
1.11

43.83
15.76 -4
14.57 -4
10.15 -4
6.72 -4
2.40 -4
1.77 -4

44.96
15.57
16.36
10.47
6.18
2.11
1.50

44.96
17.57
17.47 -4
11.75 -4
10.60 -8
5.50 -8
4.51 -8

1 509 506

0.89

1.55 -4

1.32

4.16 -8

2 592 000
7 776 000

0.50
0.15

0.77 -4
0.25 -4

0.59
0.22

2.38 -8
0.78 -8

45

Figure 19: Performance and one node scalibility results with the AMOEBA force field

46

Conclusion
We presented the native Tinker-HP multi-GPUs - multi-precision acceleration platform. The
new code is shown to be accurate and scalable across multiple GPUs cards offering unprecedented performances and new capabilities to deal with long timescale simulations on large
realistic systems using polarizable force fields such as AMOEBA. The approach strongly
reduces the time to solution offering to achieve routine simulations that would have require
thousands of CPUs on a single GPU card. Overall, the GPU -accelerated Tinker-HP reaches
the best performances ever obtained for AMOEBA simulations and extends the applicability
of polarizable force fields. The package is shown to be compatible with various computer
GPU system architectures ranging from research laboratories to modern supercomputers.
Future work will focus on adding new features (sampling methods, integrators ...) and
on further optimizing the performance on multi-nodes /multi-GPUs to address the exascale
challenge. We will improve the non-polarizable force field simulations capabilities as we
will provide the high performance implementations of additional new generation polarizable
many-body force fields models such as AMOEBA+, 55,56 SIBFA 57 and others. We will continue to develop the recently introduced adaptive sampling computing strategy enabling the
simultaneous use of hundreds (thousands) of GPU cards to further reduce time to solution
and deeper explore conformational spaces at high-resolution. 53 With such exascale-ready
simulation setup, computations that would have taken years can now be achieved in days
thanks to GPUs . Beyond this native Tinker-HP GPU platform and its various capabilities, an interface to the Plumed library 58 providing additional methodologies for enhancedsampling, free-energy calculations and the analysis of molecular dynamics simulations is also
available. Finally, the present work, that extensively exploits low precision arithmetic, highlights the key fact that high-performance computing (HPC) grounded applications such as
Tinker-HP can now efficiently use converged GPU-accelerated supercomputers, combining
HPC and artificial intelligence (AI) such as the Jean Zay machine to actually enhance their
performances.
47

Acknowledgements
This work was made possible thanks to funding from the European Research Council (ERC)
under the European Union‚Äôs Horizon 2020 research and innovation programme (grant agreement No 810367), project EMC2. This project was initiated in 2019 with a "Contrat de
Progr√®s" grant from GENCI (France) in collaboration with HPE and NVIDIA to port
Tinker-HP on the Jean Zay HPE SGI 8600 GPUs system (IDRIS supercomputer center,
GENCI-CNRS, Orsay, France) using OpenACC . FC acknowledges funding from the French
state funds managed by the CalSimLab LABEX and the ANR within the Investissements
d‚ÄôAvenir program (reference ANR11-IDEX-0004-02) and support from the Direction G√©nerale de l‚ÄôArmement (DGA) Ma√Ætrise NRBC of the French Ministry of Defense. Computations have been performed at GENCI on the Jean Zay machine (IDRIS) on grant no
A0070707671 and on the Ir√®ne Joliot Curie ATOS Sequana X1000 supercomputer (TGCC,
Bruy√®res le Chatel, CEA, France) thanks to PRACE COVID-19 special allocation (projet
COVID-HP). We thank NVIDIA (Romuald Josien and Fran√ßois Courteille, NVIDIA France)
for offering us access to A100 supercomputer systems (DGX-A100 and Selene DGX-A100
SuperPod machines). PR and JWP are grateful for support by National Institutes of Health
(R01GM106137 and R01GM114237).

Code availability
The present code has been released in phase advance in link with the High Performance Computing community COVID-19 research efforts. The software is freely accessible to Academics
via GitHub : https://github.com/TinkerTools/tinker-hp

Competing interest
The authors declare no competing interests.
48

Additional information
This information is available free of charge via the Internet at http://pubs.acs.org

49

List of Figures
1

OpenACC synchronous execution model on test kernel <fill> . . . . . . .

6

2

OpenACC asynchronous execution on both kernels <test> & <test 1>.

.

6

3

illustration of a MD timestep . . . . . . . . . . . . . . . . . . . . . . . . . .

7

4

Reciprocal space offload scheme. Charge interpolation and Force interpolation
are both written in a single kernel. They are naturally parallel except for the
atomic contributions to the grid in the first one. The approach remains the
same for data management between host and device as for real space: all data
are by default device resident to prevent any sort of useless transfer. Regarding
MPI communications, exchanges take place directly between GPUs through
interconnection. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5

12

Illustration of compute balance due to the list reordering. Unbalanced computation in the first image induces an issue called warp discrepancy: a situation
where all threads belonging to the same vector do not follow the same instructions. Minimizing that can increase kernel performance significantly since we
ensure load balancing among each thread inside the vector. . . . . . . . . . .

6

13

Representation of cuFFt‚Äôs communication/computation overlap using different streams for direct and reciprocal space. Real space computation kernels
are assigned to asynchronous stream 18. reciprocal ones goes into high priority asynchronous stream 17. real space kernel therefore recovers FFt grid
exchanges. This profile was retrieved on 2GPUs . . . . . . . . . . . . . . . .

7

15

Performance ratio between single node GPU and single node CPU performance. Reference values can be found in [Table 1]. . . . . . . . . . . . . . .

50

17

8

Profile of the matrix-vector compute kernel on the DHFR system. The left
picure is obtained with the double precision and the right one with simple
precision. In both modes, results indicate an obvious latency issue coming
from memory accessing pattern which prevents the device to reach its peak
performance.

9

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

19

Illustration of the reduction operation on a 16 variables set. Each arithmetic
operation generates an error a that is accumulated during the sequential
operation. On the other hand, parallel reduction uses intermediate variables
to significantly reduce the error. . . . . . . . . . . . . . . . . . . . . . . . . .

10

21

Absolute error between DP implementation and both FP and MP implementations on total potential energy and Forces. Forces root mean square deviation
between DP and MP for systems from 648 up to 2 592 000 atoms. As expected,
both absolute errors in SP and FP are almost identical on the energy and they
grow linearly with the system size. Logarithmic regression gives 0.99 value for
the curve slope and up to 5 kcal/mol for the largest system. However, the relative error for all systems is located under 7 √ó 10‚àí7 in comparison to DP. One
can also see that the error on the forces is independent of the system size the
theory according to which, numerical errors generated from the computation
do not depends of the system size. . . . . . . . . . . . . . . . . . . . . . . . .

11

25

Representation of interactions between two groups of atoms within Tinker-HP.
Bsize = 8 for the illustration . . . . . . . . . . . . . . . . . . . . . . . . . . .

51

26

12

Illustration of a two dimensional partition along with groups for a box of water.
The left Figure shows a 64 cells partition of ‚Ñ¶ while the right one refines this
partitioning into 88 cells. The groups are defined by re-indexing the atoms
following the cells numbering and their maximum size. Here Bsize = 16. A
unique color is associated to every atom belonging to the same group. No cell
contains more than Bsize atoms or 2 groups. Once a group is selected -23-,
searching for its neighboring groups is made through the set of cells (with
respect to the periodic boundary conditions) near to the cells it contains - 43
& 44 -. Once this set is acquired, all the group indexes greater or equal than
the selected group constitute the actual list of neighbors to take the symmetry
of the interactions into account. We see that the group‚Äôs shape modulates the
group neighborhood as illustrated with the right illustration and a spatially
flat group 23. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

13

28

Starting from the situation illustrated by Figure 12, we represent the geometry resulting from the filtering process. We significantly reduce group 23
neighborhood with the list filtering- it decreases from 144 atoms with the first
list to 77 with the filtered one -. Bcomp increases which corresponds to more
interactions computed within each group pair.

14

31

Real space kernel profiling results in mixed precision using our new groupAtoms list.

15

. . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

32

Variation of the total energy during a NVE molecular dynamics simulation
of the DHFR protein in DP and MP and FP . Energy fluctuations are respectively within 1.45 Kcal/mol 1.82 Kcal/mol 1.75 Kcal/mol 1.69 Kcal/mol
3.45 Kcal/mol from Tinker-HP DP SP FP and Tinker-OpenMM DP MP . . .

16

34

Single node mixed precision scalability on the Jean Zay Cluster (V100) using
the AMOEBA polarizable force field . . . . . . . . . . . . . . . . . . . . . .

52

38

17

Two dimensional spatial decomposition of a simulation box with MPI distribution across 16 processes. œâ6 collects all the neighboring domains of œà 6 .
Here nb < n

18

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

41

NVSHMEM memory distribution pattern across a four interconnected devices
node. A symmetric reserved space is allocated by the NVSHMEM library at
initialization. Thus, data is equally split across all devices in order. Every
time a device needs to access data allocated with NVSHMEM, either a GMA
or RMA is issued.

19

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

43

Performance and one node scalibility results with the AMOEBA force field .

46

53

List of Tables
1

Single device benchmark : MD production per day (ns/day). All simulations
were run using a RESPA/2 fs setup.

. . . . . . . . . . . . . . . . . . . . . .

17

2

Single precision MD production (ns/day) within the OpenACC implementation 18

3

device hardware specifications . . . . . . . . . . . . . . . . . . . . . . . . . .

18

4

Available features in the initial Tinker-HP GPU release . . . . . . . . . . . .

35

5

Tinker-HP performances in (ns/day) on different devices and precision modes

39

6

Tinker-OpenMM Mixed precision performances assessed with RESPA framework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

7

39

multi-node performance on Jean Zay with Sea system and BAOAB-RESPA1.
Here the latency designates the time difference between the fastest and the
slowest process. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

8

45

Performance synthesis and scalability results on the Jean Zay (V100) and
Selene (A100) machines. MD production in ns/day with the AMOEBA polarizable force field . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

54

45

References
(1) Hollingsworth, S. A.; Dror, R. O. Molecular Dynamics Simulation for All. Neuron 2018,
99, 1129 ‚Äì 1143.
(2) Dror, R. O.; Dirks, R. M.; Grossman, J.; Xu, H.; Shaw, D. E. Biomolecular Simulation: A Computational Microscope for Molecular Biology. Annual Review of Biophysics
2012, 41, 429‚Äì452, PMID: 22577825.
(3) Ponder, J. W.; Case, D. A. Advances in protein chemistry; Elsevier, 2003; Vol. 66; pp
27‚Äì85.
(4) Huang, J.; Rauscher, S.; Nawrocki, G.; Ran, T.; Feig, M.; de Groot, B. L.; Grubm√ºller, H.; MacKerell, A. D. CHARMM36m: an improved force field for folded and
intrinsically disordered proteins. Nature methods 2017, 14, 71‚Äì73.
(5) Maier, J. A.; Martinez, C.; Kasavajhala, K.; Wickstrom, L.; Hauser, K. E.; Simmerling, C. ff14SB: Improving the Accuracy of Protein Side Chain and Backbone Parameters from ff99SB. Journal of Chemical Theory and Computation 2015, 11, 3696‚Äì3713,
PMID: 26574453.
(6) Jorgensen, W. L.; Maxwell, D. S.; Tirado-Rives, J. Development and Testing of the
OPLS All-Atom Force Field on Conformational Energetics and Properties of Organic
Liquids. Journal of the American Chemical Society 1996, 118, 11225‚Äì11236.
(7) Oostenbrink, C.; Villa, A.; Mark, A. E.; Van Gunsteren, W. F. A biomolecular force
field based on the free enthalpy of hydration and solvation: The GROMOS force-field
parameter sets 53A5 and 53A6. Journal of Computational Chemistry 2004, 25, 1656‚Äì
1676.
(8) Shi, Y.; Ren, P.; Schnieders, M.; Piquemal, J.-P. Reviews in Computational Chemistry
Volume 28 ; John Wiley and Sons, Ltd, 2015; Chapter 2, pp 51‚Äì86.
55

(9) Jing, Z.; Liu, C.; Cheng, S. Y.; Qi, R.; Walker, B. D.; Piquemal, J.-P.; Ren, P. Polarizable Force Fields for Biomolecular Simulations: Recent Advances and Applications.
Ann. Rev. Biophys. 2019, 48, 371‚Äì394.
(10) Melcr, J.; Piquemal, J.-P. Accurate biomolecular simulations account for electronic
polarization. Front. Mol. Biosci. 2019, 6, 143.
(11) Bedrov, D.; Piquemal, J.-P.; Borodin, O.; MacKerell, A. D.; Roux, B.; Schr√∂der, C.
Molecular Dynamics Simulations of Ionic Liquids and Electrolytes Using Polarizable
Force Fields. Chemical Reviews 2019, 119, 7940‚Äì7995, PMID: 31141351.
(12) Lopes, P. E. M.; Huang, J.; Shim, J.; Luo, Y.; Li, H.; Roux, B.; MacKerell, A. D. Polarizable Force Field for Peptides and Proteins Based on the Classical Drude Oscillator.
Journal of Chemical Theory and Computation 2013, 9, 5430‚Äì5449, PMID: 24459460.
(13) Lemkul, J. A.; Huang, J.; Roux, B.; MacKerell, A. D. An Empirical Polarizable Force
Field Based on the Classical Drude Oscillator Model: Development History and Recent
Applications. Chemical Reviews 2016, 116, 4983‚Äì5013, PMID: 26815602.
(14) Lin, F.-Y.; Huang, J.; Pandey, P.; Rupakheti, C.; Li, J.; Roux, B.; MacKerell, A. D.
Further Optimization and Validation of the Classical Drude Polarizable Protein Force
Field. Journal of Chemical Theory and Computation 2020, 16, 3221‚Äì3239, PMID:
32282198.
(15) Ren, P. Y.; Ponder, J. W. "Polarizable Atomic Multipole Water Model for Molecular
Mechanics Simulation". J. Phys. Chem. 2003, 107, 5933‚Äì5947.
(16) Shi, Y.; Xia, Z.; Zhang, J.; Best, R.; Wu, C.; Ponder, J. W.; Ren, P. Polarizable Atomic
Multipole-Based AMOEBA Force Field for Proteins. J. Chem. Theory. Comput. 2013,
9, 4046‚Äì4063.

56

(17) Zhang, C.; Lu, C.; Jing, Z.; Wu, C.; Piquemal, J.-P.; Ponder, J. W.; Ren, P. AMOEBA
Polarizable Atomic Multipole Force Field for Nucleic Acids. J. Chem. Theory. Comput.
2018, 14, 2084‚Äì2108.
(18) Jiang, W.; Hardy, D. J.; Phillips, J. C.; MacKerell, A. D.; Schulten, K.; Roux, B. HighPerformance Scalable Molecular Dynamics Simulations of a Polarizable Force Field
Based on Classical Drude Oscillators in NAMD. The Journal of Physical Chemistry
Letters 2011, 2, 87‚Äì92, PMID: 21572567.
(19) Lemkul, J. A.; Roux, B.; van der Spoel, D.; MacKerell Jr., A. D. Implementation of
extended Lagrangian dynamics in GROMACS for polarizable simulations using the
classical Drude oscillator model. Journal of Computational Chemistry 2015, 36, 1473‚Äì
1479.
(20) Lagard√®re, L.; Jolly, L.-H.; Lipparini, F.; Aviat, F.; Stamm, B.; Jing, Z. F.; Harger, M.;
Torabifard, H.; Cisneros, G. A.; Schnieders, M. J.; Gresh, N.; Maday, Y.; Ren, P. Y.;
Ponder, J. W.; Piquemal, J.-P. Tinker-HP: a massively parallel molecular dynamics
package for multiscale simulations of large complex systems with advanced point dipole
polarizable force fields. Chem. Sci. 2018, 9, 956‚Äì972.
(21) Rackers, J. A.; Wang, Z.; Lu, C.; Laury, M. L.; Lagard√®re, L.; Schnieders, M. J.;
Piquemal, J.-P.; Ren, P.; Ponder, J. W. Tinker 8: Software Tools for Molecular Design.
J. Chem. Theory and Comput. 2018, 14, 5273‚Äì5289.
(22) Jolly, L.-H.; Duran, A.; Lagard√®re, L.; Ponder, J. W.; Ren, P.; Piquemal, J.-P. Raising
the Performance of the Tinker-HP Molecular Modeling Package [Article v1.0]. LiveCoMS. 2019, 1, 10409.
(23) Stone, J. E.; Hardy, D. J.; Ufimtsev, I. S.; Schulten, K. GPU-accelerated molecular
modeling coming of age. Journal of Molecular Graphics and Modelling 2010, 29, 116
‚Äì 125.
57

(24) G√∂tz, A. W.; Williamson, M. J.; Xu, D.; Poole, D.; Le Grand, S.; Walker, R. C. Routine Microsecond Molecular Dynamics Simulations with AMBER on GPUs. 1. Generalized Born. Journal of Chemical Theory and Computation 2012, 8, 1542‚Äì1555, PMID:
22582031.
(25) P√°ll, S.; Zhmurov, A.; Bauer, P.; Abraham, M.; Lundborg, M.; Gray, A.; Hess, B.; Lindahl, E. Heterogeneous Parallelization and Acceleration of Molecular Dynamics Simulations in GROMACS. 2020.
(26) Salomon-Ferrer, R.; G√∂tz, A. W.; Poole, D.; Le Grand, S.; Walker, R. C. Routine Microsecond Molecular Dynamics Simulations with AMBER on GPUs. 2. Explicit Solvent
Particle Mesh Ewald. Journal of Chemical Theory and Computation 2013, 9, 3878‚Äì
3888, PMID: 26592383.
(27) Eastman, P.; Swails, J.; Chodera, J. D.; McGibbon, R. T.; Zhao, Y.; Beauchamp, K. A.;
Wang, L.-P.; Simmonett, A. C.; Harrigan, M. P.; Stern, C. D.; Wiewiora, R. P.;
Brooks, B. R.; Pande, V. S. OpenMM 7: Rapid development of high performance
algorithms for molecular dynamics. PLOS Computational Biology 2017, 13, 1‚Äì17.
(28) Harger, M.; Li, D.; Wang, Z.; Dalby, K.; Lagard√®re, L.; Piquemal, J.-P.; Ponder, J.; Ren, P. Tinker-OpenMM: Absolute and relative alchemical free energies using
AMOEBA on GPUs. Journal of Computational Chemistry 2017, 38, 2047‚Äì2055.
(29) Potluri, S.; Luehr, N.; Sakharnykh, N. Simplifying Multi-GPU Communication with
NVSHMEM. GPU Technology Conference. 2016.
(30) Frenkel, D.; Smit, B. Understanding molecular simulation: from algorithms to applications; Elsevier, 2001; Vol. 1.
(31) Lagard√®re, L.; Jolly, L. H.; Lipparini, F.; Aviat, F.; Stamm, B.; Jing, Z. F.; Harger, M.;
Torabifard, H.; Cisneros, G. A.; Schnieders, M. J.; Gresh, N.; Maday, Y.; Ren, P. Y.;

58

Ponder, J. W.; Piquemal, J.-P. Tinker-HP: a massively parallel molecular dynamics
package for multiscale simulations of large complex systems with advanced point dipole
polarizable force fields. Chem. Sci. 2018, 9, 956‚Äì972.
(32) Wienke, S.; Springer, P.; Terboven, C.; an Mey, D. OpenACC ‚Äî First Experiences
with Real-World Applications. Euro-Par 2012 Parallel Processing. Berlin, Heidelberg,
2012; pp 859‚Äì870.
(33) Chandrasekaran, S.; Juckeland, G. OpenACC for Programmers: Concepts and Strategies, 1st ed.; Addison-Wesley Professional, 2017.
(34) Sanders, J.; Kandrot, E. CUDA by example: an introduction to general-purpose GPU
programming; Addison-Wesley Professional, 2010.
(35) Volkov, V. Understanding Latency Hiding on GPUs. Ph.D. thesis, EECS Department,
University of California, Berkeley, 2016.
(36) Kraus, J. An introduction to CUDA-aware MPI. Weblog entry]. PARALLEL FORALL
2013,
(37) Essmann, U.; Perera, L.; Berkowitz, M. L.; Darden, T.; Lee, H.; Pedersen, L. G. A
smooth particle mesh Ewald method. The Journal of Chemical Physics 1995, 103,
8577‚Äì8593.
(38) Lagard√®re, L.; Lipparini, F.; Polack, E.; Stamm, B.; Canc√®s, E.; Schnieders, M.; Ren, P.;
Maday, Y.; Piquemal, J.-P. Scalable Evaluation of Polarization Energy and Associated
Forces in Polarizable Molecular Dynamics: II. Toward Massively Parallel Computations
Using Smooth Particle Mesh Ewald. Journal of Chemical Theory and Computation
2015, 11, 2589‚Äì2599, PMID: 26575557.
(39) NVIDIA Corporation, CUDA Toolkit 11.1 CUFFT Library Programming Guide 2020
,http://developer.nvidia.com/nvidia-gpu-computing-documentation. 2020.
59

(40) Lipparini, F.; Lagard√®re, L.; Stamm, B.; Canc√®s, E.; Schnieders, M.; Ren, P.; Maday, Y.; Piquemal, J.-P. Scalable Evaluation of Polarization Energy and Associated
Forces in Polarizable Molecular Dynamics: I. Toward Massively Parallel Direct Space
Computations. Journal of Chemical Theory and Computation 2014, 10, 1638‚Äì1651,
PMID: 26512230.
(41) Phillips, J. C.; Gengbin Zheng,; Kumar, S.; Kale, L. V. NAMD: Biomolecular Simulation on Thousands of Processors. SC ‚Äô02: Proceedings of the 2002 ACM/IEEE
Conference on Supercomputing. 2002; pp 36‚Äì36.
(42) Tuckerman, M.; Berne, B. J.; Martyna, G. J. Reversible multiple time scale molecular
dynamics. J. Chem. Phys. 1992, 97, 1990‚Äì2001.
(43) Bussi, G.; Donadio, D.; Parrinello, M. Canonical sampling through velocity rescaling.
J. Chem. Phys. 2007, 126, 014101.
(44) Zhou, J.; Ross, K. A. Implementing database operations using SIMD instructions. Proceedings of the 2002 ACM SIGMOD international conference on Management of data.
2002; pp 145‚Äì156.
(45) Nickolls, J.; Dally, W. J. The GPU computing era. IEEE micro 2010, 30, 56‚Äì69.
(46) Le Grand, S.; G√∂tz, A. W.; Walker, R. C. SPFP: Speed without compromise‚ÄîA
mixed precision model for GPU accelerated molecular dynamics simulations. Computer
Physics Communications 2013, 184, 374 ‚Äì 380.
(47) Yatepep, R. Fixed-point arithmetic: An introduction. Digital Signal Labs 2009, 81,
198.
(48) Bowers, K. J.; Dror, R. O.; Shaw, D. E. The midpoint method for parallelization of
particle simulations. The Journal of Chemical Physics 2006, 124, 184109.

60

(49) Lagard√®re, L.; Aviat, F.; Piquemal, J.-P. Pushing the Limits of Multiple-Time-Step
Strategies for Polarizable Point Dipole Molecular Dynamics. The Journal of Physical
Chemistry Letters 2019, 10, 2593‚Äì2599.
(50) C√©lerse, F.; Lagard√®re, L.; Derat, E.; Piquemal, J.-P. Massively parallel implementation of Steered Molecular Dynamics in Tinker-HP:comparisons of polarizable and
non-polarizable simulations of realistic systems. J. Chem. Theory. Comput. 2019, 15,
3694‚Äì3709.
(51) Miao, Y.; Feher, V. A.; McCammon, J. A. Gaussian accelerated molecular dynamics: Unconstrained enhanced sampling and free energy calculation. Journal of chemical
theory and computation 2015, 11, 3584‚Äì3595.
(52) Jorgensen, W. L.; Maxwell, D. S.; Tirado-Rives, J. "Development and Testing of the
OPLS All-Atom Force Field on Conformational Energetics and Properties of Organic
Liquids". J. Am. Chem. Soc. 1996, 117, 11225‚Äì11236.
(53) Jaffrelot-Inizan, T.; C√©lerse, F.; Adjoua, O.; El Ahdab, D.; Jolly, L.-H.; Liu, C.; Ren, P.;
Montes, M.; Lagarde, N.; Lagard√®re, L., et al. High-Resolution Mining of SARS-CoV-2
Main Protease Conformational Space: Supercomputer-Driven Unsupervised Adaptive
Sampling. Chemical Science 2021,
(54) Chapman, B.; Curtis, T.; Pophale, S.; Poole, S.; Kuehn, J.; Koelbel, C.; Smith, L. Introducing OpenSHMEM: SHMEM for the PGAS community. Proceedings of the Fourth
Conference on Partitioned Global Address Space Programming Model. 2010; pp 1‚Äì3.
(55) Liu, C.; Piquemal, J.-P.; Ren, P. AMOEBA+ Classical Potential for Modeling Molecular Interactions. Journal of Chemical Theory and Computation 2019, 15, 4122‚Äì4139,
PMID: 31136175.
(56) Liu, C.; Piquemal, J.-P.; Ren, P. Implementation of Geometry-Dependent Charge Flux

61

into the Polarizable AMOEBA+ Potential. The Journal of Physical Chemistry Letters
2020, 11, 419‚Äì426, PMID: 31865706.
(57) Gresh, N.; Cisneros, G. A.; Darden, T. A.; Piquemal, J.-P. Journal of Chemical Theory
and Computation 2007, 3, 1960‚Äì1986, PMID: 18978934.
(58) Bonomi, M.; Bussi, G.; Camilloni, C.; Tribello, G. A.; Ban√°≈°, P.; Barducci, A.; Bernetti, M.; Bolhuis, P. G.; Bottaro, S.; Branduardi, D., et al. Promoting transparency
and reproducibility in enhanced molecular simulations. Nature methods 2019, 16, 670‚Äì
673.

62

Graphical TOC Entry

63

