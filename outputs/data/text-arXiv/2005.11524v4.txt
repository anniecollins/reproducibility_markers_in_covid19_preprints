Coronavirus: Comparing COVID-19, SARS, and MERS in the eyes of AI
Anas Tahira, Yazan Qiblaweya, Amith Khandakara, Tawsifur Rahmanb, Uzair Khurshida, Farayi Musharavatic, M. T. Islamd,
Serkan Kiranyaza, Muhammad E. H. Chowdhurya*
a

Department of Electrical Engineering, Qatar University, Doha-2713, Qatar
Department of Biomedical Physics & Technology, University of Dhaka, Dhaka 1000, Bangladesh
c
Mechanical & Industrial Engineering Department, Qatar University, Doha-2713, Qatar
d
Department of Electrical, Electronic & Systems Engineering, Universiti Kebangsaan Malaysia, Bangi, Selangor 43600,
Malaysia
*Correspondence: Muhammad E. H. Chowdhury; mchowdhury@qu.edu.qa,Tel.: +974-31010775
b

1

ABSTRACT
Novel Coronavirus disease (COVID-19) is an extremely contagious and quickly spreading Coronavirus infestation.
Severe Acute Respiratory Syndrome (SARS)-CoV, Middle East Respiratory Syndrome (MERS)-CoV outbreak in 2002 and
2011, and current COVID-19 pandemic all from the same family of Coronavirus. The fatality rate due to SARS and MERS
was higher than COVID-19 however, the spread of those were limited to few countries while COVID-19 affected more than
two-hundred countries worldwide. Several studies have shown that there are certain patterns in a chest X-ray and CT that are
like the manifestations of pneumonia, and they are visible in the patients diagnosed with SARS and MERS. While the data
mining technique was applied to distinguish SARS with other typical pneumonia, due to the overlapping features of the lung
infections in these diseases, there is no work available in the literature investigating similarities or dissimilarities of chest xray images of COVID-19 patients from the other two CoV family members. It is difficult for the expert radiologist to
distinguish them by the human eyes. The number of COVID-19 infected people has reached over five million however, the
publically shared chest X-ray images are quite rare to date. It is challenging to train a deep learning network without a
properly annotated large database. Thanks to transfer learning, an effective mechanism that can provide a promising solution
by transferring knowledge from generic object recognition tasks to domain-specific tasks. In this study, we have first created
the largest database consisting of 423 COVID-19 images, 134 SARS images, and 144 MERS images. We then used deep
Convolutional Neural Networks (CNNS) along with innovative image pre-processing techniques to distinguish COVID-19
images from SARS and MERS images. Several deep learning algorithms were trained and tested and four outperforming
algorithms were reported: SqueezeNet, ResNet18, Inception-v3, and DenseNet201. Original, pre-processed images were
used individually and altogether (as the 3-channel scheme) as the input(s) to the networks. It was observed that Inception-v3
outperforms other networks with a 3-channel scheme and achieves sensitivities of 99.5%, 93.1%, and 97% for classifying
COVID-19, MERS, and SARS images, respectively. Investigating deep layer activation mapping of the correctly classified
images and miss-classified images, it was observed that some overlapping patterns between COVID-19 and MERS images
were identified by the deep layer network. 10 out of 144 images were miss-classified as COVID while only one out of 423
COVID-19 images was miss-classified as MERS. None of the MERS images was miss-classified to SARS and only one
COVID-19 image was miss-classified as SARS. Therefore, it can be concluded that X-ray images from SARS are
significantly different from MERS and COVID-19 in the eyes of deep learners while there are some overlapping patterns
available between MERS and COVID-19.

2

Index Terms: Deep Convolutional Neural Networks, MERS, SARS, COVID-19 Pneumonia,
Transfer Learning, Computer-aided diagnostic tool

1. Introduction
The world has experienced outbreaks of coronavirus infections during different points of time in the last two decades.
The Severe Acute Respiratory Syndrome (SARS)-CoV outbreak in 2002-2003 from Guangdong China, Middle East
Respiratory Syndrome (MERS)-CoV outbreak in 2011 from Jeddah, Saudi Arabia and the recent COVID-19 or SARS-CoV2 pandemic from Wuhan, China are all from the same family of Coronavirus (Prompetchara et al., 2020). However, the
genomic sequence of COVID-19 showed similar but distinct genome composition of its predecessors- SARS and MERS
(Kumar et al., 2020; Prompetchara et al., 2020). Despite a lower fatality rate of COVID-19, i.e. around 7 % (CSSE, 2020)
compared to SARS (10%) and MERS (35%), COVID 19 has resulted in many fold deaths (>350,000 already) than combined
deaths (around 1700) of MERS and SARS (Mahase, 2020).
SARS-CoV and MERS-CoV were the most severe coronavirus (CoV)-associated diseases in humans until COVID-19
pandemic in 2019-2020. These viruses are originated from animals; however, can be transmitted to humans, which can cause
severe and often fatal respiratory disease in their new host. The two coronaviruses are said to have a genetic structure that
allows them to quickly replicate their presence and weaken the hostâ€™s antiviral defense mechanisms. After first infecting
humans in the Guangdong province of southern China in 2002, the SARS-CoV has spread to 26 countries of the world using
the person-to-person human contacts (W.H.O, 2020). Again in 2012, the infectious outbreak caused by MERS-CoV in Saudi
Arabia has spread to more than 1600 patients in 27 countries, resulting in over 600 deaths, 80% of which were reported in
Saudi Arabia (Gao et al., 2016; W.H.O, 2019). The recent outbreak of the coronavirus family was COVID-19 that happened
in December 2019 from the Wuhan city in China. The COVID-19 outbreak was so infectious and had spread all over the
world in a manner, that the World Health Organization (WHO) on 11th March 2020 has declared it as a pandemic (WHO,
2020). The common symptoms of SARS, MERS and COVID-19 includes fever, cough, and/or shortness of breath, and
pneumonia while some people exhibits asymptomatic symptoms like gastrointestinal symptoms, including diarrhea. Severe
cases of the CoV diseases include acute respiratory distress syndrome (ARDS) or complete respiratory failure, which
requires support from mechanical ventilation and intensive-care unit. People with compromised immune system or elderly
people or people with other chronic disease were at high risk in each disease particularly more in MERS and COVID-19 and
some patients have undergone several organs failure, particularly kidneys or septic shocks.

3

The Coronavirus Disease 2019 (COVID-19) outbreak has a tremendous impact on global health and the daily life of
people in more than two hundred countries. Most of the tests rely on detecting the genetic material of the coronavirus using
Reverse-transcription polymerase chain reaction (RT-PCR), which has a poor detection rate with a time-consuming
operation (Wang et al., 2020). Ai et al. (Ai et al., 2020) have claimed that chest CT has a higher sensitivity for the diagnosis
of COVID-19 and have suggested using it as a primary tool for the current COVID-19 detection in epidemic areas. Li et
al.(Li et al., 2020) have concluded that their deep learning model over chest CT can accurately detect COVID-19 and
differentiate it from community-acquired pneumonia and other lung diseases. Wang et al. (Wang and Wong, 2020) have later
found that the cheaper and readily available chest X-rays can also help in detecting COVID-19. Chowdhury et al.
(Chowdhury et al., 2020) have compiled a large dataset of COVID-19 with normal (healthy) and community-acquired
pneumonia chest X-rays and have shown that deep networks can distinguish between them with very high accuracy and
sensitivity. The same approach has been used in many similar studies (Bassi and Attux, 2020; Hamed et al., 2020; Horry et
al., 2020; Yan et al., 2020). There have been studies on the use of chest X-ray images in the diagnosis of MERS-CoV and
SARS-CoV. Hamimi (Hamimi, 2016) has shown that certain patterns in chest X-ray and CT are like the manifestations of
pneumonia (Horry et al., 2020). Xuanyang et al. (Xie et al., 2006) used data mining techniques to distinguish SARS and
typical pneumonia based on X-ray images.
Worldwide researchers have presented numerous clinical and experimental information regarding the SARS and MERS,
which could be useful in the fight against COVID-19 (Liu et al., 2020). There have been studies in the literature on
investigating the similarities between the genome structure of SARS, MERS, and COVID-19 (Lu et al., 2020). However, to
the best of our knowledge, there is no work available in the literature to investigate the similarities and dissimilarities of
chest X-ray images of COVID-19 patients from the other two CoV family members. Due to the overlapping patterns of lung
infections, it is very difficult to even for MDs to distinguish between the images from different CoV families.
In this study, we investigate several state-of-the-art deep classifiers for distinguishing COVID-19 from other recent
coronavirus related diseases, namely SARS and MERS so that this could provide meaningful insights and prove to be useful
for medical diagnosis. For this purpose, we employ transfer learning over several deep Convolutional Neural Networks
(CNNs) over the largest dataset encapsulating 701 chest X-ray images of SARS, MERS, and COVID-19. Furthermore, in
this study, we perform a detailed set of comparative evaluations to determine the best network model and configuration.
Finally, we also aim to see how such deep networks can be helpful in identifying potential distinguishable deep-layer
features in the X-ray images.

The rest of the paper is organized as follows: Section 2 describes the methodology adopted for the study. The
experimental setup and evaluation matrix are presented in Section 3. Section 4 presents the results and performs an extensive
set of comparative evaluations among the networks employed. Accordingly, we discuss and analyze the final results whereas
the conclusions are drawn in Section 5.

2. Methodology
This section is organized as follows: Section A describes the process of database creation using chest X-Rays images for
SARS, MERS, and COVID-19 patients, while Section B presents the pre-processing techniques applied to the X-ray images
before feeding them to deep networks. Section C briefly describes four deep CNNs used in this study to classify chest X-Ray
images from different CoV families.
In order to investigate potential enhancement on the classification performance, four different pre-processing schemes
were testes in this study. Original chest X-ray image, which did not undergo any form of pre-processing, Contrast Limited
Adaptive Histogram Equalization (CLAHE) and image complementation and finally the combination of the three (original,
CLAHE, complemented) schemes applied altogether in a 3-channels. In Figure 1, the 3-channel scheme is illustrated, where
the original X-ray image along with its equalized (CLAHE) and complemented versions was used as the inputs to a CNN
network Figure 1.

Figure 1: Schematic representation of 3-channel classification scheme. Concatenated Original, CLAHE
Equalized & Image complement of patient X-ray are feed to a pretrained CNN to be classified as COVID-19, SARS or MERS
infection. Variable network feature maps (Blue, Orange or Green frames) represents the four CNN networks used
(ResNet18, SqueezeNet, InceptioV3 or DenseNet) with different convolutional and pooling layers sizes.
2.1.

Database description
The number of worldwide infected cases for COVID-19 already exceeds 5.5 million and the death toll is around 350k.

However, little effort has been done by highly infected countries on sharing clinical and radiography data publicly. Sharing

COVID-19 data will help researchers, doctors, and engineers around the world to come up with innovative solutions for
early detection. Therefore, we have created a large dataset for COVID-19, MERS and SARS with 423, 144 and 133 chest Xray images respectively utilizing the chest X-ray images available publicly in the published or preprint articles and online
resources. In this study, we have used only posterior-to-anterior (PA) or anterior-to-posterior (AP) chest X-rays as this view
of radiography is widely used by the radiologist.
Five major sources were used to create the COVID-19 image database: Italian Society of Medical and Interventional
Radiology (SIRM) COVID-19 Database (SIRM, 2020), Novel Corona Virus 2019 Dataset, Radiopaedia (Francesco, 2020),
Chest Imaging (Spain) at thread reader and online articles and news-portals (until April 16th) (ChestImaging, 2020). SIRM
COVID-19 database (SIRM, 2020) shared 94 chest X-ray images from 71 confirmed COVID-19 positive patients until 10th
May 2020 in the database. Joseph Paul Cohen et al. (Cohen et al., 2020) have created a public database in GitHub by
collecting radiographic images of COVID-19, MERS, SARS, and ARDS from the published articles and online resources.
134 COVID-19 positive chest X-ray images were collected from the GitHub database. A physician has shared 103 images
from his hospital from Spain to the Chest imaging at thread reader, while 60 images were collected from recently published
articles and 32 images were collected from Radiopaedia. Some right-left (RL) views of chest X-ray images were also
available in the accumulated database (apart from PA/AP views of X-ray images) however, RL views were not present
among MERS, and SARS dataset and therefore, RL view of COVID-19 images was not included in the study. The articles,
news-portal and online public databases are published from different countries of the World, where COVID-19 has affected
significantly and the X-ray images, therefore, represent different age groups, gender, and ethnicity from each country.
SARS and MERS X-ray images are even scarcer compared to COVID-19, therefore, we collected and indexed X-ray
images from different publicly available online resources and articles. SARS and MERS radiographic images were collected
from 55 different articles (25-MARS, 30-SARS). A total of 260 images was collected from articles and 18 images were from
Joseph Paul Cohensâ€™ GitHub database (Cohen et al., 2020). Out of these, 70 MERS X-ray images were collected from (Rhee
et al., 2016), while 16 SARS X-ray images were from (Grinblat et al., 2003). During the collection, the authors looked to the
peer-reviewed articles in order to ensure the quality of the provided information. Extremely low-resolution images were
removed from the database. The collected dataset is highly diverse as X-ray images are from several countries around the
world and from different X-ray machines. The dataset encapsulates images of different resolution, quality, and SNR levels as
shown in Figure 2.

Figure 2: Sample X-ray images from the dataset: (A) COVID-19, (B) MERS, and (C) SARS. The dataset
encapsulates images from different countries around the world with different resolution, quality, and SNR levels. All images
are rescaled with the same factor to illustrate the diversity of the dataset.
2.2. Pre-Processing Techniques
Medical images are often affected by noise due to different sources of interference, including imaging process and data
acquisition (Rangayyan, 2000). As a result, it may become harder to evaluate them visually. Some processing methods can
be applied to improve the information provided by the image for the human eye or to use them as input for algorithms (Lu
and Guo, 2017). Histogram Equalization (HE) is a technique mainly used with images that are predominantly dark and
adjusts image intensities to enhance contrast by effectively spreading out the most frequent intensity values. HE
automatically calculates the transformation function to approximate the Uniform distribution. HE considers the entire image
to find the transform function and the transformation can be described mathematically as follows:
ğ‘¦

ğ‘‡ ğ‘¥

ğ¿

1

ğ‘ ğ‘‹

ğ‘–

where ğ‘‹ is the random variable representing the original pixel intensities, ğ‘ ğ‘‹
intensity ğ‘¥, ğ‘¦

(1)
ğ‘¥ is the probability of having the pixel

ğ‘‡ ğ‘¥ is the transformation function, ğ‘¦ are the new intensities after transformation, and ğ¿

2 is the

intensity value for an N-bit image (e.g., for 8 bit gray-scale image, L-1=255 is the maximum intensity value). A closer look
to Equation (1) will reveal the fact that ğ‘ and ğ‘‡ ğ‘¥ are the approximations of the probability and cumulative distribution

functions, respectively (Maini and Aggarwal, 2010). An improved HE variant is called Adaptive Histogram Equalization
(AHE). AHE performs HE over small regions (i.e., patches) in the image, and thus, AHE enhances the contrast of each
region individually. Therefore, it improves local contrast and edges adaptively in each region of the image to the local
distribution of pixel intensities instead of the global information of the image. However, AHE could over amplify the noise
component in the image (Pizer et al., 1987). To address this difficulty, Contrast-Limited Adaptive Histogram Equalization
(CLAHE) uses the same approach as AHE but the amount of contrast enhancement that can be produced within the selected
region is limited by a threshold parameter. Therefore, produced images are more natural in appearance than those produced
by AHE (Zimmerman et al., 1988).
When the HE technique was applied to the X-ray images it was observed that it may saturate certain regions.
CLAHE technique can address this drawback in general. For instance, Figure 3 shows the application of CLAHE and HE
techniques over a sample X-ray image. The image was saturated in the center of the lungs when HE technique was applied.
The histogram for the equalized image shows that the values are redistributed across all pixels compared with the histogram
of the original image. But some areas are becoming brighter than others and the distribution of the histogram intensity of
pixels was chosen Rayleigh distribution which made them bell-shaped. As a result, in this study, CLAHE was used for preprocessing the X-ray images instead of HE.

Figure 3: Comparison between original, HE and CLAHE equalized X-ray images with corresponding
histograms. Starting from the left, original X-ray with the corresponding histogram showing original distribution of pixel
intensities, then Histogram Equalized X-ray with almost uniform distribution of pixel intensities is shown in the middle of the
figure. Finally, The CLAHE equalized X-ray with Rayleigh distribution of pixel intensities, that is closer to original case
compared to Histogram Equalized version.

The image inversion or complement is a technique where the zeros become ones and ones become zeros so black
and white are reversed in a binary image. For an 8-bit greyscale image, the original pixel is subtracted from the highest
intensity value, 255, the difference is considered as pixel values for the new image. For x-ray images, the dark spots turn into
lighter and light spots become darker. The mathematical expression is simply:
ğ‘¦

225

ğ‘¥

(2)

where, ğ‘¥ and y are the intensity values of the original and the transformed (new) images. This technique shows the
lungs area (i.e., the region of interest) lighter and the bones are darker. As this is a standard procedure, which was used
widely by radiologists, it may equally help deep networks for a better classification. It can be noted that the histogram for the
complemented image is a flipped copy of the original image (Figure 4).

Figure 4: Comparison between an original X-ray and it is image complement. The top subplots show the original
X-ray with it corresponding histograms, while the complement version and it is histogram are shown in the bottom subplot.
Finally, as shown in Figure 5, the 3-channel scheme was used in the input layer of the deep networks where
original, CLAHE and complement images were used althogether. The pixel values for each image are concatenated into a
single matrix in order to create the new image. It may be noted that X-ray images can be chomatic (RGB) or monochromatic
(grayscale) images; however, while applying as input to the CNN networks RGB X-ray images are converted to grayscale
image. Therefore, the 3-channel scheme is expected to improve the learning performance of the networks, which is indeed
confirmed by the experiments performed in this study.

Figure 5: 3-channel scheme.
2.3. Pre-trained Networls
Transfer learning is a well-established deep learning approach, where gained knowledge from one problem is
applied to a different but a related problem. In this study, four pre-trained CNN models, ResNet18 (He et al., 2016),
SqueezeNet (Iandola et al., 2016), InceptionV3 (Szegedy et al., 2016), and DenseNet201 (Huang et al., 2017) were used to
classify X-ray images. The deep CNNs were previously trained over ImageNet database (W.H.O, 2020). The rich set of
powerful and informative features learned by these networks was utilized through transfer learning to extract specific
features of the corona infected patientsâ€™ X-ray images. The output layer of each network was replaced by a SoftMax layer
with three neurons to classify the X-ray images into one of the following classes: COVID-19, SARS, or MERS.
Overfitting, which is a well-known paradigm for deep networks trained over limited size datasets can drastically
diminish the generalization performance. The problem becomes worse when a high number of training epochs are performed
where network saturation would eventually occur due to the â€œvanishing gradientâ€ problem, especially at the first hidden
layers. With the introduction of the concept of a residual network (ResNet), vanishing gradient problem with deep CNN
networks is solved by introducing the concept of shortcut connections, where the activations of one-layer that are fed to the
next layer are fed to the deeper layers as well. ResNet consists of 8 residual blocks where each block has 2 convolutional
layers with 3x3 kernels. The depth of the layers increases every two blocks as going deeper in the network with layer sizes of
64, 128, 256, 512 kernels respectively. In addition, 7x7 Conv layer followed by a pooling layer of stride 2 is used at the start
and a SoftMax classification layer at the end of the network. InceptionV3 showed improved performance in classifying
different types of problems. Typically, larger kernels are favored for global features that are distributed over a large area of
images while smaller kernels are preferred for an area-specific feature that is distributed over image frame. This inspired the
idea of inception layers, where kernels of different sizes (1x1, 3x3, and 5x5) are concatenated within the same layer instead

of going deeper in the network. The Inception network starts with multiple conventional layers of 3x3 kernel followed by 3
inception blocks and ends with an 8x8 global average pooling layer followed by SoftMax classifier. This architecture
increases the network space, where the best features can be selected by training. SqueezeNet is the smallest network
considered in this study with 18 layers only and almost 1.24 million parameters compared to 11.7, 20, and 23.9 million
parameters for Resent18, InceptionV3, and DenseNet201 respectively. Introducing fire modules, where a squeeze
convolutional layer with 1x1 kernels is fed to an expand layer that has a mix of 3x3 and 1x1 kernels. The network begins
with a standalone convolutional layer, followed by 8 fire block and end with a convolutional layer followed by a SoftMax
layer. The number of kernels per fire module is increased gradually through the network. The network performs max-pooling
after the first convolutional layer, 4th fire module and 8th fire module. The compact architecture of SqueezeNet makes it
favorable over other networks for such problems that it can achieve a comparable performance level. Unlike the residual
networks, DenseNet concatenates all feature maps instead of summing residuals. All layers in a dense block are densely
connected to their subsequent layers, receiving more supervision from previous layers. This will create compact layers with
little redundancy in the learned feature, where dense layers can share pieces of collective knowledge. Densenet201 consists
of 4 dense blocks, where each block consists of multiple convolution layers with 1x1 and 3x3 filters. The dense blocks are
separated by transition layers consisting of batch normalization layer, 1x1 convolutional layer, and 2x2 average pooling
layer. The network starts with a 7x7 convolutional layer followed by a 3x3 max-pooling layer, both with a stride of 2, and
ends with a 7x7 global average pooling layer followed by a SoftMax layer. Choosing the best network for a specific problem
is usually a tradeoff between the following two criteria: computational complexity and classification accuracy.

3. Experimental Setup
This study was conducted on COVID-19, MERS, and SARS X-ray images of the patients infected from the CoV
family. Transfer learning was utilized to train several networks using 5-fold cross-validation (CV), with 80% train and 20%
test (unseen folds), where 20% of training data is used as a validation set to avoid overfitting. The imbalance class
distribution ratio of the dataset has a major impact on the model performance of deep learning classification problems.
Therefore, we balanced the size of each class in the train set using data augmentation. We performed data augmentation by
applying rotations of 5, 10, 20, 25, and 30 degrees. In addition, horizontal and vertical image translations were used within
the interval [-0.2, +0.2]. Table 1 summarizes the number of images per class used for training, validation, and testing at each
fold.

Table 1: Number of images per class and per fold before and after data augmentation
Class

# of Samples

Training Samples

Augmented Training
Samples

Validation Samples

Test Samples

COVID-19
MERS
SARS

423
144
134

270
92
89

1890
1932
1806

68
23
21

85
29
26

MATLAB 2020a was used to train and evaluate the pre-trained CNN networks (SqueezeNet, ResNet18,
InceptionV3, and DenseNet201), with an 8-GB NVIDIA GeForce GTX 1080 GPU card. Stochastic Gradient Descent with
momentum optimizer was used, with learning rate, ğ›¼

10 , momentum update, ğ›½

0.9 and mini-batch size of 16

images with 10-20 Back Propagation epochs. Fivefold cross-validation result was averaged to produce the final receiver
operating characteristic (ROC) curve, confusion matrix, and evaluation matrices.
The performance of different CNNs was assessed using five evaluation metrics: Accuracy, Precision, Sensitivity,
F1-score, and Specificity. Per-class values were computed over the overall confusion matrix that accumulates all test fold
results of the 5-fold cross-validation.
ğ´ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¦

_

ğ‘‡ğ‘ƒ

_

ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›

_

2

ğ¶ğ‘‚ğ‘‰ğ¼ğ·

_

ğ‘‡ğ‘
ğ¹ğ‘ƒ

ğ‘‡ğ‘ƒ

_

_

ğ¹ğ‘ƒ

ğ‘‡ğ‘ƒ

_
_

ğ¹ğ‘

_

_

ğ‘‡ğ‘

(5)

ğ‘†ğ‘’ğ‘›ğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘–ğ‘¡ğ‘¦
ğ‘†ğ‘’ğ‘›ğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘–ğ‘¡ğ‘¦
ğ‘‡ğ‘

_

_

ğ¹ğ‘ƒ

(3)

(4)

_

ğ¹ğ‘

ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›
ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›

ğ‘†ğ‘ğ‘’ğ‘ğ‘–ğ‘“ğ‘–ğ‘ğ‘–ğ‘¡ğ‘¦
where ğ‘ğ‘™ğ‘ğ‘ ğ‘ 

ğ‘‡ğ‘ƒ

_

ğ‘‡ğ‘ƒ

ğ‘†ğ‘’ğ‘›ğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘–ğ‘¡ğ‘¦

ğ¹1_ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’

ğ‘‡ğ‘ƒ
ğ‘‡ğ‘

(6)

(7)

_

19, ğ‘€ğ¸ğ‘…ğ‘† ğ‘œğ‘Ÿ ğ‘†ğ´ğ‘…ğ‘†.

The overall performance was computed using the weighted average values of each class. The weighted average
gives a better estimation of the overall performance as class frequencies vary for the presented problem.
ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›
ğ‘†ğ‘’ğ‘›ğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘–ğ‘¡ğ‘¦
ğ¹1_ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’

ğ‘›1 ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›
ğ‘›1 ğ‘†ğ‘’ğ‘›ğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘–ğ‘¡ğ‘¦
ğ‘›1 ğ¹1_ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’

ğ‘›2 ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›
ğ‘›1 ğ‘›2 ğ‘›3
ğ‘›2 ğ‘†ğ‘’ğ‘›ğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘–ğ‘¡ğ‘¦
ğ‘›1 ğ‘›2 ğ‘›3
ğ‘›2 ğ¹1_ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’
ğ‘›1 ğ‘›2 ğ‘›3

ğ‘›3 ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›
ğ‘›3 ğ‘†ğ‘’ğ‘›ğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘–ğ‘¡ğ‘¦
ğ‘›3 ğ¹1_ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’

(8)
(9)
(10)

ğ‘†ğ‘ğ‘’ğ‘ğ‘–ğ‘“ğ‘–ğ‘ğ‘–ğ‘¡ğ‘¦
ğ´ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¦

ğ‘›1 ğ‘†ğ‘ğ‘’ğ‘ğ‘–ğ‘“ğ‘–ğ‘ğ‘–ğ‘¡ğ‘¦

ğ‘›2 ğ‘†ğ‘ğ‘’ğ‘ğ‘–ğ‘“ğ‘–ğ‘ğ‘–ğ‘¡ğ‘¦
ğ‘›1 ğ‘›2 ğ‘›3

ğ‘›1 ğ´ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¦

ğ‘›2 ğ´ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¦
ğ‘›1 ğ‘›2 ğ‘›3

ğ‘›3 ğ‘†ğ‘ğ‘’ğ‘ğ‘–ğ‘“ğ‘–ğ‘ğ‘–ğ‘¡ğ‘¦

(11)

ğ‘›3 ğ´ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¦

(12)

where ğ‘›1, ğ‘›2 ğ‘ğ‘›ğ‘‘ ğ‘›3 are the total number of COVID-19, MERS and SARS cases respectively.

4. Results and Discussion
Table 2 summarizes the classification performances of the deep CNNs in-terms of the per-class performance matrix.
For each network, four different pre-processing schemes (original, CLAHE, complemented, and 3-channel) were compared
and the best performance achieved is presented in the table. For instance, it was observed that SqueezeNet achieved the best
classification performance on original images, while ResNet18 and Inceptionv3 outperformed on 3-channel images. It can be
noticed that InceptionV3 achieved the overall best classification performance among all using the 3-channel scheme.
Table 2: Comparison between SqueezeNet, ResNet18, InceptionV3 and DenseNet201 CNN networks, in term of perclass classification Accuracy, Precision, Sensitivity, F1-score and Sensitivity.
CNN Network
SqueezeNet
(Original)

ResNet18
(3-Channel)

Inceptionv3
(3-Channel)

DenseNet201
(Image complement)

Class

Accuracy

Precision

Sensitivity

F1-score

Specificity

COVID-19

88.27

89.31

91.97

90.48

82.63

MERS

91.56

84.97

72.09

77.58

96.58

SARS

91.86

77.32

81.25

78.9

94.36

Overall

89.77

86.13

85.84

85.98

88.02

COVID-19

94.04

92.99

97.88

95.29

88.21

MERS

96.03

94.34

85.49

89.5

98.75

SARS

97.16

96.17

88.89

91.97

99.12

Overall

95.02

93.88

93.61

93.74

92.41

COVID-19

97.87

97.13

99.53

98.29

95.36

MERS

98.3

98.40

93.1

95.56

99.64

SARS

99.29

99.20

97.04

98.08

99.82

Overall

98.22

97.79

97.73

97.76

97.07

COVID-19

96.17

96.55

97.18

96.85

94.64

MERS

97.02

93.57

91.72

92.63

98.39

SARS

98.86

97.23

97.04

97.05

99.3

Overall

96.84

96.07

96.03

96.05

96.28

Figure 6 shows the comparative ROC curves for different networks on different pre-processing schemes. It is
apparent from Figure 6(A) that Inceptionv3 is outperforming while DenseNet201 and ResNet18 are comparable in terms of
performance even though DenseNet201 is a very deep network compared to ResNet18 and the performance of SqueezeNet
was comparable to the significantly deeper network like DenseNet201. Interestingly, the performances of Inceptionv3,
ResNet18, and DenseNet201 are comparable in the case of CLAHE images and SqueezeNet also shows promising

performance. However, there is no notable performance improvement observed by this pre-processing scheme rather than
making the classification more or less network independent. Figure 6(C) shows that significant performance improvement
can be achieved using deeper networks with the complemented image while the performance degrades for ResNet18 and
especially for SqueezeNet. Figure 6(D) clearly depicts that the 3-channel scheme significantly improves the classification
performance of InceptionV3 and ResNet18; however, this is not the case for DenseNet201 and SqueezeNet. Interestingly,
these four pre-trained networks showed similar comparative accuracies while evaluated on ImageNet database (Mathworks,
2020).
Since InceptionV3 has become the top-performing network with the 3-channel scheme, we used this network to
investigate the role of different pre-processing schemes on the classification performance. Table 3 presents its performance
with the four pre-processing schemes used in this study. An interesting observation worth mentioning from the results is that
the networkâ€™s performance has significantly been degraded over original and especially CLAHE images. This basically
shows how crucial is the right pre-processing over the classification performance for this network. Moreover, different
preprocessing schemes could be concatenated to provide better classification performance.
Table 3: Comparison between different pre-processing schemes for InceptionV3, in term of per-class classification
Accuracy, Precision, Sensitivity, F1-score and Sensitivity.
Original
Accuracy

Precision

Sensitivity

F1-score

Specificity

COVID-19

92.86

93.68

94.54

94.08

90.28

MERS

94.86

88.48

86.08

87.24

97.12

SARS

96.57

91.25

91.03

90.98

97.88

Overall

93.87

92.15

92.13

92.14

92.93

CLAHE
COVID-19

90.56

90.17

94.79

92.4

84.11

MERS

92.85

89.22

74.19

80.72

97.67

SARS

95.71

88.23

89.54

88.71

97.17

Overall

92.04

89.6

89.56

89.58

89.43

Image complement
COVID-19

96.6

96.49

98.12

97.24

94.28

MERS

97.59

97.75

90.34

93.66

99.46

SARS

98.44

95.35

97.04

96.02

98.77

Overall

97.14

96.53

96.31

96.42

96.18

COVID-19

97.87

97.13

99.53

98.29

95.36

MERS

98.3

98.4

93.1

95.56

99.64

SARS
Overall

99.29
98.22

99.2
97.79

97.04
97.73

98.08
97.76

99.82
97.07

3-channel

Figure 6: Comparison of the AUC curve for all folds for 4 networks using original images (A), CLAHE images
(B), Complemented images (C), and 3-channel images (D).
Figure 7 shows the overall confusion matrix cumulated from all folds of InceptionV3 evaluation. It can be seen that
only two out of 423 COVID-19 images were misclassified, where one was misclassified as SARS, and the other was MERS.
However, it can be seen from Figure 8 (1 & 2) that these images are misclassified with 95% and 96% confidence level,
respectively. Therefore, the network was very confident about these false-negative cases. 4 SARS images were misclassified
while three images were misclassified as COVID-19 and one as MERS. Two SARS images were misclassified to COVID-19
with a high confidence level. However, one SARS classified to COVID-19 (Figure 8(5)) and other to MERS (Figure 8(6))
were not misclassified with high confidence level. 10 MERS images were misclassified to COVID-19 and all of them were
misclassified with a high confidence level and none was misclassified to SARS. However, it was found that in Figure 8, the
X-ray images (11), (12) and (7), (9), and (13) belong to the same subject taken on different days. Therefore, the total number
of misclassified MERS cases is actually 7. However, the network confidently classified them as COVID-19. It is, therefore,
very important to check the deep layer activation channel of these images and correctly classified images whether there can
be such co-existing patterns.

Figure 7: Confusion matrix of all folds for COVID-19, MERS, and SARS classification using InceptionV3.

Figure 8: Misclassified cases for COVID-19 (1-2), SARS (3-6), and MERS (7-16) by InceptionV3.
Finally, by using the class activation mapping (CAM) we investigate the reasoning for the misclassifications. CAM
yields a particular mapping that can grade the regions to their contribution to classification. For this purpose, we propose that
the strongest activation channel which can highlight the region of abnormality in the images is more useful than the CAM or
gradient-CAM.

Figure 9 shows three randomly chosen X-ray images from COVID-19, MERS, and SARS. They were classified
correctly and their strongest activation channel is in the 5th convolutional layer. It can be observed from Figure 8 that the 15th
neuron with the strongest activation can reveal the infected areas of the lungs for COVID-19 and MERS images. Even
though the pattern of COVID-19 and MERS are different, some overlapping features can still be observed. On the other
hand, the pattern is quite different for SARS images.

Figure 9: Three sample image cases for COVID-19 (1-3), MERS (4-6) and SARS (7-9) which were classified by
InceptionV3 with 100% confidence interval and their corresponding 15th channel image at 5th convolutional layer.
Figure 10(1) shows that even though it was a COVID-19 image, the feature map of the 15th neuron is similar to the
pattern of SARS and the network may, therefore, misclassified it as a SARS image. However, Figure 10(2) shows a COVID19 image misclassified as a MERS image, and it can be noticed from Figure 9(5) that the MERS image feature map in the
15th neuron is indeed similar to the feature map of the 15th neuron shown in Figure 10(2). Moreover, MERS images are
shown in Figure 10 (5 & 6) were also misclassified to COVID-19 due to the same reasoning. In short, we observed that there
are similarities between deep layer features of COVID-19 and MERS and this is the major reason for misclassification even
by the top-performing network, InceptionV3.

Figure 10: Sample misclassified cases for COVID-19, MERS and SARS which were missclassified by
InceptionV3 with more than 90% confidence level and their corresponding 15th channel image at 5th convolutional
layer.

5. Conclusion
In this study, we aim to investigate if it is possible to discriminate CoV family (COVID-19, MERS, and SARS) infestations

directly from the X-ray images and to evaluate recent state-of-the-art deep learning techniques for this purpose. To accomplish this
objective, we compiled the largest X-ray dataset encapsulating X-ray images from numerous countries (e.g. Italy, Spain,
China, etc.) and different X-ray machines. Consequently, the images are of different quality, resolution, and noise levels and
thus shows a high diversity. Due to the scarcity of the data, we then draw the focus on the transfer learning paradigm with
certain data augmentation and proposed four pre-processing schemes in order to improve the classification performance and
robustness. Four recent and state-of-the-art deep CNNs were evaluated. Several interesting observations can be made from
the results. First and foremost, the proposed pre-processing schemes can be useful particularly for some networks and can
improve network performance significantly. Especially, the 3-channel scheme yielded the overall best performance level for
the deep CNN, InceptionV3; however, it cannot be generalized for all networks. This basically shows that the performance
gain from a particular pre-processing scheme is both network- and problem-dependent. The top-performing network with the
3-channel pre-processing scheme has achieved the overall accuracy, precision, sensitivity, F1-score and specificity as,

98.2%, 97.8%, 97.7%, 97.8%, and 97.1% respectively. Such a high classification performance basically shows that COVID19, MERS and SARS can be discriminated directly from the X-ray images despite the fact that they all belong to the CoV
family. A close look at the failure cases reveals the fact that two COVID-19 images were misclassified to SARS and MERS
and 10 MERS images were misclassified to COVID-19. From the deep layer features, we observed that there are certain
similarities between COVID-19 and MERS, and this mainly confuses the network to misclassify those MERS images to
COVID-19 image. Even though COVID-19 is from the CoV family, it is clear from this study that the patterns in lungs due
to MERS and COVID-19 pneumonia are significantly different from SARS and therefore, no confusion has ever occurred
between MERS and SARS. Therefore, it can be concluded that X-ray images from SARS are significantly different from
MERS and COVID-19.

CRediT authorship contribution statement
Anas Tahir: Methodology, Software, Validation, Formal analysis, Writing - Review & Editing. Yazan Qiblawey: Data
Curation, Investigation, Resources, Writing - Original Draft, Writing - Review & Editing. Amith Khandakar: Methodology,
Visualization, Resources. Tawsifur Rahman: Methodology, Software, Data Curation, Writing - Original Draft. Uzair Khurshid:
Visualization, Resources, Writing - Original Draft. Farayi Musharavati: Writing - Review & Editing, Supervision, M. T. Islam: Writing
- Review & Editing, Supervision, Serkan Kiranyaz: Writing - Review & Editing, Supervision, Conceptualization. Muhammad E. H.
Chowdhury: Conceptualization, Writing - Review & Editing, Supervision, Project administration.

Acknowledgment
The authors would like to thank the Qatar National Research Fund (QNRF) for the grant (NPRP12S-0227-190164) to bear the
research personnel cost, which made this work possible.

References
Ai, T., Yang, Z., Hou, H., Zhan, C., Chen, C., Lv, W., Tao, Q., Sun, Z., Xia, L., 2020. Correlation of chest CT and
RT-PCR testing in coronavirus disease 2019 (COVID-19) in China: a report of 1014 cases. Radiology, 200642.
Bassi, P.R., Attux, R., 2020. A Deep Convolutional Neural Network for COVID-19 Detection Using Chest X-Rays.
arXiv preprint arXiv:2005.01578.
ChestImaging, 2020, COVID-19 CXR (all SARS-CoV-2 PCR+) from my hospital (Spain),
https://threadreaderapp.com/thread/1243928581983670272.html (accessed 15 of May, 2020)
Chowdhury, M.E., Rahman, T., Khandakar, A., Mazhar, R., Kadir, M.A., Mahbub, Z.B., Islam, K.R., Khan, M.S.,
Iqbal, A., Al-Emadi, N., 2020. Can AI help in screening viral and COVID-19 pneumonia? arXiv preprint arXiv:2003.13145.
Cohen, J.P., Morrison, P., Dao, L., 2020. COVID-19 image data collection. arXiv preprint arXiv:.11597.

CSSE, C.f.S.S.a.E., 2020, COVID-19 Dashboard by the Center for Systems Science and Engineering (CSSE) John
Hopkins University. https://coronavirus.jhu.edu/map.html (accessed 05th May, 2020)
Francesco, 2020, COVID-19, @radiopaedia. https://radiopaedia.org/playlists/25975?lang=us (accessed 22 of May,
2020)
Gao, H., Yao, H., Yang, S., Li, L., 2016. From SARS to MERS: evidence and speculation. Frontiers of medicine
10, 377-382.
Grinblat, L., Shulman, H., Glickman, A., Matukas, L., Paul, N., 2003. Severe Acute Respiratory Syndrome:
Radiographic Review of 40 Probable Cases in Toronto, Canada. Radiology 228, 802-809. 10.1148/radiol.2283030671.
https://doi.org/10.1148/radiol.2283030671
Hamed, A., Sobhy, A., Nassar, H., 2020. Accurate Classification of COVID-19 Based on Incomplete
Heterogeneous Data using a KNN Variant Algorithm. https://doi.org/10.21203/rs.3.rs-27186/v1+.
Hamimi, A., 2016. MERS-CoV: Middle East respiratory syndrome corona virus: Can radiology be of help? Initial
single center experience. The Egyptian Journal of Radiology and Nuclear Medicine 47, 95-106.
He, K., Zhang, X., Ren, S., Sun, J., 2016. Deep residual learning for image recognition, Proceedings of the IEEE
conference on computer vision and pattern recognition, pp. 770-778.
Horry, M.J., Paul, M., Ulhaq, A., Pradhan, B., Saha, M., Shukla, N., 2020. X-Ray Image based COVID-19
Detection using Pre-trained Deep Learning Models. engrxiv. https://doi.org/10.31224/osf.io/wx89s.
Huang, G., Liu, Z., Van Der Maaten, L., Weinberger, K.Q., 2017. Densely connected convolutional networks,
Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4700-4708.
Iandola, F.N., Han, S., Moskewicz, M.W., Ashraf, K., Dally, W.J., Keutzer, K., 2016. SqueezeNet: AlexNet-level
accuracy with 50x fewer parameters and< 0.5 MB model size. arXiv preprint arXiv:.07360.
Kumar, S., Maurya, V.K., Prasad, A.K., Bhatt, M.L., Saxena, S.K., 2020. Structural, glycosylation and antigenic
variation between 2019 novel coronavirus (2019-nCoV) and SARS coronavirus (SARS-CoV). VirusDisease, 1-9.
Li, L., Qin, L., Xu, Z., Yin, Y., Wang, X., Kong, B., Bai, J., Lu, Y., Fang, Z., Song, Q., 2020. Artificial intelligence
distinguishes COVID-19 from community acquired pneumonia on chest CT. Radiology, 200905.
Liu, J., Zheng, X., Tong, Q., Li, W., Wang, B., Sutter, K., Trilling, M., Lu, M., Dittmer, U., Yang, D., 2020.
Overlapping and discrete aspects of the pathology and pathogenesis of the emerging human pathogenic coronaviruses SARSâ€
CoV, MERSâ€CoV, and 2019â€nCoV. Journal of medical virology.
Lu, R., Zhao, X., Li, J., Niu, P., Yang, B., Wu, H., Wang, W., Song, H., Huang, B., Zhu, N., 2020. Genomic
characterisation and epidemiology of 2019 novel coronavirus: implications for virus origins and receptor binding. The
Lancet 395, 565-574.
Lu, Z.-M., Guo, S.-Z., 2017. Chapter 1 - Introduction, In: Lu, Z.-M., Guo, S.-Z. (Eds.), Lossless Information Hiding
in Images. Syngress, pp. 1-68.

Mahase, E., 2020. Coronavirus: covid-19 has killed more people than SARS and MERS combined, despite lower
case fatality rate. British Medical Journal Publishing Group.
Maini, R., Aggarwal, H., 2010. A comprehensive review of image enhancement techniques. arXiv preprint
arXiv:.1003.4053.
Mathworks, 2020, Pretrained Deep Neural Networks - MATLAB & Simulink,
https://www.mathworks.com/help/deeplearning/ug/pretrained-convolutional-neural-networks.html (accessed 22 of May,
2020)
Pizer, S.M., Amburn, E.P., Austin, J.D., Cromartie, R., Geselowitz, A., Greer, T., ter Haar Romeny, B.,
Zimmerman, J.B., Zuiderveld, K., 1987. Adaptive histogram equalization and its variations. Computer Vision, Graphics, and
Image Processing 39, 355-368. https://doi.org/10.1016/S0734-189X(87)80186-X.
http://www.sciencedirect.com/science/article/pii/S0734189X8780186X
Prompetchara, E., Ketloy, C., Palaga, T., 2020. Immune responses in COVID-19 and potential vaccines: Lessons
learned from SARS and MERS epidemic. Asian Pac J Allergy Immunol 38, 1-9.
Rangayyan, R.M., 2000. Introduction to Enhancement, In: Bankman, I.N. (Ed.), Handbook of Medical Imaging.
Academic Press, San Diego, pp. 1-2.
Rhee, J.-Y., Hong, G., Ryu, K.M., 2016. Clinical Implications of 5 Cases of Middle East Respiratory Syndrome
Coronavirus Infection in a South Korean Outbreak. Japanese Journal of Infectious Diseases 69, 361-366.
https://doi.org/10.7883/yoken.JJID.2015.445.
SIRM, 2020, COVID-19 Database, Italian Society of Medical and Interventional Radiology
https://www.sirm.org/en/italian-society-of-medical-and-interventional-radiology/ (accessed
Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., Wojna, Z., 2016. Rethinking the inception architecture for
computer vision, Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2818-2826.
W.H.O, 2019, Middle East respiratory syndrome coronavirus (MERS-CoV), World Health Organization.
https://www.who.int/news-room/q-a-detail/middle-east-respiratory-syndrome-coronavirus-(mers-cov) (accessed 15th May,
2020)
W.H.O, 2020, International travel and health infromation World Health Organization.
https://www.who.int/ith/diseases/sars/en/ (accessed 15th May, 2020)
Wang, L., Wong, A., 2020. COVID-Net: A tailored deep convolutional neural network design for detection of
COVID-19 cases from chest radiography images. arXiv preprint arXiv:2003.09871.
Wang, W., Xu, Y., Gao, R., Lu, R., Han, K., Wu, G., Tan, W., 2020. Detection of SARS-CoV-2 in Different Types
of Clinical Specimens. JAMA 323, 1843-1844. 10.1001/jama.2020.3786. https://doi.org/10.1001/jama.2020.3786
WHO, 2020, WHO Director-General's opening remarks at the media briefing on COVID-19 - 11 March 2020,
World Health Organization. https://www.who.int/dg/speeches/detail/who-director-general-s-opening-remarks-at-the-mediabriefing-on-covid-19---11-march-2020 (accessed 12 of May, 2020)
Xie, X., Li, X., Wan, S., Gong, Y., 2006. Mining x-ray images of SARS patients, Data Mining. Springer, pp. 282294.

Yan, Q., Wang, B., Gong, D., Luo, C., Zhao, W., Shen, J., Shi, Q., Jin, S., Zhang, L., You, Z., 2020. COVID-19
Chest CT Image Segmentation--A Deep Convolutional Neural Network Solution. arXiv preprint arXiv:2004.10987.
Zimmerman, J.B., Pizer, S.M., Staab, E.V., Perry, J.R., McCartney, W., Brenton, B.C., 1988. An evaluation of the
effectiveness of adaptive histogram equalization for contrast enhancement. IEEE Transactions on Medical Imaging 7, 304312. https://doi.org/10.1109/42.14513.

