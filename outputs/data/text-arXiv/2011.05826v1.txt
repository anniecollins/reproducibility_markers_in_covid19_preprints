A trial emulation approach for policy evaluations with group-level longitudinal data
Eli Ben-Michael1, Avi Feller1,2, and Elizabeth A. Stuart3
Forthcoming at Epidemiology

1 Department of Statistics, University of California, Berkeley
357 Evans Hall
Berkeley, CA 94720-3880
ebenmichael@berkeley.edu
2 Goldman School of Public Policy, University of California, Berkeley
2607 Hearst Avenue
Room 309
Berkeley, CA 94720
(510) 642-2067
afeller@berkeley.edu
3 Department of Mental Health, Johns Hopkins Bloomberg School of Public Health
624 N. Broadway
Room HH839
Baltimore, MD 21205 USA
410-502-6222
estuart@jhsph.edu

1) Eli Ben-Michael was supported by a National Science Foundation Research and Training
Grant #1745640, 2) Dr. Fellerâ€™s time was supported by the Institute of Education Sciences, U.S.
Department of Education, through Grant R305D200010. 3) Dr. Stuartâ€™s time was supported by
the National Institutes of Health through the RAND Center for Opioid Policy Tools and
Information Center (OPTIC; P50DA046351).
Replication data and code are available at https://github.com/ebenmichael/policy-trial-emulation.
We thank Elizabeth Stone, Ian Schmid, and Elena Badillo Goicoechea at Johns Hopkins for
editorial help and constructive comments.

Abstract
To limit the spread of the novel coronavirus, governments across the world implemented
extraordinary physical distancing policies, such as stay-at-home orders, and numerous studies aim
to estimate their effects. Many statistical and econometric methods, such as difference-indifferences, leverage repeated measurements and variation in timing to estimate policy effects,
including in the COVID-19 context. While these methods are less common in epidemiology,
epidemiologic researchers are well accustomed to handling similar complexities in studies of
individual-level interventions. â€œTarget trial emulationâ€ emphasizes the need to carefully design a
non-experimental study in terms of inclusion and exclusion criteria, covariates, exposure
definition, and outcome measurement â€” and the timing of those variables. We argue that policy
evaluations using group-level longitudinal (â€œpanelâ€) data need to take a similar careful approach
to study design, which we refer to as â€œpolicy trial emulation.â€ This is especially important when
intervention timing varies across jurisdictions; the main idea is to construct target trials separately
for each â€œtreatment cohortâ€ (states that implement the policy at the same time) and then aggregate.
We present a stylized analysis of the impact of state-level stay-at-home orders on total coronavirus
cases. We argue that estimates from panel methods â€” with the right data and careful modeling
and diagnostics â€” can help add to our understanding of many policies, though doing so is often
challenging.

1

Introduction
To limit the spread of the novel coronavirus, governments across the world implemented
extraordinary â€œnon-pharmaceutical interventions,â€ such as closing non-essential businesses and
imposing quarantines. The specific policies â€” and the decisions to lift them â€” varied widely, with
particularly dramatic variation within the United States.1 Learning about the impact of these
policies is important both to inform future policy decisions and to construct accurate forecasts of
the pandemic.

There are well-established epidemiologic methods for estimating the impact of an intervention that
occurs in a single location and at a single time point, dating back to John Snow and the cholera
epidemic in London.2 3 Often known as difference-in-differences, the basic approach constructs
counterfactual outcomes using group-level longitudinal data from the pre- and post-policy change
time periods and from localities (e.g., states) that did and didnâ€™t implement the policy. Variants of
this approach are known as panel data methods and include event studies and comparative
interrupted time series models.

There is no clear consensus in epidemiology, however, on how to proceed when many localities
implement a policy over time, sometimes known as staggered adoption. Fortunately,
epidemiologic researchers are well accustomed to handling similar complexities in studies of
individual-level interventions, with decades of research on strong non-experimental study designs,
including methods that handle confounding and variation in timing of treatment across individuals.
â€œTarget trial emulationâ€ emphasizes the need to carefully design a non-experimental study in terms

2

of inclusion and exclusion criteria, covariates, exposure definition, and outcome measurement â€”
and the timing of all of those variables.4 5

In this paper, we argue that policy evaluations using panel data need to take a similarly careful
approach to study design, which we refer to as â€œpolicy trial emulation.â€ The main idea is to
construct target trials separately for each â€œtreatment cohortâ€ (states that implement the policy at
the same time) and then aggregate.6 We illustrate this approach by presenting a stylized analysis
of the impact of state-level stay-at-home orders on total coronavirus cases. We believe this new
connection to trial emulation is an important conceptual advance, though the underlying statistical
methods we discuss are well-established,7 and many of these points have been made in other
contexts.8 We argue that estimates from panel methods â€” with the right data and careful modeling
and diagnostics â€” can help add to our understanding of policy impacts. The underlying
assumptions, however, are often strong and the application to COVID-19 anti-contagion policies
is particularly challenging.

The elements of â€œpolicy trial emulationâ€
We now describe key steps in conducting â€œpolicy trial emulation,â€ which are necessary and
obvious when designing a randomized trial and are becoming more common in the design of nonexperimental studies. We argue that these steps are just as important when evaluating policies with
aggregate longitudinal panel data.

We illustrate the key idea with a stylized policy evaluation: measuring the impact of US states
adopting a â€œshelter in placeâ€ or â€œstay-at-homeâ€ policy on COVID-19 case counts. These orders
urge or require citizens to remain at home except to conduct essential business, e.g., grocery
3

shopping or exercise; we use NYT Tracker to define policy enactment dates and obtain daily case
counts.9 No IRB approval was needed for this use of publicly available aggregate data.

Defining units and exposures
First, we must have a consistent definition of the exposure. Specifically, there is only one form of
treatment, and the outcome we see under a particular policy environment is equal to the potential
outcome under that policy environment.10 For stay-at-home policies, different states enacted
different requirements that broadly fall under this header, and the NYT definition is just one. This
introduces a trade-off. We could consider multiple types of treatment separately, such as closing
schools versus closing non-essential businesses. However, this greatly expands the dimensionality
of the specific exposure under consideration. Instead we consider â€œpackaging treatmentsâ€ together,
allowing for some variation in the specific implementation of the policy across units. As a result,
the estimated effect averages over different policy-specific effects within the data, which may be
less interpretable and may violate the consistency assumption.8 11

Second, there is growing evidence that stay-at-home orders had only modest impacts on individual
behavior â€” in many states, individuals reduced their mobility even in the absence of official policy
changes.12 Thus, we focus here on emulating an intent-to-treat (ITT) analysis, where individuals
are randomized to treatment conditions, but the amount of treatment actually received (e.g., the
dose) may differ across people. The ITT is often relevant for examining whether the policy is
effective overall, regardless of specific implementation. If we had data on, e.g., state-wide
implementation of the policy or the level of adherence as determined by mobility measures, we
could conduct an analysis analogous to a per protocol effect, e.g., estimating the effect of â€œfullâ€
4

policy implementation (i.e., all individuals following the stay at home guidelines).13 The trial
emulation framework helps clarify the additional assumptions necessary for conducting a per
protocol analysis in this context.

Finally, an important complication is reasoning about interactions between units, e.g. people with
COVID-19 traveling across state lines and spreading infection. The trial emulation framework
makes clear that we must pay close attention to this when defining our target trials: nearly all
standard tools for policy evaluation assume no interference between units, that is, a stateâ€™s outcome
only depends on that stateâ€™s intervention. Violations of this assumption, sometimes known as
spillovers or contagion, complicate the definitions of units and exposures and can lead to bias.14
Understanding and explicitly modeling these violations is paramount when studying policies to
control infectious diseases. For example, Holtz et al. use travel and social network information to
account for spillovers across states.15 We refer interested readers to the large literature on
observational causal inference with interference in general16 and on panel data methods in
particular.17

Causal contrasts of interest
After defining units and exposures, the next step in the trial emulation framework is to define the
estimand of interest.13 As discussed above, we focus on the ITT for treated states. Formally, let
ğ‘Š!" be an indicator that state i has a stay-at-home order at time t; and let ğ‘Œ!" be the corresponding
observed outcome. We can express the causal quantity of interest via potential outcomes:
ğ‘Œ!" (ğ‘Š!" = 1) is the outcome if the stay-at-home order is enacted, and ğ‘Œ!" (ğ‘Š!" = 0) is the outcome

5

if the order is not enacted. The causal contrast of interest is then a difference between these
potential outcomes, ğ‘Œ!" (1) âˆ’ ğ‘Œ!" (0), averaged over the states that implemented the policy and
over post-treatment time periods.7

We also focus on the impact of â€œturning onâ€ these policies, but of course states also turn them
â€œoff.â€ Just as in the individual exposure case, modeling individuals or locations turning exposures
on AND off is complex. If that seems ambitious in a trial setting it is often even more ambitious
in a non-experimental context!

Defining time zero
The next step in trial emulation is to define â€œtime zeroâ€, i.e., the point in time when individuals or
states would have been randomized to treatment conditions.6 This is crucial for clearly
distinguishing baseline (pre-treatment) measures from outcomes (post-treatment): inappropriately
conditioning on or selecting on post-treatment variables can cause as much bias (including
immortal time bias) as can confounding.5 18.

In standard target trial emulation, time zero is often defined based on when individuals meet the
specified eligibility criteria and is applied equally to both treated (exposed) and control
(unexposed) units. In policy trial emulation, states are often â€œeligibleâ€ to implement a policy at
any point, though this can also occur in standard target trial emulation.13 For treated states, we
typically use the date the policy is enacted as time zero; for comparison states, however, analysts
essentially need to identify the moment in time that a state â€œcould haveâ€ implemented the policy
but did not.
6

One option is to align states based on calendar time. For instance, below we focus on a target trial
for states that enact stay-at-home orders on March 23; we similarly set March 23 as â€œtime zeroâ€
for the comparison states. In the COVID setting, however, we might instead want to measure time
since the start of the pandemic in a specific location --- given the sudden emergence of the
pandemic, case counts are essentially undefined before this time. This presentation is in line with
many of the epidemiologic models.19 We refer to this as case time and, as an illustration, index
time by the number of days since the 10th confirmed case.

Figure 1 (left panel) shows the timing of statewide orders in calendar time beginning in mid-March
with â€œearly adoptersâ€ such as California, New Jersey, Illinois, and New York. Statewide orders
continued through early April, with â€œlate adoptersâ€ including Florida and Alabama. Several states,
including Iowa and Arkansas, never enacted a state-wide order. Figure 1 (right panel) shows the
timing in case time. From this perspective, early adopters, including West Virginia and Idaho,
enacted stay-at-home orders within days of the tenth case, while California â€” the first state to
enact a statewide stay-at-home order â€” was relatively late to do so.

7

Figure 1: Timing of statewide stay-at-home orders, by calendar date (left) and case time (right).
Calendar dates with fewer than 10 cases and case times after April 26th, 2020 are unshaded.

The choice of time zero will depend on the context. In the COVID setting, case time is more
consistent with models of infectious disease growth but is also more sensitive to measurement
error. Calendar time is more natural for accounting for â€œcommon shocksâ€ to states, such as changes
in Federal policy, which occur on a specific date. Moreover, for some specific questions, such as
impacts on employment outcomes, this distinction might not be relevant. For ease of exposition,
we focus on calendar time in the main text and give analogous results in case time in the Appendix.

Defining outcomes
The next step in policy trial emulation is to clearly define the outcomes, both the measures
themselves and their timing (ğ‘¡ in the notation above). In a typical trial, the outcomes might be

8

something like â€œmortality within 6 months.â€ In our COVID case study we focus on two different
outcome measures: (a) the (log) number of cases, and (b) the log-ratio of case counts from the
previous day. The first is a measure of the cumulative effect, while the latter is a measure of the
day-by-day changes in growth. We focus on log transformed data because exponential disease
growth can result in different pre-intervention trends on the raw outcome scale; we further discuss
the risk of pre-trends below, and present results for raw case counts and case growth in the
Appendix. Data quality is also a key concern. Care needs to be taken to select outcomes that can
be measured accurately; in particular, differential changes in the testing regime across states over
time can lead to the illusion of an effect. Finally, we focus on outcomes that are measured at the
aggregate (e.g., state) level, which is natural given the outcomes we examine. However, it is
sometimes more reasonable to consider targeting a cluster- (rather than individual-) randomized
trial in which treatment occurs at some aggregate level but outcome data are measured at the
individual level data.19 The same trial emulation principles apply, and hierarchical modeling can
be used to account for the multilevel structure.

Single target trial
We now describe a single target trial, and then turn to describing a sequence of nested target trials,
which more fully captures the staggered adoption setting. Here the goal is to estimate the impact
for a single cohort of states that enact a stay-at-home order at the same time. For now, we focus
on the five states that enacted the policy on March 23, 2020: Connecticut, Louisiana, Ohio, Oregon,
and Washington.

Selecting treated and comparison states

9

In a randomized trial, researchers have the â€œluxuryâ€ of knowing that the exposed and unexposed
units are similar on all baseline characteristics in expectation. This is not the case in nonexperimental studies such as policy evaluations. Thus, a key step in policy trial emulation is
carefully selecting comparison units. We discuss statistical approaches for adjusting for
differences in the next section.

An important consideration in selecting comparison units is the length of follow up â€” once a
comparison state enacts the treatment, it is no longer a useful comparison without strong modeling
assumptions. Only 19 days passed between when the first and last states enacted stay-at-home
orders, and if we compare the March 23 cohort to late-adopting states, we can observe effects for
at most 10 days. In general, across cohorts, the longer the follow up, the fewer available
comparison states.*

Due to the lag between virus exposure, symptoms, and testing, we expect stay-at-home-orders to
have delayed or gradual effects on the number of confirmed cases and case growth. How we define
the start of the â€œoutcomeâ€ time period (to then allow for different patterns of effects) is intimately
related to the question of defining time zero, discussed above. Therefore, we compare the treated
cohort to the eight â€œnever treatedâ€ states, allowing for estimates of longer-term effects.â€  In
principle, we could use â€œnot yet treatedâ€ states in the comparison group, dropping states from the
comparison group at the time they enact the policy. However, the set of â€œnot yet treatedâ€ states
will change throughout the follow up period. It may then be difficult to assess whether changes in

*

The choice of time scale is especially important if comparing with late adopters, since a state might be a valid
comparison in calendar time but not case time: Ohioâ€™s stay-at-home order was enacted after California in calendar
time but before California in case time.
â€ 
These are Arkansas, North Dakota, South Dakota, Iowa, Nebraska, Oklahoma, Wyoming, and Utah.

10

effects are merely due to the changing composition of the comparison states. Additionally, for each
set of comparison states at each follow up period we will need to perform the diagnostic checks
we describe below, potentially leading to an unwieldy number of diagnostics.

Estimating treatment effects
Once we have specified the target trial, the final stage is estimating the treatment effects and
evaluating diagnostics for underlying assumptions. While similar to standard trial emulation
contexts in many ways, there are particular nuances and complications in the policy trial emulation
setting given the longitudinal time-series nature of the data and the relatively small number of units
(e.g., 50 states).

We illustrate this setup with simple estimators, especially the canonical

â€œdifference-in-differencesâ€ estimator; we discuss alternative estimators below.

Table 1: Average log growth rate in daily case counts for the March 23 Cohort and the never
treated states (% day-over-day growth in parentheses). The pre-period is from March 8 to March
22; the post period is from March 23 to April 26.
Stay-at-Home Order
Pre

Post

March 23 Cohort

0.31 (37%)

0.09 (10%)

-0.22 (-20%)

Never Treated
Cohort

0.24 (27%)

0.10 (11%)

-0.14 (-12%)

+0.07 (+10%)

-0.01 (-1%)

-0.08 (-8%)

Difference

Difference

11

Difference-in-Differences Fundamentals. The basic building block of traditional panel data
estimation is â€œdifference-in-differencesâ€ (DiD). To build up to the DiD estimator, consider two
possible (flawed) comparisons. We could compare the growth rate in the March 23 cohort before
and after the stay-at-home order; Table 1 shows these growth rates, implying a decrease in the
average log growth rate of 0.22, a reduction of about 20 percentage points. However, this simple
comparison relies on the heroic assumption that the stay-at-home order is the only change affecting
the growth rate following March 23. Instead, we could directly compare the post March 23 growth
rates for the two cohorts. From March 23 to April 26 the treated statesâ€™ average log growth rate
was 0.01 (1 percentage point) lower than the never treated states. While this approach protects
against shared â€œshocksâ€ between the two cohorts â€” e.g., changes in national policy or testing â€”
it does not adjust for any differences in pre-intervention cases.

Difference-in-differences combines these two approaches. First take the pre/post estimate of a
decrease in the log growth rate by 0.22 in the March 23 cohort and compare it to a pre/post change
in the never treated states, a decrease of 0.14. Taking the difference of these differences (hence
â€œdifference-in-differencesâ€) yields an estimated reduction of the log growth rate by 0.08, or an 8percentage point decrease in the growth rate (Table 1). Formally, this estimator is:
+ $ = ,ğ‘Œ
----------- ----ğ·ğ¼ğ·
%$ âˆ’ ğ‘Œ&$ . âˆ’ (ğ‘Œ%' âˆ’ ğ‘Œ&' )
------where ğ‘Œ
&$ and ğ‘Œ%$ denote the pre- and post-treatment average outcomes for cohort ğ‘”, and ğ‘” =
âˆ denotes the never treated cohort.

12

Assumptions. The key to the differences-in-differences framework is a parallel counterfactual
trends assumption: loosely, in the absence of any treatment, the trends for the treated cohort would
be the same as the trends for the never-treated states, on average.3

20

This assumption is inherently

dependent on the outcome scale: if trends in log cases are equal, then trends in â€œrawâ€ case numbers
cannot also be equal. This assumption would be violated if:

â— Anticipation: behavior changes before the state-wide shutdown, since pre-time zero
measures would no longer truly be â€œpre-treatment.â€ In the case of stay-at-home orders,
there is strong evidence of such anticipatory behavior.12

â— Time-varying confounding: the policy implementation decision making process
depends on features other than baseline levels, i.e., the average level of the outcome of
interest in the baseline time period. For example, this would be violated if governors
enacted stay-at-home orders in response to trends in case counts.

Finally, this approach relies entirely on outcome modeling and therefore differs from many
common methods in epidemiology that instead model treatment, especially inverse probability of
treatment weighting (IPTW). Recent â€œdoubly robustâ€ implementations of difference-indifferences also incorporate IPTW and therefore rest on different assumptions than outcome
modeling alone, including the positivity assumption that all units have a non-zero probability in
being in each of the treatment conditions. Standard difference-in-difference models avoid
positivity by instead relying on a parametric outcome model that potentially extrapolates across
groups.7 22

13

Diagnostics and allowing effects to vary over time. The basic 2x2 table DiD estimator is a blunt
tool: it estimates the effect averaged over the entire post-treatment period. By combining various
2x2 DiD estimators we can estimate how effects phase in after March 23. First, we pick a common
reference date, often the time period immediately preceding treatment (here, March 22). Then for
every other time period we estimate the 2x2 DiD relative to that date. Concretely, to estimate the
effect k periods before/after treatment we compute the 2x2 estimator:
+ ($ = ,ğ‘Œ
------------- -----ğ·ğ¼ğ·
($ âˆ’ ğ‘Œ)%$ . âˆ’ (ğ‘Œ(' âˆ’ ğ‘Œ)%' ),
---where ğ‘Œ
($ is the average for cohort ğ‘”, ğ‘˜ periods before/after treatment. Figure 2 shows these
estimates for the March 23 cohort, sometimes known as event study plots,23 with uncertainty
quantified via a leave-one-unit-out jackknife.7

14

Figure 2: Difference-in-differences estimates for the effect of statewide stay-at-home orders on
log daily case growth and log cases for states in the March 23 cohort. Standard errors computed
via jackknife.

This procedure has several advantages. First, this provides a diagnostic of the parallel trends
+ ($ for ğ‘˜ < âˆ’1 (to the left of
assumption, similar in spirit to a balance check. The estimated ğ·ğ¼ğ·
the dotted line) are â€œplaceboâ€ estimates of the impact of treatment k periods before treatment is
enacted; if the parallel trends assumption holds, these should be close to zero. Although this is not
a direct test of the actual assumption (since that involves counterfactual outcomes in the postpolicy period) assessing the pre-period trends can be thought of as a proxy for evaluating the
assumption. As with all diagnostics, these are not a panacea: there is often limited statistical power
to detect differences, and noisy estimates around zero do not absolve researchers from making the
case for why the assumptions should hold.23

15

As we see in Figure 2, in the week prior to March 23 the placebo estimates are near zero, but two
weeks prior there is higher variance and some evidence that growth rates were systematically
higher in the treated cohort than the never treated cohort, relative to March 22. For log cases we
see even more stark violations of the parallel trends assumption. Relative to the never-treated
states, the March 23 cohort saw a larger increase in the number of cases, possibly evidence of timevarying confounding, although with such few units there is a large amount of uncertainty.

Finally, for ğ‘˜ â‰¥ 0 we estimate a different treatment effect for each period succeeding treatment,
without imposing any assumptions on how we expect the treatment effects to phase in or out. From
Figure 2 we see that in this single target trial there is insufficient precision to differentiate the
effects from zero, let alone to distinguish a trend.

Nested target trials
Selecting units
We now estimate the overall average impact by repeating the trial emulation approach for all 42
states that eventually adopt a stay-at-home order. As above, the first step is to divide treated states
into 17 cohorts based on adoption date. For each cohort, we then emulate a single target trial,
selecting the same eight never-treated states as comparisons for every target trial. Finally, we
aggregate results across these target trials.

16

These are nested target trials in the sense that each target trial can have a different starting point
and length of follow up.24 This approach is sometimes known as stacking or event-by-event
analysis in the econometrics literature.26 The specific approach we implement here is equivalent
to that in Abraham and Sun (2020) and Callaway and Santâ€™Anna (2019) without any covariates.9
7 22

Treatment effect estimation
In each single target trial, we estimate a series of two-period difference-in-differences estimates
for that cohort as in the previous section. There are many ways to aggregate these estimates across
cohorts.7 Here we aggregate based on days since treatment (sometimes called â€œevent timeâ€):
*

1
3( = 6 ğ‘›%$ ğ·ğ¼ğ·
3
ğ·ğ¼ğ·
($
ğ‘›%
$+%

where ğº is the total number of cohorts, ğ‘›1$ is the number of treated units in cohort ğ‘”, and ğ‘›1 is the
total number of treated states. We refer to these as nested estimates.

Figure 3 shows the estimates from this approach, both for the effect on log case growth and on log
cases. As with the single target trial, estimates to the right of zero are the treatment effects of
interest, and estimates to the left of zero are â€œplacebo estimatesâ€ of the impact of the treatment
prior to the treatment itself. For the left panel (log case growth), the placebo estimates for the ten
days before a state enacts a stay at home order are precisely estimated near zero; however, placebo
effects prior to this are highly variable and show a downward trend over time. This suggests
caution in interpreting the negative estimates to the right of zero. For the right panel (log cases),
the placebo estimates are even more starkly different from zero, suggesting that this would not be

17

an appropriate analysis and that the estimated effects are likely merely a reflection of these
differential trends.

Figure 3: Nested estimates of the impact of statewide stay-at-home orders on log cases and log
case growth, in calendar time. Standard errors estimated with the jackknife.

Discussion
Epidemiologists regularly confront settings where multiple jurisdictions adopt a policy over time
and the data available are aggregate longitudinal data on those and other jurisdictions. Policy trial
emulation provides a principled framework for estimating causal effects in this setting.

18

The specific approach we advocate is not new; there is a growing literature in statistics and
econometrics proposing robust methods for panel data. Here we show that these ideas fit naturally
into the trial emulation framework, especially the notion of aggregating across multiple target
trials. As a result, we can leverage recent methodological advances to enable more sophisticated
estimation that allows for looser assumptions (e.g., parallel trends conditioned on covariates),
including inverse propensity score weighting, doubly robust estimation, synthetic controls, and
matching. 7

27 28 29

We could also impose stronger modeling assumptions on the time series, e.g., a

linear trend, such as in Comparative Interrupted Time Series.30

One approach we caution against is the common practice of using regression to fit a longitudinal
model to all the data, with fixed effects for state and time. As with individual data with timevarying treatments and confounders, naive regression models can mask important issues. In
particular, it has been shown that the coefficient in this pooled model estimates a weighted average
over all possible 2x2 difference-in-differences estimates, where the weights can be negative.22
Moreover, some of these estimates are not in the spirit of trial emulation, e.g., by comparing two
states that are entirely post treatment. In practice, these complications mean that the sign of the
estimated effect can be flipped relative to the nested estimate. While some approaches, such as
â€œevent study models,â€ are less susceptible to this critique,23 we believe that the trial emulation
framework we outline here is more transparent and less prone to error, partly by being explicit
about all the causal contrasts.

The issues that we highlight are just some of many major challenges in estimating policy effects
more generally, including: differences in the characteristics of states that do and donâ€™t implement

19

the policy and challenges in identifying the timing of effects, including limited statistical power.31
The COVID-19 pandemic adds additional complexities to these policy evaluations.8 For example,
the disease transmission process, and the up to two-week lag in the time from exposure to
symptoms, makes it difficult to identify the precise timing of expected effects. Data on outcomes
of interest are also limited or problematic; for example, case rates need to be considered within the
context of the extent of testing.32 Finally, methods that do not account for spillovers and contagion
are likely to be biased in this setting, and so properly addressing interference is a key
methodological and practical concern.

These issues â€” and the strong underlying assumptions â€” suggest caution in using difference-indifference methods for estimating impacts of COVID-19 physical distancing policies. At the same
time, the policy trial emulation framework suggests a rubric by which we can assess the quality of
evidence presented in these studies. We anticipate that high-quality panel data methods will add
to our understanding of these policies, especially when considered alongside other sources of
evidence.

20

References
1. The New York Times. See How All 50 States Are Reopening. Accessed August 2, 2020.
https://www.nytimes.com/interactive/2020/us/states-reopen-map-coronavirus.html
2. Wing C, Simon K, Bello-Gomez RA. Designing Difference in Difference Studies: Best
Practices for Public Health Policy Research. Annual Review of Public Health 2018; 39:1,
453-469
3. Zeldow B, Hatfield LA. Confounding and Regression Adjustment in Difference-inDifferences. 2019; https://arxiv.org/abs/1911.12185 [3][24]
4. Danaei G, GarcÃ­a RodrÃ­guez LA, Cantero OF, Logan RW, HernÃ¡n MA. Electronic
medical records can be used to emulate target trials of sustained treatment strategies.
Journal of Clinical Epidemiology. 2018;96:12â€22. doi:10.1016/j.jclinepi.2017.11.021
5. Dickerman BA, GarcÃ­a-AlbÃ©niz X, Logan RW, Denaxas S, Hernan MA. Avoidable flaws
in observational analyses: an application to statins and cancer. Nature Medicine. 2019;
25, 1601â€“1606. https://doi.org/10.1038/s41591-019-0597-x [5][19]
6. HernÃ¡n MA, Robins JM. Per-Protocol Analyses of Pragmatic Trials. New England
Journal of Medicine. 2017;377(14):1391â€1398. doi:10.1056/NEJMsm1605385 [6][18]
7. Callaway B and Sant'Anna Pedro HC. Difference-in-Differences with Multiple Time
Periods (March 1, 2019). Available at SSRN: https://ssrn.com/abstract=3148250 or
http://dx.doi.org/10.2139/ssrn.3148250 [7] [17][26][28][32][33]
8. Goodman-Bacon A, Marcus J. Using Difference-in-Differences to Identify Causal Effects
of COVID-19 Policies. Unpublished, 2020. https://cdn.vanderbilt.edu/vu-my/wpcontent/uploads/sites/2318/2020/05/11154933/Covid-DD_v2.pdf [8][11][41]
9. The New York Times. Coronavirus (Covid-19) Data in the United States. Accessed
August 2, 2020. https://github.com/nytimes/covid-19-data/blob/master/README.md
10. Cole SR, Frangakis CE. The Consistency Statement in Causal Inference: A Definition or
an Assumption? Epidemiology: January 2009 - Volume 20 - Issue 1 - p 3-5 doi:
10.1097/EDE.0b013e31818ef366
11. HernÃ¡n MA. Does water kill? A call for less casual causal inferences. Ann. Epidemiol.
2016;26(10):674-80.

21

12. Goolsbee A, Syverson C. Fear, Lockdown, and Diversion: Comparing Drivers of
Pandemic Economic Decline 2020 NBER Working Paper. 2020; 27432. June JEL No.
E6, H7, L51
13. Danaei G, RodrÃ­guez LA, Cantero OF, Logan R, HernÃ¡n MA. Observational data for
comparative effectiveness research: An emulation of randomised trials of statins and
primary prevention of coronary heart disease. Stat Methods Med Res. 2013;22(1):70-96.
doi:10.1177/0962280211403603 [12][16][20].
14. Halloran ME, Hudgens MG. Dependent Happenings: A Recent Methodological Review.
Curr Epidemiol Rep. 2016;3(4):297-305. doi:10.1007/s40471-016-0086-4
15. Holtz D, Zhaoa M, Benzellb SG, et al. Interdependence and the Cost of Uncoordinated
Responses to COVID-19. Working paper, 2020.
http://ide.mit.edu/sites/default/files/publications/Interdependence_COVID_522.pdf
16. Ogburn EL, VanderWeele TJ. Vaccines, contagion, and social networks. Annals of
Applied Statistics. 2017;11-2, 919--948. doi:10.1214/17-AOAS1023.
https://projecteuclid.org/euclid.aoas/1500537729

17. Levesque L, Hanley J, Kezouh A, Suissa S. Problem of immortal time bias in cohort
studies: example using statins for preventing progression of diabetes. BMJ.
2010;340(mar12 1):b5087-b5087. doi:10.1136/bmj.b5087
18. Di Gennaro D, Pellegrini G. Policy Evaluation In Presence Of Interferences: A Spatial
Multilevel DiD Approach, CREI UniversitÃ  degli Studi Roma Tre. Working Paper. 2020;
0416.
19. Flaxman S, Mishra S, Gandy A, Unwin HJ, Coupland H, Mellan TA, et al. Estimating the
number of infections and the impact of non-pharmaceutical interventions on COVID-19
in 11 European countries. Imperial College COVID-19 Response Team. 2020; 13.
https://www.imperial.ac.uk/media/imperial-college/medicine/mrc-gida/2020-03-30COVID19-Report-13.pdf
20. Page LC, Lenard MA, and Keele L. The Design of Clustered Observational Studies in
Education. EdWorkingPaper. 2020; 20-234. Retrieved from Annenberg Institute at
Brown University: https://doi.org/10.26300/y9ek-px57
21. Angrist JD, Pischke JS. Mostly Harmless Econometrics: An Empiricist's Companion.
Economics Books, Princeton University Press, edition 1:8769; 2009.

22

22. Goodman-Bacon A. Difference-in-Differences with Variation in Treatment Timing.
National Bureau of Economic Research. Working Paper, 2018; 25018.
https://www.nber.org/papers/w25018
23. Abraham S, Liyang Sun L. Estimating Dynamic Treatment Effects in Event Studies with
Heterogeneous Treatment Effects. arXiv:1804.05785 [27][39]
24. Rambachan A, Roth J. An Honest Approach to Parallel Trends, 2019. Unpublished
https://scholar.harvard.edu/jroth/publications/Roth_JMP_Honest_Parallel_Trends
25. Cengiz D, Dube A, Lindner A, Zipperer B. The Effect of Minimum Wages on Low-Wage
Jobs, The Quarterly Journal of Economics. 2019; 134:3:1405â€“1454,
https://doi.org/10.1093/qje/qjz014
26. HernÃ¡n MA, Sauer BC, HernÃ¡ndez-DÃ­az S, Platt R, Shrier I. Specifying a target trial
prevents immortal time bias and other self-inflicted injuries in observational analyses.
Journal of Clinical Epidemiology. 2016; 79, 70â€“75.
https://doi.org/10.1016/j.jclinepi.2016.04.014
27. Arkhangelsky D, Athey S, Hirshberg DA, Imbens GW, Wager S. Synthetic Difference in
Differences. 2019; Working Paper https://arxiv.org/pdf/1812.09970.pdf
28. Ben-Michael E, Feller A, and Rothstein J. The Augmented Synthetic Control Method.
2020; arXiv:1811.04170
29. Daw JR, Hatfield LA. Matching and Regression to the Mean in Difference-in-Differences
Analysis.

Health

Services

Research.

2018;53(6):4138â€4156.

doi:10.1111/1475-

6773.12993
30. Health

Policy

Data

Science

Lab.

Difference-in-Differences.

2019;

https://diff.healthpolicydatascience.org Accessed online May 24, 2020.
31. Schell TL, Griffin BA, Andrew RM. Evaluating Methods to Estimate the Effect of State
Laws on Firearm Deaths: A Simulation Study. Santa Monica, CA: RAND Corporation;
2018. https://www.rand.org/pubs/research_reports/RR2685.html
32. Lachmann A, Jagodnik KM, Giorgi FM, Ray F. Correcting under-reported COVID-19
case numbers: estimating the true scale of the pandemic. Working Paper. medRxiv 2020;
20036178; doi: https://doi.org/10.1101/2020.03.14.20036178

23

Appendix Figures

Appendix Figure 1: Nested estimates of the impact of statewide stay-at-home orders on log cases
and log case growth, in case time. Standard errors estimated with the jackknife.

24

Appendix Figure 2: Nested estimates of the impact of statewide stay-at-home orders on raw case
counts, in both calendar and case time. Standard errors estimated with the jackknife.

25

Appendix Figure 3: Nested estimates of the impact of statewide stay-at-home orders on raw daily
case growth, in both calendar and case time. Standard errors estimated with the jackknife.

26

