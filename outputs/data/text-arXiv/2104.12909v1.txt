Algorithm is Experiment:
Machine Learning, Market Design, and Policy Eligibility Rules
Yusuke Narita

Kohei Yata‚àó

arXiv:2104.12909v1 [econ.EM] 26 Apr 2021

April 28, 2021

Abstract
Algorithms produce a growing portion of decisions and recommendations both in policy and business. Such algorithmic decisions are natural experiments (conditionally quasirandomly assigned instruments) since the algorithms make decisions based only on observable
input variables. We use this observation to develop a treatment-effect estimator for a class
of stochastic and deterministic algorithms. Our estimator is shown to be consistent and
asymptotically normal for well-defined causal effects. A key special case of our estimator
is a high-dimensional regression discontinuity design. The proofs use tools from differential
geometry and geometric measure theory, which may be of independent interest.
The practical performance of our method is first demonstrated in a high-dimensional
simulation resembling decision-making by machine learning algorithms. Our estimator has
smaller mean squared errors compared to alternative estimators. We finally apply our estimator to evaluate the effect of Coronavirus Aid, Relief, and Economic Security (CARES)
Act, where more than $10 billion worth of relief funding is allocated to hospitals via an
algorithmic rule. The estimates suggest that the relief funding has little effects on COVID19-related hospital activity levels. Naive OLS and IV estimates exhibit substantial selection
bias.

‚àó

Narita: Yale University, email: yusuke.narita@yale.edu. Yata: Yale University, email: kohei.yata@yale.edu.
We are especially indebted to Aneesha Parvathaneni and Richard Liu for expert research assistance. For their
suggestions, we are grateful to Tim Armstrong, Pat Kline and seminar participants at American Economic Association, Caltech, Columbia, CEMFI, Counterfactual Machine Learning Workshop, Econometric Society, European
Economic Association, Hitotsubashi, JSAI, Stanford, UC Irvine, the University of Tokyo, and Yale.

1

Introduction

Today‚Äôs society increasingly resorts to machine learning (‚ÄúAI‚Äù) algorithms for decision-making
and resource allocation. For example, judges in the US make legal judgements aided by predictions from supervised machine learning (descriptive regression). Supervised learning is also
used by governments to detect potential criminals and terrorists, and finance companies (such as
banks and insurance companies) to screen potential customers. Tech companies like Facebook,
Microsoft, and Netflix allocate digital content by reinforcement learning and bandit algorithms.
Uber and other ride sharing services adjust prices using their surge pricing algorithms to take
into account local demand and supply information. Retailers and e-commerce platforms engage
in algorithmic pricing. Similar algorithms are encroaching on increasingly high-stakes settings,
such as in healthcare and the military.
Other types of algorithms also loom large. School districts, college admissions systems,
and labor markets use matching algorithms for position and seat allocations. Objects worth
astronomical sums of money change hands every day in algorithmically run auctions ‚Äì not only
household objects, art and antiquities, but also securities, energy, and public procurements.
Many public policy domains like Medicaid often decide who are eligible based on algorithmic
rules.
All of the above, diverse examples share a common trait: a decision-making algorithm makes
decisions based only on its observable input variables. Conditional on the observable variables,
therefore, algorithmic treatment decisions are (quasi-)randomly assigned in the sense that they
are independent of any potential outcome or unobserved heterogeneity. This property turns
algorithm-based treatment decisions into instrumental variables (IVs) that can be used for measuring the causal effect of the final treatment assignment. The algorithm-based instrument may
produce regression-discontinuity-style local variation (e.g., machine judges), stratified randomization (e.g., several bandit and reinforcement learning algorithms), or some combination of the
two.
Based on the above observation, this paper shows how to use data obtained from algorithmic decision-making to identify and estimate causal effects. In our framework, the analyst
observes a random sample {(Yi , Xi , Di , Zi )}ni=1 , where Yi is the outcome of interest, Xi ‚àà Rp
is a vector of pre-treatment covariates (algorithm‚Äôs input variables), Di is the binary treatment
assignment, possibly made by humans, and Zi is the binary treatment recommendation made
by some algorithm. The treatment recommendation Zi is randomly determined based on the
known probability M L(Xi ) = Pr(Zi = 1|Xi ) independently of everything else conditional on
Xi . The central assumption is that the analyst knows function M L and is able to simulate it.
That is, the analyst is able to compute the recommendation probability M L(x) given any input
value x ‚àà Rp . The algorithm‚Äôs recommendation Zi may influence the final treatment assignment Di , determined as Di = Zi Di (1) + (1 ‚àí Zi )Di (0), where Di (z) is the potential treatment
assignment that would be realized if Zi = z. Finally, the observed outcome Yi is determined
as Yi = Di Yi (1) + (1 ‚àí Di )Yi (0), where Yi (1) and Yi (0) are potential outcomes that would be
realized if the individual were treated and not treated, respectively. This setup is an IV model
where the IV satisfies the conditional independence condition but may not satisfy the overlap

1

(full-support) condition. There appears to be no standard estimator for this setup.
Example. There is a rapidly growing trend of development and real-world implementation of
automated disease detection algorithms (Gulshan et al., 2016). Machine learning, in particular
deep learning, is used to detect various diseases and to predict patients at risk. Using our
framework described above, a detection algorithm predicts whether an individual i has a certain
disease (Zi = 1) or not (Zi = 0) based on a digital image Xi ‚àà Rp of a part of the individual‚Äôs
body, where each Xij ‚àà R denotes the intensity value of a pixel in the image. The algorithm
uses training data and machine learning (e.g., deep learning) to construct a binary classifier
M L : Rp ‚Üí {0, 1}. The classifier takes an image of individual i as input and makes a binary
prediction of whether the individual has the disease:
Zi ‚â° M L(Xi ).
The algorithm‚Äôs diagnosis Zi may influence the doctor‚Äôs treatment decision for the individual,
denoted by Di ‚àà {0, 1}. We are interested in how the treatment decision Di affects the individual‚Äôs outcome Yi .
Within this framework, we first characterize the whole sources of causal-effect identification
(quasi-experimental variation) for a class of algorithms, nesting both stochastic and deterministic
ones. This class includes all of the aforementioned examples, thus nesting existing insights on
quasi-experimental variation in particular algorithms, such as surge pricing (Cohen, Hahn, Hall,
Levitt and Metcalfe, 2016), bandit (Li, Chu, Langford and Schapire, 2010), reinforcement learning
(Precup, 2000), supervised learning (Cowgill, 2018; Bundorf, Polyakova and Tai-Seale, 2019),
and market-design algorithms (Abdulkadiroƒülu, Angrist, Narita and Pathak, 2017, Forthcoming;
Narita, 2020, 2021). Our framework also reveals new sources of identification for algorithms that,
at first sight, do not appear to produce a natural experiment.
The sources of causal-effect identification turn out to be summarized by a suitable modification of the Propensity Score (Rosenbaum and Rubin, 1983), which we call the Quasi Propensity
Score (QPS). The Quasi Propensity Score at covariate value x is the average probability of a
treatment recommendation in a shrinking neighborhood around x, defined as
R
‚àó
‚àó
B(x,Œ¥) M L(x )dx
ML
R
p (x) ‚â° lim
,
‚àó
Œ¥‚Üí0
B(x,Œ¥) dx
where B(x, Œ¥) is a p-dimensional ball with radius Œ¥ centered at x. Conditional on the Quasi
Propensity Score, algorithmic decisions are quasi-randomly assigned. The Quasi Propensity Score
provides an easy-to-check condition for what causal effects the data from an algorithm allow us
to identify; non-degeneracy of the Quasi Propensity Score is both sufficient and necessary for
identification of average causal effects conditional on covariates. In particular, we show that the
conditional local average treatment effect (LATE; Imbens and Angrist, 1994) is identified for the
subpopulation with nondegenerate Quasi Propensity Score.
Based on the identification analysis, we offer a way of estimating treatment effects using the
algorithm-produced data. The treatment effects can be estimated by two-stage least squares
2

(2SLS) where we regress the outcome on the treatment with the algorithm‚Äôs recommendation
as an IV.1 To make the algorithmic recommendation a conditionally independent IV, we need
to control for appropriate variables. Motivated by the fact that algorithmic decision IVs are
quasi-randomly assigned conditional on the Quasi Propensity Score, we propose controlling for
the Quasi Propensity Score as follows.
1. Standardize each characteristic Xij to have mean zero and variance one for each j = 1, ..., p,
where p is the number of input characteristics.
2. For small bandwidth Œ¥ > 0 and a large number of simulation draws S, compute
ps (Xi ; Œ¥) =

S
1X
‚àó
M L(Xi,s
),
S
s=1

‚àó , ..., X ‚àó are S independent simulation draws from the uniform distribution on
where Xi,1
i,S
B(Xi , Œ¥).2

3. Using the observations with ps (Xi ; Œ¥) ‚àà (0, 1), run the following 2SLS IV regression:
Di = Œ≥0 + Œ≥1 Zi + Œ≥2 ps (Xi ; Œ¥) + ŒΩi (First Stage)
Yi = Œ≤0 + Œ≤1 Di + Œ≤2 ps (Xi ; Œ¥) + i (Second Stage).
Let Œ≤ÃÇ1s be the estimated coefficient on Di .
As the main theoretical result, we prove the 2SLS estimator Œ≤ÃÇ1s is a consistent and asymptotically normal estimator of a well-defined causal effect (weighted average of conditional local average treatment effects). We also show that inference based on the conventional 2SLS
heteroskedasticity-robust standard errors is asymptotically valid as long as the bandwidth Œ¥ goes
to zero at an appropriate rate. We prove the asymptotic properties by exploiting results from
differential geometry and geometric measure theory. There appears to be no existing estimator
with these asymptotic properties even for a multidimensional RDD, a special case of our framework where the decision-making algorithm is deterministic and uses multiple input (running)
variables for determining treatment recommendations. Our estimator can therefore be considered as one of the few consistent and asymptotically normal estimators for a high-dimensional
RDD. Moreover, our asymptotic result applies to much more general settings with stochastic
algorithms, deterministic algorithms, and combinations of the two.
1

Recent empirical studies document that algorithmic treatment recommendations have impacts on final treatment assignment by humans (Cowgill, 2018; Bundorf et al., 2019).
2
For the bandwidth Œ¥, we suggest to the analyst to consider several different values and check if the 2SLS
estimates are robust to bandwidth changes, as we often do in regression discontinuity design (RDD) applications.
It is hard to pick Œ¥ in a data-driven way. Common methods for bandwidth selection in univariate RDDs include
Imbens and Kalyanaraman (2012) and Calonico, Cattaneo and Titiunik (2014), who estimate the bandwidth
that minimizes the asymptotic mean squared error (AMSE). It is not straightforward to estimate the AMSEoptimal bandwidth in our setting with many running variables and complex IV assignment, since it requires
nonparametric estimation of functions on the high-dimensional covariate space such as conditional mean functions,
their derivatives, the curvature of the RDD boundary, etc.

3

The practical value of our estimator is demonstrated through simulation and an original application. We first conduct a Monte Carlo simulation mimicking real-world decision-making based
on machine learning. We consider a data-generating process combining stochastic and deterministic algorithms. Treatment recommendations are randomly assigned for a small experimental
segment of the population and are determined by a deterministic machine learning algorithm for
the rest of the population. The deterministic algorithm uses high-dimensional predictors. Our
estimator is shown to have smaller mean squared errors compared to alternative estimators.
Our empirical application is an analysis of COVID-19 relief funding. The Coronavirus Aid,
Relief, and Economic Security (CARES) Act and Paycheck Protection Program designated $175
billion for COVID-19 response efforts and reimbursement to health care entities for expenses or
lost revenues (Kakani, Chandra, Mullainathan and Obermeyer, 2020), as ‚Äúfinancially insecure
hospitals may be less capable of investing in COVID-19 response efforts" (Khullar, Bond and
Schpero, 2020). We ask whether this problem is alleviated by the relief funding to hospitals.
We identify the causal effects of the relief funding by exploiting the funding eligibility rule.
The government employs an algorithmic rule to decide which hospitals are eligible for funding.
This fact allows us to apply our 2SLS with Quasi-Propensity-Score controls to estimate the effect
of relief funding. Specifically, 2SLS estimators use eligibility status as an instrumental variable
for funding amounts, while controlling for the Quasi Propensity Score induced by the eligibilitydetermining algorithm. The resulting estimates suggest that COVID-19 relief funding has little
effect on outcomes, such as the number of COVID-19 patients hospitalized and in ICU at each
hospital. The estimated effects of causal relief funding are much smaller and less significant
than the naive ordinary least squares (OLS) or 2SLS estimates with no controls. This finding
contributes to emerging work on how healthcare providers respond to financial shocks (Duggan,
2000; Dranove, Garthwaite and Ody, 2017).

Related Literature
Theoretically, our framework integrates the classic propensity-score (selection-on-observables)
scenario with a multidimensional extension of the RDD. We analyze this integrated setup in the
IV world with noncompliance. Our estimator applies to this general setting, which allows for
both stochastic IV assignment (the propensity-score scenario) and deterministic IV assignment
(the high-dimensional RDD). This general setting appears to have no prior established estimator.
Armstrong and Koles√°r (2020) provide an estimator for a related setting with perfect compliance.3
When we adapt our estimator to the sharp multidimensional RDD case, our estimator has
three features. First, it is a consistent and asymptotically normal estimator of a well-interpreted
causal effect (average of conditional treatment effects along the RDD boundary) even if treatment
effects are heterogeneous. Second, it uses observations near all the boundary points as opposed
to using only observations near one specific boundary point, which avoids variance explosion
3

Building on their prior work (Armstrong and Koles√°r, 2018), Armstrong and Koles√°r (2020) consider estimation and inference on average treatment effects under the assumption that the final treatment assignment is
independent of potential outcomes conditional on observables. Their estimator is not applicable to the IV world
we consider. Their method and our method also achieve different goals; their goal lies in finite-sample optimality
and asymptotically valid inference while our goal is to obtain consistency and asymptotic normality.

4

even when Xi is high dimensional. Third, it can be easily implemented even in cases with highdimensional data and complex algorithms (RDD boundaries), where identifying the decision
boundary from a general decision algorithm is hard. No prior estimator appears to have all of
these properties (Papay, Willett and Murnane, 2011; Zajonc, 2012; Wong, Steiner and Cook,
2013; Keele and Titiunik, 2015; Cattaneo, Titiunik, Vazquez-Bare and Keele, 2016; Imbens and
Wager, 2019). In Appendix A.1, we provide a detailed review of the most closely related papers
on the multidimensional RDD.
The Quasi Propensity Score used in this paper also shares its spirit with the local random
assignment interpretation of the RDD, discussed by Hahn, Todd and van der Klaauw (2001),
Fr√∂lich (2007), Cattaneo, Frandsen and Titiunik (2015), Cattaneo, Titiunik and Vazquez-Bare
(2017), Frandsen (2017), Sekhon and Titiunik (2017), Fr√∂lich and Huber (2019) and Abdulkadiroƒülu et al. (Forthcoming). These papers provide special cases of this paper‚Äôs framework.
Substantively, there are heated discussions about whether algorithmic decisions are ‚Äúbetter‚Äù
than human decisions, where ‚Äúbetter‚Äù is in terms of fairness and efficiency (Hoffman, Kahn and
Li, 2017; Horton, 2017; Kleinberg, Lakkaraju, Leskovec, Ludwig and Mullainathan, 2017). In
this study, we take a complementary perspective in that we take a decision algorithm as given, no
matter whether it is good or bad, and study how to use its produced data for impact evaluation.
This paper also relates to the emerging literature on the integration of machine learning,
causal inference, and the social sciences. While we are interested in machine learning as a
data-production tool, the existing literature (except the above mentioned strand) focuses on
machine learning as a data-analysis tool. For example, a set of predictive studies applies machine
learning to make predictions important for social policy questions (Kleinberg et al., 2017; Einav,
Finkelstein, Mullainathan and Obermeyer, 2018). Another set of causal and structural work
repurposes machine learning to aid with causal inference and structural econometrics (Athey and
Imbens, 2017; Belloni, Chernozhukov, Fern√°ndez-Val and Hansen, 2017; Bonhomme, Lamadon
and Manresa, 2019; Mullainathan and Spiess, 2017). We supplement these studies by highlighting
the role of machine learning as a data-production tool.

2

Framework

Our framework is a mix of the conditional independence, high-dimensional RDD, and instrumental variable scenarios. We are interested in the effect of some binary treatment Di ‚àà {0, 1}
on some outcome of interest Yi ‚àà R in the setup in the introduction. As is standard in the
literature, we impose the exclusion restriction that the treatment recommendation Zi ‚àà {0, 1}
does not affect the observed outcome other than through the treatment assignment Di . This
allows us to define the potential outcomes indexed against the treatment assignment Di alone.4
We consider algorithms that make treatment recommendations based solely on individual i‚Äôs
predetermined, observable covariates Xi = (Xi1 , ..., Xip )0 ‚àà Rp . Let the function M L : Rp ‚Üí
[0, 1] represent the decision algorithm, where M L(Xi ) = Pr(Zi = 1|Xi ) is the probability that
4

Formally, let Yi (d, z) denote the potential outcome that would be realized if i‚Äôs treatment assignment and
recommendation were d and z, respectively. The exclusion restriction assumes that Yi (d, 1) = Yi (d, 0) for d ‚àà {0, 1}
(Imbens and Angrist, 1994).

5

the treatment is recommended for individual i with covariates Xi .5 We assume that the analyst
knows the algorithm M L and is able to simulate it. That is, the analyst is able to compute the
recommendation probability M L(x) given any input value x ‚àà Rp . In typical machine-learning
scenarios, an algorithm first applies machine learning on Xi to make some prediction and then
uses the prediction to output the recommendation probability M L(Xi ). The treatment recommendation Zi for individual i is then randomly determined based on the probability M L(Xi )
independently of everything else. Consequently, the following conditional independence property holds regardless of how the algorithm is constructed and how the algorithm computes a
recommendation probability.
Property 1 (Conditional Independence). Zi ‚ä•
‚ä•(Yi (1), Yi (0), Di (1), Di (0))|Xi .
Let Yzi be defined as Yzi ‚â° Di (z)Yi (1) + (1 ‚àí Di (z))Yi (0) for z ‚àà {0, 1}. Yzi is the potential outcome when the treatment recommendation is Zi = z. It follows from Property 1 that
Zi ‚ä•
‚ä•(Y1i , Y0i )|Xi .
Note that the codomain of M L contains 0 and 1, allowing for deterministic treatment assignments conditional on Xi . Our framework therefore nests the RDD as a special case.6 Another
special case of our framework is the classic conditional independence scenario with the common
support condition (M L(Xi ) ‚àà (0, 1) almost surely). In addition to these simple settings, this
framework nests many other situations, such as multidimensional RDDs and complex machine
learning and market-design algorithms, as illustrated in Section 7.
We put a few assumptions on the covariates Xi and the M L algorithm. To simplify the
exposition, the main text assumes that the distribution of Xi is absolutely continuous with
respect to the Lebesgue measure. In practice, the input variables of an algorithm often include
discrete variables. Appendix A.3 extends the analysis and proposed method to the case where
some covariates in Xi are discrete. Let X be the support of Xi , X0 = {x ‚àà X : M L(x) = 0},
X1 = {x ‚àà X : M L(x) = 1}, Lp be the Lebesgue measure on Rp , and int(A) denote the interior
of a set A ‚äÇ Rp .
Assumption 1.
(a) (Almost Everywhere Continuity of M L) M L is continuous almost everywhere with respect
to the Lebesgue measure.
(b) (Measure Zero Boundaries of X0 and X1 ) Lp (Xk ) = Lp (int(Xk )) for k = 0, 1.
Assumption 1 (a) allows the function M L to be discontinuous on a set of points with the
Lebesgue measure zero. For example, M L is allowed to be a discontinuous step function as long
as it is continuous almost everywhere. Assumption 1 (b) holds if the Lebesgue measures of the
boundaries of X0 and X1 are zero.
5

We assume that the function M L is supported on Rp irrespective of the support of Xi .
Most of the existing studies on RDDs define the potential treatment assignment indexed against the running variable like Di (x), which represents the counterfactual treatment assignment the individual i would have
received if her running variable had been set to x. Unlike prior work, we define it indexed against the treatment
recommendation z.
6

6

3

Identification

What causal effects can be learned from data (Yi , Xi , Di , Zi ) generated by the M L algorithm?
A key step toward answering this question is what we call the Quasi Propensity Score (QPS).
To define it, let:
R
‚àó
‚àó
B(x,Œ¥) M L(x )dx
R
pM L (x; Œ¥) ‚â°
,
‚àó
B(x,Œ¥) dx
where B(x, Œ¥) = {x‚àó ‚àà Rp : kx ‚àí x‚àó k < Œ¥} is the (open) Œ¥-ball around x ‚àà X .7 Here, k ¬∑ k
denotes the Euclidean distance on Rp . To make common Œ¥ for all dimensions reasonable, we
normalize Xij to have mean zero and variance one for each j = 1, ..., p.8 We assume that M L is
a Lp -measurable function so that the integrals exist. We then define QPS as follows:
pM L (x) ‚â° lim pM L (x; Œ¥).
Œ¥‚Üí0

QPS at x is the average probability of a treatment recommendation in a shrinking ball around
x.9 We call this the Quasi Propensity Score, since this score modifies the standard propensity
score M L(Xi ) to incorporate local variation in the score. We discuss when QPS exists in Section
3.1.
Figure 1 illustrates QPS. In the example, Xi is two dimensional, and the support of Xi is
divided into three sets depending on the value of M L. For the interior points of each set, QPS
is equal to M L (as formally implied by Part 2 of Corollary 2 below). On the border of any two
sets, QPS is the average of the M L values in the two sets. Thus, pM L (x) = 21 (0 + 0.5) = 0.25
for any x in the open line segment AB, pM L (x) = 21 (0.5 + 1) = 0.75 for any x in the open line
segment BC, and pM L (x) = 12 (0 + 1) = 0.5 for any x in the open line segment BD.
QPS provides an easy-to-check condition for whether an algorithm allows us to identify causal
effects. Here we say that a causal effect is identified if it is uniquely determined by the joint
distribution of (Yi , Xi , Di , Zi ). Our identification analysis uses the following continuity condition.
Assumption 2 (Local Mean Continuity). For z ‚àà {0, 1}, the conditional expectation functions
E[Yzi |Xi ] and E[Di (z)|Xi ] are continuous at any point x ‚àà X such that pM L (x) ‚àà (0, 1) and
M L(x) ‚àà {0, 1}.
7

Whether we use an open ball or closed ball does not affect pM L (x; Œ¥). When we instead use a rectangle,
ellipsoid, or any standard kernel function to define pM L (x; Œ¥), the limit limŒ¥‚Üí0 pM L (x; Œ¥) may be different at some
points (e.g., at discontinuity points of M L), but the same identification results hold under suitable conditions.
We use a ball for simplicity and practicality.
8
This normalization is without loss of generality in the following sense. Take a vector Xi‚àó of any continuous
random variables and M L‚àó : Rp ‚Üí [0, 1]. The normalization induces the random vector Xi = A(Xi‚àó ‚àí E[Xi‚àó ]),
where A is a diagonal matrix with diagonal entries Var(X1‚àó )1/2 , ..., Var(X1‚àó )1/2 . Let M L(x) = M L‚àó (A‚àí1 x+E[Xi‚àó ]).
i1

ip

Then (Xi‚àó , M L‚àó ) is equivalent to (Xi , M L) in the sense that M L(Xi ) = M L‚àó (Xi‚àó ) for any individual i.
9
The idea behind QPS shares some of its spirit with the local randomization interpretation of RDDs (Fr√∂lich,
2007; Cattaneo et al., 2015, 2017): the treatment assignment is considered as good as randomly assigned in a
neighborhood of the cutoff.

7

Assumption 2 is a natural multivariate extension of the local mean continuity condition that is
frequently assumed in the RDD.10 M L(x) ‚àà {0, 1} means that the treatment recommendation Zi
is deterministic conditional on Xi = x. If QPS at the point x is nondegenerate (pM L (x) ‚àà (0, 1)),
however, there exists a point close to x that has a different value of M L from x‚Äôs, which creates
variation in the treatment recommendation near x. For any such point x, Assumption 2 requires
that the points close to x have similar conditional means of the outcome Yzi and treatment
assignment Di (z).11 Note that Assumption 2 does not require continuity of the conditional
means at x for which M L(x) ‚àà (0, 1), since the identification of the conditional means at such
points follows from Property 1 without continuity.
Under the above assumptions, the following identification result holds.
Proposition 1 (Identification). Under Assumptions 1 and 2:
(a) E[Y1i ‚àí Y0i |Xi = x] and E[Di (1) ‚àí Di (0)|Xi = x] are identified for every x ‚àà int(X ) such
that pM L (x) ‚àà (0, 1).12
(b) Let A be any open subset of X such that pM L (x) exists for all x ‚àà A. Then either E[Y1i ‚àí
Y0i |Xi ‚àà A] or E[Di (1) ‚àí Di (0)|Xi ‚àà A] or both are identified only if pM L (x) ‚àà (0, 1) for
almost every x ‚àà A (with respect to the Lebesgue measure).13
Proof. See Appendix C.1.
Proposition 1 characterizes a necessary and sufficient condition for identification. Part (a)
says that the average effects of the treatment recommendation Zi on the outcome Yi and on
the treatment assignment Di for the individuals with Xi = x are both identified if QPS at x is
neither 0 nor 1. Non-degeneracy of QPS at x implies that there are both types of individuals
who receive Zi = 1 and Zi = 0 among those whose Xi is close to x. Assumption 2 ensures that
these individuals are similar in terms of average potential outcomes and treatment assignments.
We can therefore identify the average effects conditional on Xi = x. In Figure 1, pM L (x) ‚àà (0, 1)
holds for any x in the shaded region (the union of the minor circular segment made by the chord
AC and the line segment BD).
10

In the RDD with a single running variable, the point x for which pM L (x) ‚àà (0, 1) and M L(x) ‚àà {0, 1} is the
cutoff point at which the treatment probability discontinuously changes.
11
In the context of the RDD with a single running variable, one sufficient condition for continuity of E[Yzi |Xi ] is
a local independence condition in the spirit of Hahn et al. (2001): (Yi (1), Yi (0), Di (1), Di (0)) is independent of Xi
near x. A weaker sufficient condition, which allows such dependence, is that E[Yi (d)|Di (1) = d1 , Di (0) = d0 , Xi ]
and Pr(Di (1) = d1 , Di (0) = d0 |Xi ) are continuous at x for every d ‚àà {0, 1} and (d1 , d0 ) ‚àà {0, 1}2 (Dong, 2018).
This assumes that the conditional means of the potential outcomes for each of the four types determined based
on the potential treatment assignment Di (z) and the conditional probabilities of those types are continuous at
the cutoff. These two sets of conditions are sufficient for continuity of E[Yzi |Xi ] regardless of the dimension of
Xi , accommodating multidimensional RDDs.
12
The causal effects may not be identified at a boundary point x of X for which pM L (x) ‚àà (0, 1). For example,
if M L(x‚àó ) = 1 for all x‚àó ‚àà B(x, Œ¥) ‚à© X and M L(x‚àó ) = 0 for all x‚àó ‚àà B(x, Œ¥) \ X for any sufficiently small Œ¥ > 0,
pM L (x) ‚àà (0, 1) but the causal effects are not identified at x since Pr(Zi = 0|Xi ‚àà B(x, Œ¥)) = 0.
13
We assume that pM L is a Lp -measurable function so that {x ‚àà A : pM L (x) = 0} and {x ‚àà A : pM L (x) = 1}
are Lp -measurable.

8

R
A consequence of Part (a) is that it is possible to identify {x‚àó ‚ààint(X ):pM L (x‚àó )‚àà(0,1)} œâ(x)E[Y1i ‚àí
R
Y0i |Xi = x]d¬µ(x) and {x‚àó ‚ààint(X ):pM L (x‚àó )‚àà(0,1)} œâ(x)E[Di (1) ‚àí Di (0)|Xi = x]d¬µ(x) for any known
or identified function œâ : Rp ‚Üí R and any measure ¬µ provided that the integrals exist.
Part (a) nests two well-known identification results as special cases. First, suppose that
M L(x) ‚àà (0, 1) for every x ‚àà X . This corresponds to the classic conditional independence
setting (or stratified randomization setting) with nondegenerate assignment probability, in which
conditional average causal effects are identified (see for example Angrist and Pischke (2008)).
Second, suppose that M L(x) ‚àà {0, 1} for all x ‚àà X but the value of M L discontinuously changes
at some point x‚àó so that pM L (x‚àó ) ‚àà (0, 1). This case corresponds to an RDD, in which the
average causal effect at a boundary point is identified under continuity of conditional expectation
functions of potential outcomes (Hahn et al., 2001; Keele and Titiunik, 2015).
Part (b) provides a necessary condition for identification. It says that if the average effect
of the treatment recommendation conditional on Xi being in some open set A is identified, then
we must have pM L (x) ‚àà (0, 1) for almost every x ‚àà A. If, to the contrary, there is a subset of
A of nonzero measure for which pM L (x) = 1 (or pM L (x) = 0), then Zi has no variation in the
subset, which makes it impossible to identify the average effect for the subset.
Proposition 1 concerns causal effects of treatment recommendation, not of treatment assignment. The proposition implies that the conditional average treatment effects and the conditional
local average treatment effects (LATEs) are identified under additional assumptions.
Corollary 1 (Perfect and Imperfect Compliance). Under Assumptions 1 and 2:
(a) The average treatment effect conditional on Xi = x, E[Yi (1) ‚àí Yi (0)|Xi = x], is identified
for every x ‚àà int(X ) such that pM L (x) ‚àà (0, 1) and Pr(Di (1) > Di (0)|Xi = x) = 1 (perfect
compliance).
(b) Let A be any open subset of X such that pM L (x) exists for all x ‚àà A, and Pr(Di (1) >
Di (0)|Xi ‚àà A) = 1. Then E[Yi (1) ‚àí Yi (0)|Xi ‚àà A] is identified only if pM L (x) ‚àà (0, 1) for
almost every x ‚àà A.
(c) The local average treatment effect conditional on Xi = x, E[Yi (1)‚àíYi (0)|Di (1) 6= Di (0), Xi =
x], is identified for every x ‚àà int(X ) such that pM L (x) ‚àà (0, 1), Pr(Di (1) ‚â• Di (0)|Xi =
x) = 1 (monotonicity), and Pr(Di (1) 6= Di (0)|Xi = x) > 0 (existence of compliers).
(d) Let A be any open subset of X such that pM L (x) exists for all x ‚àà A, Pr(Di (1) ‚â• Di (0)|Xi ‚àà
A) = 1, and Pr(Di (1) 6= Di (0)|Xi ‚àà A) > 0. Then E[Yi (1) ‚àí Yi (0)|Di (1) 6= Di (0), Xi ‚àà A]
is identified only if pM L (x) ‚àà (0, 1) for almost every x ‚àà A.
Proof. See Appendix C.2.
Non-degeneracy of QPS pM L (x) therefore summarizes what causal effects the data from M L
identify. Note that the key condition (pM L (x) ‚àà (0, 1)) holds for some points x for every standard
algorithm except trivial algorithms that always recommend a treatment with probability 0 or 1.
Therefore, the data from almost every algorithm identify some causal effect.

9

3.1

Existence of the Quasi Propensity Score

The above results assume that QPS exists, but is it fair to assume so? In general, QPS may fail
to exist; we provide such an example in Appendix A.2. Nevertheless, it exists for most covariate
points and typical M L algorithms. For each x ‚àà X and each q ‚àà Supp(M L(Xi )), define
Ux,q ‚â° {u ‚àà B(0, 1) : lim M L(x + Œ¥u) = q},
Œ¥‚Üí0

where 0 ‚àà Rp is a vector of zeros. Ux,q is the set of vectors in B(0, 1) such that the value of M L
approaches q as we approach x from the direction of the vector. With this notation, we obtain
a sufficient condition for the existence of QPS at a point x.
Proposition 2. Take any x ‚àà X . If there exists a countable set Q ‚äÇ Supp(M L(Xi )) such that
Lp (‚à™q‚ààQ Ux,q ) = Lp (B(0, 1)) and Ux,q is Lp -measurable for all q ‚àà Q, then pM L (x) exists and is
given by
P
p
q‚ààQ qL (Ux,q )
ML
p (x) =
.
Lp (B(0, 1))
Proof. See Appendix C.3.
If almost every point in B(0, 1) is contained by one of countably many Ux,q ‚Äôs, therefore, QPS
exists and is equal to the weighted average of the values of q with the weight proportional to the
hypervolume of Ux,q . This result implies that QPS exists in practically important cases.
Corollary 2.
1. (Continuity points) If M L is continuous at x ‚àà X , then pM L (x) exists and pM L (x) =
M L(x).
2. (Interior points) Let Xq = {x ‚àà X : M L(x) = q} for some q ‚àà [0, 1]. Then, for any interior
point x ‚àà int(Xq ), pM L (x) exists and pM L (x) = q.
3. (Smooth boundary points) Suppose that {x ‚àà X : M L(x) = q1 } = {x ‚àà X : f (x) ‚â• 0} and
{x ‚àà X : M L(x) = q2 } = {x ‚àà X : f (x) < 0} for some q1 , q2 ‚àà [0, 1], where f : Rp ‚Üí R.
Let x ‚àà X be a boundary point such that f (x) = 0, and suppose that f is continuously
differentiable in a neighborhood of x with ‚àáf (x) = ( ‚àÇf‚àÇx(x)
, ..., ‚àÇf‚àÇx(x)
)0 6= 0. In this case,
p
1
pM L (x) exists and pM L (x) = 21 (q1 + q2 ).
4. (Intersection points under CART and random forests) Let p = 2, and suppose that {x ‚àà X :
M L(x) = q1 } = {(x1 , x2 )0 ‚àà X : x1 ‚â§ 0 or x2 ‚â§ 0}, {x ‚àà X : M L(x) = q2 } = {(x1 , x2 )0 ‚àà
X : x1 > 0, x2 > 0}, and 0 = (0, 0)0 ‚àà X . This is an example in which tree-based algorithms
such as Classification And Regression Tree (CART) and random forests are used to create
M L. In this case, pM L (0) exists and pM L (0) = 43 q1 + 14 q2 .
Proof. See Appendix C.4.

10

4

Estimation

The sources of quasi-random assignment characterized in Proposition 1 suggest a way of estimating causal effects of the treatment. In view of Proposition 1, it is possible to nonparametrically
estimate conditional average causal effects E[Y1i ‚àí Y0i |Xi = x] and E[Di (1) ‚àí Di (0)|Xi = x] for
points x such that pM L (x) ‚àà (0, 1). This approach is hard to use in practice, however, when Xi
is high dimensional.
We instead seek an estimator that aggregates conditional effects at different points into a single average causal effect. Proposition 1 suggests that conditioning on QPS makes algorithm-based
treatment recommendation quasi-randomly assigned. This motivates the use of an algorithm‚Äôs
recommendation as an instrument conditional on QPS, which we operationalize as follows.

4.1

Two-Stage Least Squares Meets QPS

Suppose that we observe a random sample {(Yi , Xi , Di , Zi )}ni=1 of size n from the population
whose data generating process is described in the introduction and Section 2. Consider the
following 2SLS regression using the observations with pM L (Xi ; Œ¥n ) ‚àà (0, 1):
Di = Œ≥0 + Œ≥1 Zi + Œ≥2 pM L (Xi ; Œ¥n ) + ŒΩi
Yi = Œ≤0 + Œ≤1 Di + Œ≤2 p

ML

(Xi ; Œ¥n ) + i ,

(1)
(2)

where bandwidth Œ¥n shrinks toward zero as the sample size n increases. Let Ii,n = 1{pM L (Xi ; Œ¥n ) ‚àà
(0, 1)}, Di,n = (1, Di , pM L (Xi ; Œ¥n ))0 , and Zi,n = (1, Zi , pM L (Xi ; Œ¥n ))0 . The 2SLS estimator Œ≤ÃÇ is
then given by
n
n
X
X
Œ≤ÃÇ = (
Zi,n D0i,n Ii,n )‚àí1
Zi,n Yi Ii,n .
i=1

i=1

Let Œ≤ÃÇ1 denote the 2SLS estimator of Œ≤1 in the above regression.
The above regression uses true QPS pM L (Xi ; Œ¥n ), but it may be difficult to analytically
compute if M L is complex. In such a case, we propose to approximate pM L (Xi ; Œ¥n ) using brute
force simulation. We draw a value of x from the uniform distribution on B(Xi , Œ¥n ) a number
of times, compute M L(x) for each draw, and take the average of M L(x) over the draws.14
‚àó , ..., X ‚àó
Formally, let Xi,1
i,Sn be Sn independent draws from the uniform distribution on B(Xi , Œ¥n ),
and calculate
Sn
1 X
s
‚àó
M L(Xi,s
).
p (Xi ; Œ¥n ) =
Sn
s=1

We compute
for each i = 1, ..., n independently across i so that ps (X1 ; Œ¥n ), ..., ps (Xn ; Œ¥n )
are independent of each other. For fixed n and Xi , the approximation error relative to true
‚àö
pM L (Xi ; Œ¥n ) has a 1/ Sn rate of convergence.15 This rate does not depend on the dimension of
Xi , so the simulation error can be made negligible even when Xi is high dimensional.
ps (Xi ; Œ¥n )

14

See Appendix A.5 for how to efficiently sample from the uniform
distribution on a p-dimensional ball.
‚àö
More precisely, we have |ps (Xi ; Œ¥n ) ‚àí pM L (Xi ; Œ¥n )| = Ops (1/ Sn ), where Ops indicates the stochastic boundedness in terms of the probability distribution of the Sn simulation draws.
15

11

Now consider the following simulation version of the 2SLS regression using the observations
with ps (Xi ; Œ¥n ) ‚àà (0, 1):
Di = Œ≥0 + Œ≥1 Zi + Œ≥2 ps (Xi ; Œ¥n ) + ŒΩi
s

Yi = Œ≤0 + Œ≤1 Di + Œ≤2 p (Xi ; Œ¥n ) + i .

(3)
(4)

Let Œ≤ÃÇ1s denote the 2SLS estimator of Œ≤1 in the simulation-based regression. This regression is
the same as the 2SLS regression (1) and (2) except that we use the simulated QPS ps (Xi ; Œ¥n ) in
place of pM L (Xi ; Œ¥n ).16

4.2

Consistency and Asymptotic Normality

We establish the consistency and asymptotic normality of the 2SLS estimators Œ≤ÃÇ1 and Œ≤ÃÇ1s . Our
consistency and asymptotic normality result uses the following assumptions.
Assumption 3.
(a) (Finite Moment) E[Yi4 ] < ‚àû.
(b) (Nonzero First Stage) There exists a constant c > 0 such that E[Di (1) ‚àí Di (0)|Xi = x] > c
for every x ‚àà X such that pM L (x) ‚àà (0, 1).
(c) (Nonzero Conditional Variance) If Pr(M L(Xi ) ‚àà (0, 1)) > 0, then Var(M L(Xi )|M L(Xi ) ‚àà
(0, 1)) > 0.
If Pr(M L(Xi ) ‚àà (0, 1)) = 0, then the following conditions (d)‚Äì(g) hold.
(d) (Nonzero Variance) Var(M L(Xi )) > 0.
For a set A ‚äÇ Rp , let cl(A) denote the closure of A and let ‚àÇA denote the boundary of A,
i.e., ‚àÇA = cl(A) \ int(A).
(e) (C 2 Boundary of ‚Ñ¶‚àó ) There exists a partition {‚Ñ¶‚àó1 , ..., ‚Ñ¶‚àóM } of ‚Ñ¶‚àó = {x ‚àà Rp : M L(x) = 1}
(the set of the covariate points whose M L value is one) such that
(i) dist(‚Ñ¶‚àóm , ‚Ñ¶‚àóm0 ) > 0 for any m, m0 ‚àà {1, ..., M } such that m 6= m0 . Here dist(A, B) =
inf x‚ààA,y‚ààB kx ‚àí yk is the distance between two sets A and B ‚äÇ Rp ;
(ii) ‚Ñ¶‚àóm is nonempty, bounded, open, connected and twice continuously differentiable for
each m ‚àà {1, ..., M }. Here we say that a bounded open set A ‚äÇ Rp is twice continuously
differentiable if for every x ‚àà A, there exists a ball B(x, ) and a one-to-one mapping
16

In many industry and policy applications, the analyst is only able to change the algorithm‚Äôs recommendation
Zi by redesigning the algorithm. In this case, the effect of recommendation Zi on outcome Yi may also be of
interest to the practitioner. We can estimate the effect of recommendation by running the following ordinary least
squares (OLS) regression using the observations with ps (Xi ; Œ¥) ‚àà (0, 1):
Yi = Œ±0 + Œ±1 Zi + Œ±2 ps (Xi ; Œ¥) + ui .
The estimated coefficient on Zi , Œ±ÃÇ1s , is our preferred estimator of the recommendation effect.

12

œà from B(x, ) onto an open set D ‚äÇ Rp such that œà and œà ‚àí1 are twice continuously
differentiable, œà(B(x, ) ‚à© A) ‚äÇ {(x1 , ..., xp ) ‚àà Rp : xp > 0} and œà(B(x, ) ‚à© ‚àÇA) ‚äÇ
{(x1 , ..., xp ) ‚àà Rp : xp = 0}.
Let fX denote the probability density function of Xi and let Hk denote the k-dimensional
Hausdorff measure on Rp .17
(f ) (Regularity of Deterministic M L)
R
(i) Hp‚àí1 (‚àÇ‚Ñ¶‚àó ) < ‚àû, and ‚àÇ‚Ñ¶‚àó fX (x)dHp‚àí1 (x) > 0.
(ii) There exists Œ¥ > 0 such that M L(x) = 0 for almost every x ‚àà N (X , Œ¥) \ ‚Ñ¶‚àó , where
N (A, Œ¥) = {x ‚àà Rp : kx ‚àí yk < Œ¥ for some y ‚àà A} for a set A ‚äÇ Rp and Œ¥ > 0.
(g) (Conditional Moments and Density near ‚àÇ‚Ñ¶‚àó ) There exists Œ¥ > 0 such that
(i) E[Y1i |Xi ], E[Y0i |Xi ], E[Di (1)|Xi ], E[Di (0)|Xi ] and fX are continuously differentiable
and have bounded partial derivatives on N (‚àÇ‚Ñ¶‚àó , Œ¥);
(ii) E[Y1i2 |Xi ], E[Y0i2 |Xi ], E[Y1i Di (1)|Xi ] and E[Y0i Di (0)|Xi ] are continuous on N (‚àÇ‚Ñ¶‚àó , Œ¥);
(iii) E[Yi4 |Xi ] is bounded on N (‚àÇ‚Ñ¶‚àó , Œ¥).
Assumption 3 is a set of conditions for establishing consistency. Assumption 3 (b) assumes
that, conditional on each value of Xi for which QPS is nondegenerate, more individuals would
change their treatment assignment status from 0 to 1 in response to treatment recommendation
than would change it from 1 to 0.18 Under this assumption, the estimated first-stage coefficient on
Zi converges to a positive quantity. Note that, if there exists c < 0 such that E[Di (1)‚àíDi (0)|Xi =
x] < c for every x ‚àà X with pM L (x) ‚àà (0, 1), changing the labels of treatment recommendation
makes Assumption 3 (b) hold.
Assumption 3 (c) rules out potential multicollinearity. If the support of M L(Xi ) contains
only one value in (0, 1), pM L (Xi ; Œ¥n ) is asymptotically constant and equal to M L(Xi ) conditional on pM L (Xi ; Œ¥n ) ‚àà (0, 1), resulting in the multicollinearity between pM L (Xi ; Œ¥n ) and the
constant term. Although dropping the constant term from the 2SLS regression solves this issue,
Assumption 3 (c) allows us to only consider the regression with a constant for the purpose of
simplifying the presentation. In Appendix C.6, we provide 2SLS estimators that are consistent
and asymptotically normal even if we do not know whether Assumption 3 (c) holds.
Assumption 3 (d)‚Äì(g) are a set of conditions we require for proving consistency and asymptotic normality of Œ≤ÃÇ1 when M L is deterministic and produces only multidimensional regressiondiscontinuity variation. Assumption 3 (d) says that M L produces variation in the treatment
recommendation.
17

The k-dimensional Hausdorff measure on Rp is defined as follows. Let Œ£ be the Lebesgue œÉ-algebra on Rp
P
k
(the set of all Lebesgue measurable sets on Rp ). For A ‚àà Œ£ and Œ¥ > 0, let HŒ¥k (A) = inf{ ‚àû
j=1 d(Ej ) : A ‚äÇ
‚àû
p
‚à™j=1 Ej , d(Ej ) < Œ¥, Ej ‚äÇ R for all j}, where d(E) = sup{kx ‚àí yk : x, y ‚àà E}. The k-dimensional Hausdorff
measure of A on Rp is Hk (A) = limŒ¥‚Üí0 HŒ¥k (A).
18
At the cost of making the presentation more complex, the assumption can be relaxed so that the sign of
E[Di (1) ‚àí Di (0)|Xi = x] is allowed to vary over x with pM L (x) ‚àà (0, 1).

13

Assumption 3 (e) imposes the differentiability of the boundary of ‚Ñ¶‚àó = {x ‚àà Rp : M L(x) =
1}. The conditions are satisfied if, for example, ‚Ñ¶‚àó = {x ‚àà Rp : f (x) ‚â• 0} for some twice
continuously differentiable function f : Rp ‚Üí R such that ‚àáf (x) 6= 0 for all x ‚àà Rp with f (x) = 0.
‚Ñ¶‚àó takes this form, for example, when the conditional treatment effect E[Yi (1) ‚àí Yi (0)|X] is
predicted by supervised learning based on smooth models such as lasso and ridge regressions, and
treatment is recommended to individuals who are estimated to experience nonnegative treatment
effects.
In general, the differentiability of ‚Ñ¶‚àó may not hold. For example, if tree-based algorithms
such as Classification And Regression Tree (CART) and random forests are used to predict the
conditional treatment effect, the predicted conditional treatment effect function is not differentiable at some points. Although the resulting ‚Ñ¶‚àó does not exactly satisfy Assumption 3 (e), the
assumptions approximately hold in that ‚Ñ¶‚àó is arbitrarily well approximated by a set that satisfies
the differentiability condition.19
Part (i) of Assumption 3 (f) says that the boundary of ‚Ñ¶‚àó is (p ‚àí 1) dimensional and that
the boundary has nonzero density. Part (ii) puts a weak restriction on the values M L takes on
outside the support of Xi . It requires that M L(x) = 0 for almost every x ‚àà
/ ‚Ñ¶‚àó that is outside
X but is in the neighborhood of X . M L(x) may take on any value if x is not close to X . These
conditions hold in practice.20
Assumption 3 (g) imposes continuity, continuous differentiability and boundedness on the
conditional moments of potential outcomes and the probability density near the boundary of ‚Ñ¶‚àó .
When M L is stochastic, asymptotic normality requires additional assumptions. Let
C ‚àó = {x ‚àà Rp : M L is continuously differentiable at x},
and let D‚àó = Rp \ C ‚àó be the set of points at which M L is not continuously differentiable.
Assumption 4. If Pr(M L(Xi ) ‚àà (0, 1)) > 0, then the following conditions (a)‚Äì(c) hold.
(a) (Probability of Neighborhood of D‚àó ) Pr(Xi ‚àà N (D‚àó , Œ¥)) = O(Œ¥).
(b) (Bounded Partial Derivatives of M L) The partial derivatives of M L are bounded on C ‚àó .
(c) (Bounded Conditional Mean) E[Yi |Xi ] is bounded on X .
Assumption 4 is required for proving asymptotic normality of Œ≤ÃÇ1 when M L is stochastic. To
explain the role of Assumption 4 (a), consider a path of covariate points xŒ¥ ‚àà N (D‚àó , Œ¥) ‚à© C ‚àó
indexed by Œ¥ > 0. Since M L is continuous at xŒ¥ , pM L (xŒ¥ ) = M L(xŒ¥ ) as implied by Part 1
of Corollary 2. However, pM L (xŒ¥ ; Œ¥) does not necessarily get sufficiently close to M L(xŒ¥ ) even
Consider the example in Part 4 of Corollary 2 with q1 = 0 and q2 = 1. In this example, ‚Ñ¶‚àó = {x ‚àà R2 : x1 >
2
2
1
0, x2 > 0}. Let {‚Ñ¶k }‚àû
k=1 be a sequence of subsets of R , where ‚Ñ¶k = {x ‚àà R : x2 ‚â• kx1 , x1 > 0} for each k. ‚Ñ¶k
‚àó
is twice continuously differentiable for all k, and well approximates ‚Ñ¶ for a large k in that dH (‚Ñ¶‚àó , ‚Ñ¶k ) ‚Üí 0 as
k ‚Üí ‚àû, where dH (A, B) = max{supx‚ààA inf y‚ààB kx ‚àí yk, supy‚ààB inf x‚ààA kx ‚àí yk} is the Hausdorff distance between
two sets A and B ‚äÇ Rp .
20
The boundary of ‚Ñ¶‚àó fails to be (p ‚àí 1) dimensional, for example, when the covariate space is three dimensional
(p = 3) and ‚Ñ¶‚àó is a straight line, not a set with nonzero volume nor even a plane. In this example, the boundary
is the same as ‚Ñ¶‚àó , and its two-dimensional Hausdorff measure is zero.
19

14

as Œ¥ ‚Üí 0, since xŒ¥ is in the Œ¥-neighborhood of D‚àó and hence M L may discontinuously change
within the Œ¥-ball B(xŒ¥ , Œ¥). Assumption 4 (a) requires that the probability of Xi being in the Œ¥neighborhood of D‚àó shrinks to zero at the rate of Œ¥, which makes the points in the neighborhood
negligible.
Assumption 4 (a) often holds in practice. If M L is continuously differentiable on X , then
‚àó
D ‚à© X = ‚àÖ, so this condition holds. If, for example, the treatment recommendation is randomly
assigned based on a stratified randomized experiment or on the -Greedy algorithm (see Part 2
(a) of Example 1 in Section 7), D‚àó is the boundary at which the recommendation probability
changes discontinuously. For any boundary of standard shape, the probability of Xi being in the
Œ¥-neighborhood of the boundary vanishes at the rate of Œ¥, and the required condition is satisfied.
We provide a sufficient condition for this condition in Appendix A.4.
Assumption 4 (b) and (c) are regularity conditions, imposing the boundedness of the partial
derivatives of M L and of the conditional mean of the outcome.
Assumption 5 (The Number of Simulation Draws). n‚àí1/2 Sn ‚Üí ‚àû, and Pr(pM L (Xi ; Œ¥n ) ‚àà
n
log n
‚àí1/2 Œ¥ 1/2 ) for some Œ≥ > 1 .
(0, Œ≥ log
n
Sn ) ‚à™ (1 ‚àí Œ≥ Sn , 1)) = o(n
2
Assumption 5 is the key to proving asymptotic normality of the simulation-based estimator
Assumption 5 says that we need to choose the number of simulation draws Sn so that it
grows to infinity faster than n1/2 , and that the probability that pM L (Xi ; Œ¥n ) lies on the tails
n
log n
‚àí1/2 Œ¥ 1/2 . This condition makes the bias caused
(0, Œ≥ log
n
Sn ) ‚à™ (1 ‚àí Œ≥ Sn , 1) vanishes faster than n
s
M
L
by using p (Xi ; Œ¥n ) instead of p (Xi ; Œ¥n ) asymptotically negligible. To illustrate how the second
part of this assumption restricts the rate at which Sn goes to infinity, consider an example where
Pr(pM L (Xi ; Œ¥n ) ‚àà (0, 1)) = O(Œ¥n ), and pM L (Xi ; Œ¥n ) is approximately uniformly distributed on
n
log n
M L (X ; Œ¥ ) ‚àà (0, Œ≥ log n ) ‚à™ (1 ‚àí Œ≥ log n , 1)) =
the tails (0, Œ≥ log
i n
Sn ) ‚à™ (1 ‚àí Œ≥ Sn , 1). In this case, Pr(p
Sn
Sn
log n
O(Œ¥n Sn ), and the second part of Assumption 5 requires that Sn grow sufficiently fast so that
Œ≤ÃÇ1s .

1/2

n1/2 Œ¥n log n
Sn

1/2

= o(1). One choice of Sn satisfying this is Sn = Œ±nŒ∫ Œ¥n

for some Œ± > 0 and Œ∫ > 12 ,

1/2
n1/2 Œ¥n
Sn

log n
n
in which case
= Œ±nlog
Œ∫‚àí1/2 = o(1).
Under the above conditions, the 2SLS estimators Œ≤ÃÇ1 and Œ≤ÃÇ1s are consistent and asymptotically
normal estimators of a weighted average treatment effect.

Theorem 1 (Consistency and Asymptotic Normality). Suppose that Assumptions 1 and 3 hold,
and that Œ¥n ‚Üí 0, nŒ¥n ‚Üí ‚àû and Sn ‚Üí ‚àû as n ‚Üí ‚àû. Then the 2SLS estimators Œ≤ÃÇ1 and Œ≤ÃÇ1s
converge in probability to
Œ≤1 ‚â° lim E[œâi (Œ¥)(Yi (1) ‚àí Yi (0))],
Œ¥‚Üí0

where
œâi (Œ¥) =

pM L (Xi ; Œ¥)(1 ‚àí pM L (Xi ; Œ¥))(Di (1) ‚àí Di (0))
.
E[pM L (Xi ; Œ¥)(1 ‚àí pM L (Xi ; Œ¥))(Di (1) ‚àí Di (0))]

Suppose, in addition, that Assumptions 4 and 5 hold and that nŒ¥n2 ‚Üí 0 as n ‚Üí ‚àû. Then
d

œÉÃÇn‚àí1 (Œ≤ÃÇ1 ‚àí Œ≤1 ) ‚àí‚Üí N (0, 1),
d

(œÉÃÇns )‚àí1 (Œ≤ÃÇ1s ‚àí Œ≤1 ) ‚àí‚Üí N (0, 1),
15

where we define œÉÃÇn‚àí1 and (œÉÃÇns )‚àí1 as follows. Let
n
n
n
X
X
X
Œ£ÃÇn = (
Zi,n D0i,n Ii,n )‚àí1 (
ÀÜ2i,n Zi,n Z0i,n Ii,n )(
Di,n Z0i,n Ii,n )‚àí1 ,
i=1

i=1

i=1

where
ÀÜi,n = Yi ‚àí D0i,n Œ≤ÃÇ.
Œ£ÃÇn is the conventional heteroskedasticity-robust estimator for the variance of the 2SLS estimator.
œÉÃÇn2 is the second diagonal element of Œ£ÃÇn . (œÉÃÇns )2 is the analogously-defined estimator for the
variance of Œ≤ÃÇ1s from the simulation-based regression.
Proof. See Appendix C.6.
Theorem 1 says that the 2SLS estimators converge to a weighted average of causal effects
for the subpopulation whose QPS is nondegenerate (pM L (Xi ; Œ¥) ‚àà (0, 1)) and who would switch
their treatment status in response to the treatment recommendation (Di (1) 6= Di (0)).21 The
limit limŒ¥‚Üí0 E[œâi (Œ¥)(Yi (1) ‚àí Yi (0))] always exists under the assumptions of Theorem 1. It also
shows that inference based on the conventional 2SLS heteroskedasticity-robust standard errors
is asymptotically valid if Œ¥n goes to zero at an appropriate rate. The convergence rate of Œ≤ÃÇ1 is
‚àö
‚àö
Op (1/ n) if Pr(M L(Xi ) ‚àà (0, 1)) > 0 and is Op (1/ nŒ¥n ) if Pr(M L(Xi ) ‚àà (0, 1)) = 0.
Our consistency result requires that Œ¥n goes to zero slower than n‚àí1 . The rate condition
ensures that, when Pr(M L(Xi ) ‚àà (0, 1)) = 0, we have sufficiently many observations in the
Œ¥n -neighborhood of the boundary of ‚Ñ¶‚àó . Importantly, the rate condition does not depend on the
dimension of Xi , unlike other bandwidth-based estimation methods such as kernel methods. This
is because we use all the observations in the Œ¥-neighborhood of the boundary, and the number
of those observations is of order nŒ¥n regardless of the dimension of Xi if the dimension of the
boundary is one less than the dimension of Xi , i.e., (p ‚àí 1).
The asymptotic normality result requires that Œ¥n goes to zero sufficiently quickly. When
Pr(M L(Xi ) ‚àà (0, 1)) > 0, we need to use a small enough Œ¥n so that pM L (Xi ; Œ¥n ) converges
to pM L (Xi ) at a fast rate and Œ¥n -neighborhood of D‚àó is asymptotically small enough. When
Pr(M L(Xi ) ‚àà (0, 1)) = 0, the asymptotic normality is based on undersmoothing, which eliminates the asymptotic bias by using the observations sufficiently close to the boundary of ‚Ñ¶‚àó .
Whether or not Pr(M L(Xi ) ‚àà (0, 1)) = 0, when we use simulated QPS, the consistency
result requires that the number of simulation draws Sn goes to infinity as n increases while the
asymptotic normality result requires a sufficiently fast growth rate of Sn to make the bias caused
by using ps (Xi ; Œ¥n ) negligible.
Finally, note that the weight œâi (Œ¥) given in Theorem 1 is negative if Di (1) < Di (0), so
E[œâi (Œ¥)(Yi (1) ‚àí Yi (0))] may not be a causally interpretable convex combination of treatment
effects Yi (1) ‚àí Yi (0). This can happen because the treatment effect of those whose treatment
assignment switches from 1 to 0 in response to the treatment recommendation (defiers) negatively
contributes to E[œâi (Œ¥)(Yi (1) ‚àí Yi (0))]. Additional assumptions prevent this problem. If the
21

In principle, it is possible to estimate other weighted averages and the unweighted average by reweighting
different observations appropriately.

16

treatment effect is constant, for example, the 2SLS estimators are consistent for the treatment
effect.
Corollary 3. Suppose that Assumptions 1 and 3 hold, that the treatment effect is constant, i.e.,
Yi (1) ‚àí Yi (0) = b for some constant b, and that Œ¥n ‚Üí 0, nŒ¥n ‚Üí ‚àû, and Sn ‚Üí ‚àû as n ‚Üí ‚àû.
Then the 2SLS estimators Œ≤ÃÇ1 and Œ≤ÃÇ1s converge in probability to b.
Another approach is to impose monotonicity (Imbens and Angrist, 1994). If Pr(Di (1) ‚â•
Di (0)|Xi = x) = 1, we have
E[(Di (1) ‚àí Di (0))(Yi (1) ‚àí Yi (0))|Xi = x] = E[Di (1) ‚àí Di (0)|Xi = x]LAT E(x),
where LAT E(x) = E[Yi (1) ‚àí Yi (0)|Di (1) 6= Di (0), Xi = x] is the local average treatment effect
(LATE) conditional on Xi = x. The 2SLS estimators are then consistent for a weighted average
of conditional LATEs with all weights nonnegative.
Corollary 4. Suppose that Assumptions 1 and 3 hold, that Pr(Di (1) ‚â• Di (0)|Xi = x) = 1 for
any x ‚àà X with pM L (x) ‚àà (0, 1), and that Œ¥n ‚Üí 0, nŒ¥n ‚Üí ‚àû and Sn ‚Üí ‚àû as n ‚Üí ‚àû. Then the
2SLS estimators Œ≤ÃÇ1 and Œ≤ÃÇ1s converge in probability to
lim E[œâ(Xi ; Œ¥)LAT E(Xi )],

Œ¥‚Üí0

where
œâ(x; Œ¥) =

pM L (x; Œ¥)(1 ‚àí pM L (x; Œ¥))E[Di (1) ‚àí Di (0)|Xi = x]
.
E[pM L (Xi ; Œ¥)(1 ‚àí pM L (Xi ; Œ¥))(Di (1) ‚àí Di (0))]

The probability limit of the 2SLS estimators is a weighted average of conditional LATEs
over all values of Xi with nondegenerate QPS pM L (Xi ; Œ¥n ). The weights are proportional to
pM L (Xi ; Œ¥n )(1 ‚àí pM L (Xi ; Œ¥n )), and to the proportion of compliers, E[Di (1) ‚àí Di (0)|Xi ].

4.3

Special Cases

The result in Theorem 1 holds whether M L is stochastic (Pr(M L(Xi ) ‚àà (0, 1)) > 0) or deterministic (Pr(M L(Xi ) ‚àà (0, 1)) = 0). As shown in the proof of Theorem 1, if we consider these
two underlying cases separately, the probability limit of the 2SLS estimators has a more specific
expression. If Pr(M L(Xi ) ‚àà (0, 1)) > 0,
plim Œ≤ÃÇ1 = plim Œ≤ÃÇ1s =

E[M L(Xi )(1 ‚àí M L(Xi ))(Di (1) ‚àí Di (0))(Yi (1) ‚àí Yi (0))]
.
E[M L(Xi )(1 ‚àí M L(Xi ))(Di (1) ‚àí Di (0))]

(5)

The 2SLS estimators converge to a weighted average of treatment effects for the subpopulation
with nondegenerate M L(Xi ). To relate this result to existing work, consider the following 2SLS
regression with the (standard) propensity score M L(Xi ) control:
Di = Œ≥0 + Œ≥1 Zi + Œ≥2 M L(Xi ) + ŒΩi

(6)

Yi = Œ≤0 + Œ≤1 Di + Œ≤2 M L(Xi ) + i .

(7)

17

Existing results show that under conditional independence, the 2SLS estimator from this
regression converges in probability to the treatment-variance weighted average of treatment effects in (5) (Angrist and Pischke, 2008; Hull, 2018).22 Not surprisingly, for this selection-onobservables case, our result shows that the 2SLS estimator is consistent for the same treatment
effect whether we use as a control the propensity score, QPS, or simulated QPS.
Importantly, using QPS as a control allows us to consistently estimate a causal effect even
if M L is deterministic and produces high-dimensional regression-discontinuity variation.23 If
Pr(M L(Xi ) ‚àà (0, 1)) = 0,
R
p‚àí1 (x)
‚àó E[(Di (1) ‚àí Di (0))(Yi (1) ‚àí Yi (0))|Xi = x]fX (x)dH
s
R
plim Œ≤ÃÇ1 = plim Œ≤ÃÇ1 = ‚àÇ‚Ñ¶
. (9)
p‚àí1 (x)
‚àÇ‚Ñ¶‚àó E[Di (1) ‚àí Di (0)|Xi = x]fX (x)dH
The 2SLS estimators converge to a weighted average of treatment effects for the subpopulation
who are on the boundary of the treated region.
Recall that the 2SLS regression uses the observations with pM L (Xi ; Œ¥n ) ‚àà (0, 1) (or ps (Xi ; Œ¥n ) ‚àà
(0, 1) when we use simulated QPS) only. By definition, if pM L (Xi ; Œ¥) ‚àà (0, 1), Xi must be in
the Œ¥-neighborhood of the boundary of ‚Ñ¶‚àó . Therefore, to derive the probability limit of Œ≤ÃÇ1 , it
is necessary to derive the limits of the integrals of relevant variables over the Œ¥-neighborhood
R
(e.g., N (‚àÇ‚Ñ¶‚àó ,Œ¥) E[Yi |Xi = x]fX (x)dx) as Œ¥ shrinks to zero. We take an approach drawing on
change of variables techniques from differential geometry and geometric measure theory.24 In
this approach, we first use the coarea formula (Lemma B.3 in Appendix B.3) to write the integral
of an integrable function g over N (‚àÇ‚Ñ¶‚àó , Œ¥) in terms of the iterated integral over the levels sets of
the signed distance function of ‚Ñ¶‚àó :
Z
Z Œ¥Z
g(x)dx =
g(x)dHp‚àí1 (x)dŒª,
(10)
N (‚àÇ‚Ñ¶‚àó ,Œ¥)

‚àíŒ¥

{x0 ‚ààRp :ds‚Ñ¶‚àó (x0 )=Œª}

22

Precisely speaking, Angrist and Pischke (2008) consider the OLS regression of Yi (or Di ) on Zi controlling a
dummy variable for every value taken on by Xi (i.e., the model is saturated in Xi ) when Xi is a discrete variable:
X
Yi = Œ±1 Zi +
Œ±2,x 1{Xi = x} + ui .
(8)
x‚ààX
i ‚àíE[Zi |Xi ])Yi ]
.
By the Frisch-Waugh Theorem, the population coefficient on Zi from (8) is given by Œ±1 = E[(Z
E[(Zi ‚àíE[Zi |Xi ])2 ]
Angrist and Pischke (2008) show that this expression is reduced to the treatment-variance weighted average of
i )(1‚àíM L(Xi ))(Y1i ‚àíY0i )]
under the conditional independence assumption. Their derivation
treatment effects E[M L(X
E[M L(Xi )(1‚àíM L(Xi ))
follows even when Xi is continuous and we control the propensity score linearly.
23
In the standard RDD with a single running variable Xi and cutoff c, pM L (Xi ; Œ¥n ) = X2Œ¥i ‚àíc
+ 21 if Xi ‚àà
n
ML
ML
[c ‚àí Œ¥n , c + Œ¥n ] and p (Xi ; Œ¥n ) ‚àà {0, 1} otherwise. Since p (Xi ; Œ¥n ) is linear in the running variable Xi if
pM L (Xi ; Œ¥n ) ‚àà (0, 1), the estimator Œ≤ÃÇ1 becomes a local regression estimator with the box kernel that places the
same slope coefficient of Xi on both sides of the cutoff. Under our assumptions, Œ≤ÃÇ1 and standard local linear
estimators are shown to have the same fastest possible convergence rate.
24
Our approach using geometric theory shows that Œ≤ÃÇ1 converges to an integral of the conditional treatment
effect over boundary points with respect to the Hausdorff measure. In constrast, prior studies on multidimensional
RDDs express treatment effect estimands in terms of expectations conditional on Xi being in the boundary like
E[Y1i ‚àí Y0i |Xi ‚àà ‚àÇ‚Ñ¶‚àó ] (Zajonc, 2012). However, those conditional expectations are, formally, not well-defined,
since Lp (‚àÇ‚Ñ¶‚àó ) = 0 and hence Pr(Xi ‚àà ‚àÇ‚Ñ¶‚àó ) = 0. We therefore prefer our expression in terms of an integral with
respect to the Hausdorff measure to any expressions in terms of conditional expectations on the boundary. Arias,
Rubio-Ram√≠rez and Waggoner (2018), Bornn, Shephard and Solgi (2019), and Qiao (2021) use similar tools from
differential geometry and geometric measure theory, but for different purposes.

18

where ds‚Ñ¶‚àó is the signed distance function of ‚Ñ¶‚àó (see Appendix B.2 for the definition). The set
{x0 ‚àà Rp : ds‚Ñ¶‚àó (x0 ) = Œª} is a level set of ds‚Ñ¶‚àó , which collects the points in ‚Ñ¶‚àó when Œª > 0 and
the points in Rp \ ‚Ñ¶‚àó when Œª < 0 whose distance to the boundary ‚àÇ‚Ñ¶‚àó is |Œª|. Figure 2a shows a
visual illustration.
We then use the area formula (Lemma B.4 in Appendix B.3) to write the integral over each
level set in terms of the integral over the boundary ‚àÇ‚Ñ¶‚àó :
Z
Z
‚àÇ‚Ñ¶‚àó
g(x)dHp‚àí1 (x) =
g(x‚àó + ŒªŒΩ‚Ñ¶‚àó (x‚àó ))Jp‚àí1
œà‚Ñ¶‚àó (x‚àó , Œª)dHp‚àí1 (x‚àó ),
(11)
{x0 ‚ààRp :ds‚Ñ¶‚àó (x0 )=Œª}

‚àÇ‚Ñ¶‚àó

where ŒΩ‚Ñ¶‚àó (x‚àó ) is the inward unit normal vector of ‚àÇ‚Ñ¶‚àó at x‚àó (the unit vector orthogonal to all
‚àÇ‚Ñ¶‚àó œà ‚àó (x‚àó , Œª)
vectors in the tangent space of ‚àÇ‚Ñ¶‚àó at x‚àó that points toward the inside of ‚Ñ¶‚àó ), and Jp‚àí1
‚Ñ¶
is the Jacobian of the transformation œà‚Ñ¶‚àó (x‚àó , Œª) = x‚àó +ŒªŒΩ‚Ñ¶‚àó (x‚àó ). Figure 2b illustrates this change
of variables formula. Finally, combining (10) and (11) and proceeding with further analysis, we
prove in Appendix C.6.3 that when g is continuous,
Z

Z
p‚àí1
g(x)dx = Œ¥
g(x)dH (x) + o(1) .
N (‚àÇ‚Ñ¶‚àó ,Œ¥)

‚àÇ‚Ñ¶‚àó

Thus, the integral over the Œ¥-neighborhood of ‚àÇ‚Ñ¶‚àó scaled up by Œ¥ ‚àí1 converges to the integral
over boundary points with respect to the (p ‚àí 1)-dimensional Hausdorff measure. This result is
used to derive the expression of the probability limit of Œ≤ÃÇ1 given by (9).

5

Machine Learning Simulation

We conduct a Monte Carlo experiment to assess the feasibility and performance of our method.
Consider a tech company that conducts a randomized experiment (randomized controlled trial;
RCT) using a small segment of the population and, at the same time, applies a deterministic decision algorithm to the rest of the population. We generate a random sample {(Yi , Xi , Di , Zi )}ni=1
of size n = 10, 000 as follows. There are 100 covariates (p = 100), and Xi ‚àº N (0, Œ£). Yi (0) is
generated as Yi (0) = 0.75Xi0 Œ±0 + 0.250i , where Œ±0 ‚àà R100 , and 0i ‚àº N (0, 1). We consider two
models for Yi (1), one in which the the treatment effect Yi (1) ‚àí Yi (0) does not depend on Xi and
one in which the effect depends on Xi .
Model A. Yi (1) = Yi (0) + 1i , where 1i ‚àº N (0, 1).
Model B. Yi (1) = Yi (0) + Xi0 Œ±1 , where Œ±1 ‚àà R100 .
The choice of parameters Œ£, Œ±0 and Œ±1 is explained in Appendix D. Di (0) and Di (1) are generated
as Di (0) = 0 and Di (1) = 1{Yi (1)‚àíYi (0) > ui }, where ui ‚àº N (0, 1). To generate Zi , let q0.495 and
q0.505 be the 49.5th and 50.5th (empirical) quantiles of the first covariate X1i , and let œÑpred (Xi )
be a real-valued function of Xi , which we regard as a prediction of the effect of recommendation
on the outcome for individual i obtained from past data. We will explain how we construct œÑpred

19

in the next paragraph. Zi is then generated as
Ô£±
‚àó
Ô£¥
Ô£¥
Ô£≤Zi ‚àº Bernoulli(0.5) if X1i ‚àà [q0.495 , q0.505 ]
Zi = 1
if X1i ‚àà
/ [q0.495 , q0.505 ] and œÑpred (Xi ) ‚â• 0
Ô£¥
Ô£¥
Ô£≥0
if X1i ‚àà
/ [q0.495 , q0.505 ] and œÑpred (Xi ) < 0.
The first case corresponds to the RCT segment while the latter two cases to the deterministic
algorithm segment. The function M L is given by
Ô£±
Ô£¥
Ô£¥
Ô£≤0.5 if x1 ‚àà [q0.495 , q0.505 ]
M L(x) =

1
Ô£¥
Ô£¥
Ô£≥0

if x1 ‚àà
/ [q0.495 , q0.505 ] and œÑpred (x) ‚â• 0

if x1 ‚àà
/ [q0.495 , q0.505 ] and œÑpred (x) < 0.

Finally, Di and Yi are generated as Di = Zi Di (1)+(1‚àíZi )Di (0) and Yi = Di Yi (1)+(1‚àíDi )Yi (0),
respectively.
We simulate 1, 000 hypothetical samples from the above data-generating process. Before
obtaining 1, 000 samples, we construct œÑpred using an independent sample {(YÃÉi , XÃÉi , DÃÉi , ZÃÉi )}nÃÉi=1 of
size nÃÉ = 2, 000. The distribution of (YÃÉi , XÃÉi , DÃÉi , ZÃÉi ) is the same as that of (Yi , Xi , Di , Zi ) except
(1) that YÃÉi (1) is generated as YÃÉi (1) = YÃÉi (0) + 0.5XÃÉi0 Œ±1 + 0.51i , where 1i ‚àº N (0, 1) and (2) that
ZÃÉi ‚àº Bernoulli(0.5). This can be viewed as data from a past randomized experiment conducted
to construct an algorithm. We then use random forests separately for the subsamples with ZÃÉi = 1
and ZÃÉi = 0 to make a prediction of YÃÉi from XÃÉi . Let ¬µz (x) be the trained prediction model, and
set œÑpred (x) = ¬µ1 (x) ‚àí ¬µ0 (x).
This mimics a situation in which the decision maker first conducts an experiment that randomly assigns Zi to predict the conditional average effect of Zi and then constructs an algorithm
that greedily chooses the treatment predicted to perform better based on the predicted effect.
We generate the sample {(YÃÉi , XÃÉi , DÃÉi , ZÃÉi )}nÃÉi=1 and construct œÑpred only once, and we use it for all
of the 1, 000 samples. The distribution of the sample {(Yi , Xi , Di , Zi )}ni=1 is thus held fixed for
all simulations.

5.1

Estimators and Estimands

We use the data {(Yi , Xi , Di , Zi )}ni=1 to estimate treatment effect parameters. Our main approach
is 2SLS with QPS controls in Theorem 1. To compute QPS, we use S = 400 simulation draws
for each observation.
We compare our approach with two alternatives. The first alternative is 2SLS with M L
controls. This method uses the observations with M L(Xi ) ‚àà (0, 1) to run the 2SLS regression of
Yi on a constant, Di , and M L(Xi ) using Zi as an instrument for Di (see (6) and (7) in Section
4.3) and reports the coefficient on Di . The second alternative is OLS of Yi on a constant and Di
(i.e., the difference in the sample mean of Yi between the treated group and untreated group)
using all observations.
We consider four parameters as target estimands: ATE ‚â° E[Yi (1) ‚àí Yi (0)], ATE(RCT) ‚â°
E[Yi (1)‚àíYi (0)|X1i ‚àà [q0.495 , q0.505 ]], LATE ‚â° E[Yi (1)‚àíYi (0)|Di (1) 6= Di (0)], and LATE(RCT) ‚â°
E[Yi (1) ‚àí Yi (0)|Di (1) 6= Di (0), X1i ‚àà [q0.495 , q0.505 ]]. In the case where the treatment effect does
20

not depend on Xi (Model A), the conditional effects are homogeneous, and ATE and LATE are
the same as ATE(RCT) and LATE(RCT), respectively. In the case where the treatment effect
depends on Xi (Model B), the conditional effects are heterogeneous. However, since the RCT
segment consists of those in the middle of the distribution of X1i , the average effect for the RCT
segment is close to the unconditional average effect. As a result, ATE is equal to ATE(RCT)
and LATE is similar to LATE(RCT) under this data-generating process.
For both models, the 2SLS estimator converges in probability to LATE(RCT) whether we
control for QPS or M L.25 However, 2SLS with M L controls uses only the individuals for the
RCT segment while 2SLS with QPS controls additionally uses the individuals near the decision
boundary of the deterministic algorithm (i.e., the boundary of the region for which œÑpred (x) ‚â• 0).
Therefore, 2SLS with QPS controls is expected to produce a more precise estimate than 2SLS
with M L controls if the conditional effects for those near the boundary are not far from the
target estimand.

5.2

Results

Table 1 reports the bias, standard deviation (SD), and root mean squared error (RMSE) of each
estimator. Panels A and B present the results for the cases where the conditional effects are
homogeneous and heterogeneous, respectively. Note first that OLS with no controls is significantly biased, showing the importance of correcting for omitted variable bias. 2SLS with QPS
achieves this goal, as suggested by its smaller biases across all possible treatment effect models,
target parameters, and values of Œ¥. 2SLS with QPS controls shows a consistent pattern; as the
bandwidth Œ¥ grows, the bias increases while the variance declines. For several values of Œ¥, 2SLS
with QPS controls outperforms 2SLS with M L controls in terms of the RMSE. This finding implies that exploiting individuals near the high-dimensional decision boundary of the deterministic
algorithm can lead to better performance than using only the individuals in the RCT segment.
To evaluate our inference procedure based on Theorem 1, we also report the coverage probabilities of the 95% confidence intervals for LATE(RCT) constructed from the 2SLS estimates
and their heteroskedasticity-robust standard errors. The confidence intervals still offer nearly
correct coverage when Œ¥ is small, which supports the implication of Theorem 1 that the inference
procedure is valid when we use a sufficiently small Œ¥.

6
6.1

Empirical Policy Application
Hospital Relief Funding during the COVID-19 Pandemic

We also provide a real-world empirical application. As part of the 3-phase Coronavirus Aid,
Relief, and Economic Security (CARES) Act, the government has distributed tens of billions of
dollars of relief funding to hospitals since April 2020. We focus on an initial portion of this funding
($10 billion), which was allocated to hospitals that qualified as ‚Äúsafety net hospitals‚Äù according
to a specific eligibility criterion. This eligibility criterion intends to focus on hospitals that
25
The 2SLS estimators converge in probability to the right-hand side of Eq.
LATE(RCT) under the data-generating process of this simulation.

21

(5), which is the same as

‚Äúdisproportionately provide care to the most vulnerable, and operate on thin margins.‚Äù Specifically,
an acute care hospital was deemed eligible for funding if the following conditions hold:
‚Ä¢ Medicare Disproportionate Patient Percentage (DPP) of 20.2% or greater. DPP is equal
to the sum of the percentage of Medicare inpatient days attributable to patients eligible
for both Medicare Part A and Supplemental Security Income (SSI), and the percentage of
total inpatient days attributable to patients eligible for Medicaid but not Medicare Part
A.26
‚Ä¢ Annual Uncompensated Care (UCC) of at least $25, 000 per bed. UCC is a measure of
hospital care provided for which no payment was received from the patient or insurer. It
is the sum of a hospital‚Äôs bad debt and the financial assistance it provides.27
‚Ä¢ Profit Margin (Net income/(Net patient revenue + Total other income)) of 3.0% or less.
Hospitals that do not qualify on any of the three dimensions are funding ineligible. Figure 3 visualizes how the three dimensions determine safety net eligibility. As the bottom two-dimensional
planes show, eligibility discontinuously changes as hospitals cross the eligibility boundary in the
space of the three characteristics. This setting is a three-dimensional RDD, falling under our
framework.
The final funding amount is calculated as follows. Each eligible hospital is assigned an
individual facility score, which is calculated as the product of DPP and the number of beds in
that hospital. This facility score determines the share of funding allocated to the hospital, out
of the total $10 billion. The share received by each hospital is determined by the ratio of the
hospital‚Äôs facility score to the sum of facility scores across all eligible hospitals. The amount of
funding that can be received is bounded below at $5 million and capped above at $50 million.
We use publicly available data from the Healthcare Cost Report Information System (HCRIS)28 ,
to replicate29 the funding eligibility status as well as the amount of funding received. To obtain outcome measures of interest, we use the publicly available COVID-19 Reported Patient
Impact and Hospital Capacity by Facility dataset. This provides facility-level data on hospital
utilization aggregated on a weekly basis, from July 31st 2020 onwards.30 Summary measures of
outcome variables and hospital characteristics are documented in Table 2. Safety net hospitals
have higher levels of inpatient beds and ICU beds occupied by patients who have lab-confirmed
or suspected COVID-19. They also have a higher number of employees and beds and shorter
lengths of inpatient stay.
26

Source: https://www.cms.gov/Medicare/Medicare-Fee-for-Service-Payment/AcuteInpatientPPS/dsh
Source: https://www.aha.org/fact-sheets/2020-01-06-fact-sheet-uncompensated-hospital-care-cost
28
We use the RAND cleaned version of this dataset which can be accessed at https://www.hospitaldatasets.
org/
29
We use the methodology detailed in the CARE ACT website to project funding based on 2018 financial year
cost reports.
30
Source:
https://healthdata.gov/Hospital/COVID-19-Reported-Patient-Impact-and-Hospital-Capa/
anag-cw7u
27

22

6.2

Covariate Balance Estimates

Using the above data, we study the effect of safety net funding on relevant hospital outcomes,
such as the total number of inpatient beds and the number of staffed ICU beds occupied by adult
COVID patients reported between July 31st 2020 and August 6th 2020.
We first evaluate the balancing property of QPS conditioning using QPS-controlled differences
in covariate means for hospitals who are and are not deemed eligible for safety net funding.
Specifically, we run the following OLS regression of hospital-level characteristics on the eligibility
status using observations with ps (Xi ; Œ¥n ) ‚àà (0, 1):
Wi = Œ≥0 + Œ≥1 Zi + Œ≥2 ps (Xi ; Œ¥n ) + Œ∑i ,
where Wi is one of the predetermined financial and utilization characteristics of the hospital,
Zi is a funding eligibility dummy, Xi is a vector of three input variables (DPP, UCC, and
profit margin) that determine the funding eligibility, and ps (Xi ; Œ¥n ) is the simulated QPS. The
estimated coefficient on Zi is the QPS-controlled difference in the mean of the covariate between
eligible and ineligible hospitals. For comparison, we also run the OLS regression of hospital
characteristics on the eligibility status with no controls using the whole sample.
Table 3 reports the covariate balance estimates. Column 1 shows that, without controlling
for QPS, eligible hospitals are significantly different from ineligible hospitals. We find that all
the relevant hospital eligibility characteristics are strongly associated with eligibility.
Once we control for QPS, eligible and ineligible hospitals have similar financial and utilization
characteristics, as reported in columns 2‚Äì6 of Table 3. These estimates are consistent with our
theoretical results, establishing the empirical relevance of QPS controls.

6.3

2SLS Estimates

Causal effects of safety net funding are estimated by 2SLS using funding eligibility as an instrument for the amount of funding received. We run the following 2SLS regression on four different
hospital-level outcome variables:
Di = Œ≥0 + Œ≥1 Zi + Œ≥2 ps (Xi ; Œ¥n ) + vi
Yi = Œ≤0 + Œ≤1 Di + Œ≤2 ps (Xi ; Œ¥n ) + i ,
where Yi is a hospital-level outcome and Di is the amount of relief funding received.31 We also
run the OLS and 2SLS regressions with no controls, computed using the sample of all hospitals,
as benchmark estimators.
The first stage effects of safety net eligibility on funding amount (in millions), shown in
columns 2‚Äì9 of Table 4, suggest that safety net eligibility boosts funding significantly. For
example, in column 2 of Table 4, we can see that being eligible for the funding increases the
safety net funding by approximately 14 million dollars.
31

This specification uses a continuous treatment, unlike our theoretical framework with a binary treatment.
We obtain similar results when the treatment is a binary transformation of the amount of relief funding received
(e.g., a dummy indicating whether the amount exceeds a certain value). Results are available upon request.

23

OLS estimates of funding effects, reported as the benchmark in column 1 of Table 4, indicate
that safety net funding is associated with a higher number of adult inpatient beds and higher
number of staffed ICU beds utilized by patients who have lab-confirmed or suspected COVID.
The estimates indicate that a million dollar increase in funding is associated with 5.52 more adult
inpatient beds occupied by patients with lab-confirmed or suspected COVID. The corresponding
increase in total adult inpatient beds occupied by those who have lab-confirmed COVID is 4.50
and the increase in staffed ICU beds occupied by those who have lab-confirmed or suspected
COVID is 1.66. The estimated increase in staffed ICU beds occupied by lab-confirmed COVID
patients is 1.50. Naive 2SLS estimates with no controls produce similar results.
In contrast with the OLS or uncontrolled 2SLS estimates, the 2SLS estimates with QPS
controls in columns 3‚Äì9 show a different picture. The gains in number of inpatient beds and
staffed ICU beds occupied by suspected and lab-confirmed COVID patients become much smaller
and lose significance across all bandwidth specifications. These results suggest that QPS reveals
important selection bias in the estimated effects of safety net funding. Once we control for QPS
to eliminate the bias, the safety net relief funding has little to no effect on the hospital utilization
level by COVID-19 patients.

7

Other Examples

Here we give real-world examples and discuss the applicability of our framework.
Example 1 (Bandit and Reinforcement Learning). We are constantly exposed to digital information (movie, music, news, search results, advertisements, and recommendations) through a
variety of devices and platforms. Tech companies allocate these pieces of content by using bandit
and reinforcement learning algorithms. Our method is applicable to many popular bandit and
reinforcement learning algorithms. For simplicity, assume that individuals perfectly comply with
the treatment recommendation (Di = Zi ).
1. (Bandit Algorithms) The algorithms below first use past data and supervised learning to estimate the conditional means and variances of potential outcomes, E[Yi (z)|Xi ] and Var(Yi (z)|Xi ),
for each z ‚àà {0, 1}. Let ¬µz and œÉz2 denote the estimated functions. The algorithms use ¬µz (Xi )
and œÉz2 (Xi ) to determine the treatment assignment for individual i.
(a) (Thompson Sampling Using Gaussian Priors) The algorithm first samples potential outcomes from the normal distribution with mean (¬µ0 (Xi ), ¬µ1 (Xi )) and variance-covariance
matrix diag(œÉ02 (Xi ), œÉ12 (Xi )). The algorithm then chooses the treatment with the highest
sampled potential outcome:
ZiT S ‚â° arg max y(z), M LT S (Xi ) = E[arg max y(z)|Xi ],
z‚àà{0,1}

z‚àà{0,1}

where y(z) ‚àº N (¬µz (Xi ), œÉz2 (Xi )) independently across z. These algorithms often induce
quasi-experimental variation in treatment assignment, as a strand of the computer science

24

literature has observed (Precup, 2000; Li et al., 2010; Narita, Yasui and Yata, 2019; Saito,
Aihara, Matsutani and Narita, 2021). The function M L has an analytical expression:
!
¬µ
(x)
‚àí
¬µ
(x)
0
1
,
M LT S (x) = 1 ‚àí Œ¶ p 2
œÉ0 (x) + œÉ12 (x)
where Œ¶ is the cumulative distribution function of a standard normal distribution. Suppose
that the functions ¬µ0 , ¬µ1 , œÉ02 and œÉ12 are continuous. QPS for this case is given by
!
¬µ
(x)
‚àí
¬µ
(x)
0
1
.
pT S (x) = 1 ‚àí Œ¶ p 2
œÉ0 (x) + œÉ12 (x)
This QPS is nondegenerate, meaning that the data from the algorithm allow for causal-effect
identification.
(b) (Upper Confidence Bound, UCB) Unlike the above stochastic one, the UCB algorithm
(Li et al., 2010) is a deterministic algorithm, producing a less obvious example of our
framework. This algorithm chooses the treatment with the highest upper confidence bound
for the potential outcome:
ZiU CB ‚â° arg max{¬µz (Xi ) + Œ±œÉz (Xi )}, M LU CB (x) = arg max{¬µz (x) + Œ±œÉz (x)},
z=0,1

z=0,1

where Œ± is chosen so that |¬µz (x)‚àíE[Yi (z)|Xi = x]| ‚â§ Œ±œÉz (x) at least with some probability,
for example, 0.95, for every x. Suppose that the function g = ¬µ1 ‚àí ¬µ0 + Œ±(œÉ1 ‚àí œÉ0 ) is
continuous on X and is continuously differentiable in a neighborhood of x with ‚àág(x) 6= 0
for any x ‚àà X such that g(x) = 0. QPS for this case is given by
Ô£±
Ô£¥
if ¬µ1 (x) + Œ±œÉ1 (x) < ¬µ0 (x) + Œ±œÉ0 (x)
Ô£¥
Ô£≤0
pU CB (x) = 0.5
if ¬µ1 (x) + Œ±œÉ1 (x) = ¬µ0 (x) + Œ±œÉ0 (x)
Ô£¥
Ô£¥
Ô£≥1
if ¬µ (x) + Œ±œÉ (x) > ¬µ (x) + Œ±œÉ (x).
1

1

0

0

This means that the UCB algorithm produces potentially complicated quasi-experimental
variation along the boundary in the covariate space where the algorithm‚Äôs treatment recommendation changes from one to the other. It is possible to identify and estimate causal
effects across the boundary.
2. (Reinforcement Learning Algorithms) Extending bandit algorithms to dynamically changing
environments, reinforcement learning algorithms optimize decisions in dynamic environments,
where the state (the set of observables that the agent receives from the environment) and action
in the current period can affect the future states and outcomes. Let {(Xti , Zti , Yti )}‚àû
t=0 denote
the trajectory of the states, treatment assignments, and outcomes in periods t = 0, 1, 2, ¬∑ ¬∑ ¬∑ for
individual i. For simplicity, we assume that the trajectory follows a Markov decision process.32
32

Under a Markov decision process, the distribution of the state Xti only depends on the last state and treatment
assignment (Xt‚àí1,i , Zt‚àí1,i ), the distribution of the outcome Yti only depends on the current state and treatment
assignment (Xti , Zti ), and these distributions are stationary over periods.

25

Let Yti (1) and Yti (0) represent the potential outcomes in period t. Let Q : X √ó {0, 1} ‚Üí R be
the optimal state-action value function, called the Q-function: for (x, z) ‚àà X √ó {0, 1},
"‚àû
#
X
Q(x, z) ‚â° max E
Œ≥ t (Yti (1)œÄ(Xti ) + Yti (0)(1 ‚àí œÄ(Xti ))|X0i = x, Z0i = z ,
œÄ:X ‚Üí[0,1]

t=0

where Œ≥ ‚àà [0, 1) is a discount factor, and œÄ is a policy function that assigns the probability of
treatment to each possible state.
(a) (-Greedy) This algorithm first uses past data to yield QÃÇ, an estimate of the Q-function.
For example, the fitted Q iteration (Ernst, Geurts and Wehenkel, 2005) is used to estimate
Q.33 The algorithm then chooses the best treatment based on QÃÇ(Xti , z) with probability
1 ‚àí 2 and chooses the other treatment with probability 2 : for each t,
Zti ‚â°
M L (x) =

(
arg maxz=0,1 QÃÇ(Xti , z)

with probability 1 ‚àí


2

1 ‚àí arg maxz=0,1 QÃÇ(Xti , z)
with probability 2 ,
(

if QÃÇ(x, 1) < QÃÇ(x, 0)
2
1‚àí


2

if QÃÇ(x, 1) > QÃÇ(x, 0).

Suppose that the function g(¬∑) = QÃÇ(¬∑, 1) ‚àí QÃÇ(¬∑, 0) is continuous on X and is continuously
differentiable in a neighborhood of x with ‚àág(x) 6= 0 for any x ‚àà X such that g(x) = 0.
QPS for this case is given by
Ô£±

Ô£¥
if QÃÇ(x, 1) < QÃÇ(x, 0)
Ô£¥2
Ô£≤

p (x) = 0.5
if QÃÇ(x, 1) = QÃÇ(x, 0)
Ô£¥
Ô£¥
Ô£≥1 ‚àí 
if QÃÇ(x, 1) > QÃÇ(x, 0).
2

(b) (Policy Gradient Methods) Policy gradient methods such as REINFORCE (Williams, 1992)
and Actor-Critic approximate the optimal policy function by parametrization and learn
the parameter using stochastic gradient ascent. Let œÄ(x; Œ∏) be a parametrization of the
policy function that is differentiable with respect to Œ∏.34 Suppose that we have collected
l
a set of L trajectories {(xlt , ztl , ytl )Tt=0
: l = 1, ..., L} by running the policy œÄ(x; Œ∏0 ) for L
individuals. Policy gradient methods use the trajectories to update the policy parameter
33

Suppose that we have collected a set of L four-tuples {(xltl , ztll , ytll , xltl +1 )}L
l=1 as a result of the agent interacting
with the dynamic environment. Given the dataset and an initial approximation QÃÇ of Q (e.g., QÃÇ(x, z) = 0 for all
(x, z)), we repeat the following steps until some stopping condition is reached: 1. For each l = 1, ..., L, calculate
q l = ytll + Œ≥ maxz‚àà{0,1} QÃÇ(xltl +1 , z); 2. Use {(xltl , ztll , q l )}L
l=1 and a supervised learning method to train a model
that predicts q from (x, z). Let the model be a new approximation QÃÇ of Q. Possible supervised learning methods
used in the second step include tree-based methods, neural networks (Neural Fitted Q Iteration) and deep neural
networks (Deep Fitted Q Iteration).
exp(x0 Œ∏)
34
For example, œÄ might be a softmax function with a linear index: œÄ(x; Œ∏) = 1+exp(x
0 Œ∏) . Another example
is a neural network whose input is a representation of the state x, whose output is the treatment assignment
probability, and whose weights are represented by the parameter Œ∏.

26

to Œ∏1 by stochastic gradient ascent. The algorithms then use the updated policy function
œÄ(x; Œ∏1 ) to determine the treatment assignment for new episodes. For each t,
ZtiP G

‚â°

(
1

with probability œÄ(Xti ; Œ∏1 )

0

with probability 1 ‚àí œÄ(Xti

; Œ∏1 ),

M LT G (x) = œÄ(x; Œ∏1 ).

Suppose that the function œÄ(¬∑; Œ∏1 ) is continuous. QPS for this case is given by
pT G (x) = œÄ(x; Œ∏1 ).
Example 2 (Unsupervised Learning). Customer segmentation is a core marketing practice that
divides a company‚Äôs customers into groups based on their characteristics and behavior so that
the company can effectively target marketing activities at each group. Many businesses today
use unsupervised learning algorithms, clustering algorithms in particular, to perform customer
segmentation. Using our notation, assume that a company decides whether it targets a campaign
at customer i (Zi = 1) or not (Zi = 0). The company first uses a clustering algorithm such as
K-means clustering or Gaussian mixture model clustering to divide customers into K groups,
making a partition {S1 , ..., SK } of the covariate space Rp . The company then conducts the
campaign targeted at some of the groups:
ZiCL ‚â° 1{Xi ‚àà ‚à™k‚ààT Sk }, M LCL (x) = 1{x ‚àà ‚à™k‚ààT Sk },
where T ‚äÇ {1, .., K} is the set of the indices of the target groups.
For example, suppose that the company uses K-means clustering, which creates a partition
in which a covariate value x belongs to the group with the nearest centroid. Let c1 , ..., cK be the
centroids of the K groups, and define a set-valued function C : Rp ‚Üí 2{1,...,K} , where 2{1,...,K}
is the power set of {1, ..., K}, as C(x) ‚â° arg mink‚àà{1,...,K} kx ‚àí ck k. If C(x) is a singleton, x
belongs to the unique group in C(x). If C(x) contains more than one indices, the group to which
x belongs is arbitrarily determined. QPS for this case is given by
Ô£±
Ô£¥
if C(x) ‚à© T = ‚àÖ
Ô£¥
Ô£≤0
CL
p (x) = 0.5
if |C(x)| = 2, x ‚àà ‚àÇ(‚à™k‚ààT Sk )
Ô£¥
Ô£¥
Ô£≥1
if C(x) ‚äÇ T
and pCL (x) ‚àà (0, 1) if |C(x)| ‚â• 3 and x ‚àà ‚àÇ(‚à™k‚ààT Sk ), where |C(x)| is the number of elements in
C(x).35 Thus, it is possible to identify causal effects across the boundary ‚àÇ(‚à™k‚ààT Sk ).
Example 3 (Supervised Learning). Millions of times each year, judges make jail-or-release
decisions that hinge on a prediction of what a defendant would do if released. Many judges
now use proprietary algorithms (like COMPAS criminal risk score) to make such predictions
and use the predictions to support jail-or-release decisions. Using our notation, assume that a
35

If |C(x)| = 2 and x ‚àà ‚àÇ(‚à™k‚ààT Sk ), x is on a linear boundary between one target group and one non-target
group, and hence QPS is 0.5. If |C(x)| ‚â• 3 and x ‚àà ‚àÇ(‚à™k‚ààT Sk ), x is a common endpoint of several group
boundaries, and QPS is determined by the angles at which the boundaries intersect.

27

criminal risk algorithm recommends jailing (Zi = 1) or releasing (Zi = 0) for each defendant i.
The algorithm uses defendant i‚Äôs observable characteristics Xi , including criminal history and
demographics. The algorithm first translates Xi into a risk score r(Xi ), where r : Rp ‚Üí R is
a function estimated by supervised leaning based on past data and assumed to be fixed. For
example, Kleinberg et al. (2017) construct a version of r(Xi ) using gradient boosted decision
trees. The algorithm then uses the risk score to make the final recommendation:
ZiSL ‚â° 1{r(Xi ) > c}, M LSL (x) = 1{r(x) > c},
where c ‚àà R is a constant threshold that is set ex ante.36 A similar procedure applies to
the screening of potential borrowers by banks and insurance companies based on credit scores
estimated by supervised learning (Agarwal, Chomsisengphet, Mahoney and Stroebel, 2017).
A widely-used approach to identifying and estimating treatment effects in these settings is
to use the score r(Xi ) as a continuous univariate running variable and apply a univariate RDD
method (Cowgill, 2018). However, whether r(Xi ) is continuously distributed or not depends on
how the function r is constructed. For example, suppose that r is constructed by a tree-based
algorithm and is the following simple regression tree with three terminal nodes:
Ô£±
Ô£¥
if x1 ‚â§ 0
Ô£¥
Ô£≤r1
r(x) =

r2
Ô£¥
Ô£¥
Ô£≥r
3

if x1 > 0, x2 ‚â§ 0

if x1 > 0, x2 > 0,

where r1 < r2 < c < r3 .37 In this case, the score r(Xi ) is a discrete variable, and hence it may
not be suitable to apply a standard univariate RDD method.
Our approach is applicable to this case as long as at least one of the original multi-dimensional
covariates Xi are continuously distributed. QPS for this case is given by
Ô£±
Ô£¥
0
if x1 < 0 or x2 < 0
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≤0.25
if x1 = x2 = 0
pSL (x) =
Ô£¥
if (x1 = 0, x2 > 0) or (x1 > 0, x2 = 0)
Ô£¥
Ô£¥0.5
Ô£¥
Ô£¥
Ô£≥1
if x1 > 0, x2 > 0.
It is therefore possible to identify causal effects across the boundary {x ‚àà X : (x1 = 0, x2 ‚â•
0) or (x1 > 0, x2 = 0)}.
Example 4 (Policy Eligibility Rules). Medicaid and other welfare policies often decide who
are eligible based on algorithmic rules, as studied by Currie and Gruber (1996) and Brown,
Kowalski and Lurie (2020).38 Using our notation, the state government determines whether each
36

The algorithm sometimes discretizes the original risk score r(Xi ) into d(r(Xi )), where d : R ‚Üí N (Cowgill,
2018). In this case, the algorithm uses the discretized risk score to make the final recommendation: ZiSL ‚â°
1{d(r(Xi )) > c}.
37
If the regression tree is larger, or ensemble methods such as random forests and gradient boosted decision
trees are used to construct r, r is of similar form but has a more complicated expression.
38
These papers estimate the effect of Medicaid eligibility by exploiting variation in the eligibility rule across
states and over time (simulated instrumental variable method). In contrast, our method exploits local variation
in the eligibility status across different individuals given a fixed eligibility rule.

28

individual i is eligible (Zi = 1) or not (Zi = 0) for Medicare. The state government‚Äôs eligibility
rule M LM edicaid maps individual characteristics Xi (e.g. income, family composition) into an
eligibility decision ZiM edicare . A similar procedure also applies to bankruptcy laws (Mahoney,
2015). These policy eligibility rules produce quasi-experimental variation as in Example 3.
Example 5 (Mechanism Design: Matching and Auction). Centralized economic mechanisms
such as matching and auction are also suitable examples, as summarized below:
i
Xi
Zi
Di
Yi

Matching (e.g., School Choice)

Auction

Student
Preference/Priority/Tie-breaker
Whether student i is
assigned treatment school
Whether student i
attends treatment school
Student i‚Äôs
future test score

Bidder
Bid
Whether bidder i
wins the good
same as Zi
Bidder i‚Äôs future
economic performance

In mechanism design and other algorithms with capacity constraints, the treatment recommendation for individual i may depend not only on Xi but also on the characteristics of others.
These interactive situations can be accommodated by our framework if we consider the following
large market setting.39 Suppose that there is a continuum of individuals i ‚àà [0, 1] and that the
recommendation probability for individual i with covariate Xi is determined by a function M as
follows:
Pr(Zi = 1|Xi ; FX‚àíi ) = M (Xi ; FX‚àíi ).
Here FX‚àíi = Pr({j ‚àà [0, 1] \ {i} : Xj ‚â§ x}) is the distribution of X among all individuals
j ‚àà [0, 1] \ {i}. The function M : Rp √ó F ‚Üí [0, 1], where F is a set of distributions on Rp ,
gives the recommendation probability for each individual in the market. With a continuum of
individuals, for any i ‚àà [0, 1], FX‚àíi is the same as the distribution of X in the whole market,
denoted by FX . Therefore, the data generated by the mechanism M are equivalent to the data
generated by the algorithm M L : Rp ‚Üí [0, 1] such that M L(x) ‚â° M (x; FX ) for all x ‚àà Rp . Our
framework is applicable to this large-market interactive setting.
The above discussions can be summarized as follows.
Corollary 5. In all the above examples, there exists x ‚àà int(X ) such that pM L (x) ‚àà (0, 1).
Therefore, a causal effect is identified under Assumptions 1 and 2.

8

Conclusion

As algorithmic decisions become the new norm, the world becomes a mountain of natural experiments and instruments. These instruments enable us to estimate causal treatment effects,
39

The approach proposed by Borusyak and Hull (2020) is applicable to finite-sample settings if the treatment
recommendation probability, which may depend on all individuals‚Äô characteristics, is nondegenerate for multiple
individuals.

29

as we formalize and illustrate in this paper. Our analysis clarifies a few implications for policy
and management practices around algorithmic decision-making. It is important to record the
implementation of algorithms in a replicable, simulatable way, including what input variables Xi
are used to make algorithmic recommendation Zi . Another key point is to record an algorithm‚Äôs
recommendation Zi even if they are superseded by a human decision Di . These data retention
efforts would go a long way to exploit the full potential of algorithms as natural experiments.
In addition to estimating treatment effects, instruments induced by algorithms can also help
inform the improvement of algorithms. To see this, suppose some algorithm M L1 is in use.
As we characterize in this paper, this algorithm M L1 produces instrument IV1 . We can then
use instrument IV1 to make counterfactual predictions about what would happen if we change
M L1 to another algorithm M L2 . We‚Äôd then switch to M L2 if it is predicted to be better than
the previous algorithm. This algorithm change in turn would produce another cycle of natural
experiments and improvements:
M L1 ‚Üí IV1 ‚Üí Algorithm Improvement1 ‚Üí M L2 ‚Üí IV2 ‚Üí Algorithm Improvement2 ...
This cycle of natural experiments and improvements may provide an alternative to wellestablished A/B testing (randomized experiment). A/B testing is often technically, politically,
or managerially infeasible, since deploying a new algorithm is time- and money-consuming, and
entails a risk of failure and ethical concerns (Narita, 2021). This difficulty with randomized
experiment may be alleviated by additionally making use of algorithms as natural experiments.
Our agenda for future research includes a formalization of such optimal policy (algorithm)
learning. Another important topic is data-driven bandwidth selection. This work needs to extend
Imbens and Kalyanaraman (2012) and Calonico et al. (2014)‚Äôs bandwidth selection methods in
the univariate RDD to our setting. Inference on treatment effects in our framework relies on
conventional large sample reasoning. It seems natural to additionally consider permutation or
randomization inference. It will also be challenging but interesting to develop finite-sample
optimal estimation and inference strategies such as those recently introduced by Armstrong and
Koles√°r (2018, 2020) and Imbens and Wager (2019). Finite-sample bias is also a related important
topic for further work (Narita, 2020). Finally, we look forward to empirical applications of our
method in a variety of business, policy, and scientific domains.

30

References
Abdulkadiroƒülu, A., Angrist, J. D., Narita, Y. and Pathak, P. A. (2017). Research
Design Meets Market Design: Using Centralized Assignment for Impact Evaluation. Econometrica, 85 (5), 1373‚Äì1432.
‚Äî, ‚Äî, ‚Äî and Pathak, P. A. (Forthcoming). Breaking Ties: Regression Discontinuity Design
Meets Market Design. Econometrica.
Agarwal, S., Chomsisengphet, S., Mahoney, N. and Stroebel, J. (2017). Do Banks
Pass Through Credit Expansions to Consumers Who Want to Borrow? Quarterly Journal of
Economics, 133 (1), 129‚Äì190.
Angrist, J. D. and Pischke, J.-S. (2008). Mostly Harmless Econometrics: An Empiricist‚Äôs
Companion. Princeton University Press.
Arias, J. E., Rubio-Ram√≠rez, J. F. and Waggoner, D. F. (2018). Inference Based on
Structural Vector Autoregressions Identified with Sign and Zero Restrictions: Theory and
Applications. Econometrica, 86 (2), 685‚Äì720.
Armstrong, T. B. and Koles√°r, M. (2018). Optimal Inference in a Class of Regression
Models. Econometrica, 86 (2), 655‚Äì683.
‚Äî and Koles√°r, M. (2020). Finite-Sample Optimal Estimation and Inference on Average Treatment Effects Under Unconfoundedness. Econometrica.
Athey, S. and Imbens, G. W. (2017). The State of Applied Econometrics: Causality and
Policy Evaluation. Journal of Economic Perspectives, 31 (2), 3‚Äì32.
Belloni, A., Chernozhukov, V., Fern√°ndez-Val, I. and Hansen, C. (2017). Program
Evaluation and Causal Inference with High-Dimensional Data. Econometrica, 85 (1), 233‚Äì
298.
Bonhomme, S., Lamadon, T. and Manresa, E. (2019). Discretizing Unobserved Heterogeneity. University of Chicago, Becker Friedman Institute for Economics Working Paper No.
2019-16, unpublished Manuscript, University of Chicago.
Bornn, L., Shephard, N. and Solgi, R. (2019). Moment conditions and bayesian nonparametrics. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 81 (1),
5‚Äì43.
Borusyak, K. and Hull, P. (2020). Non-Random Exposure to Exogenous Shocks: Theory
and Applications. NBER Working Paper No. 27845, unpublished Manuscript, University of
Chicago.
Brown, D., Kowalski, A. E. and Lurie, I. Z. (2020). Long-Term Impacts of Childhood
Medicaid Expansions on Outcomes in Adulthood. The Review of Economic Studies, 87 (2),
729‚Äì821.
31

Bundorf, K., Polyakova, M. and Tai-Seale, M. (2019). How Do Humans Interact with
Algorithms? Experimental Evidence from Health Insurance. NBER Working Paper No. 25976.
Calonico, S., Cattaneo, M. D. and Titiunik, R. (2014). Robust Nonparametric Confidence
Intervals for Regression-Discontinuity Designs. Econometrica, 82 (6), 2295‚Äì2326.
Cattaneo, M. D., Frandsen, B. R. and Titiunik, R. (2015). Randomization Inference in
the Regression Discontinuity Design: An Application to Party Advantages in the US Senate.
Journal of Causal Inference, 3 (1), 1‚Äì24.
‚Äî, Titiunik, R. and Vazquez-Bare, G. (2017). Comparing Inference Approaches for RD
Designs: A Reexamination of the Effect of Head Start on Child Mortality. Journal of Policy
Analysis and Management, 36 (3), 643‚Äì681.
‚Äî, ‚Äî, ‚Äî and Keele, L. (2016). Interpreting Regression Discontinuity Designs with Multiple
Cutoffs. Journal of Politics, 78 (4), 1229‚Äì1248.
Cohen, P., Hahn, R., Hall, J., Levitt, S. and Metcalfe, R. (2016). Using Big Data to
Estimate Consumer Surplus: The Case of Uber. NBER Working Paper No. 22627, unpublished
Manuscript, University of Chicago.
Cowgill, B. (2018). The Impact of Algorithms on Judicial Discretion: Evidence from Regression
Discontinuities. Unpublished Manuscript, Columbia Business School.
Crasta, G. and Malusa, A. (2007). The Distance Function from the Boundary in a Minkowski
Space. Transactions of the American Mathematical Society, 359, 5725‚Äì5759.
Currie, J. and Gruber, J. (1996). Health Insurance Eligibility, Utilization of Medical Care,
and Child Health. Quarterly Journal of Economics, 111 (2), 431‚Äì466.
Dong, Y. (2018). Alternative Assumptions to Identify LATE in Fuzzy Regression Discontinuity
Designs. Oxford Bulletin of Economics and Statistics, 80 (5), 1020‚Äì1027.
Dranove, D., Garthwaite, C. and Ody, C. (2017). How do nonprofits respond to negative
wealth shocks? the impact of the 2008 stock market collapse on hospitals. RAND Journal of
Economics, 48 (2), 485‚Äì525.
Duggan, M. G. (2000). Hospital Ownership and Public Medical Spending. Quarterly Journal
of Economics, 115 (4), 1343‚Äì1373.
Einav, L., Finkelstein, A., Mullainathan, S. and Obermeyer, Z. (2018). Predictive
Modeling of U.S. Health Care Spending in Late Life. Science, 360 (6396), 1462‚Äì1465.
Ernst, D., Geurts, P. and Wehenkel, L. (2005). Tree-Based Batch Mode Reinforcement
Learning. Journal of Machine Learning Research, 6, 503‚Äì556.
Frandsen, B. R. (2017). Party Bias in Union Representation Elections: Testing for Manipulation in the Regression Discontinuity Design When the Running Variable is Discrete. In
32

Regression Discontinuity Designs: Theory and Applications, Emerald Publishing Limited, pp.
281‚Äì315.
Fr√∂lich, M. (2007). Regression Discontinuity Design with Covariates. IZA Discussion Paper
No. 3024, unpublished Manuscript, University of St. Gallen.
Fr√∂lich, M. and Huber, M. (2019). Including Covariates in the Regression Discontinuity
Design. Journal of Business and Economic Statistics, 37 (4), 736‚Äì748.
Gulshan, V. et al. (2016). Development and Validation of a Deep Learning Algorithm for
Detection of Diabetic Retinopathy in Retinal Fundus Photographs. Journal of the American
Medical Association, 316 (22), 2402‚Äì2410.
Hahn, J., Todd, P. and van der Klaauw, W. (2001). Identification and Estimation of
Treatment Effects with a Regression-Discontinuity Design. Econometrica, 69 (1), 201‚Äì209.
Hoffman, M., Kahn, L. B. and Li, D. (2017). Discretion in Hiring. Quarterly Journal of
Economics, 133 (2), 765‚Äì800.
Horton, J. J. (2017). The Effects of Algorithmic Labor Market Recommendations: Evidence
from a Field Experiment. Journal of Labor Economics, 35 (2), 345‚Äì385.
Hull, P. (2018). Subtracting the Propensity Score in Linear Models. Unpublished Manuscript,
MIT.
Imbens, G. and Kalyanaraman, K. (2012). Optimal Bandwidth Choice for the Regression
Discontinuity Estimator. The Review of Economic Studies, 79 (3), 933‚Äì959.
‚Äî and Wager, S. (2019). Optimized regression discontinuity designs. Review of Economics and
Statistics, 101 (2), 264‚Äì278.
Imbens, G. W. and Angrist, J. D. (1994). Identification and Estimation of Local Average
Treatment Effects. Econometrica, 62 (2), 467‚Äì475.
Kakani, P., Chandra, A., Mullainathan, S. and Obermeyer, Z. (2020). Allocation of
COVID-19 Relief Funding to Disproportionately Black Counties. Journal of the American
Medical Association (JAMA), 324 (10), 1000‚Äì1003.
Keele, L. J. and Titiunik, R. (2015). Geographic Boundaries as Regression Discontinuities.
Political Analysis, 23 (1), 127‚Äì155.
Khullar, D., Bond, A. M. and Schpero, W. L. (2020). COVID-19 and the Financial Health
of US Hospitals. Journal of the American Medical Association (JAMA), 323 (21), 2127‚Äì2128.
Kleinberg, J., Lakkaraju, H., Leskovec, J., Ludwig, J. and Mullainathan, S. (2017).
Human Decisions and Machine Predictions. Quarterly Journal of Economics, 133 (1), 237‚Äì293.
Krantz, S. G. and Parks, H. R. (2008). Geometric Integration Theory. Birkh√§user Basel.
33

Li, L., Chu, W., Langford, J. and Schapire, R. E. (2010). A Contextual-Bandit Approach
to Personalized News Article Recommendation. Proceedings of the 19th international conference on World Wide Web (WWW), pp. 661‚Äì670.
Li, S. (2011). Concise Formulas for the Area and Volume of a Hyperspherical Cap. Asian Journal
of Mathematics and Statistics, 4, 66‚Äì70.
Mahoney, N. (2015). Bankruptcy as Implicit Health Insurance. American Economic Review,
105 (2), 710‚Äì46.
Mullainathan, S. and Spiess, J. (2017). Machine learning: An applied econometric approach.
Journal of Economic Perspectives, 31 (2), 87‚Äì106.
Narita, Y. (2020). A Theory of Quasi-Experimental Evaluation of School Quality. Management
Science.
‚Äî (2021). Incorporating ethics and welfare into randomized experiments. Proceedings of the
National Academy of Sciences, 118 (1).
‚Äî, Yasui, S. and Yata, K. (2019). Efficient Counterfactual Learning from Bandit Feedback.
Proceedings of the 33rd AAAI Conference on Artificial Intelligence, pp. 4634‚Äì4641.
Papay, J. P., Willett, J. B. and Murnane, R. J. (2011). Extending the RegressionDiscontinuity Approach to Multiple Assignment Variables. Journal of Econometrics, 161 (2),
203‚Äì207.
Precup, D. (2000). Eligibility Traces for Off-Policy Policy Evaluation. Proceedings of the Seventeenth International Conference on Machine Learning, pp. 759‚Äì766.
Qiao, W. (2021). Nonparametric Estimation of Surface Integrals on Level Sets. Bernoulli, 27 (1),
155‚Äì191.
Rosenbaum, P. R. and Rubin, D. B. (1983). The Central Role of the Propensity Score in
Observational Studies for Causal Effects. Biometrika, 70 (1), 41‚Äì55.
Saito, Y., Aihara, S., Matsutani, M. and Narita, Y. (2021). Open bandit dataset and
pipeline: Towards realistic and reproducible off-policy evaluation. Unpublished Manuscript,
Tokyo Institute of Technology.
Sekhon, J. S. and Titiunik, R. (2017). On Interpreting the Regression Discontinuity Design as
a Local Experiment. In Regression Discontinuity Designs: Theory and Applications, Emerald
Publishing Limited, pp. 1‚Äì28.
Stein, E. M. and Shakarchi, R. (2005). Real Analysis: Measure Theory, Integration, and
Hilbert Spaces. Princeton Lectures in Analysis, Princeton, NJ: Princeton Univ. Press.
Voelker, A. R., Gosmann, J. and Stewart, T. C. (2017). Efficiently Sampling Vectors and
Coordinates from the n-Sphere and n-Ball. Centre for Theoretical Neuroscience - Technical
Report.
34

Williams, R. J. (1992). Simple Statistical Gradient-Following Algorithms for Connectionist
Reinforcement Learning. Machine Learning, 8, 229‚Äì256.
Wong, V. C., Steiner, P. M. and Cook, T. D. (2013). Analyzing Regression-Discontinuity
Designs with Multiple Assignment Variables: A Comparative Study of Four Estimation Methods. Journal of Educational and Behavioral Statistics, 38 (2), 107‚Äì141.
Zajonc, T. (2012). Regression Discontinuity Design with Multiple Forcing Variables. Essays on
Causal Inference for Public Policy, pp. 45‚Äì81.

35

Figure 1: Example of the Quasi Propensity Score

Figure 2: Illustration of the Change of Variables Techniques
(a)

(b)

36

Table 1: Bias, SD, and RMSE of Estimators and Coverage of 95% Confidence Intervals
Œ¥ = 0.01

Our Method: 2SLS with Quasi Propensity Score Controls
Œ¥ = 0.05
Œ¥ = 0.1
Œ¥ = 0.25
Œ¥ = 0.5

Œ¥=1

2SLS
with M L
Controls

OLS
with No
Controls

Panel A: Homogeneous Conditional Effects
Estimand: ATE = ATE(RCT) = 0
Bias
.603
.634
SD
.304
.205
RMSE
.675
.667

.644
.157
.663

.659
.110
.668

.684
.078
.689

.740
.061
.842

.572
.372
.683

.754
.024
.754

Estimand: LATE = LATE(RCT) = 0.564
Bias
.039
.070
.080
SD
.304
.205
.157
RMSE
.306
.217
.176

.095
.110
.145

.120
.078
.143

.176
.061
.186

.008
.372
.372

.190
.024
.191

Coverage
Avg N

94.8%

92.8%

92.9%

84.6%

69.6%

18.6%

‚Äî

‚Äî

235

727

1275

2567

3995

5561

100

10000

Panel B: Heterogeneous Conditional Effects
Estimand: ATE = ATE(RCT) = 0
Bias
.568
.587
SD
.331
.222
RMSE
.657
.628

.589
.170
.613

.604
.118
.615

.636
.083
.642

.709
.063
.712

.545
.399
.676

1.192
.025
1.193

Estimand: LATE = 0.564
Bias
.004
SD
.331
RMSE
.331

.023
.222
.223

.025
.170
.172

.040
.118
.125

.072
.083
.110

.145
.063
.158

‚àí.019
.399
.399

.628
.025
.629

Estimand: LATE(RCT) = 0.559
Bias
.009
.028
SD
.331
.222
RMSE
.331
.224

.030
.170
.173

.045
.118
.127

.077
.083
.114

.150
.063
.163

‚àí.014
.399
.399

.633
.025
.634

Coverage
Avg N

95.9%

94.8%

95.0%

93.2%

87.1%

37.4%

‚Äî

‚Äî

235

723

1274

2567

3993

5561

100

10000

Notes: This table shows the bias, the standard deviation (SD) and the root mean squared error (RMSE) of 2SLS with
Quasi Propensity Score controls, 2SLS with M L controls, and OLS with no controls. These statistics are computed with
the estimand set to ATE, ATE(RCT), LATE, or LATE(RCT). The row ‚ÄúCoverage‚Äù in each panel shows the probabilities
s , Œ≤ÃÇ s + 1.96œÉÃÇ s ] contains LATE(RCT), where Œ≤ÃÇ s is the 2SLS
that the 95% confidence intervals of the form [Œ≤ÃÇ1s ‚àí 1.96œÉÃÇn
n
1
1
s is its heteroskedasticity-robust standard error. We use 1, 000
estimate with Quasi Propensity Score controls and œÉÃÇn
replications of a size 10, 000 simulated sample to compute these statistics. We use several possible values of Œ¥ to compute
the Quasi Propensity Score. All Quasi Propensity Scores are computed by averaging 400 simulation draws of the M L
value. Panel A reports the results under the model in which the treatment effect does not depend on Xi . Panel B reports
the results under the model in which the treatment effect depends on Xi . The bottom row ‚ÄúAvg N ‚Äù in each panel shows
the average number of observations used for estimation (i.e., the average number of observations for which the Quasi
Propensity Score or the M L value is strictly between 0 and 1).

37

Figure 3: Three-dimensional Regression Discontinuity in Hospital Funding Eligibility.

Notes: We remove hospitals above the 99th percentile of disproportionate patient percentage
and uncompensated care per bed, for visibility purposes. The top figure visualizes the three
dimensions that determine safety net eligibility. The bottom figures show the data points plotted
along 2 out of 3 dimensions. The bottom left panel plots disproportionate patient percentage
against profit margin, while the bottom right panel plots uncompensated care per bed against
profit margin.
38

Table 2: Outcome Variables and Hospital Characteristics
All

Ineligible
Hospitals

Eligible
Hospitals

Patients in adult inpatient beds with lab-confirmed or suspected COVID
Patients in adult inpatient beds with lab-confirmed COVID (including those
with both lab-confirmed COVID and influenza)
Patients in adult ICU beds with lab-confirmed or suspected COVID
Patients in adult ICU beds who have lab-confirmed COVID
(including those with both lab-confirmed COVID and influenza)
Observations

105.37

98.32

135.60

80.12
31.40

73.90
28.93

107.65
42.17

26.67
4,008

24.43
3,291

36.71
717

Panel B: Hospital Characteristics
Beds
Interns and residents (full-time equivalents) per bed
Adult and pediatric hospital beds
Ownership: Proprietary (for-profit)
Ownership: Governmental
Ownership: Voluntary (non-profit)
Inpatient length of stay
Employees on payroll (full-time equivalents)
Observations

143.66
.06
120.26
.19
.22
.58
9.21
973.90
4,633

134.60
.05
113.29
.20
.22
.58
10.14
897.31
3,852

188.35
.11
154.66
.18
.23
.59
4.66
1351.57
781

Panel A: Outcome Variable Means

Notes: This table reports averages of outcome variables and hospital characteristics by safety net eligibility. A safety net
hospital is defined as any acute care hospital with disproportionate patient percentage of 20.2% or greater, annual
uncompensated care of at least $25,000 per bed and profit margin of 3.0% or less. Panel A reports the outcome variable
means. Outcome variable estimates are 7 day sums for the week spanning July 31st 2020 to August 6th 2020. Inpatient
bed totals also include observation beds. Panel B reports the means for hospital characteristics for the financial year 2018.

39

Table 3: Covariate Balance Regressions
Our Method with Quasi Propensity Score Controls

No
Controls
(1)

Œ¥ = 0.01
(2)

Œ¥ = 0.025
(3)

Œ¥ = 0.05
(4)

Œ¥ = 0.075
(5)

Œ¥ = 0.1
(6)

Œ¥ = 0.25
(7)

Œ¥ = 0.5
(8)

Mean
(9)

Beds

53.75***
(7.05)
N=4633

204.96
(106.65)
N=89

28.85
(67.20)
N=235

9.92
(47.17)
N=473

0.42
(38.63)
N=656

4.22
(33.69)
N=852

16.01
(20.11)
N=1699

8.95
(14.36)
N=2339

134.60

Costs per discharge
(in thousands)

‚àí49.95**
(17.93)
N=3539

4.12
(2.12)
N=89

3.52*
(1.51)
N=235

1.72
(1.24)
N=473

‚àí6.76
(8.34)
N=656

‚àí0.43
(2.06)
N=852

5.68
(4.25)
N=1699

6.33
(4.80)
N=2339

66.28

Disproportionate
payment percent

0.21***
(0.01)
N=4633

‚àí0.09
(0.09)
N=89

‚àí0.09
(0.07)
N=235

‚àí0.09
(0.07)
N=473

‚àí0.08
(0.05)
N=656

‚àí0.09
(0.05)
N=852

‚àí0.06*
(0.02)
N=1699

‚àí0.07***
(0.02)
N=2339

.18

Full time employees

454.26***
(69.23)
N=4626

2,841.76
(1,729.87)
N=89

307.37
(1,009.69)
N=234

127.92
(652.56)
N=472

27.38
(491.24)
N=655

‚àí11.29
(428.97)
N=851

200.42
(218.73)
N=1696

114.27
(141.57)
N=2336

897.32

Medicare net revenue
(in millions)

18.36***
(2.39)
N=4511

37.35
(30.38)
N=88

‚àí9.10
(18.55)
N=234

‚àí4.61
(14.19)
N=471

‚àí2.60
(11.80)
N=653

0.05
(10.77)
N=848

3.59
(6.66)
N=1659

‚àí0.28
(4.62)
N=2295

20.04

Occupancy

0.07***
(0.01)
N=4624

0.19
(0.10)
N=89

0.07
(0.06)
N=235

‚àí0.00
(0.04)
N=473

0.01
(0.04)
N=656

0.01
(0.03)
N=852

0.03
(0.02)
N=1699

0.04**
(0.01)
N=2339

.44

Operating margin

‚àí0.11***
(0.01)
N=4541

‚àí0.04
(0.06)
N=88

‚àí0.01
(0.05)
N=234

0.03
(0.03)
N=465

0.02
(0.03)
N=646

0.03
(0.03)
N=841

0.06***
(0.02)
N=1651

0.07***
(0.01)
N=2285

.02

Profit margin

‚àí0.11***
(0.01)
N=4633

‚àí0.03
(0.06)
N=89

‚àí0.01
(0.04)
N=235

0.02
(0.03)
N=473

0.01
(0.03)
N=656

0.02
(0.02)
N=852

0.04**
(0.01)
N=1699

0.06***
(0.01)
N=2339

.04

11,010.77
(10,352.08)
N=235

‚àí4,644.86
(8,868.36)
N=473

‚àí10,167.80
(7,606.21)
N=656

‚àí11,096.86
(7,274.64)
N=852

‚àí7,850.91
(4,520.95)
N=1699

‚àí6,018.15
(3,638.71)
N=2339

56,556.02

.439

.565

.738

.236

.001

0

Uncompensated
care per bed
p-value for joint significance

19,540.28*** 3,654.68
(3,827.22)
(12,124.80)
N=4633
N=89
0

.697

Notes: This table shows the results of the covariate balance regressions at the hospital level. The dependent variables for these regressions are drawn from the
Healthcare Cost Report Information System for the financial year 2018. Disproportionate patient percentage, profit margin and uncompensated care per bed
are used to determine the hospital‚Äôs safety net funding eligibility. Other dependent variables shown indicate the financial health and utilization of the hospitals.
In column 1, we regress the dependent variables on the safety net eligibility of the hospital with no controls. In columns 2‚Äì8, we regress the dependent
variables on funding eligibility controlling for the Quasi Propensity Score with different values of bandwidth Œ¥. All Quasi Propensity Scores are computed by
averaging 1,000 simulation draws. Column 9 shows the mean of dependent variables for hospitals that are ineligible to receive safety net funding. Robust
standard errors are reported in the parenthesis and number of observations are reported separately for each regression. The last row reports the p-value of the
joint significance test.

40

Table 4: Estimated Effects of Safety Net Funding on Hospital Utilization
OLS
with No
Controls

2SLS
with No
Controls

(1)

(2)

Our Method: 2SLS with Quasi Propensity Score Controls
Œ¥=
0.01
(3)

Œ¥=
0.025
(4)

Œ¥=
0.05
(5)

Œ¥=
0.075
(6)

Œ¥ = 0.1
(7)

Œ¥=
0.25
(8)

Œ¥ = 0.5
(9)

7 day sum of patient currently hospitalized in an adult inpatient bed (including observation beds) who have
lab-confirmed or suspected COVID
First stage
(in millions)
$1mm of funding
Observations

5.52***
(0.68)
3544

13.79***
(0.49)
2.70***
(0.58)
3544

15.07*
(5.79)
‚àí1.16
(5.36)
74

13.81***
(3.56)
‚àí1.24
(5.11)
192

14.61***
(2.29)
‚àí2.40
(4.79)
386

14.02***
(1.86)
‚àí4.55
(4.64)
535

13.85***
(1.64)
‚àí3.16
(3.70)
698

13.84***
(1.02)
0.15
(1.64)
1375

13.11***
(0.73)
‚àí0.28
(1.22)
1934

7 day sum of patient currently hospitalized in an adult inpatient bed (including observation beds) who have
lab-confirmed COVID (including those with both lab-confirmed COVID and influenza)
First stage
(in millions)
$1mm of funding
Observations

4.50***
(0.63)
3572

13.91***
(0.50)
2.43***
(0.50)
3572

16.70**
(6.10)
‚àí0.09
(4.09)
71

14.81***
(3.68)
‚àí1.77
(3.70)
188

15.32***
(2.34)
1.79
(2.17)
378

14.62***
(1.91)
‚àí0.21
(2.00)
527

14.41***
(1.67)
‚àí0.07
(1.74)
689

14.01***
(1.04)
‚àí0.08
(1.17)
1355

13.24***
(0.74)
‚àí0.52
(0.97)
1911

7 day sum of patient currently hospitalized in a designated adult ICU bed who have lab-confirmed or suspected COVID
First stage
(in millions)
$1mm of funding
Observations

1.66***
(0.21)
3452

13.92***
(0.50)
0.95***
(0.18)
3452

14.70*
(5.58)
0.89
(1.39)
72

13.89***
(3.50)
0.87
(1.18)
183

16.02***
(2.34)
0.47
(0.73)
367

15.12***
(1.92)
‚àí0.16
(0.70)
507

14.68***
(1.69)
0.06
(0.61)
659

14.26***
(1.06)
0.03
(0.42)
1300

13.27***
(0.75)
‚àí0.26
(0.36)
1832

7 day sum of patient currently hospitalized in a designated adult ICU bed who have lab-confirmed COVID
(including those with both lab-confirmed COVID and influenza)
First stage
(in millions)
$1mm of funding
Observations

1.50***
(0.21)
3510

13.93***
(0.50)
0.88***
(0.17)
3510

15.78*
(6.07)
0.49
(1.45)
67

14.29***
(3.73)
0.08
(1.26)
178

16.06***
(2.42)
0.28
(0.70)
363

15.34***
(2.00)
‚àí0.10
(0.64)
503

14.92***
(1.75)
0.04
(0.57)
648

14.37***
(1.09)
‚àí0.10
(0.40)
1305

13.50***
(0.76)
‚àí0.29
(0.34)
1853

Notes: In this table we regress relevant outcomes at the hospital level on safety net funding. Column 1 presents the results
of OLS regression of the outcome variables on safety net funding without any controls. In columns 2‚Äì9, we instrument
safety net funding with eligibility to receive this funding and present the results of 2SLS regressions. In columns 2‚Äì9, the
first stage shows the effect of being deemed eligible on the amount of relief funding received by hospitals, in millions of
dollars. Column 2 shows the results of a 2SLS regression with no controls. In columns 3‚Äì9, we run this regression
controlling for the Quasi Propensity Score with different values of bandwidth Œ¥ on the sample with nondegenerate Quasi
Propensity Score. All Quasi Propensity Scores are computed by averaging 1,000 simulation draws. Robust standard errors
are reported in parentheses.

41

A
A.1

Extensions and Discussions
Related Literature: Details

In this section, we discuss the related methodological literature on the multidimensional RDD in
detail. Imbens and Wager (2019) propose the finite-sample-minimax linear estimator of the form
Pn
i=1 Œ≥i Yi and uniform confidence intervals for treatment effects in the multidimensional RDD.
One version of their approach constructs a linear estimator by choosing the weight (Œ≥i )ni=1 greedily
to make the inference as precise as possible. Although their estimator is favorable in terms of
precision, it is not obvious what estimand the estimator estimates, without assuming a constant
treatment effect. The other version of Imbens and Wager (2019)‚Äôs approach and some other
existing approaches (Zajonc, 2012; Keele and Titiunik, 2015) consider nonparametric estimation
of the conditional average treatment effect E[Yi (1) ‚àí Yi (0)|Xi = x] for a specified boundary point
x. The estimand has a clear interpretation, but ‚Äúwhen curvature is nonnegligible, equation (6)
can effectively make use of only data near the specified focal point c, thus resulting in relatively
long confidence intervals‚Äù (Imbens and Wager, 2019, p. 268), where equation (6) defines their
estimator.
To obtain more precise estimates while keeping interpretability, several papers studying a twodimensional RDD, including Zajonc (2012) and Keele and Titiunik (2015), propose to estimate
an integral of conditional average treatment effects over the boundary. Their approach first
nonparametrically estimates E[Yi (1) ‚àí Yi (0)|Xi = x] and the density of Xi for a large number of
points x in the boundary and then computes the weighted average of the estimated conditional
average treatment effects with the weight set to the estimated density.
The above approach is difficult to implement, however, when Xi is high dimensional or the
decision algorithm is a complex, black box function of Xi , for the following reasons. First, it
is computationally demanding to estimate E[Yi (1) ‚àí Yi (0)|Xi = x] for numerous points in the
boundary such that the weighted average well approximates the integral of E[Yi (1)‚àíYi (0)|Xi = x]
over the boundary. Second, identifying boundary points from a general decision algorithm itself
is hard unless it has a known analytical form. By contrast, we develop an estimator that uses
observations near all the boundary points without tracing out the boundary or knowing its
analytical form, thus alleviating the limitations of existing estimators.

A.2

QPS May Not Exist But Does Exist for Almost All x

Figure 4 shows an example where QPS does not exist at 0. In this example, Xi is two dimensional,
and
(
1 if 3( 21 )k‚àí1 < kxk ‚â§ 4( 12 )k‚àí1 for some k = 1, 2, ¬∑ ¬∑ ¬∑
M L(x) =
0 if 2( 12 )k‚àí1 < kxk ‚â§ 3( 12 )k‚àí1 for some k = 1, 2, ¬∑ ¬∑ ¬∑ .
It is shown that
(
pM L (0; Œ¥) =

7
12
7
27

if Œ¥ = 4( 12 )k‚àí1 for some k = 1, 2, ¬∑ ¬∑ ¬∑
if Œ¥ = 3( 12 )k‚àí1 for some k = 1, 2, ¬∑ ¬∑ ¬∑ .
42

Figure 4: An example of the M L algorithm for which the Quasi Propensity Score fails to exist
Therefore, limŒ¥‚Üí0 pM L (0; Œ¥) does not exist.
Nevertheless, the Quasi Propensity Score exists for almost every x, as shown in the following
proposition.
Proposition A.1. pM L (x) exists and is equal to M L(x) for almost every x ‚àà X (with respect
to the Lebesgue measure).
Proof. See Appendix C.5.

A.3

Discrete Covariates

In this section, we provide the definition of QPS and identification and consistency results when
Xi includes discrete covariates. Suppose that Xi = (Xdi , Xci ), where Xdi ‚àà Rpd is a vector
of discrete covariates, and Xci ‚àà Rpc is a vector of continuous covariates. Let Xd denote the
support of Xdi and be assumed to be finite. We also assume that Xci is continuously distributed
conditional on Xdi , and let Xc (xd ) denote the support of Xci conditional on Xdi = xd for each
xd ‚àà Xd . Let Xc,0 (xd ) = {xc ‚àà Xc (xd ) : M L(xd , xc ) = 0} and Xc,1 (xd ) = {xc ‚àà Xc (xd ) :
M L(xd , xc ) = 1}.
Define QPS as follows: for each x = (xd , xc ) ‚àà X ,
R
‚àó
‚àó
B(xc ,Œ¥) M L(xd , xc )dxc
ML
R
p (x; Œ¥) ‚â°
,
‚àó
B(xc ,Œ¥) dxc
pM L (x) ‚â° lim pM L (x; Œ¥),
Œ¥‚Üí0

where B(xc , Œ¥) = {x‚àóc ‚àà Rpc : kxc ‚àí x‚àóc k ‚â§ Œ¥} is the Œ¥-ball around xc ‚àà Rpc . In other words, we
take the average of the M L(xd , x‚àóc ) values when x‚àóc is uniformly distributed on B(xc , Œ¥) holding
xd fixed, and let Œ¥ ‚Üí 0. Below, we assume that Assumptions 1, 2, 3 and 4 hold conditional on
Xdi .
43

Assumption A.1 (Almost Everywhere Continuity of M L).
(a) For every xd ‚àà Xd , M L(xd , ¬∑) is continuous almost everywhere with respect to the Lebesgue
measure Lpc .
(b) For every xd ‚àà Xd , Lpc (Xc,k (xd )) = Lpc (int(Xc,k (xd ))) for k = 0, 1.
A.3.1

Identification

Assumption A.2 (Local Mean Continuity). For every xd ‚àà Xd and z ‚àà {0, 1}, the conditional
expectation functions E[Yzi |Xi = (xd , xc )] and E[Di (z)|Xi = (xd , xc )] are continuous in xc at
any point xc ‚àà Xc (xd ) such that pM L (xd , xc ) ‚àà (0, 1) and M L(xd , xc ) ‚àà {0, 1}.
Let intc (X ) = {(xd , xc ) ‚àà X : xc ‚àà int(Xc (xd ))}. We say that a set A ‚äÇ Rp is open
relative to X if there exists an open set U ‚äÇ Rp such that A = U ‚à© X . For a set A ‚äÇ Rp , let
XdA = {xd ‚àà Xd : (xd , xc ) ‚àà A for some xc ‚àà Rpc } and XcA (xd ) = {xc ‚àà Xc : (xd , xc ) ‚àà A} for
each xd ‚àà XdA .
Proposition A.2. Under Assumptions A.1 and A.2:
(a) E[Y1i ‚àí Y0i |Xi = x] and E[Di (1) ‚àí Di (0)|Xi = x] are identified for every x ‚àà intc (X ) such
that pM L (x) ‚àà (0, 1).
(b) Let A be any subset of X open relative to X such that pM L (x) exists for all x ‚àà A. Then
either E[Y1i ‚àí Y0i |Xi ‚àà A] or E[Di (1) ‚àí Di (0)|Xi ‚àà A], or both are identified only if
pM L (x) ‚àà (0, 1) for almost every xc ‚àà XcA (xd ) for every xd ‚àà XdA .
Proof. See Appendix C.7.
A.3.2

Estimation

For each xd ‚àà Xd , let ‚Ñ¶‚àó (xd ) = {xc ‚àà Rpc : M L(xd , xc ) = 1}. Also, let Xd‚àó = {xd ‚àà Xd :
Var(M L(Xi )|Xdi = xd ) > 0}, and let fXc |Xd denote the probability density function of Xci
conditional on Xdi . In addition, for each xd ‚àà Xd , let
C ‚àó (xd ) = {xc ‚àà Rpc : M L(xd , ¬∑) is continuously differentiable at xc },
and let D‚àó (xd ) = Rpc \ C ‚àó (xd ).
Assumption A.3.
(a) (Finite Moments) E[Yi4 ] < ‚àû.
(b) (Nonzero First Stage) There exists a constant c > 0 such that E[Di (1) ‚àí Di (0)|Xi = x] > c
for every x ‚àà X such that pM L (x) ‚àà (0, 1).
(c) (Nonzero Conditional Variance) If Pr(M L(Xi ) ‚àà (0, 1)) > 0, then Var(M L(Xi )|M L(Xi ) ‚àà
(0, 1)) > 0.
44

If Pr(M L(Xi ) ‚àà (0, 1)) = 0, then the following conditions (d)‚Äì(g) hold.
(d) (Nonzero Variance) Xd‚àó 6= ‚àÖ.
(e) (C 2 Boundary of ‚Ñ¶‚àó (xd )) For each xd ‚àà Xd‚àó , there exists a partition {‚Ñ¶‚àó1 (xd ), ..., ‚Ñ¶‚àóM (xd )}
of ‚Ñ¶‚àó (xd ) such that
(i) dist(‚Ñ¶‚àóm (xd ), ‚Ñ¶‚àóm0 (xd )) > 0 for any m, m0 ‚àà {1, ..., M } such that m 6= m0 ;
(ii) ‚Ñ¶‚àóm (xd ) is nonempty, bounded, open, connected and twice continuously differentiable
for each m ‚àà {1, ..., M }.
(f ) (Regularity of Deterministic M L)
(i) For each xd ‚àà Xd‚àó , Hpc ‚àí1 (‚àÇ‚Ñ¶‚àó (xd )) < ‚àû, and

R

‚àÇ‚Ñ¶‚àó (xd ) fXc |Xd (xc |xd )dH

pc ‚àí1 (x )
c

> 0.

(ii) There exists Œ¥ > 0 such that M L(xd , xc ) = 0 for almost every xc ‚àà N (Xc (xd ), Œ¥) \
‚Ñ¶‚àó (xd ).
(g) (Conditional Means and Density near ‚àÇ‚Ñ¶‚àó (xd )) For each xd ‚àà Xd‚àó , there exists Œ¥ > 0 such
that
(i) E[Y1i |Xi = (xd , ¬∑)], E[Y0i |Xi = (xd , ¬∑)], E[Di (1)|Xi = (xd , ¬∑)], E[Di (0)|Xi = (xd , ¬∑)]
and fXc |Xd (¬∑|xd ) are continuously differentiable and have bounded partial derivatives
on N (‚àÇ‚Ñ¶‚àó (xd ), Œ¥);
(ii) E[Y1i2 |Xi = (xd , ¬∑)], E[Y0i2 |Xi = (xd , ¬∑)], E[Y1i Di (1)|Xi = (xd , ¬∑)] and E[Y0i Di (0)|Xi =
(xd , ¬∑)] are continuous on N (‚àÇ‚Ñ¶‚àó (xd ), Œ¥);
(iii) E[Yi4 |Xi = (xd , ¬∑)] is bounded on N (‚àÇ‚Ñ¶‚àó (xd ), Œ¥).
Assumption A.4. If Pr(M L(Xi ) ‚àà (0, 1)) > 0, then the following conditions (a)‚Äì(c) hold.
(a) (Probability of Neighborhood of D‚àó (xd )) For each xd ‚àà Xd‚àó , Pr(Xi ‚àà N (D‚àó (xd ), Œ¥)) =
O(Œ¥).
(b) (Bounded Partial Derivatives of M L) For each xd ‚àà Xd‚àó , the partial derivatives of M L(xd , ¬∑)
are bounded on C ‚àó (xd ).
(c) (Bounded Conditional Mean) For each xd ‚àà Xd‚àó , E[Yi |Xi = (xd , ¬∑)] is bounded on Xc (xd ).
Theorem A.1. Suppose that Assumptions A.1 and A.3 hold, and that Œ¥n ‚Üí 0, nŒ¥n ‚Üí ‚àû and
Sn ‚Üí ‚àû as n ‚Üí ‚àû. Then the 2SLS estimators Œ≤ÃÇ1 and Œ≤ÃÇ1s converge in probability to
Œ≤1 ‚â° lim E[œâi (Œ¥)(Yi (1) ‚àí Yi (0))],
Œ¥‚Üí0

where
œâi (Œ¥) =

pM L (Xi ; Œ¥)(1 ‚àí pM L (Xi ; Œ¥))(Di (1) ‚àí Di (0))
.
E[pM L (Xi ; Œ¥)(1 ‚àí pM L (Xi ; Œ¥))(Di (1) ‚àí Di (0))]

Suppose, in addition, that Assumptions A.4 and 5 hold and that nŒ¥n2 ‚Üí 0 as n ‚Üí ‚àû. Then
d

œÉÃÇn‚àí1 (Œ≤ÃÇ1 ‚àí Œ≤1 ) ‚àí‚Üí N (0, 1),
d

(œÉÃÇns )‚àí1 (Œ≤ÃÇ1s ‚àí Œ≤1 ) ‚àí‚Üí N (0, 1).
45

Proof. See Appendix C.8.
As in the case in which all covariates are continuous, the probability limit of the 2SLS
estimators has more specific expressions depending on whether Pr(M L(Xi ) ‚àà (0, 1)) > 0 or not.
If Pr(M L(Xi ) ‚àà (0, 1)) > 0,
plim Œ≤ÃÇ1 = plim Œ≤ÃÇ1s =

E[M L(Xi )(1 ‚àí M L(Xi ))(Di (1) ‚àí Di (0))(Yi (1) ‚àí Yi (0))]
.
E[M L(Xi )(1 ‚àí M L(Xi ))(Di (1) ‚àí Di (0))]

If Pr(M L(Xi ) ‚àà (0, 1)) = 0,
plim Œ≤ÃÇ1
= plim Œ≤ÃÇ1s
P
=

xd ‚ààXd‚àó

R
Pr(Xdi = xd ) ‚àÇ‚Ñ¶‚àó (xd ) E[(Di (1) ‚àí Di (0))(Yi (1) ‚àí Yi (0))|Xi = x]fXc |Xd (xc |xd )dHpc ‚àí1 (xc )
R
P
.
pc ‚àí1 (x )
c
xd ‚ààX ‚àó Pr(Xdi = xd ) ‚àÇ‚Ñ¶‚àó (xd ) E[Di (1) ‚àí Di (0)|Xi = x]fXc |Xd (xc |xd )dH
d

A.4

A Sufficient Condition for Assumption 4 (a)

We provide a sufficient condition for Assumption 4 (a).
Assumption A.5.
‚àó ‚äÇ Rp such that
(a) (Twice Continuous Differentiability of D‚àó ) There exist C1‚àó , ..., CM
‚àó
(i) ‚àÇ(CÃÉ ‚àó ) = D‚àó , where CÃÉ ‚àó ‚â° ‚à™M
m=1 Cm ;
‚àó , C ‚àó ) > 0 for any m, m0 ‚àà {1, ..., M } such that m 6= m0 ;
(ii) dist(Cm
m0
‚àó is nonempty, bounded, open, connected and twice continuously differentiable for
(iii) Cm
each m ‚àà {1, ..., M }.

(b) (Regularity of D‚àó ) Hp‚àí1 (D‚àó ) < ‚àû.
(c) (Bounded Density near D‚àó ) There exists Œ¥ > 0 such that fX is bounded on N (D‚àó , Œ¥).
The key condition is the twice continuous differentiability of D‚àó . This condition holds if,
for example, the -Greedy algorithm described in Part 2 (a) of Example 1 in Section 7 uses an
estimated Q-function that is twice continuously differentiable in x.
Under Assumption A.5 (a), by Lemma B.4 in Appendix B.3 and with change of variables
v = ŒªŒ¥ , for any sufficiently small Œ¥ > 0,
Pr(Xi ‚àà N (D‚àó , Œ¥)) =

Z

Œ¥

Z

‚àíŒ¥

Z

‚àó

D‚àó
1

D
fX (u + ŒªŒΩCÃÉ ‚àó (u))Jp‚àí1
œàCÃÉ ‚àó (u, Œª)dHp‚àí1 (u)dŒª

Z

=Œ¥
‚àí1

D‚àó

‚àó

D
fX (u + Œ¥vŒΩCÃÉ ‚àó (u))Jp‚àí1
œàCÃÉ ‚àó (u, Œ¥v)dHp‚àí1 (u)dv.

(See Appendix B for the notation.) If fX is bounded on N (D‚àó , Œ¥) and Hp‚àí1 (D‚àó ) < ‚àû, the
right-hand side is O(Œ¥).
46

A.5

Sampling from Uniform Distribution on p-Dimensional Ball

When we calculate QPS by simulation, we need to uniformly sample from B(Xi , Œ¥). We introduce
three existing methods to uniformly sample from a p-dimensional unit ball B(0, 1). By multiplying the sampled vector by Œ¥ and adding Xi to it, we can sample from a uniform distribution
on B(Xi , Œ¥).
Method 1.
1. Sample x1 , ..., xp independently from the uniform distribution on [‚àí1, 1].
P
2. Accept the vector x = (x1 , ..., xp ) if pk=1 x2k ‚â§ 1 and reject it otherwise.
Method 1 is a practical choice when p is small (e.g. p = 2, 3), but is inefficient for higher
dimensions, since the acceptance rate decreases to zero quickly as p increases. The conventional
method used for higher dimensions is the following.
Method 2.
1. Sample x‚àó1 , ..., x‚àóp independently from the standard normal distribution, and compute the
qP
p
‚àó 2
vector s = (x‚àó1 , ..., x‚àóp )/
k=1 (xk ) .
2. Sample u from the uniform distribution on [0, 1].
3. Return the vector x = u1/p s.
There is yet another method efficient for higher dimensions, which is recently proposed by
Voelker, Gosmann and Stewart (2017).
Method 3.
1. Sample x‚àó1 , ..., x‚àóp+2 independently from the standard normal distribution, and compute the
qP
p+2 ‚àó 2
vector s = (x‚àó1 , ..., x‚àóp+2 )/
k=1 (xk ) .
2. Return the vector x = (s1 , ..., sp ).

B
B.1

Notation and Lemmas
Basic Notations

For a scalar-valued differentiable function f : A ‚äÇ Rn ‚Üí R, let ‚àáf : A ‚Üí Rn be a gradient of f :
for every x ‚àà A,


‚àÇf (x)
‚àÇf (x) 0
‚àáf (x) =
,¬∑¬∑¬∑ ,
.
‚àÇx1
‚àÇxn
Also, when the second-order partial derivatives of f exist, let D2 f (x) be the Hessian matrix:
Ô£Æ ‚àÇ 2 f (x)
..
.

¬∑¬∑¬∑
..
.

‚àÇ 2 f (x)
‚àÇxn ‚àÇx1

¬∑¬∑¬∑

‚àÇx21

Ô£Ø
D2 f (x) = Ô£Ø
Ô£∞
for each x ‚àà A.

47

‚àÇ 2 f (x)
‚àÇx1 ‚àÇxn

..
.

‚àÇ 2 f (x)
‚àÇx2n

Ô£π
Ô£∫
Ô£∫
Ô£ª

Let f : A ‚äÇ Rm ‚Üí Rn be a function such that its first-order partial derivatives exist. For
each x ‚àà A, let Jf (x) be the Jacobian matrix of f at x:
Ô£Æ
Ô£π
‚àÇf1 (x)
¬∑ ¬∑ ¬∑ ‚àÇf‚àÇx1 (x)
‚àÇx1
m
Ô£Ø .
.. Ô£∫
..
..
Jf (x) = Ô£Ø
.
. Ô£∫
Ô£∞
Ô£ª.
‚àÇfn (x)
‚àÇfn (x)
¬∑ ¬∑ ¬∑ ‚àÇxm
‚àÇx1
For a positive integer n, let In denote the n √ó n identity matrix.

B.2

Differential Geometry

We provide some concepts and facts from differential geometry of twice continuously differentiable
sets, following Crasta and Malusa (2007). Let A ‚äÇ Rp be a twice continuously differentiable set.
For each x ‚àà ‚àÇA, we denote by ŒΩA (x) ‚àà Rp the inward unit normal vector of ‚àÇA at x, that is,
the unit vector orthogonal to all vectors in the tangent space of ‚àÇA at x that points toward the
inside of A. For a set A ‚äÇ Rp , let dsA : Rp ‚Üí R be the signed distance function of A, defined by
(
d(x, ‚àÇA)
if x ‚àà cl(A)
s
dA (x) =
‚àíd(x, ‚àÇA)
if x ‚àà Rp \ cl(A),
where d(x, B) = inf y‚ààB ky ‚àí xk for any x ‚àà Rp for a set B ‚äÇ Rp . Note that we can write
N (‚àÇA, Œ¥) = {x ‚àà Rp : ‚àíŒ¥ < dsA (x) < Œ¥} for Œ¥ > 0. Lastly, let Œ†‚àÇA (x) = {y ‚àà ‚àÇA : ky ‚àí xk =
d(x, ‚àÇA)} be the set of projections of x on ‚àÇA.
Lemma B.1 (Corollary of Theorem 4.16, Crasta and Malusa (2007)). Let A ‚äÇ Rp be nonempty,
bounded, open, connected and twice continuously differentiable. Then the function dsA is twice
continuously differentiable on N (‚àÇA, ¬µ) for some ¬µ > 0. In addition, for every x0 ‚àà ‚àÇA,
Œ†‚àÇA (x0 + tŒΩA (x0 )) = {x0 } for every t ‚àà (‚àí¬µ, ¬µ). Furthermore, for every x ‚àà N (‚àÇA, ¬µ), Œ†‚àÇA (x)
is a singleton, ‚àádsA (x) = ŒΩA (y) and x = y + dsA (x)ŒΩA (y) for y ‚àà Œ†‚àÇA (x), and k‚àádsA (x)k = 1.
Proof. We apply results from Crasta and Malusa (2007). Let K = {x ‚àà Rp : kxk ‚â§ 1}. K is
nonempty, compact, convex subset of Rp with the origin as an interior point. The polar body
of K, defined as K0 = {y ‚àà Rp : y ¬∑ x ‚â§ 1 for all x ‚àà K}, is K itself. The gauge functions
œÅK , œÅK0 : Rp ‚Üí [0, ‚àû] of K and K0 are given by
œÅK (x) ‚â° inf{t ‚â• 0 : x ‚àà tK} = kxk,
œÅK0 (x) ‚â° inf{t ‚â• 0 : x ‚àà tK0 } = kxk.
Given œÅK0 , the Minkowski distance from a set S ‚äÇ Rp is defined as
Œ¥S (x) ‚â° inf œÅK0 (x ‚àí y),
y‚ààS

x ‚àà Rp .

Note that we can write
dsA (x)

=

(
Œ¥‚àÇA (x)

if x ‚àà cl(A)
if x ‚àà Rp \ cl(A).

‚àíŒ¥‚àÇA (x)
48

It then follows from Theorem 4.16 of Crasta and Malusa (2007) that dsA is twice continuously
differentiable on N (‚àÇA, ¬µ) for some ¬µ > 0, and for every x0 ‚àà ‚àÇA,
‚àádsA (x0 ) =

ŒΩA (x0 )
ŒΩA (x0 )
=
= ŒΩA (x0 ),
œÅK (ŒΩA (x0 ))
kŒΩA (x0 )k

where the last equality follows since ŒΩA (x0 ) is a unit vector. It then follows that k‚àádsA (x0 )k =
kŒΩA (x0 )k = 1 for every x0 ‚àà ‚àÇA. Also, it is obvious that, for every x0 ‚àà ‚àÇA, Œ†‚àÇA (x0 ) = {x0 } and
x0 = x0 + dsA (x0 )ŒΩA (x0 ), since dsA (x0 ) = 0. In addition, as stated in the proof of Theorem 4.16
of Crasta and Malusa (2007), ¬µ is chosen so that (4.7) in Proposition 4.6 of Crasta and Malusa
(2007) holds for every x0 ‚àà ‚àÇA and every t ‚àà (‚àí¬µ, ¬µ). That is, Œ†‚àÇA (x0 + t‚àáœÅK (ŒΩA (x0 ))) =
(x0 )
= ŒΩA (x0 ),
{x0 } for every x0 ‚àà ‚àÇA and every t ‚àà (‚àí¬µ, ¬µ). Since ‚àáœÅK (ŒΩA (x0 )) = kŒΩŒΩA
A (x0 )k
Œ†‚àÇA (x0 + tŒΩA (x0 )) = {x0 } for every x0 ‚àà ‚àÇA and every t ‚àà (‚àí¬µ, ¬µ).
Furthermore, for every x ‚àà N (‚àÇA, ¬µ) \ ‚àÇA, Œ†‚àÇA (x) is a singleton as shown in the proof of
Theorem 4.16 of Crasta and Malusa (2007). Let œÄ‚àÇA (x) be the unique element in Œ†‚àÇA (x). By
Lemma 4.3 of Crasta and Malusa (2007), for every x ‚àà N (‚àÇA, ¬µ) \ ‚àÇA,
‚àádsA (x) =

ŒΩA (œÄ‚àÇA (x))
ŒΩA (œÄ‚àÇA (x))
=
= ŒΩA (œÄ‚àÇA (x)),
œÅK (ŒΩA (œÄ‚àÇA (x)))
kŒΩA (œÄ‚àÇA (x))k

where the last equality follows since ŒΩA (œÄ‚àÇA (x)) is a unit vector. It then follows that k‚àádsA (x)k =
kŒΩA (œÄ‚àÇA (x))k = 1 for every x ‚àà N (‚àÇA, ¬µ) \ ‚àÇA.
Lastly, note that
(
dsA (x)
if x ‚àà N (‚àÇA, ¬µ) ‚à© int(A)
Œ¥‚àÇA (x) =
‚àídsA (x)
if x ‚àà N (‚àÇA, ¬µ) \ cl(A),
and
‚àáŒ¥‚àÇA (x) =

(
‚àádsA (x)

if x ‚àà N (‚àÇA, ¬µ) ‚à© int(A)
if x ‚àà N (‚àÇA, ¬µ) \ cl(A),

‚àí‚àádsA (x)

so Œ¥‚àÇA (x)‚àáŒ¥‚àÇA (x) = dsA (x)‚àádsA (x) = dsA (x)ŒΩA (œÄ‚àÇA (x)) for every x ‚àà N (‚àÇA, ¬µ) \ ‚àÇA. By Proposition 3.3 (i) of Crasta and Malusa (2007), for every x ‚àà N (‚àÇA, ¬µ) \ ‚àÇA,
‚àáœÅK (‚àáŒ¥‚àÇA (x)) =

x ‚àí œÄ‚àÇA (x)
,
Œ¥‚àÇA (x)

which implies that
x = œÄ‚àÇA (x) + Œ¥‚àÇA (x)‚àáœÅK (‚àáŒ¥‚àÇA (x))
= œÄ‚àÇA (x) + Œ¥‚àÇA (x)

‚àáŒ¥‚àÇA (x)
= œÄ‚àÇA (x) + dsA (x)ŒΩA (œÄ‚àÇA (x)).
k‚àáŒ¥‚àÇA (x)k

We say that a set A ‚äÇ Rn is a m-dimensional C 1 submanifold of Rn if for every point x ‚àà A,
there exist an open neighborhood V ‚äÇ Rn of x and a one-to-one continuously differentiable
function œÜ from an open set U ‚äÇ Rm to Rn such that the Jacobian matrix JœÜ(u) is of rank m
for all u ‚àà U , and œÜ(U ) = V ‚à© A.
49

Lemma B.2. Let A ‚äÇ Rp be nonempty, bounded, open, connected and twice continuously differentiable. Then ‚àÇA is a (p ‚àí 1)-dimensional C 1 submanifold of Rp ,
Proof. Fix any x‚àó ‚àà ‚àÇA. By Lemma B.1, ‚àádsA (x‚àó ) is nonzero. Without loss of generality, let
‚àÇdsA (x‚àó )
6= 0. Let œà : Rp ‚Üí Rp be the function such that œà(x) = (x1 , ..., xp‚àí1 , dsA (x)). œà is
‚àÇxp
continuously differentiable, and the Jacobian matrix of œà at x‚àó is given by
Ô£∂
Ô£´
Ô£∂
Ô£´ ‚àÇœà
0
‚àÇœà1
‚àó
‚àó
1
Ô£¨
‚àÇx1 (x ) ¬∑ ¬∑ ¬∑ ‚àÇxp (x )
.. Ô£∑
Ô£∑ Ô£¨
Ô£¨
Ip‚àí1
. Ô£∑
..
..
.
‚àó
Ô£∑.
Ô£∑
Ô£¨
Ô£¨
.
Jœà(x ) = Ô£≠
.
.
.
Ô£∑
Ô£∏=Ô£¨
0
Ô£∏
Ô£≠ s ‚àó
‚àÇœàp
‚àó ) ¬∑ ¬∑ ¬∑ ‚àÇœàp (x‚àó )
s
‚àó
s
‚àó
(x
‚àÇdA (x )
‚àÇdA (x )
‚àÇdA (x )
‚àÇx1
‚àÇxp
¬∑
¬∑
¬∑
‚àÇx1
‚àÇxp‚àí1
‚àÇxp
‚àÇds (x‚àó )

A
Since ‚àÇx
6= 0, the Jacobian matrix is invertible. By the Inverse Function Theorem, there
p
exist an open set V containing x‚àó and an open set W containing œà(x‚àó ) such that œà : V ‚Üí W
has an inverse function œà ‚àí1 : W ‚Üí V that is continuously differentiable. We make V small
‚àÇdsA (x)
enough so that ‚àÇx
6= 0 for every x ‚àà V . The Jacobian matrix of œà ‚àí1 is given by Jœà ‚àí1 (y) =
p
Jœà(œà ‚àí1 (y))‚àí1 for all y ‚àà W .
Now note that œà(x) = (x1 , ..., xp‚àí1 , 0) for all x ‚àà V ‚à© ‚àÇA by the definition of dsA . Let U =
{(x1 , ..., xp‚àí1 ) ‚àà Rp‚àí1 : x ‚àà V ‚à© ‚àÇA} and œÜ : U ‚Üí Rp be a function such that œÜ(u) = œà ‚àí1 ((u, 0))
for all u ‚àà U . Below we verify that œÜ is one-to-one and continously differentiable, that JœÜ(u) is
of rank p ‚àí 1 for all u ‚àà U , that œÜ(U ) = V ‚à© ‚àÇA, and that U is open.
First, œÜ is one-to-one, since œà ‚àí1 is one-to-one, and (u, 0) 6= (u0 , 0) if u 6= u0 . Second, œÜ is
continuously differentiable, since œà ‚àí1 is so. The Jacobian matrix of œÜ at u ‚àà U is by definition
Ô£´ ‚àí1
Ô£∂
‚àÇœà1
‚àÇœà1‚àí1
((u,
0))
¬∑
¬∑
¬∑
((u,
0))
‚àÇyp‚àí1
Ô£∑
Ô£¨ ‚àÇy1
Ô£∑
Ô£¨
..
..
.
.
JœÜ(u) = Ô£¨
Ô£∑.
.
.
.
Ô£∏
Ô£≠ ‚Äò
‚àí1
‚àÇœàp ‚àí1
‚àÇœàp
((u,
0))
¬∑
¬∑
¬∑
((u,
0))
‚àÇy1
‚àÇyp‚àí1

Note that this is the left p √ó (p ‚àí 1) submatrix of Jœà ‚àí1 ((u, 0)). Since Jœà ‚àí1 ((u, 0)) has full rank,
JœÜ(u) is of rank p ‚àí 1. Moreover,
œÜ(U ) = {œà ‚àí1 ((u, 0)) : u ‚àà U }
= {œà ‚àí1 ((x1 , ..., xp‚àí1 , 0)) : x ‚àà V ‚à© ‚àÇA}
= {œà ‚àí1 (œà(x)) : x ‚àà V ‚à© ‚àÇA}
= V ‚à© ‚àÇA.
Lastly, we show that U is open. Pick any uÃÑ ‚àà U . Then, there exists xÃÑp ‚àà R such that
‚àÇds ((uÃÑ,xÃÑ ))
(uÃÑ, xÃÑp ) ‚àà V ‚à© ‚àÇA. As (uÃÑ, xÃÑp ) ‚àà V ‚à© ‚àÇA, dsA ((uÃÑ, xÃÑp )) = 0. Since A ‚àÇxp p 6= 0, it follows by the
Implicit Function Theorem that there exist an open set S ‚äÇ Rp‚àí1 containing uÃÑ and a continuously
differentiable function g : S ‚Üí R such that g(uÃÑ) = xÃÑp and dsA (u, g(u)) = 0 for all u ‚àà S. Since
g is continuous, (uÃÑ, g(uÃÑ)) ‚àà V and V is open, there exists an open set S 0 ‚äÇ S containing uÃÑ such
that (u, g(u)) ‚àà V for all u ‚àà S 0 . By the definition of dsA , dsA (x) = 0 if and only if x ‚àà ‚àÇA.
Therefore, if u ‚àà S 0 , (u, g(u)) must be contained by ‚àÇA, for otherwise dsA (u, g(u)) 6= 0, which is
a contradiction. Thus, (u, g(u)) ‚àà V ‚à© ‚àÇA and hence u ‚àà U for all u ‚àà S 0 . This implies that S 0
is an open subset of U containing uÃÑ, which proves that U is open.
50

B.3

Geometric Measure Theory

We provide some concepts and facts from geometric measure theory, following Krantz and Parks
(2008). Recall that for a function f : A ‚äÇ Rm ‚Üí Rn and a point x ‚àà A at which f is differentiable,
Jf (x) denotes the Jacobian matrix of f at x.
Lemma B.3 (Coarea Formula, Lemma 5.1.4 and Corollary 5.2.6 of Krantz and Parks (2008)).
If f : Rm ‚Üí Rn is a Lipschitz function and m ‚â• n, then
Z Z
Z
m
g(x)dHm‚àín (x)dLn (y)
g(x)Jn f (x)dL (x) =
Rn

A

{x0 ‚ààA:f (x0 )=y}

for every Lebesgue measurable subset A of Rm and every Lm -measurable function g : A ‚Üí R,
where for each x ‚àà Rm at which f is differentiable,
p
Jn f (x) = det((Jf (x))(Jf (x))0 ).
Let A be an m-dimensional C 1 submanifold of Rn . Let x ‚àà A and let œÜ : U ‚äÇ Rm ‚Üí Rn be
as in the definition of m-dimensional C 1 submanifold. We denote by TA (x) the tangent space of
A at x, {JœÜ(u)v : v ‚àà Rm }, where u = œÜ‚àí1 (x).
Lemma B.4 (Area Formula, Lemma 5.3.5 and Theorem 5.3.7 of Krantz and Parks (2008)).
Suppose m ‚â§ ŒΩ and f : Rn ‚Üí RŒΩ is Lipschitz. If A is an m-dimensional C 1 submanifold of Rn ,
then
Z
Z
X
A
m
g(x)Jm f (x)dH (x) =
g(x)dHm (y)
RŒΩ x‚ààA:f (x)=y

A

for every Hm -measurable function g : A ‚Üí R, where for each x ‚àà Rn at which f is differentiable,
A
Jm
f (x) =

Hm ({Jf (x)y : y ‚àà P })
Hm (P )

for an arbitrary m-dimensional parallelepiped P contained in TA (x).
Let A ‚äÇ Rp . For each x ‚àà Rp at which dsA is differentiable and for each Œª ‚àà R, let œàA (x, Œª) =
x + Œª‚àádsA (x).
Lemma B.5. Let ‚Ñ¶ ‚äÇ Rp , and suppose that there exists a partition {‚Ñ¶1 , ..., ‚Ñ¶M } of ‚Ñ¶ such that
(i) dist(‚Ñ¶m , ‚Ñ¶m0 ) > 0 for any m, m0 ‚àà {1, ..., M } such that m 6= m0 ;
(ii) ‚Ñ¶m is nonempty, bounded, open, connected and twice continuously differentiable for each
m ‚àà {1, ..., M }.
Then there exists ¬µ > 0 such that ds‚Ñ¶ is twice continuously differentiable on N (‚àÇ‚Ñ¶, ¬µ) and that
Z
Z Œ¥Z
‚àÇ‚Ñ¶
g(x)dx =
g(u + ŒªŒΩ‚Ñ¶ (u))Jp‚àí1
œà‚Ñ¶ (u, Œª)dHp‚àí1 (u)dŒª
N (‚àÇ‚Ñ¶,Œ¥)

‚àíŒ¥

‚àÇ‚Ñ¶

for every Œ¥ ‚àà (0, ¬µ) and every function g : Rp ‚Üí R that is integrable on N (‚àÇ‚Ñ¶, Œ¥), where for
‚àÇ‚Ñ¶ œà (¬∑, Œª) is calculated by applying the operation J ‚àÇ‚Ñ¶ to the function
each fixed Œª ‚àà (‚àí¬µ, ¬µ), Jp‚àí1
‚Ñ¶
p‚àí1
‚àÇ‚Ñ¶ œà (x, ¬∑) is continuously differentiable in Œª and J ‚àÇ‚Ñ¶ œà (x, 0) = 1 for
œà‚Ñ¶ (¬∑, Œª). Futhermore, Jp‚àí1
‚Ñ¶
p‚àí1 ‚Ñ¶
‚àÇ‚Ñ¶ œà (¬∑, ¬∑) and
every x ‚àà ‚àÇ‚Ñ¶, and Jp‚àí1
‚Ñ¶

‚àÇ‚Ñ¶ œà (¬∑,¬∑)
‚àÇJp‚àí1
‚Ñ¶
‚àÇŒª

are bounded on ‚àÇ‚Ñ¶ √ó (‚àí¬µ, ¬µ).
51

Proof. Let ¬µÃÑ = 21 minm,m0 ‚àà{1,...,M },m6=m0 dist(‚Ñ¶‚àóm , ‚Ñ¶m0 ) so that {N (‚àÇ‚Ñ¶m , ¬µÃÑ)}M
m=1 is a partition of
s
s
N (‚àÇ‚Ñ¶, ¬µÃÑ). Note that for every m ‚àà {1, ..., M }, d‚Ñ¶ (x) = d‚Ñ¶m (x) for every x ‚àà N (‚àÇ‚Ñ¶m , ¬µÃÑ). By
Lemma B.1, for every m ‚àà {1, ..., M }, there exists ¬µÃÑm > 0 such that ds‚Ñ¶m is twice continuously
differentiable on N (‚àÇ‚Ñ¶m , ¬µÃÑm ). Letting ¬µ ‚àà (0, min{¬µÃÑ, ¬µÃÑ1 , ..., ¬µÃÑM }), we have that ds‚Ñ¶ is twice
continuously differentiable on N (‚àÇ‚Ñ¶, ¬µ). This implies that ds‚Ñ¶ is Lipschitz on N (‚àÇ‚Ñ¶, ¬µ). For
every Œ¥ ‚àà (0, ¬µ) and every function g : Rp ‚Üí R that is integrable on N (‚àÇ‚Ñ¶, Œ¥),
Z
Z
q
g(x) det(k‚àáds‚Ñ¶ (x)k)dx
g(x)dx =
{x0 ‚ààRp :ds‚Ñ¶ (x0 )‚àà(‚àíŒ¥,Œ¥)}

N (‚àÇ‚Ñ¶,Œ¥)

Z
=
{x0 ‚ààRp :ds‚Ñ¶ (x0 )‚àà(‚àíŒ¥,Œ¥)}

Z
=
{x0 ‚ààRp :ds‚Ñ¶ (x0 )‚àà(‚àíŒ¥,Œ¥)}

q
g(x) det(‚àáds‚Ñ¶ (x)0 ‚àáds‚Ñ¶ (x))dx
q
g(x) det((Jds‚Ñ¶ (x))(Jds‚Ñ¶ (x))0 )dx

Z Z

g(x)dHp‚àí1 (x)dŒª

=
R

Z

Œ¥

{x0 ‚ààRp :ds‚Ñ¶ (x0 )‚àà(‚àíŒ¥,Œ¥),ds‚Ñ¶ (x0 )=Œª}

Z

g(x)dHp‚àí1 (x)dŒª,

=
‚àíŒ¥

{x0 ‚ààRp :ds‚Ñ¶ (x0 )=Œª}

(12)

where the first equality follows since k‚àáds‚Ñ¶ (x)k = 1 for every x ‚àà N (‚àÇ‚Ñ¶, Œ¥) by Lemma B.1, the
third equality follows from the definition of the Jacobian matrix, and the fourth equality follows
from Lemma B.3.
Let Œì(Œª) = {x ‚àà Rp : ds‚Ñ¶ (x) = Œª} for each Œª ‚àà (‚àí¬µ, ¬µ). Since ‚àáds‚Ñ¶ is differentiable on
N (‚àÇ‚Ñ¶, ¬µ), œà‚Ñ¶ (x, Œª) is defined on N (‚àÇ‚Ñ¶, ¬µ) √ó R. We show that {œà‚Ñ¶ (x0 , Œª) : x0 ‚àà ‚àÇ‚Ñ¶} ‚äÇ Œì(Œª) for
every Œª ‚àà (‚àí¬µ, ¬µ). By Lemma B.1, for every x0 ‚àà ‚àÇ‚Ñ¶, œà‚Ñ¶ (x0 , Œª) = x0 + ŒªŒΩ‚Ñ¶ (x0 ) and
Œ†‚àÇ‚Ñ¶ (œà‚Ñ¶ (x0 , Œª)) = Œ†‚àÇ‚Ñ¶ (x0 + ŒªŒΩ‚Ñ¶ (x0 )) = {x0 }.
Hence,
d(œà‚Ñ¶ (x0 , Œª), ‚àÇ‚Ñ¶) = kœà‚Ñ¶ (x0 , Œª) ‚àí x0 k = kŒªŒΩ‚Ñ¶ (x0 )k = |Œª|.
Since ŒΩ‚Ñ¶ (x0 ) is an inward normal vector, œà‚Ñ¶ (x0 , Œª) ‚àà cl(A) if 0 ‚â§ Œª < ¬µ, and œà‚Ñ¶ (x, Œª0 ) ‚àà
Rp \ cl(A) if ‚àí¬µ < Œª < 0. It follows that
dsA (œà‚Ñ¶ (x0 , Œª))

=

(
|Œª|
‚àí|Œª|

if 0 ‚â§ Œª < ¬µ
if ¬µ < Œª < 0

= Œª,
so {œà‚Ñ¶ (x0 , Œª) : x0 ‚àà ‚àÇ‚Ñ¶} ‚äÇ Œì(Œª). It also holds that Œì(Œª) ‚äÇ {œà‚Ñ¶ (x0 , Œª) : x0 ‚àà ‚àÇ‚Ñ¶}, since by
Lemma B.1, for every x ‚àà Œì(Œª),
œà‚Ñ¶ (œÄ‚àÇ‚Ñ¶ (x), Œª) = œÄ‚àÇ‚Ñ¶ (x) + Œª‚àáds‚Ñ¶ (œÄ‚àÇ‚Ñ¶ (x)) = œÄ‚àÇ‚Ñ¶ (x) + ds‚Ñ¶ (x)ŒΩ‚Ñ¶ (œÄ‚àÇ‚Ñ¶ (x)) = x,
where œÄ‚àÇ‚Ñ¶ (x) is the unique element in Œ†‚àÇ‚Ñ¶ (x). Thus, {œà‚Ñ¶ (x0 , Œª) : x0 ‚àà ‚àÇ‚Ñ¶} = Œì(Œª).
52

0
Now note that {‚àÇ‚Ñ¶m }M
m=1 is a partition of ‚àÇ‚Ñ¶, since dist(‚Ñ¶m , ‚Ñ¶m0 ) > 0 for any m, m ‚àà
{1, ..., M } such that m 6= m0 . By Lemma B.2, ‚àÇ‚Ñ¶m is a (p ‚àí 1)-dimensional C 1 submanifold
of Rp for every m ‚àà {1, ..., M }, and hence ‚àÇ‚Ñ¶ is a (p ‚àí 1)-dimensional C 1 submanifold of
Rp . Furthermore, since ‚àáds‚Ñ¶ is continuously differentiable on N (‚àÇ‚Ñ¶, ¬µ), œà‚Ñ¶ (¬∑, Œª) is continuously
differentiable on N (‚àÇ‚Ñ¶, ¬µ), which implies that œà‚Ñ¶ (¬∑, Œª) is Lipschitz on N (‚àÇ‚Ñ¶, ¬µ) for every Œª ‚àà R.
Applying Lemma B.4, we have that for every Œª ‚àà (‚àí¬µ, ¬µ),
Z
Z
‚àÇ‚Ñ¶
‚àÇ‚Ñ¶
p‚àí1
g(œà‚Ñ¶ (u, Œª))Jp‚àí1
œà‚Ñ¶ (u, Œª)dHp‚àí1 (u)
g(u + ŒªŒΩ‚Ñ¶ (u))Jp‚àí1 œà‚Ñ¶ (u, Œª)dH (u) =
‚àÇ‚Ñ¶
‚àÇ‚Ñ¶
Z
X
g(œà‚Ñ¶ (u, Œª))dHp‚àí1 (x).
(13)
=
Rp u‚àà‚àÇ‚Ñ¶:œà (u,Œª)=x
‚Ñ¶

If x ‚àà
/ {œà‚Ñ¶ (u, Œª) : u ‚àà ‚àÇ‚Ñ¶}, {u ‚àà ‚àÇ‚Ñ¶ : œà‚Ñ¶ (u, Œª) = x} = ‚àÖ. If x ‚àà {œà‚Ñ¶ (u, Œª) : u ‚àà ‚àÇ‚Ñ¶}, there exists
u ‚àà ‚àÇ‚Ñ¶ such that x = œà‚Ñ¶ (u, Œª). Since Œ†‚àÇ‚Ñ¶ (x) = Œ†‚àÇ‚Ñ¶ (u + Œª‚àáds‚Ñ¶ (u)) = Œ†‚àÇ‚Ñ¶ (u + ŒªŒΩ‚Ñ¶ (u)) = {u}
by Lemma B.1, such u is unique, and hence {u ‚àà ‚àÇ‚Ñ¶ : œà‚Ñ¶ (u, Œª) = x} is a singleton. It follow
that
Z
Z
X
p‚àí1
g(œà‚Ñ¶ (u, Œª))dH (x) =
g(x)dHp‚àí1 (x)
Rp u‚àà‚àÇ‚Ñ¶:œà (u,Œª)=x
‚Ñ¶

{œà‚Ñ¶ (u,Œª):u‚àà‚àÇ‚Ñ¶}

Z
=

g(x)dHp‚àí1 (x),

(14)

Œì(Œª)

where the last equality holds since {œà‚Ñ¶ (u, Œª) : u ‚àà ‚àÇ‚Ñ¶} = Œì(Œª). Combining (12), (13) and (14),
we obtain
Z
Z Œ¥Z
‚àÇ‚Ñ¶
g(x)dx =
g(u + ŒªŒΩ‚Ñ¶ (u))Jp‚àí1
œà‚Ñ¶ (u, Œª)dHp‚àí1 (u)dŒª.
N (‚àÇ‚Ñ¶,Œ¥)

‚àíŒ¥

‚àÇ‚Ñ¶

‚àÇ‚Ñ¶ œà (x, 0) = 1
We next show that
is continuously differentiable in Œª and Jp‚àí1
‚Ñ¶
for every x ‚àà ‚àÇ‚Ñ¶. Fix an x ‚àà ‚àÇ‚Ñ¶, and let V‚Ñ¶ (x) be an arbitrary p √ó (p ‚àí 1) matrix whose
columns v1 (x), ..., vp‚àí1 (x) ‚àà Rp form an orthonormal basis of T‚àÇ‚Ñ¶ (x). Let P (x) ‚äÇ T‚àÇ‚Ñ¶ (x)
Pp‚àí1
be a parallelepiped determined by v1 (x), ..., vp‚àí1 (x), that is, let P (x) = { k=1
ck vk (x) : 0 ‚â§
ck ‚â§ 1 for k = 1, ..., p ‚àí 1}. Since v1 (x), ..., vp‚àí1 (x) are linearly independent, P (x) is a (p ‚àí 1)dimensional parallelepiped. It follows that for each fixed Œª ‚àà R,
‚àÇ‚Ñ¶ œà (x, ¬∑)
Jp‚àí1
‚Ñ¶

{Jœà‚Ñ¶ (x, Œª)y : y ‚àà P (x)} = {Jœà‚Ñ¶ (x, Œª)

p‚àí1
X

ck vk (x) : 0 ‚â§ ck ‚â§ 1 for k = 1, ..., p ‚àí 1}

k=1

={

p‚àí1
X

ck Jœà‚Ñ¶ (x, Œª)vk (x) : 0 ‚â§ ck ‚â§ 1 for k = 1, ..., p ‚àí 1}

k=1

={

p‚àí1
X

ck wk (x, Œª) : 0 ‚â§ ck ‚â§ 1 for k = 1, ..., p ‚àí 1},

k=1

where wk (x, Œª) = Jœà‚Ñ¶ (x, Œª)vk (x) for k = 1, ..., p ‚àí 1. Since Jœà‚Ñ¶ (x, Œª)vk (x) is the k-th column
of Jœà‚Ñ¶ (x, Œª)V‚Ñ¶ (x), {Jœà‚Ñ¶ (x, Œª)y : y ‚àà P (x)} is the parallelepiped determined by the columns of

53

Jœà‚Ñ¶ (x, Œª)V‚Ñ¶ (x). By Proposition 5.1.2 of Krantz and Parks (2008), we have that
‚àÇ‚Ñ¶
œà‚Ñ¶ (x, Œª)
Jp‚àí1

=

Hp‚àí1 ({

Pp‚àí1

: 0 ‚â§ ck ‚â§ 1 for k = 1, ..., p ‚àí 1})
Hp‚àí1 (P (x))

k=1 ck wk (x, Œª)

p
det((Jœà‚Ñ¶ (x, Œª)V‚Ñ¶ (x))0 (Jœà‚Ñ¶ (x, Œª)V‚Ñ¶ (x)))
p
=
det(V‚Ñ¶ (x)0 V‚Ñ¶ (x))
p
det((V‚Ñ¶ (x) + ŒªD2 ds‚Ñ¶ (x)V‚Ñ¶ (x))0 (V‚Ñ¶ (x) + ŒªD2 ds‚Ñ¶ (x)V‚Ñ¶ (x)))
p
=
det(Ip‚àí1 )
q
= det(V‚Ñ¶ (x)0 V‚Ñ¶ (x) + 2V‚Ñ¶ (x)0 ŒªD2 ds‚Ñ¶ (x)V‚Ñ¶ (x) + V‚Ñ¶ (x)0 (ŒªD2 ds‚Ñ¶ (x))2 V‚Ñ¶ (x))
q
= det(Ip‚àí1 + ŒªV‚Ñ¶ (x)0 (2D2 ds‚Ñ¶ (x) + Œª(D2 ds‚Ñ¶ (x))2 )V‚Ñ¶ (x)))
q
= det(Ip + ŒªV‚Ñ¶ (x)V‚Ñ¶ (x)0 (2D2 ds‚Ñ¶ (x) + Œª(D2 ds‚Ñ¶ (x))2 )),
where we use the fact that V‚Ñ¶ (x)0 V‚Ñ¶ (x) = Ip‚àí1 and the fact that det(Im + AB) = det(In + BA)
for an m √ó n matrix A and an n √ó m matrix B (the Weinstein-Aronszajn identity). For every
p
‚àÇ‚Ñ¶ œà (x, ¬∑) is continuously differentiable in Œª, and J ‚àÇ‚Ñ¶ œà (x, 0) =
det(Ip ) = 1.
x ‚àà ‚àÇ‚Ñ¶, Jp‚àí1
‚Ñ¶
p‚àí1 ‚Ñ¶
‚àÇJ ‚àÇ‚Ñ¶ œà‚Ñ¶ (¬∑,¬∑)

p‚àí1
‚àÇ‚Ñ¶ œà (¬∑, ¬∑) and
Lastly, we show that Jp‚àí1
‚Ñ¶
‚àÇŒª
‚àÇ‚Ñ¶ √ó Rp√ó(p‚àí1) ‚Üí Rp√óp be functions such that

are bounded on ‚àÇ‚Ñ¶ √ó (‚àí¬µ, ¬µ). Let f, h :

f (x, A) = 2AA0 D2 ds‚Ñ¶ (x),
h(x, A) = AA0 (D2 ds‚Ñ¶ (x))2 .
Also, let k : ‚àÇ‚Ñ¶ √ó R √ó Rp√ó(p‚àí1) ‚Üí R be a function such that
q
k(x, Œª, A) = det(Ip + Œªf (x, A) + Œª2 h(x, A)).
Observe that
‚àÇ‚Ñ¶
Jp‚àí1
œà‚Ñ¶ (x, Œª) = k(x, Œª, V‚Ñ¶ (x))

and that
‚àÇ‚Ñ¶ œà (x, Œª)
‚àÇJp‚àí1
‚Ñ¶
‚àÇŒª
‚àÇk(x, Œª, A)
=
‚àÇŒª
A=V‚Ñ¶ (x)

=

X ‚àÇdet(Ip + Œªf (x, A) + Œª2 h(x, A))
1
(fij (x, A) + 2Œªhij (x, A))
2k(x, Œª, A)
‚àÇbij
i,j

,
A=V‚Ñ¶ (x)

where ‚àÇdet(B)
denotes the partial derivative of the function det : Rp√óp ‚Üí R with respect to the
‚àÇbij
(i, j) entry of B.
Note that k(¬∑, ¬∑, ¬∑) and ‚àÇk(¬∑,¬∑,¬∑)
are continuous on ‚àÇ‚Ñ¶ √ó R √ó Rp√ó(p‚àí1) (except at the points
‚àÇŒª
for which k(x, Œª, A) = 0), since det is infinitely differentiable, and f and h are continuous on
54

‚àÇ‚Ñ¶ √ó Rp√ó(p‚àí1) . Let S = {(x, Œª, A) ‚àà ‚àÇ‚Ñ¶ √ó [‚àí¬µ, ¬µ] √ó Rp√ó(p‚àí1) : kaj k = 1 for k = 1, ..., p ‚àí 1},
where aj denotes the jth column of A. Since k(¬∑, ¬∑, ¬∑) and ‚àÇk(¬∑,¬∑,¬∑)
are continuous and S is
‚àÇŒª
0
closed and bounded, kÃÑ = max(x,Œª,A)‚ààS |k(x, Œª, A)| and kÃÑ = max(x,Œª,A)‚ààS | ‚àÇk(x,Œª,A)
| exist. Since
‚àÇŒª
‚àÇ‚Ñ¶ œà (x, Œª)| ‚â§ kÃÑ and
(x, Œª, V‚Ñ¶ (x)) ‚àà S for every (x, Œª) ‚àà ‚àÇ‚Ñ¶ √ó (‚àí¬µ, ¬µ), it follows that |Jp‚àí1
‚Ñ¶
|

‚àÇ‚Ñ¶ œà (x,Œª)
‚àÇJp‚àí1
‚Ñ¶
|
‚àÇŒª

B.4

‚â§ kÃÑ 0 for every (x, Œª) ‚àà ‚àÇ‚Ñ¶ √ó (‚àí¬µ, ¬µ).

Other Lemmas

2
Lemma B.6. Let {Vi }‚àû
i=1 be i.i.d. random variables such that E[Vi ] < ‚àû. If Assumption 1
holds, then for l ‚â• 0 and m = 0, 1,

E[Vi pM L (Xi ; Œ¥)l 1{pM L (Xi ; Œ¥) ‚àà (0, 1)}m ] ‚Üí E[Vi M L(Xi )l 1{M L(Xi ) ‚àà (0, 1)}m ]
as Œ¥ ‚Üí 0. Moreover, if, in addition, Œ¥n ‚Üí 0 as n ‚Üí ‚àû, then for l ‚â• 0,
n

1X
p
Vi pM L (Xi ; Œ¥n )l Ii,n ‚àí‚Üí E[Vi M L(Xi )l 1{M L(Xi ) ‚àà (0, 1)}]
n
i=1

as n ‚Üí ‚àû.
Proof. Note that E[ n1
show that

Pn

i=1 Vi p

= E[Vi pM L (Xi ; Œ¥n )l 1{pM L (Xi ; Œ¥n ) ‚àà (0, 1)}]. We

M L (X ; Œ¥ )l I ]
i n
i,n

E[Vi pM L (Xi ; Œ¥)l 1{pM L (Xi ; Œ¥) ‚àà (0, 1)}m ] ‚Üí E[Vi M L(Xi )l 1{M L(Xi ) ‚àà (0, 1)}m ]
for l ‚â• 0 and m = 0, 1 as Œ¥ ‚Üí 0, and that
n

1X
Var(
Vi pM L (Xi ; Œ¥n )l Ii,n ) ‚Üí 0
n
i=1

for l ‚â• 0 as n ‚Üí ‚àû. For the first part, we have
Z
ML
l
ML
m
E[Vi |Xi = x]pM L (x; Œ¥)l 1{pM L (x; Œ¥) ‚àà (0, 1)}m fX (x)dx.
E[Vi p (Xi ; Œ¥) 1{p (Xi ; Œ¥) ‚àà (0, 1)} ] =
X

Suppose M L is continuous at x and M L(x) ‚àà (0, 1). Then limŒ¥‚Üí0 pM L (x; Œ¥) = M L(x) by
Part 1 of Corollary 2, and hence pM L (x; Œ¥) ‚àà (0, 1) for sufficiently small Œ¥ > 0. It follows that
1{pM L (x; Œ¥) ‚àà (0, 1)} ‚Üí 1 = 1{M L(x) ‚àà (0, 1)} as Œ¥ ‚Üí 0. Suppose x ‚àà int(X0 ) ‚à™ int(X1 ). Then
B(x, Œ¥) ‚äÇ X0 or B(x, Œ¥) ‚äÇ X1 for sufficiently small Œ¥ > 0 by the fact that int(X0 ) and int(X1 )
are open, and hence 1{pM L (x; Œ¥) ‚àà (0, 1)} ‚Üí 0 = 1{M L(x) ‚àà (0, 1)} as Œ¥ ‚Üí 0. Therefore,
limŒ¥‚Üí0 pM L (x; Œ¥) = M L(x) and limŒ¥‚Üí0 1{pM L (x; Œ¥) ‚àà (0, 1)} = 1{M L(x) ‚àà (0, 1)} for almost
every x ‚àà X , since M L is continuous at x for almost every x ‚àà X by Assumption 1 (a), and
either M L(x) ‚àà (0, 1) or x ‚àà int(X0 ) ‚à™ int(X1 ) for almost every x ‚àà X by Assumption 1 (b). By
the Dominated Convergence Theorem,
Z
ML
l
ML
m
E[Vi p (Xi ; Œ¥) 1{p (Xi ; Œ¥) ‚àà (0, 1)} ] ‚Üí
E[Vi |Xi = x]M L(x)l 1{M L(x) ‚àà (0, 1)}m fX (x)dx
X

= E[Vi M L(Xi )l 1{M L(Xi ) ‚àà (0, 1)}m ]
55

as Œ¥ ‚Üí 0. As for variance,
n

Var(

1X
1
Vi pM L (Xi ; Œ¥n )l Ii,n ) ‚â§ E[Vi2 pM L (Xi ; Œ¥n )2l (Ii,n )2 ]
n
n
i=1

1
E[Vi2 ]
n
‚Üí0
‚â§

as n ‚Üí ‚àû.
Lemma B.7. Let {(Œ¥n , Sn )}‚àû
n=1 be any sequence of positive numbers and positive integers. Fix
x ‚àà X , and let X1‚àó , ..., XS‚àón be Sn independent draws from the uniform distribution on B(x, Œ¥n )
so that
Sn
1 X
M L(Xs‚àó ).
ps (x; Œ¥n ) =
Sn
s=1

Then,
E[ps (x; Œ¥n ) ‚àí pM L (x; Œ¥n )] = 0,
1
E[(ps (x; Œ¥n ) ‚àí pM L (x; Œ¥n ))2 ] ‚â§
,
Sn
1
|E[ps (x; Œ¥n )2 ‚àí pM L (x; Œ¥n )2 ]| ‚â§
,
Sn
4
E[(ps (x; Œ¥n )2 ‚àí pM L (x; Œ¥n )2 )2 ] ‚â§
,
Sn
Pr(ps (x; Œ¥n ) ‚àà {0, 1}) ‚â§ (1 ‚àí pM L (x; Œ¥n ))Sn + pM L (x; Œ¥n )Sn .
Moreover, for any  > 0,
E[|ps (x; Œ¥n ) ‚àí pM L (x; Œ¥n )|] ‚â§

1
+ ,
Sn 2

and if Sn ‚Üí ‚àû, then
E[|ps (x; Œ¥n ) ‚àí pM L (x; Œ¥n )|] ‚Üí 0
as n ‚Üí ‚àû.
Proof. By construction, E[M L(Xs‚àó )] = pM L (x; Œ¥n ), so
s

E[p (x; Œ¥n ) ‚àí p

ML

Sn
1 X
(x; Œ¥n )] = E[
M L(Xs‚àó )] ‚àí pM L (x; Œ¥n )
Sn
s=1

= E[M L(Xs‚àó )] ‚àí pM L (x; Œ¥n )
= 0.

56

We have
E[(ps (x; Œ¥n ) ‚àí pM L (x; Œ¥n ))2 ] = Var(ps (x; Œ¥n ))
= Var(

Sn
1 X
M L(Xs‚àó ))
Sn
s=1

1
Var(M L(Xs‚àó ))
=
Sn
1
‚â§
E[M L(Xs‚àó )2 ]
Sn
1
‚â§
,
Sn
|E[ps (x; Œ¥n )2 ‚àí pM L (x; Œ¥n )2 ]| = |Var(ps (x; Œ¥n )) + (E[ps (x; Œ¥n )])2 ‚àí pM L (x; Œ¥n )2 |
1
‚â§
+ |(pM L (x; Œ¥n ))2 ‚àí pM L (x; Œ¥n )2 |
Sn
1
=
,
Sn
and
E[(ps (x; Œ¥n )2 ‚àí pM L (x; Œ¥n )2 )2 ]
= E[(ps (x; Œ¥n ) + pM L (x; Œ¥n ))2 (ps (x; Œ¥n ) ‚àí pM L (x; Œ¥n ))2 ]
‚â§ 4E[(ps (x; Œ¥n ) ‚àí pM L (x; Œ¥n ))2 ]
4
.
‚â§
Sn
Now note that we have the following bounds on Pr(M L(Xs‚àó ) = 0) and Pr(M L(Xs‚àó ) = 1):
0 ‚â§ Pr(M L(Xs‚àó ) = 0) ‚â§ 1 ‚àí pM L (x; Œ¥n ),
0 ‚â§ Pr(M L(Xs‚àó ) = 1) ‚â§ pM L (x; Œ¥n ).
It follows that
0 ‚â§ Pr(ps (x; Œ¥n ) ‚àà {0, 1})
= Pr(M L(Xs‚àó ) = 0)Sn + Pr(M L(Xs‚àó ) = 1)Sn
‚â§ (1 ‚àí pM L (x; Œ¥n ))Sn + pM L (x; Œ¥n )Sn .
Lastly, for any  > 0,
E[|ps (x; Œ¥n ) ‚àí pM L (x; Œ¥n )|]
= E[|ps (x; Œ¥n ) ‚àí pM L (x; Œ¥n )|||ps (x; Œ¥n ) ‚àí pM L (x; Œ¥n )| ‚â• ] Pr(|ps (x; Œ¥n ) ‚àí pM L (x; Œ¥n )| ‚â• )
+ E[|ps (x; Œ¥n ) ‚àí pM L (x; Œ¥n )|||ps (x; Œ¥n ) ‚àí pM L (x; Œ¥n )| < ] Pr(|ps (x; Œ¥n ) ‚àí pM L (x; Œ¥n )| < )
Var(ps (x; Œ¥n ))
+¬∑1
2
1
‚â§
+ ,
Sn 2
< 1¬∑

57

where we use Chebyshev‚Äôs inequality for the first inequality. We can make E[|ps (x; Œ¥n ) ‚àí
pM L (x; Œ¥n )|] arbitrarily close to zero by taking sufficiently small  > 0 and sufficiently large
Sn , which implies that E[|ps (x; Œ¥n ) ‚àí pM L (x; Œ¥n )|] = o(1) if Sn ‚Üí ‚àû.
s = 1{ps (X ; Œ¥ ) ‚àà (0, 1)}, and let {V }‚àû be i.i.d. random variables such
Lemma B.8. Let Ii,n
i n
i i=1
2
that E[Vi ] < ‚àû. If Assumption 1 holds, Sn ‚Üí ‚àû, and Œ¥n ‚Üí 0, then
n

n

i=1

i=1

1X
1X
s
Vi ps (Xi ; Œ¥n )l Ii,n
‚àí
Vi pM L (Xi ; Œ¥n )l Ii,n = op (1)
n
n
for l = 0, 1, 2, 3, 4. If, in addition, Assumption 5 holds, and E[Vi |Xi ] is bounded, then
n

n

i=1

i=1

1 X
1 X
s
‚àö
Vi ps (Xi ; Œ¥n )l Ii,n
‚àí‚àö
Vi pM L (Xi ; Œ¥n )l Ii,n = op (1)
n
n
for l = 0, 1, 2.
Proof. We have
n

n

i=1

i=1

1X
1X
s
Vi ps (Xi ; Œ¥n )l Ii,n
‚àí
Vi pM L (Xi ; Œ¥n )l Ii,n
n
n
n
n
1X
1X
s
=
Vi ps (Xi ; Œ¥n )l (Ii,n
‚àí Ii,n ) +
Vi (ps (Xi ; Œ¥n )l ‚àí pM L (Xi ; Œ¥n )l )Ii,n .
n
n
i=1

We first consider

i=1

1
n

Pn

s
l
i=1 Vi (p (Xi ; Œ¥n )

‚àí pM L (Xi ; Œ¥n )l )Ii,n . By Lemma B.7, for l = 0, 1, 2,

n

|E[

1X
Vi (ps (Xi ; Œ¥n )l ‚àí pM L (Xi ; Œ¥n )l )Ii,n ]|
n
i=1
s

= |E[Vi (p (Xi ; Œ¥n )l ‚àí pM L (Xi ; Œ¥n )l )Ii,n ]|
= E[|E[Vi |Xi ]||E[ps (Xi ; Œ¥n )l ‚àí pM L (Xi ; Œ¥n )l |Xi ]|Ii,n ]
1
‚â§
E[|E[Vi |Xi ]|Ii,n ]
Sn
= O(Sn‚àí1 ).
Also, by Lemma B.7,
n

|E[

1X
Vi (ps (Xi ; Œ¥n )3 ‚àí pM L (Xi ; Œ¥n )3 )Ii,n ]|
n
i=1
s

= |E[Vi (p (Xi ; Œ¥n ) ‚àí pM L (Xi ; Œ¥n ))(ps (Xi ; Œ¥n )2 + ps (Xi ; Œ¥n )pM L (Xi ; Œ¥n ) + pM L (Xi ; Œ¥n )2 )Ii,n ]|
‚â§ E[|E[Vi |Xi ]||E[(ps (Xi ; Œ¥n ) ‚àí pM L (Xi ; Œ¥n ))(ps (Xi ; Œ¥n )2 + ps (Xi ; Œ¥n )pM L (Xi ; Œ¥n ) + pM L (Xi ; Œ¥n )2 )|Xi ]|Ii,n ]
‚â§ 3E[|E[Vi |Xi ]|E[|ps (Xi ; Œ¥n ) ‚àí pM L (Xi ; Œ¥n )||Xi ]Ii,n ]
= o(1),
58

and
n

1X
|E[
Vi (ps (Xi ; Œ¥n )4 ‚àí pM L (Xi ; Œ¥n )4 )Ii,n ]|
n
i=1
s

= |E[Vi (p (Xi ; Œ¥n )2 + pM L (Xi ; Œ¥n )2 )(ps (Xi ; Œ¥n ) + pM L (Xi ; Œ¥n ))(ps (Xi ; Œ¥n ) ‚àí pM L (Xi ; Œ¥n ))Ii,n ]|
‚â§ E[|E[Vi |Xi ]||E[(ps (Xi ; Œ¥n )2 + pM L (Xi ; Œ¥n )2 )(ps (Xi ; Œ¥n ) + pM L (Xi ; Œ¥n ))(ps (Xi ; Œ¥n ) ‚àí pM L (Xi ; Œ¥n ))|Xi ]|Ii,n ]
‚â§ 4E[|E[Vi |Xi ]|E[|ps (Xi ; Œ¥n ) ‚àí pM L (Xi ; Œ¥n )||Xi ]Ii,n ]
= o(1).
As for variance, for l = 0, 1, 2,
n

Var(

1X
1
Vi (ps (Xi ; Œ¥n )l ‚àí pM L (Xi ; Œ¥n )l )Ii,n ) ‚â§ E[Vi2 (ps (Xi ; Œ¥n )l ‚àí pM L (Xi ; Œ¥n )l )2 Ii,n ]
n
n
i=1

1
E[E[Vi2 |Xi ]E[(ps (Xi ; Œ¥n )l ‚àí pM L (Xi ; Œ¥n )l )2 |Xi ]Ii,n ]
n
4
E[E[Vi2 |Xi ]Ii,n ]
‚â§
nSn
= O((nSn )‚àí1 ),
‚â§

and for l = 3, 4,
n

Var(

1X
1
Vi (ps (Xi ; Œ¥n )l ‚àí pM L (Xi ; Œ¥n )l )Ii,n ) ‚â§ E[Vi2 (ps (Xi ; Œ¥n )l ‚àí pM L (Xi ; Œ¥n )l )2 Ii,n ]
n
n
i=1

1
E[Vi2 Ii,n ]
n
= o(1).

‚â§

P
Therefore, n1 ni=1 Vi (ps (Xi ; Œ¥n )l ‚àí pM L (Xi ; Œ¥n )l )Ii,n = op (1) if Sn ‚Üí ‚àû for l = 0, 1, 2, 3, 4, and
Pn
s
l
M L (X ; Œ¥ )l )I
‚àí1/2 S ‚Üí ‚àû for l = 0, 1, 2.
‚àö1
i n
i,n = op (1) if n
n
i=1 Vi (p (Xi ; Œ¥n ) ‚àí p
n
P
n
1
s
l
s
We next show that n i=1 Vi p (Xi ; Œ¥n ) (Ii,n ‚àí Ii,n ) = op (1) if Sn ‚Üí ‚àû and Œ¥n ‚Üí 0 for l ‚â• 0.
We have
n

|E[

1X
s
s
Vi ps (Xi ; Œ¥n )l (Ii,n
‚àí Ii,n )]| = |E[Vi ps (Xi ; Œ¥n )l (Ii,n
‚àí Ii,n )]|
n
i=1

s
‚â§ E[|E[Vi |Xi ]||E[ps (Xi ; Œ¥n )l (Ii,n
‚àí Ii,n )|Xi ]|]
s
‚â§ E[|E[Vi |Xi ]|E[|Ii,n
‚àí Ii,n ||Xi ]].

Note that by construction, 1{ps (Xi ; Œ¥n ) ‚àà (0, 1)} ‚â§ 1{pM L (Xi ; Œ¥n ) ‚àà (0, 1)} with probability one
conditional on Xi = x, so that
s
s
E[|Ii,n
‚àí Ii,n ||Xi = x] = ‚àíE[Ii,n
‚àí Ii,n |Xi = x].

Suppose M L is continuous at x and M L(x) ‚àà (0, 1). Then limŒ¥‚Üí0 pM L (x; Œ¥) = M L(x) ‚àà (0, 1)
by Part 1 of Corollary 2, and hence pM L (x; Œ¥n ) ‚àà [, 1 ‚àí ] for sufficiently small Œ¥n > 0 for some
59

constant  ‚àà (0, 1/2). It follows that
s
E[Ii,n
|Xi = x] = 1 ‚àí Pr(ps (x; Œ¥n ) ‚àà {0, 1})

‚â• 1 ‚àí (1 ‚àí pM L (x; Œ¥n ))Sn ‚àí pM L (x; Œ¥n )Sn
‚â• 1 ‚àí 2(1 ‚àí )Sn
‚Üí1
s ‚àí
as Sn ‚Üí ‚àû, where the first inequality follows from Lemma B.7. This implies that E[Ii,n
Ii,n |Xi = x] ‚Üí 0 as n ‚Üí ‚àû. Suppose x ‚àà int(X0 )‚à™int(X1 ). Then B(x, Œ¥n ) ‚äÇ X0 or B(x, Œ¥n ) ‚äÇ X1
for sufficiently small Œ¥n > 0 by the fact that int(X0 ) and int(X1 ) are open, and hence pM L (x; Œ¥n ) ‚àà
s ‚àí I |X = x] ‚Üí 0 as
{0, 1} and ps (x; Œ¥n ) ‚àà {0, 1} for sufficiently small Œ¥n > 0, so that E[Ii,n
i,n
i
s
n ‚Üí ‚àû. Therefore, E[Ii,n ‚àí Ii,n |Xi = x] ‚Üí 0 for almost every x ‚àà X , since M L is continuous at
x for almost every x ‚àà X by Assumption 1 (a), and either M L(x) ‚àà (0, 1) or x ‚àà int(X0 )‚à™int(X1 )
for almost every x ‚àà X by Assumption 1 (b). By the Dominated Convergence Theorem,
s
‚àíE[|E[Vi |Xi ]|E[Ii,n
‚àí Ii,n |Xi ]] ‚Üí 0

as n ‚Üí ‚àû.
As for variance,
n

Var(

1
1X
s
s
Vi ps (Xi ; Œ¥n )l (Ii,n
‚àí Ii,n )) ‚â§ E[Vi2 ps (Xi ; Œ¥n )2l (Ii,n
‚àí Ii,n )2 ]
n
n
i=1

1
E[Vi2 ]
n
‚Üí 0.
‚â§

Lastly, we show that, for l ‚â• 0,
holds, and E[Vi |Xi ] is bounded. Let
We have

‚àö1
n

Pn

s
l s
i=1 Vi p (Xi ; Œ¥n ) (Ii,n ‚àí Ii,n )
n
Œ∑n = Œ≥ log
Sn , where Œ≥ is the one

= op (1) if Assumption 5
satisfying Assumption 5.

n

1 X
s
|E[ ‚àö
Vi ps (Xi ; Œ¥n )l (Ii,n
‚àí Ii,n )]|
n
i=1
‚àö
s
‚àí Ii,n ||Xi ]]
‚â§ nE[|E[Vi |Xi ]|E[|Ii,n
‚àö
s
= ‚àí nE[|E[Vi |Xi ]|E[Ii,n ‚àí 1|Xi ]Ii,n ]
‚àö
‚â§ nE[|E[Vi |Xi ]|((1 ‚àí pM L (Xi ; Œ¥n ))Sn + pM L (Xi ; Œ¥n )Sn ))Ii,n ]
‚àö
= nE[|E[Vi |Xi ]|((1 ‚àí pM L (Xi ; Œ¥n ))Sn + pM L (Xi ; Œ¥n )Sn ))1{pM L (Xi ; Œ¥n ) ‚àà (0, Œ∑n ) ‚à™ (1 ‚àí Œ∑n , 1)}]
‚àö
+ nE[|E[Vi |Xi ]|((1 ‚àí pM L (Xi ; Œ¥n ))Sn + pM L (Xi ; Œ¥n )Sn ))1{pM L (Xi ; Œ¥n ) ‚àà [Œ∑n , 1 ‚àí Œ∑n ]}]
‚àö
‚àö
‚â§ (sup |E[Vi |Xi = x]|)( n Pr(pM L (Xi ; Œ¥n ) ‚àà (0, Œ∑n ) ‚à™ (1 ‚àí Œ∑n , 1)) + 2 n(1 ‚àí Œ∑n )Sn ),
x‚ààX
s ‚â§ I
where the second equality follows from the fact that Ii,n
i,n with strict inequality only if
‚àö
‚àö
M
L
Ii,n = 1. By Assumption 5, n Pr(p (Xi ; Œ¥n ) ‚àà (0, Œ∑n )‚à™(1‚àíŒ∑n , 1)) = o(1). As for n(1‚àíŒ∑n )Sn ,
n
log n
1
‚àí1/2 S ‚Üí ‚àû and log n ‚Üí 0. Using the
first observe that Œ∑n = Œ≥ log
n
Sn = Œ≥ n1/2 n‚àí1/2 S ‚Üí 0, since n
n1/2
n

60

fact that et ‚â• 1 + t for every t ‚àà R, we have
‚àö
‚àö
n(1 ‚àí Œ∑n )Sn ‚â§ n(e‚àíŒ∑n )Sn
‚àö
= ne‚àíŒ∑n Sn
‚àö
= ne‚àíŒ≥ log n
‚àö
= nn‚àíŒ≥
= n1/2‚àíŒ≥
‚Üí 0,
since Œ≥ > 1/2. As for variance,
n

1 X
s
s
Var( ‚àö
Vi ps (Xi ; Œ¥n )l (Ii,n
‚àí Ii,n )) ‚â§ E[Vi2 ps (Xi ; Œ¥n )2l (Ii,n
‚àí Ii,n )2 ]
n
i=1

s
‚â§ E[Vi2 |Ii,n
‚àí Ii,n |]
s
= E[E[Vi2 |Xi ]E[|Ii,n
‚àí Ii,n ||Xi ]]

= o(1).

C
C.1

Proofs
Proof of Proposition 1

Suppose that Assumptions 1 and 2 hold. Here, we only show that
(a) E[Y1i ‚àí Y0i |Xi = x] is identified for every x ‚àà int(X ) such that pM L (x) ‚àà (0, 1).
(b) Let A be any open subset of X such that pM L (x) exists for all x ‚àà A. Then E[Y1i ‚àíY0i |Xi ‚àà
A] is identified only if pM L (x) ‚àà (0, 1) for almost every x ‚àà A.
The results for E[Di (1) ‚àí Di (0)|Xi = x] and E[Di (1) ‚àí Di (0)|Xi ‚àà A] are obtained by a similar
argument.
Proof of Part (a). Pick an x ‚àà int(X ) such that pM L (x) ‚àà (0, 1). If M L(x) ‚àà (0, 1),
E[Y1i ‚àí Y0i |Xi = x] and E[Di (1) ‚àí Di (0)|Xi = x] are trivially identified by Property 1:
E[Yi |Xi = x, Zi = 1] ‚àí E[Yi |Xi = x, Zi = 0] = E[Y1i ‚àí Y0i |Xi = x].
We next consider the case where M L(x) ‚àà {0, 1}. Since x ‚àà int(X ), B(x, Œ¥) ‚äÇ X for any
sufficiently small Œ¥ > 0. Moreover, since pM L (x) = limŒ¥‚Üí0 pM L (x; Œ¥) ‚àà (0, 1), pM L (x; Œ¥) ‚àà (0, 1)
for any sufficiently small Œ¥ > 0. This implies that we can find points x0,Œ¥ , x1,Œ¥ ‚àà B(x, Œ¥)(‚äÇ X )
such that M L(x0,Œ¥ ) < 1 and M L(x1,Œ¥ ) > 0 for any sufficiently small Œ¥ > 0, for otherwise
pM L (x; Œ¥) ‚àà {0, 1}. Noting that x0,Œ¥ ‚Üí x and x1,Œ¥ ‚Üí x as Œ¥ ‚Üí 0,
lim (E[Yi |Xi = x1,Œ¥ , Zi = 1] ‚àí E[Yi |Xi = x0,Œ¥ , Zi = 0]) = lim (E[Yi1 |Xi = x1,Œ¥ ] ‚àí E[Yi0 |Xi = x0,Œ¥ ])

Œ¥‚Üí0

Œ¥‚Üí0

= E[Y1i ‚àí Y0i |Xi = x],
61

where the first equality follows from Property 1, and the second from Assumption 2.
Proof of Part (b).
Suppose to the contrary that Lp ({x ‚àà A : pM L (x) ‚àà {0, 1}}) > 0. Without loss of generality,
assume Lp ({x ‚àà A : pM L (x) = 1}) > 0. The proof proceeds in four steps.
Step C.1.1. Lp (A ‚à© X1 ) > 0.
Proof. By Assumption 1, M L is continuous almost everywhere. Part 1 of Cororally 2 then
implies that pM L (x) = M L(x) for almost every x ‚àà {x‚àó ‚àà A : pM L (x‚àó ) = 1}. Since Lp ({x ‚àà A :
pM L (x) = 1}) > 0, Lp ({x ‚àà A : pM L (x) = 1, pM L (x) = M L(x)}) > 0, and hence Lp (A ‚à© X1 ) >
0.
Step C.1.2. A ‚à© int(X1 ) 6= ‚àÖ.
Proof. Suppose that A ‚à© int(X1 ) = ‚àÖ. Then, we must have that A ‚à© X1 ‚äÇ X1 \ int(X1 ). It then
follows that Lp (A ‚à© X1 ) ‚â§ Lp (X1 \ int(X1 )) = Lp (X1 ) ‚àí Lp (int(X1 )) = 0, where the last equality
holds by Assumption 1. But this is a contradiction to the result from Step C.1.1.
Step C.1.3. pM L (x) = 1 for any x ‚àà int(X1 ).
Proof. Pick any x ‚àà int(X1 ). By the definition of interior, B(x, Œ¥) ‚äÇ X1 for any sufficiently small
Œ¥ > 0. Therefore, pM L (x; Œ¥) = 1 for any sufficiently small Œ¥ > 0.
Step C.1.4. E[Y1i ‚àí Y0i |Xi ‚àà A] is not identified.
Proof. We first introduce some notation. Let Q be the set of all distributions of (Y1i , Y0i , Xi , Zi )
satisfying Property 1 and Assumptions 1 and 2. Let P be the set of all distributions of (Yi , Xi , Zi ).
Let T : Q ‚Üí P be a function such that, for Q ‚àà Q, T (Q) is the distribution of (Zi Y1i + (1 ‚àí
Zi )Y0i , Xi , Zi ), where the distribution of (Y1i , Y0i , Xi , Zi ) is Q. Let Q0 and P0 denote the true
distributions of (Y1i , Y0i , Xi , Zi ) and (Yi , Xi , Zi ), respectively. Given P0 , the identified set of
E[Y1i ‚àí Y0i |Xi ‚àà A] is given by {EQ [Y1i ‚àí Y0i |Xi ‚àà A] : P0 = T (Q), Q ‚àà Q}, where EQ [¬∑] is the
expectation operator under distribution Q. We show that this set contains two distinct values.
In what follows, Pr(¬∑) and E[¬∑] without a subscript denote the probability and expectation under
the true distributions Q0 and P0 as up until now.
Now pick any x‚àó ‚àà A ‚à© int(X1 ). Since A and int(X1 ) are open, there is some Œ¥ > 0 such
that B(x‚àó , Œ¥) ‚äÇ A ‚à© int(X1 ). Let  = 2Œ¥ , and consider a function f : X ‚Üí R such that f (x) =
E[Y0i |X = x] for all x ‚àà X \ B(x‚àó , ) and f (x) = E[Y0i |X = x] ‚àí 1 for all x ‚àà B(x‚àó , ). Below,
we show that f is continuous at any point x ‚àà X such that pM L (x) ‚àà (0, 1) and M L(x) ‚àà {0, 1}.
Pick any x ‚àà X such that pM L (x) ‚àà (0, 1) and M L(x) ‚àà {0, 1}. Since B(x‚àó , Œ¥) ‚äÇ int(X1 ) and
int(X1 ) ‚äÇ {x0 ‚àà X : pM L (x0 ) = 1} by Step C.1.3, x ‚àà
/ B(x‚àó , Œ¥). Hence, B(x, ) ‚äÇ X \ B(x‚àó , ).
By Assumption 2 and the definition of f , f is continuous at x.
Now take any random vector (Y1i‚àó , Y0i‚àó , Xi‚àó , Zi‚àó ) that is distributed according to the true distribution Q0 . Let Q be the distribution of (Y1iQ , Y0iQ , XiQ , ZiQ ), where (Y1iQ , XiQ , ZiQ ) = (Y1i‚àó , Xi‚àó , Zi‚àó ),
and
(
if Xi‚àó ‚àà X \ B(x‚àó , )
Y0i‚àó
Q
Y0i =
Y0i‚àó ‚àí 1
if Xi‚àó ‚àà B(x‚àó , )
62

Note first that Q ‚àà Q, since EQ [Y1iQ |XiQ = x] = E[Y1i‚àó |Xi‚àó = x] and EQ [Y0iQ |XiQ = x] = f (x),
where E[Y1i‚àó |Xi‚àó ] and f are both continuous at any point x ‚àà X such that pM L (x) ‚àà (0, 1) and
M L(x) ‚àà {0, 1}. Also, ZiQ = Zi‚àó = 1 if Xi‚àó ‚àà B(x‚àó , ). It then follows that
YiQ = ZiQ Y1iQ + (1 ‚àí ZiQ )Y0iQ
(
Zi‚àó Y1i‚àó + (1 ‚àí Zi‚àó )Y0i‚àó
=
Zi‚àó Y1i‚àó

if Xi‚àó ‚àà X \ B(x‚àó , )

Yi‚àó = Zi‚àó Y1i‚àó + (1 ‚àí Zi‚àó )Y0i‚àó
(
Zi‚àó Y1i‚àó + (1 ‚àí Zi‚àó )Y0i‚àó
=
Zi‚àó Y1i‚àó

if Xi‚àó ‚àà X \ B(x‚àó , )

if Xi‚àó ‚àà B(x‚àó , )

and

if Xi‚àó ‚àà B(x‚àó , ).

Thus, YiQ = Yi‚àó , and hence T (Q) = T (Q0 ) = P0 .
Using EQ [Y1iQ |XiQ = x] = E[Y1i‚àó |Xi‚àó = x] and EQ [Y0iQ |XiQ = x] = f (x), we have
EQ [Y1iQ ‚àí Y0iQ |XiQ ‚àà A]
= EQ [EQ [Y1iQ |XiQ ]|XiQ ‚àà A]
‚àí EQ [EQ [Y0iQ |XiQ ]|XiQ ‚àà A, XiQ ‚àà
/ B(x‚àó , )]PrQ (XiQ ‚àà
/ B(x‚àó , )|XiQ ‚àà A)
‚àí EQ [EQ [Y0iQ |XiQ ]|XiQ ‚àà B(x‚àó , )]PrQ (XiQ ‚àà B(x‚àó , )|XiQ ‚àà A)
= E[E[Y1i‚àó |Xi‚àó ]|Xi‚àó ‚àà A] ‚àí E[f (Xi‚àó )|Xi‚àó ‚àà A, Xi‚àó ‚àà
/ B(x‚àó , )] Pr(Xi‚àó ‚àà
/ B(x‚àó , )|Xi‚àó ‚àà A)
‚àí E[f (Xi‚àó )|Xi‚àó ‚àà B(x‚àó , )] Pr(Xi‚àó ‚àà B(x‚àó , )|Xi‚àó ‚àà A)
= E[Y1i‚àó |Xi‚àó ‚àà A] ‚àí E[Y0i‚àó |Xi‚àó ‚àà A, Xi‚àó ‚àà
/ B(x‚àó , )] Pr(Xi‚àó ‚àà
/ B(x‚àó , )|Xi‚àó ‚àà A)
‚àí E[Y0i‚àó ‚àí 1|Xi‚àó ‚àà B(x‚àó , )] Pr(Xi‚àó ‚àà B(x‚àó , )|Xi‚àó ‚àà A)
= E[Y1i‚àó ‚àí Y0i‚àó |Xi‚àó ‚àà A] + Pr(Xi‚àó ‚àà B(x‚àó , )|Xi‚àó ‚àà A).
By the definition of support, Pr(Xi‚àó ‚àà B(x‚àó , )) > 0. Since T (Q) = T (Q0 ) = P0 but EQ [Y1iQ ‚àí
Y0iQ |XiQ ‚àà A] 6= E[Y1i‚àó ‚àí Y0i‚àó |Xi‚àó ‚àà A], E[Y1i ‚àí Y0i |Xi ‚àà A] is not identified.

C.2

Proof of Corollary 1

If Pr(Di (1) ‚àí Di (0) = 1|Xi = x) = 1, Pr(Y1i ‚àí Y0i = Yi (1) ‚àí Yi (0)|Xi = x) = 1, and hence
E[Y1i ‚àí Y0i |Xi = x] = E[Yi (1) ‚àí Yi (0)|Xi = x]. Then, Parts (a) and (b) follow from Proposition
1. If Pr(Di (1) ‚â• Di (0)|Xi = x) = 1, we have
E[Y1i ‚àí Y0i |Xi = x] = E[(Di (1) ‚àí Di (0))(Yi (1) ‚àí Yi (0))|Xi = x]
= Pr(Di (1) 6= Di (0)|Xi = x)E[Yi (1) ‚àí Yi (0)|Di (1) 6= Di (0), Xi = x].
63

If in addition Pr(Di (1) 6= Di (0)|Xi = x) > 0, we obtain
E[Y1i ‚àí Y0i |Xi = x]
Pr(Di (1) 6= Di (0)|Xi = x)
E[Y1i ‚àí Y0i |Xi = x]
=
.
E[Di (1) ‚àí Di (0)|Xi = x]

E[Yi (1) ‚àí Yi (0)|Di (1) 6= Di (0), Xi = x] =

Then, Part (c) follows from Proposition 1 (a). Part (d) is established by following the procedure
used to show Proposition 1 (b).

C.3

Proof of Proposition 2

With change of variables u =
R

x‚àó ‚àíx
Œ¥ ,

B(x,Œ¥) M L(x

pM L (x; Œ¥) =

R
R
p

Œ¥
=

‚àó

B(0,1) M L(x + Œ¥u)du
R
Œ¥ p B(0,1) du

‚à™q‚ààQ Ux,q

M L(x + Œ¥u)du +
R

R
B(0,1)\‚à™q‚ààQ Ux,q

M L(x + Œ¥u)du

B(0,1) du

P
=

‚àó )dx‚àó

B(x,Œ¥) dx

R
=

we have

R

q‚ààQ Ux,q

M L(x + Œ¥u)du
,

R

B(0,1) du

where the last equality follows from the assumption that Lp (‚à™q‚ààQ Ux,q ) = Lp (B(0, 1)). By the
definition of Ux,q , for each q ‚àà Q, limŒ¥‚Üí0 M L(x + Œ¥u) = q for any u ‚àà Ux,q . By the Dominated
Convergence Theorem,
pM L (x) = lim pM L (x; Œ¥)
Œ¥‚Üí0
P
p
q‚ààQ qL (Ux,q )
.
=
Lp (B(0, 1))
P
The numerator exists, since q ‚â§ 1 for all q ‚àà Q and q‚ààQ Lp (Ux,q ) = Lp (B(0, 1)).

C.4

Proof of Corollary 2

1. Suppose that M L is continuous at x ‚àà X , and let q = M L(x). Then, by definition,
Ux,q = B(0, 1). By Proposition 2, pM L (x) exists, and pM L (x) = q.
2. Pick any x ‚àà int(Xq ). M L is continuous at x, since there exists Œ¥ > 0 such that B(x, Œ¥) ‚äÇ Xq
by the definition of interior. By the previous result, pM L (x) exists, and pM L (x) = q.
3. Let N be the neighborhood of x on which f is continuously differentiable. By the mean
value theorem, for any sufficiently small Œ¥ > 0,
f (x + Œ¥u) = f (x) + ‚àáf (xÃÉŒ¥ ) ¬∑ Œ¥u
= ‚àáf (xÃÉŒ¥ ) ¬∑ Œ¥u
64

for some xÃÉŒ¥ which is on the line segment connecting x and x + Œ¥u. Since xÃÉŒ¥ ‚Üí x as Œ¥ ‚Üí 0
and ‚àáf is continuous on N , ‚àáf (xÃÉŒ¥ ) ¬∑ u ‚Üí ‚àáf (x) ¬∑ u as Œ¥ ‚Üí 0. Therefore, if ‚àáf (x) ¬∑ u > 0,
then f (x + Œ¥u) = ‚àáf (xÃÉŒ¥ ) ¬∑ Œ¥u > 0 for any sufficiently small Œ¥ > 0, and if ‚àáf (x) ¬∑ u < 0,
then f (x + Œ¥u) = ‚àáf (xÃÉŒ¥ ) ¬∑ Œ¥u < 0 for any sufficiently small Œ¥ > 0. We then have
Ux+ ‚â° {u ‚àà B(0, 1) : ‚àáf (x) ¬∑ u > 0} ‚äÇ Ux,q1
Ux‚àí ‚â° {u ‚àà B(0, 1) : ‚àáf (x) ¬∑ u < 0} ‚äÇ Ux,q2 .
Let V be the Lebesgue measure of a half p-dimensional unit ball. Since V = Lp (Ux+ ) ‚â§
Lp (Ux,q1 ), V = Lp (Ux‚àí ) ‚â§ Lp (Ux,q2 ), and Lp (Ux,q1 ) + Lp (Ux,q2 ) ‚â§ Lp (B(0, 1)) = 2V , it
follows that Lp (Ux,q1 ) = Lp (Ux,q2 ) = V . By Proposition 2, pM L (x) exists, and pM L (x) =
1
2 (q1 + q2 ).
4. We have that U0,q1 = {(u1 , u2 )0 ‚àà B(0, 1) : u1 ‚â§ 0 or u2 ‚â§ 0} and U0,q2 = {(u1 , u2 )0 ‚àà
B(0, 1) : u1 > 0, u2 > 0}. By Proposition 2, pM L (x) exists, and pM L (x) =
3
4 q1

C.5

+

q1 L2 (U0,q1 )+q2 L2 (U0,q2 )
L2 (B(0,1))

1
4 q2 .

Proof of Proposition A.1

Since M L is a Lp -measurable and bounded function, M L is locally integrable with respect to
R
the Lebesgue measure, i.e., for every ball B ‚äÇ Rp , B M L(x)dx exists. An application of the
Lebesgue differentiation theorem (see e.g. Theorem 1.4 in Chapter 3 of Stein and Shakarchi
(2005)) to the function M L shows that
R
‚àó
‚àó
B(x,Œ¥) M L(x )dx
R
lim
= M L(x)
‚àó
Œ¥‚Üí0
B(x,Œ¥) dx
for almost every x ‚àà Rp .

C.6

Proof of Theorem 1

We prove consistency and asymptotic normality of the following estimators without imposing
Assumption 3 (c). These estimators are aymptotically equivalent to the estimators defined in
Section 4.1 if Assumption 3 (c) holds.
First, consider the following 2SLS regression using the observations with pM L (Xi ; Œ¥n ) ‚àà (0, 1):
Di = Œ≥0 (1 ‚àí In ) + Œ≥1 Zi + Œ≥2 pM L (Xi ; Œ¥n ) + ŒΩi

(15)

Yi = Œ≤0 (1 ‚àí In ) + Œ≤1 Di + Œ≤2 pM L (Xi ; Œ¥n ) + i .

(16)

Here In is a dummy random variable which equals one if there exists a constant q ‚àà (0, 1)
such that M L(Xi ) ‚àà {0, q, 1} for all i ‚àà {1, ..., n}. In is the indicator that M L(Xi ) takes
on only one nondegenerate value in the sample. If the support of M L(Xi ) (in the population) contains only one value in (0, 1), pM L (Xi ; Œ¥n ) is asymptotically constant conditional on
pM L (Xi ; Œ¥n ) ‚àà (0, 1). To avoid the multicollinearity between asymptotically constant pM L (Xi ; Œ¥n )
and a constant, we do not include the constant term if In = 1. Let Ii,n = 1{pM L (Xi ; Œ¥n ) ‚àà (0, 1)},
65

=

M L (X ; Œ¥ ))0 , and Znc =
Di,n = (1, Di , pM L (Xi ; Œ¥n ))0 , Zi,n = (1, Zi , pM L (Xi ; Œ¥n ))0 , Dnc
i n
i,n = (Di , p
i,n
M
L
0
(Zi , p (Xi ; Œ¥n )) . The 2SLS estimator Œ≤ÃÇ from this regression is then given by
(P
P
( ni=1 Zi,n D0i,n Ii,n )‚àí1 ni=1 Zi,n Yi Ii,n
if In =0
Œ≤ÃÇ = Pn
Pn
nc
nc
0
‚àí1
nc
( i=1 Zi,n (Di,n ) Ii,n )
if In =1.
i=1 Zi,n Yi Ii,n

Let Œ≤ÃÇ1 denote the 2SLS estimator of Œ≤1 in the above regression.
Similarly, consider the following simulation version of the 2SLS regression using the observations with ps (Xi ; Œ¥n ) ‚àà (0, 1):
(17)

Di = Œ≥0 (1 ‚àí In ) + Œ≥1 Zi + Œ≥2 ps (Xi ; Œ¥n ) + ŒΩi

(18)

s

Yi = Œ≤0 (1 ‚àí In ) + Œ≤1 Di + Œ≤2 p (Xi ; Œ¥n ) + i .
Let Œ≤ÃÇ1s denote the 2SLS estimator of Œ≤1 in the simulation-based regression.
Below, we prove the following result.

Theorem C.1. Suppose that Assumptions 1 and 3 hold except Assumption 3 (c), and that
Œ¥n ‚Üí 0, nŒ¥n ‚Üí ‚àû and Sn ‚Üí ‚àû as n ‚Üí ‚àû. Then the 2SLS estimators Œ≤ÃÇ1 and Œ≤ÃÇ1s converge in
probability to
Œ≤1 ‚â° lim E[œâi (Œ¥)(Yi (1) ‚àí Yi (0))],
Œ¥‚Üí0

where
œâi (Œ¥) =

pM L (Xi ; Œ¥)(1 ‚àí pM L (Xi ; Œ¥))(Di (1) ‚àí Di (0))
.
E[pM L (Xi ; Œ¥)(1 ‚àí pM L (Xi ; Œ¥))(Di (1) ‚àí Di (0))]

Suppose, in addition, that Assumptions 4 and 5 hold and that nŒ¥n2 ‚Üí 0 as n ‚Üí ‚àû. Then
d

œÉÃÇn‚àí1 (Œ≤ÃÇ1 ‚àí Œ≤1 ) ‚àí‚Üí N (0, 1),
d

(œÉÃÇns )‚àí1 (Œ≤ÃÇ1s ‚àí Œ≤1 ) ‚àí‚Üí N (0, 1).
where we define œÉÃÇn‚àí1 and (œÉÃÇns )‚àí1 as follows: let
(P
P
P
( ni=1 Zi,n D0i,n Ii,n )‚àí1 ( ni=1 ÀÜ2i,n Zi,n Z0i,n Ii,n )( ni=1 Di,n Z0i,n Ii,n )‚àí1
Œ£ÃÇn = Pn
P
Pn 2 nc nc 0
nc 0
‚àí1
nc 0
‚àí1
ÀÜi,n Zi,n (Zi,n ) Ii,n )( ni=1 Dnc
( i=1 Znc
i,n (Zi,n ) Ii,n )
i,n (Di,n ) Ii,n ) ( i=1 

if In = 0
if In = 1,

where
ÀÜi,n =

(
Yi ‚àí D0i,n Œ≤ÃÇ
0
Yi ‚àí (Dnc
i,n ) Œ≤ÃÇ

if In = 0
if In = 1.

Let œÉÃÇn2 denote the estimator for the variance of Œ≤ÃÇ1 . That is, œÉÃÇn2 is the second diagonal element of
Œ£ÃÇn when In = 0 and is the first diagonal element of Œ£ÃÇn when In = 1. (œÉÃÇns )2 is the analogouslydefined estimator for the variance of Œ≤ÃÇ1s from the simulation-based regression.
Throughout the proof, we omit the subscript n from Ii,n , Di,n , Zi,n , ÀÜi,n , Œ£ÃÇn , œÉÃÇn , etc.
for notational brevity. We provide proofs separately for the two cases, the case in which
Pr(M L(Xi ) ‚àà (0, 1)) > 0 and the case in which Pr(M L(Xi ) ‚àà (0, 1)) = 0. For each case, we first
prove consistency and asymptotic normality of Œ≤ÃÇ1 , and then prove consistency and asymptotic
normality of Œ≤ÃÇ1s .
66

C.6.1

Consistency and Asymptotic Normality of Œ≤ÃÇ1 When Pr(M L(Xi ) ‚àà (0, 1)) > 0

By Lemma B.6,
lim E[pM L (Xi ; Œ¥)(1 ‚àí pM L (Xi ; Œ¥))(Di (1) ‚àí Di (0))] = E[M L(Xi )(1 ‚àí M L(Xi ))(Di (1) ‚àí Di (0))].

Œ¥‚Üí0

When Pr(M L(Xi ) ‚àà (0, 1)) > 0, E[M L(Xi )(1 ‚àí M L(Xi ))(Di (1) ‚àí Di (0))] = E[pM L (Xi )(1 ‚àí
pM L (Xi ))(Di (1) ‚àí Di (0))], since pM L (x) = M L(x) for almost every x ‚àà X by Proposition A.1.
Under Assumption 3 (b), E[pM L (Xi )(1 ‚àí pM L (Xi ))(Di (1) ‚àí Di (0))] > 0. Again by Lemma B.6,
lim E[œâi (Œ¥)(Yi (1) ‚àí Yi (0))] =

Œ¥‚Üí0

Let Œ≤1 =

E[M L(Xi )(1 ‚àí M L(Xi ))(Di (1) ‚àí Di (0))(Yi (1) ‚àí Yi (0)]
.
E[M L(Xi )(1 ‚àí M L(Xi ))(Di (1) ‚àí Di (0))]

E[M L(Xi )(1‚àíM L(Xi ))(Di (1)‚àíDi (0))(Yi (1)‚àíYi (0)]
.
E[M L(Xi )(1‚àíM L(Xi ))(Di (1)‚àíDi (0))]

Let

n
n
X
X
0
‚àí1
Œ≤ÃÇ = (
Zi D i I i )
Zi Yi Ii
c

i=1

Œ≤ÃÇ nc

i=1

n
n
X
X
nc
nc 0
‚àí1
=(
Zi (Di ) Ii )
Znc
i Yi Ii ,
i=1

i=1

and let Œ≤ÃÇ1c = (0, 1, 0)Œ≤ÃÇ c and Œ≤ÃÇ1nc = (1, 0)Œ≤ÃÇ nc . Œ≤ÃÇ1 is given by
Œ≤ÃÇ1 = Œ≤ÃÇ1c (1 ‚àí In ) + Œ≤ÃÇ1nc In .
0
nc
0
Also, let DÃÉi = (1, Di , M L(Xi ))0 , ZÃÉi = (1, Zi , M L(Xi ))0 , DÃÉnc
i = (Di , M L(Xi )) , ZÃÉi = (Zi , M L(Xi )) ,
M
L
and Ii = 1{M L(Xi ) ‚àà (0, 1)}.
We claim that Pr(In = 1) ‚Üí 0 when Var(M L(Xi )|IiM L = 1) > 0, and that Pr(In = 1) ‚Üí 1
when Var(M L(Xi )|IiM L = 1) = 0. To show the first claim, observe that In = 1 if and only if
VÃÇn = 0, where
Pn
Pn
M L(Xi )IiM L 2 M L
i=1
Pn
) Ii
ML
i=1 (M L(Xi ) ‚àí
i=1 Ii
Pn
VÃÇn =
ML
i=1 Ii

is the sample variance of M L(Xi ) conditional on IiM L = 1. When Var(M L(Xi )|IiM L = 1) > 0,
Pr(In = 1) = Pr(VÃÇn = 0)
‚â§ Pr(|VÃÇn ‚àí Var(M L(Xi )|IiM L = 1)| ‚â• Var(M L(Xi )|IiM L = 1))
‚Üí 0,
p

where the convergence follows since VÃÇn ‚àí‚Üí Var(M L(Xi )|IiM L = 1) > 0.
To show the second claim, note that, when Var(M L(Xi )|IiM L = 1) = 0, there exists q ‚àà (0, 1)
such that Pr(M L(Xi ) = q|IiM L = 1) = 1. It follows that
Pr(In = 0) = Pr(M L(Xi ) ‚àà {0, 1} for all i = 1, ..., n)
+ Pr(M L(Xi ) = q 0 and M L(Xj ) = q 00 for some q 0 , q 00 ‚àà (0, 1) with q 0 6= q 00
for some i, j ‚àà {1, ..., n})
= Pr(M L(Xi ) ‚àà {0, 1} for all i = 1, ..., n)
= (1 ‚àí Pr(M L(Xi ) ‚àà (0, 1)))n ,
67

which converges to zero as n ‚Üí ‚àû, since Pr(M L(Xi ) ‚àà (0, 1)) > 0.
The above claims imply that Œ≤ÃÇ1 = Œ≤ÃÇ1c with probability approaching one when Var(M L(Xi )|IiM L =
1) > 0, and that Œ≤ÃÇ1 = Œ≤ÃÇ1nc with probability approaching one when Var(M L(Xi )|IiM L = 1) = 0.
Therefore, to prove consistency and asymptotic normality of Œ≤ÃÇ1 , it suffices to show those of Œ≤ÃÇ1c
when Var(M L(Xi )|IiM L = 1) > 0 and those of Œ≤ÃÇ1nc when Var(M L(Xi )|IiM L = 1) = 0.
Below we first show that, if Assumptions 1 and 3 hold and Œ¥n ‚Üí 0 as n ‚Üí ‚àû, then
p
Œ≤ÃÇ1 ‚àí‚Üí Œ≤1 . We then show that, if, in addition, Assumption 4 holds and nŒ¥n2 ‚Üí 0 as n ‚Üí ‚àû, then
d

œÉÃÇ ‚àí1 (Œ≤ÃÇ1 ‚àí Œ≤1 ) ‚àí‚Üí N (0, 1).
p

Proof of Consistency. To prove consistency of Œ≤ÃÇ1 , we first show that Œ≤ÃÇ1c ‚àí‚Üí Œ≤1 when
p
Var(M L(Xi )|IiM L = 1) > 0. We then show that Œ≤ÃÇ1nc ‚àí‚Üí Œ≤1 whether or not Var(M L(Xi )|IiM L =
1) > 0. By Lemma B.6,
n
n
X
X
p
Œ≤ÃÇ c = (
Zi D0i Ii )‚àí1
Zi Yi Ii ‚àí‚Üí (E[ZÃÉi DÃÉ0i IiM L ])‚àí1 E[ZÃÉi Yi IiM L ]
i=1

i=1

provided that E[ZÃÉi DÃÉ0i IiM L ] is invertible. After a few lines of algebra, we have
det(E[ZÃÉi DÃÉ0i IiM L ])
= Pr(IiM L = 1)2 Var(M L(Xi )|IiM L = 1)E[Di (Zi ‚àí M L(Xi ))IiM L ]
= Pr(IiM L = 1)2 Var(M L(Xi )|IiM L = 1)E[(Zi Di (1) + (1 ‚àí Zi )Di (0))(Zi ‚àí M L(Xi ))IiM L ]
= Pr(IiM L = 1)2 Var(M L(Xi )|IiM L = 1)E[((Zi ‚àí Zi M L(Xi ))Di (1) ‚àí (1 ‚àí Zi )M L(Xi )Di (0))IiM L ]
= Pr(IiM L = 1)2 Var(M L(Xi )|IiM L = 1)E[((M L(Xi ) ‚àí M L(Xi )2 )Di (1) ‚àí (1 ‚àí M L(Xi ))M L(Xi )Di (0))IiM L ]
= Pr(IiM L = 1)2 Var(M L(Xi )|IiM L = 1)E[M L(Xi )(1 ‚àí M L(Xi ))(Di (1) ‚àí Di (0))IiM L ]
= Pr(IiM L = 1)2 Var(M L(Xi )|IiM L = 1)E[M L(Xi )(1 ‚àí M L(Xi ))(Di (1) ‚àí Di (0))],
where the fourth equality follows from Property 1. Therefore, E[ZÃÉi DÃÉ0i IiM L ]
Var(M L(Xi )|IiM L = 1) > 0. Another few lines of algebra gives
Ô£Æ
‚àó
1
Ô£Ø
0 M L ‚àí1
(E[ZÃÉi DÃÉi Ii ]) =
Ô£∞0
E[M L(Xi )(1 ‚àí M L(Xi ))(Di (1) ‚àí Di (0))]
‚àó

68

is invertible when
Ô£π
‚àó ‚àó
Ô£∫
1 ‚àí1Ô£ª
‚àó ‚àó

when Var(M L(Xi )|IiM L = 1) > 0. Therefore, when Var(M L(Xi )|IiM L = 1) > 0,
E[Zi Yi IiM L ] ‚àí E[M L(Xi )Yi IiM L ]
E[M L(Xi )(1 ‚àí M L(Xi ))(Di (1) ‚àí Di (0))]
E[Zi Y1i IiM L ] ‚àí E[M L(Xi )(Zi Y1i + (1 ‚àí Zi )Y0i )IiM L ]
E[M L(Xi )(1 ‚àí M L(Xi ))(Di (1) ‚àí Di (0))]
E[M L(Xi )Y1i IiM L ] ‚àí E[M L(Xi )(M L(Xi )Y1i + (1 ‚àí M L(Xi ))Y0i )IiM L ]
E[M L(Xi )(1 ‚àí M L(Xi ))(Di (1) ‚àí Di (0))]
E[M L(Xi )(1 ‚àí M L(Xi ))(Y1i ‚àí Y0i )IiM L ]
E[M L(Xi )(1 ‚àí M L(Xi ))(Di (1) ‚àí Di (0))]
E[M L(Xi )(1 ‚àí M L(Xi ))((Di (1) ‚àí Di (0))(Yi (1) ‚àí Yi (0))]
E[M L(Xi )(1 ‚àí M L(Xi ))(Di (1) ‚àí Di (0))]

p

Œ≤ÃÇ1c ‚àí‚Üí
=
=
=
=

= Œ≤1 ,
where the third line follows from Property 1, and the second last follows from the definitions of
Y1i and Y0i .
We next consider Œ≤ÃÇ1nc . By Lemma B.6,
Œ≤ÃÇ

nc

n
n
X
X
p
nc
nc 0
‚àí1
nc
nc 0 M L ‚àí1
ML
=(
Zi (Di ) Ii )
Znc
]) E[ZÃÉnc
]
i Yi Ii ‚àí‚Üí (E[ZÃÉi (DÃÉi ) Ii
i Yi Ii
i=1

i=1

nc 0 M L ] is invertible. After a few lines of algebra, we have
provided that E[ZÃÉnc
i (DÃÉi ) Ii
nc 0 M L
det(E[ZÃÉnc
]) = E[M L(Xi )2 IiM L ]E[Di (Zi ‚àí M L(Xi ))IiM L ]
i (DÃÉi ) Ii

= E[M L(Xi )2 IiM L ]E[M L(Xi )(1 ‚àí M L(Xi ))(Di (1) ‚àí Di (0))]
> 0.
Another few lines of algebra gives
nc 0 M L ‚àí1
(E[ZÃÉnc
])
i (DÃÉi ) Ii

"
#
1
1 ‚àí1
.
=
E[M L(Xi )(1 ‚àí M L(Xi ))(Di (1) ‚àí Di (0))] ‚àó ‚àó

Therefore,
p

Œ≤ÃÇ1nc ‚àí‚Üí

E[Zi Yi IiM L ] ‚àí E[M L(Xi )Yi IiM L ]
= Œ≤1 .
E[M L(Xi )(1 ‚àí M L(Xi ))(Di (1) ‚àí Di (0))]

Proof of Asymptotic Normality. Let (œÉÃÇ c )2 be the second diagonal element of
n
n
n
X
X
X
Œ£ÃÇc = (
Zi D0i Ii )‚àí1 (
ÀÜ2i Zi Z0i Ii )(
Di Z0i Ii )‚àí1
i=1

and

(œÉÃÇ nc )2

i=1

i=1

be the first diagonal element of
nc

Œ£ÃÇ

n
n
n
X
X
X
nc
nc 0
‚àí1
2
nc
nc 0
nc 0
‚àí1
=(
Zi,n (Di,n ) Ii ) (
ÀÜi,n Zi,n (Zi,n ) Ii )(
Dnc
i,n (Zi,n ) Ii ) .
i=1

i=1

i=1

69

d

We only show that (œÉÃÇ c )‚àí1 (Œ≤ÃÇ1c ‚àí Œ≤1 ) ‚àí‚Üí N (0, 1) when Var(M L(Xi )|IiM L = 1) > 0. We can show
d

that (œÉÃÇ nc )‚àí1 (Œ≤ÃÇ1nc ‚àí Œ≤1 ) ‚àí‚Üí N (0, 1) by an analogous argument. The proof proceeds in six steps.
Step C.6.1.1. Let Œ≤ÃÉn = (E[ZÃÉi DÃÉ0i Ii ])‚àí1 E[ZÃÉi Yi Ii ], and let Œ≤ÃÉ1,n denote the second element of Œ≤ÃÉn .
Then Œ≤ÃÉ1,n = Œ≤1 for any choice of Œ¥n > 0.
Proof. Note first that, for every Œ¥ > 0, pM L (x; Œ¥) ‚àà (0, 1) for almost every x ‚àà {x0 ‚àà X :
M L(x0 ) ‚àà (0, 1)}, since by almost everywhere continuity of M L, for almost every x ‚àà {x0 ‚àà X :
M L(x0 ) ‚àà (0, 1)}, there exists an open ball B ‚äÇ B(x, Œ¥) such that M L(x0 ) ‚àà (0, 1) for every
x0 ‚àà B. After a few lines of algebra, we have
det(E[ZÃÉi DÃÉ0i Ii ]) = Pr(Ii = 1)2 Var(M L(Xi )|Ii = 1)E[Di (Zi ‚àí M L(Xi ))Ii ]
= Pr(Ii = 1)2 Var(M L(Xi )|Ii = 1)E[M L(Xi )(1 ‚àí M L(Xi ))(Di (1) ‚àí Di (0))Ii ]
= Pr(Ii = 1)2 Var(M L(Xi )|Ii = 1)E[M L(Xi )(1 ‚àí M L(Xi ))(Di (1) ‚àí Di (0))],
where the last equality holds since pM L (x; Œ¥) ‚àà (0, 1) for almost every x ‚àà {x0 ‚àà X : M L(x0 ) ‚àà
(0, 1)}. By the law of total conditional variance,
Var(M L(Xi )|Ii = 1)
= E[Var(M L(Xi )|Ii = 1, IiM L )|Ii = 1] + Var(E[M L(Xi )|Ii = 1, IiM L ]|Ii = 1)
X
‚â•
Var(M L(Xi )|Ii = 1, IiM L = t) Pr(IiM L = t|Ii = 1)
t‚àà{0,1}

‚â• Var(M L(Xi )|Ii = 1, IiM L = 1) Pr(IiM L = 1|Ii = 1)
= Var(M L(Xi )|IiM L = 1) Pr(IiM L = 1|Ii = 1)
> 0.
Therefore, E[ZÃÉi DÃÉ0i Ii ] is invertible. Another few lines of algebra gives
(E[ZÃÉi DÃÉ0i Ii ])‚àí1

Ô£Æ
‚àó
1
Ô£Ø
=
Ô£∞0
E[M L(Xi )(1 ‚àí M L(Xi ))(Di (1) ‚àí Di (0))]
‚àó

Ô£π
‚àó ‚àó
Ô£∫
1 ‚àí1Ô£ª .
‚àó ‚àó

It follows that
E[Zi Yi Ii ] ‚àí E[M L(Xi )Yi Ii ]
E[M L(Xi )(1 ‚àí M L(Xi ))(Di (1) ‚àí Di (0))]
E[M L(Xi )(1 ‚àí M L(Xi ))(Di (1) ‚àí Di (0))(Yi (1) ‚àí Yi (0))Ii ]
=
E[M L(Xi )(1 ‚àí M L(Xi ))(Di (1) ‚àí Di (0))]
E[M L(Xi )(1 ‚àí M L(Xi ))(Di (1) ‚àí Di (0))(Yi (1) ‚àí Yi (0))]
=
E[M L(Xi )(1 ‚àí M L(Xi ))(Di (1) ‚àí Di (0))]

Œ≤ÃÉ1,n =

= Œ≤1 .

70

We can write
n
n
n
n
X
X
‚àö
1X
1X
c
0
‚àí1 1
0
‚àí1 1
n(Œ≤ÃÇ ‚àí Œ≤ÃÉn ) = (
Zi D i I i ) ‚àö
ZÃÉi DÃÉi Ii ) ‚àö
Zi Yi Ii ‚àí (
ZÃÉi Yi Ii
n
n
n
n
i=1
i=1
i=1
i=1
{z
}
|
=(A)

+(

1
n

n
X
i=1

1
ZÃÉi DÃÉ0i Ii )‚àí1 ‚àö
n

n
X

‚àö
ZÃÉi Yi Ii ‚àí (E[ZÃÉi DÃÉ0i Ii ])‚àí1 nE[ZÃÉi Yi Ii ] .

i=1

|

{z

=(B)

}

We first consider (B). Let Àúi,n = Yi ‚àí DÃÉ0i Œ≤ÃÉn so that
E[ZÃÉi Àúi,n Ii ] = E[ZÃÉi (Yi ‚àí DÃÉ0i Œ≤ÃÉn )Ii ] = E[ZÃÉi Yi Ii ] ‚àí E[ZÃÉi DÃÉ0i Ii ]Œ≤ÃÉn = 0.
Then
n

n

i=1

i=1

‚àö
1X
1 X
ZÃÉi DÃÉ0i Ii )‚àí1 ‚àö
ZÃÉi (DÃÉ0i Œ≤ÃÉn + Àúi,n )Ii ‚àí (E[ZÃÉi DÃÉ0i Ii ])‚àí1 nE[ZÃÉi (DÃÉ0i Œ≤ÃÉn + Àúi,n )Ii ]
(B) = (
n
n
n
n
X
‚àö
‚àö
1X
0
‚àí1 1
= n(Œ≤ÃÉn ‚àí Œ≤ÃÉn ) + (
ZÃÉi DÃÉi Ii ) ‚àö
ZÃÉi Àúi,n Ii ‚àí (E[ZÃÉi DÃÉ0i Ii ])‚àí1 nE[ZÃÉi Àúi,n Ii ]
n
n
i=1

i=1

n
n
X
1X
0
‚àí1 1
=(
ZÃÉi DÃÉi Ii ) ‚àö
ZÃÉi Àúi,n Ii .
n
n
i=1

i=1

Step C.6.1.2.

n

1 X
d
‚àö
ZÃÉi Àúi,n Ii ‚àí‚Üí N (0, E[Àú
2i ZÃÉi ZÃÉ0i IiM L ]).
n
i=1

Proof. We use the triangular-array Lyapunov CLT and the Cram√©r-Wold device. Pick a nonzero
Œª ‚àà Rp , and let Vi,n = ‚àö1n Œª0 ZÃÉi Àúi,n Ii . First, we have
n
X

2
E[Vi,n
] = Œª0 E[Àú
2i,n ZÃÉi ZÃÉ0i Ii ]Œª.

i=1

By Lemma B.6,
Œ≤ÃÉn ‚Üí (E[ZÃÉi DÃÉ0i IiM L ])‚àí1 E[ZÃÉi Yi IiM L ]
as n ‚Üí ‚àû. Let Œ≤ = (E[ZÃÉi DÃÉ0i IiM L ])‚àí1 E[ZÃÉi Yi IiM L ] and Àúi = Yi ‚àí DÃÉ0i Œ≤. We have
E[Àú
2i,n ZÃÉi ZÃÉ0i Ii ] = E[(Yi ‚àí DÃÉ0i Œ≤ÃÉn )2 ZÃÉi ZÃÉ0i Ii ]
= E[(Àú
i ‚àí DÃÉ0i (Œ≤ÃÉn ‚àí Œ≤))2 ZÃÉi ZÃÉ0i Ii ]
= E[Àú
2i ZÃÉi ZÃÉ0i Ii ] ‚àí 2E[Àú
i ((Œ≤ÃÉ0,n ‚àí Œ≤0 ) + Di (Œ≤ÃÉ1,n ‚àí Œ≤1 ) + M L(Xi )(Œ≤ÃÉ2,n ‚àí Œ≤2 ))ZÃÉi ZÃÉ0i Ii ]
+ E[((Œ≤ÃÉ0,n ‚àí Œ≤0 ) + Di (Œ≤ÃÉ1,n ‚àí Œ≤1 ) + M L(Xi )(Œ≤ÃÉ2,n ‚àí Œ≤2 ))2 ZÃÉi ZÃÉ0i Ii ]
‚Üí E[Àú
2i ZÃÉi ZÃÉ0i IiM L ]
71

as n ‚Üí ‚àû, where the convergence follows from Lemma B.6 and from the fact that Œ≤ÃÉn ‚Üí Œ≤.
Therefore,
n
X
2
E[Vi,n
] ‚Üí Œª0 E[Àú
2i ZÃÉi ZÃÉ0i IiM L ]Œª.
i=1

We next verify the Lyapunov condition: for some t > 0,
n
X

E[|Vi,n |2+t ] ‚Üí 0.

i=1

We have
n
X

E[|Vi,n |4 ] =

i=1

1
E[|Œª0 ZÃÉi Àúi,n Ii |4 ].
n

We use the cr -inequality: E[|X + Y |r ] ‚â§ 2r‚àí1 E[|X|r + |Y |r ] for r ‚â• 1. Repeating using the
cr -inequality gives
E[|Œª0 ZÃÉi Àúi,n Ii |4 ] = E[|Œª0 ZÃÉi (Yi ‚àí Œ≤ÃÉ0,n ‚àí Œ≤ÃÉ1,n Di ‚àí Œ≤ÃÉ2,n M L(Xi ))|4 Ii ]
‚â§ 23c E[(|Œª0 ZÃÉi |4 )(|Yi |4 + |Œ≤ÃÉ0,n |4 + |Œ≤ÃÉ1,n |4 Di + |Œ≤ÃÉ2,n |4 M L(Xi )4 )Ii ]
4
4
4
‚â§ 23c (Œª1 + Œª2 + Œª3 )4 (E[Yi4 ] + Œ≤ÃÉ0,n
+ Œ≤ÃÉ1,n
+ Œ≤ÃÉ2,n
)

for some finite constant c, and the right-hand side converges to
23c (Œª1 + Œª2 + Œª3 )4 (E[Yi4 ] + Œ≤ÃÉ04 + Œ≤ÃÉ14 + Œ≤ÃÉ24 ),
which is finite under Assumption 3 (a). Therefore,
n
X

E[|Vi,n |4 ] ‚Üí 0,

i=1

and the conclusion follows from the Lyapunov CLT and the Cram√©r-Wold device.
We next consider (A). We can write
n

n

i=1

i=1

1X
1 X
Zi D0i Ii )‚àí1 ‚àö
(Zi Yi Ii ‚àí ZÃÉi Yi Ii )
(A) = (
n
n
n
n
n
n
X
X
1X
1X
0
‚àí1 1
0
‚àí1 1
0
0
‚àí(
Zi Di Ii ) [ ‚àö
(Zi Di Ii ‚àí ZÃÉi DÃÉi Ii )](
ZÃÉi DÃÉi Ii )
ZÃÉi Yi Ii .
n
n
n
n
i=1

i=1

i=1

i=1

Step C.6.1.3. Let {Vi }‚àû
i=1 be i.i.d. random variables such that E[|Vi |] < ‚àû and that E[Vi |Xi ]
‚àó
0
is bounded on N (D , Œ¥ ) ‚à© X for some Œ¥ 0 > 0. Then,
E[Vi pM L (Xi ; Œ¥)l (pM L (Xi ; Œ¥) ‚àí M L(Xi ))1{pM L (Xi ; Œ¥) ‚àà (0, 1)}] = O(Œ¥)
for l = 0, 1.
72

Proof. For every x ‚àà
/ N (D‚àó , Œ¥), B(x, Œ¥)‚à©D‚àó = ‚àÖ, so M L is continuously differentiable on B(x, Œ¥).
By the mean value theorem, for every x ‚àà
/ N (D‚àó , Œ¥) and a ‚àà B(0, Œ¥),
M L(x + a) = M L(x) + ‚àáM L(y(x, a))0 a
for some point y(x, a) on the line segment connecting x and x + a. For every x ‚àà
/ N (D‚àó , Œ¥),
R
B(0,1) M L(x + Œ¥u)du
R
pM L (x; Œ¥) =
B(0,1) du
R
0
B(0,1) (M L(x) + Œ¥‚àáM L(y(x, Œ¥u)) u)du
R
=
B(0,1) du
R
0
B(0,1) ‚àáM L(y(x, Œ¥u)) udu
R
= M L(x) + Œ¥
.
B(0,1) du
Now, we can write
E[Vi pM L (Xi ; Œ¥)l (pM L (Xi ; Œ¥) ‚àí M L(Xi ))1{pM L (Xi ; Œ¥) ‚àà (0, 1)}]
= E[Vi pM L (Xi ; Œ¥)l (pM L (Xi ; Œ¥) ‚àí M L(Xi ))1{pM L (Xi ; Œ¥) ‚àà (0, 1)}1{Xi ‚àà
/ N (D‚àó , Œ¥)}]
+ E[Vi pM L (Xi ; Œ¥)l (pM L (Xi ; Œ¥) ‚àí M L(Xi ))1{pM L (Xi ; Œ¥) ‚àà (0, 1)}1{Xi ‚àà N (D‚àó , Œ¥)}].
For the first term,
|E[Vi pM L (Xi ; Œ¥)l (pM L (Xi ; Œ¥) ‚àí M L(Xi ))1{pM L (Xi ; Œ¥) ‚àà (0, 1)}1{Xi ‚àà
/ N (D‚àó , Œ¥)}]|
R
0
B(0,1) ‚àáM L(y(Xi , Œ¥u)) udu
R
1{pM L (Xi ; Œ¥) ‚àà (0, 1)}1{Xi ‚àà
/ N (D‚àó , Œ¥)}]|
= Œ¥|E[Vi pM L (Xi ; Œ¥)l
du
B(0,1)
R
Pp
‚àÇM L(y(Xi ,Œ¥u))
||uk |du
k=1 |
‚àÇxk
ML
l B(0,1)
R
‚â§ Œ¥E[|Vi |p (Xi ; Œ¥)
1{pM L (Xi ; Œ¥) ‚àà (0, 1)}1{Xi ‚àà
/ N (D‚àó , Œ¥)}]
du
B(0,1)
R
p
X
‚àÇM L(x) B(0,1) |uk |du
R
‚â§ Œ¥E[|Vi |]
sup
‚àÇxk
x‚ààC ‚àó
B(0,1) du
k=1

= O(Œ¥),
where we use the assumption that the partial derivatives of M L is bounded on C ‚àó . For the
second term, for sufficiently small Œ¥ > 0,
|E[Vi pM L (Xi ; Œ¥)l (pM L (Xi ; Œ¥) ‚àí M L(Xi ))1{pM L (Xi ; Œ¥) ‚àà (0, 1)}1{Xi ‚àà N (D‚àó , Œ¥)}]|
‚â§ E[|E[Vi |Xi ]|1{Xi ‚àà N (D‚àó , Œ¥)}]
‚â§ CE[1{Xi ‚àà N (D‚àó , Œ¥)}]
= C Pr(Xi ‚àà N (D‚àó , Œ¥))
= O(Œ¥),
where C is some constant, the second inequality follows from the assumption that E[Vi |Xi ] is
bounded on N (D‚àó , Œ¥ 0 ) ‚à© X for some Œ¥ 0 > 0, and the last equality follows from Assumption 4
(a).
73

Step C.6.1.4.

‚àö1
n

Pn

i=1 (Zi Yi Ii

‚àí ZÃÉi Yi Ii ) = op (1) and

‚àö1
n

Pn

0
i=1 (Zi Di Ii

‚àí ZÃÉi DÃÉ0i Ii ) = op (1).

P
Proof. We only show that ‚àö1n ni=1 (pM L (Xi ; Œ¥n )2 ‚àí M L(Xi )2 )Ii = op (1). The proofs for the
other elements are similar. As for bias,
n

1 X ML
E[ ‚àö
(p (Xi ; Œ¥n )2 ‚àí M L(Xi )2 )Ii ]
n
i=1
‚àö
= nE[(pM L (Xi ; Œ¥n )2 ‚àí M L(Xi )2 )Ii ]
‚àö
= nE[(pM L (Xi ; Œ¥n ) + M L(Xi ))(pM L (Xi ; Œ¥n ) ‚àí M L(Xi ))Ii ]
‚àö
= nO(Œ¥n )
= 0,
where the third equality follows from Step C.6.1.3 and the last from the assumption that nŒ¥n2 ‚Üí 0.
As for variance, by Lemma B.6,
n

1 X ML
Var( ‚àö
(p (Xi ; Œ¥n )2 ‚àí M L(Xi )2 )Ii )
n
i=1

ML

‚â§ E[(p

(Xi ; Œ¥n )2 ‚àí M L(Xi )2 )2 Ii ]

= E[(pM L (Xi ; Œ¥n )4 ‚àí 2pM L (Xi ; Œ¥n )2 M L(Xi )2 + M L(Xi )4 )Ii ]
‚Üí E[(M L(Xi )4 ‚àí 2M L(Xi )2 M L(Xi )2 + M L(Xi )4 )IiM L ]
= 0.

p

Step C.6.1.5. nŒ£ÃÇc ‚àí‚Üí (E[ZÃÉi DÃÉ0i IiM L ])‚àí1 E[Àú
2i ZÃÉi ZÃÉ0i IiM L ](E[DÃÉi ZÃÉ0i IiM L ])‚àí1 .
Proof. Let i = Yi ‚àí D0i Œ≤. We have
n

n

1X 2
1X
ÀÜi Zi Z0i Ii =
(Yi ‚àí D0i Œ≤ÃÇ c )2 Zi Z0i Ii
n
n
i=1

i=1

n
1X
(i ‚àí D0i (Œ≤ÃÇ c ‚àí Œ≤))2 Zi Z0i Ii
=
n
i=1

n
1X 2
i Zi Z0i Ii
=
n
i=1

n

2X
‚àí
(Yi ‚àí D0i Œ≤)((Œ≤ÃÇ0c ‚àí Œ≤0 ) + Di (Œ≤ÃÇ1c ‚àí Œ≤1 ) + pM L (Xi ; Œ¥n )(Œ≤ÃÇ2c ‚àí Œ≤2 ))Zi Z0i Ii
n
i=1

n
1X c
+
((Œ≤ÃÇ0 ‚àí Œ≤0 ) + Di (Œ≤ÃÇ1c ‚àí Œ≤1 ) + pM L (Xi ; Œ¥n )(Œ≤ÃÇ2c ‚àí Œ≤2 ))2 Zi Z0i Ii
n
i=1

n
1X 2
=
i Zi Z0i Ii + op (1)Op (1),
n
i=1

74

where the last equality follows from the result that Œ≤ÃÇ c ‚àí Œ≤ = op (1) and from Lemma B.6. Again
by Lemma B.6,
n

n

1X 2
1X 2
i Zi Z0i Ii =
(Yi ‚àí 2Yi D0i Œ≤ + Œ≤ 0 Di D0i Œ≤)Zi Z0i Ii
n
n
i=1

i=1

p

‚àí‚Üí E[(Yi2 ‚àí 2Yi DÃÉ0i Œ≤ + Œ≤ 0 DÃÉi DÃÉ0i Œ≤)ZÃÉi ZÃÉ0i IiM L ]
= E[Àú
2i ZÃÉi ZÃÉ0i IiM L ],
and

n

1X
p
Zi D0i Ii ‚àí‚Üí E[ZÃÉi DÃÉ0i IiM L ].
n
i=1

The conclusion then follows.
d

Step C.6.1.6. (œÉÃÇ c )‚àí1 (Œ≤ÃÇ1c ‚àí Œ≤1 ) ‚àí‚Üí N (0, 1).
Proof. By combining the results from Steps C.6.1.2‚ÄìC.6.1.4 and by Lemma B.6,
p

(A) ‚àí‚Üí 0,
d

(B) ‚àí‚Üí N (0, (E[ZÃÉi DÃÉ0i IiM L ])‚àí1 E[Àú
2i ZÃÉi ZÃÉ0i IiM L ](E[DÃÉi ZÃÉ0i IiM L ])‚àí1 ),
and therefore,
‚àö

d

n(Œ≤ÃÇ c ‚àí Œ≤ÃÉn ) ‚àí‚Üí N (0, (E[ZÃÉi DÃÉ0i IiM L ])‚àí1 E[Àú
2i ZÃÉi ZÃÉ0i IiM L ](E[DÃÉi ZÃÉ0i IiM L ])‚àí1 ).

The conclusion then follows from Steps C.6.1.1 and C.6.1.5.

C.6.2

Consistency and Asymptotic Normality of Œ≤ÃÇ1s When Pr(M L(Xi ) ‚àà (0, 1)) > 0

Let Iis = 1{ps (Xi ; Œ¥n ) ‚àà (0, 1)}, Dsi = (1, Di , ps (Xi ; Œ¥n ))0 and Zsi = (1, Zi , ps (Xi ; Œ¥n ))0 . Let
Œ≤ÃÇ

c,s

n
n
X
X
s
s 0 s ‚àí1
=(
Zi (Di ) Ii )
Zsi Yi Iis
i=1

and
c,s

Œ£ÃÇ

i=1

n
n
n
X
X
X
s
s 0 s ‚àí1
s 2 s
s 0 s
=(
Zi (Di ) Ii ) ( (ÀÜ
i ) Zi (Zi ) Ii )(
Dsi (Zsi )0 Iis )‚àí1 ,
i=1

i=1

i=1
p

where ÀÜsi = Yi ‚àí (Dsi )0 Œ≤ÃÇ c,s . Here, we only show that Œ≤ÃÇ1c,s ‚àí‚Üí Œ≤1 if Sn ‚Üí ‚àû and that (œÉÃÇ s )‚àí1 (Œ≤ÃÇ1c,s ‚àí
d

Œ≤1 ) ‚àí‚Üí N (0, 1) if Assumption 5 holds when Var(M L(Xi )|IiM L = 1) > 0. For that, it suffices to
show that
Œ≤ÃÇ c,s ‚àí Œ≤ÃÇ c = op (1)
75

if Sn ‚Üí ‚àû and that
‚àö

n(Œ≤ÃÇ c,s ‚àí Œ≤ÃÇ c ) = op (1),
p

nŒ£ÃÇc,s ‚àí‚Üí (E[ZÃÉi DÃÉ0i IiM L ])‚àí1 E[Àú
2i ZÃÉi ZÃÉ0i IiM L ](E[DÃÉi ZÃÉ0i IiM L ])‚àí1
if Assumption 5 holds. We have
n

Œ≤ÃÇ c,s ‚àí Œ≤ÃÇ c = (
=(

n

n

n

i=1
n
X

i=1

1 X s s 0 s ‚àí1 1 X s s
1X
1X
Zi (Di ) Ii )
Zi Yi Ii ‚àí (
Zi D0i Ii )‚àí1
Zi Yi Ii
n
n
n
n
i=1
n
X

1
n

Zsi (Dsi )0 Iis )‚àí1 (

i=1

i=1
n
X

1
n

i=1

n

‚àí(

Zsi Yi Iis ‚àí

1
n

Zi Yi Ii )

i=1

n

n

n

n

i=1

i=1

1 X s s 0 s ‚àí1 1 X s s 0 s 1 X
1X
1X
Zi (Di ) Ii ) (
Zi (Di ) Ii ‚àí
Zi D0i Ii )(
Zi D0i Ii )‚àí1
Zi Yi Ii .
n
n
n
n
n
i=1

i=1

i=1

‚àö

= op (1) under the boundedness
By Lemma B.8,
= op (1) if Sn ‚Üí ‚àû, and
imposed by Assumption 4 (c) if Assumption 5 holds.
By proceeding as in Step C.6.1.5 in Section C.6.1, we have
Œ≤ÃÇ c,s

‚àí Œ≤ÃÇ c

n

n

i=1

i=1

n(Œ≤ÃÇ c,s

‚àí Œ≤ÃÇ c )

1X s 2 s s 0 s
1X s 2 s s 0 s
(ÀÜ
i ) Zi (Zi ) Ii =
(i ) Zi (Zi ) Ii + op (1),
n
n
where

si

= Yi ‚àí

(Dsi )0 Œ≤.

Then, by Lemma B.8,

n

n

i=1

i=1

1X s 2 s s 0 s 1X 2
(ÀÜ
i ) Zi (Zi ) Ii ‚àí
i Zi Z0i Ii
n
n
n
n
1X 2
1X 2
s 0
0 s
s 0
s
s 0 s
=
(Yi ‚àí 2Yi (Di ) Œ≤ + Œ≤ Di (Di ) Œ≤)Zi (Zi ) Ii ‚àí
(Yi ‚àí 2Yi D0i Œ≤ + Œ≤ 0 Di D0i Œ≤)Zi Z0i Ii + op (1)
n
n
i=1

i=1

= op (1)
so that

n

1X s 2 s s 0 s p
(ÀÜ
i ) Zi (Zi ) Ii ‚àí‚Üí E[Àú
2i ZÃÉi ZÃÉ0i IiM L ].
n
i=1

Also,

1
n

C.6.3

Pn

s
s 0 s
i=1 Zi (Di ) Ii

p

‚àí‚Üí E[ZÃÉi DÃÉ0i IiM L ] by using Lemma B.8. The conclusion then follows.

Consistency and Asymptotic Normality of Œ≤ÃÇ1 When Pr(M L(Xi ) ‚àà (0, 1)) = 0

Since Pr(M L(Xi ) ‚àà (0, 1)) = 0, In = 0 with probability one. Hence,
n
n
X
X
0
‚àí1
Œ≤ÃÇ = (
Zi Di Ii )
Zi Yi Ii
i=1

i=1

with probability one. We use the notation and results provided in Appendix B. By Lemma B.5,
under Assumption 3 (e), there exists ¬µ > 0 such that ds‚Ñ¶‚àó is twice continuously differentiable on
N (‚àÇ‚Ñ¶‚àó , ¬µ) and that
Z
Z Œ¥Z
‚àÇ‚Ñ¶‚àó
g(x)dx =
g(u + ŒªŒΩ‚Ñ¶‚àó (u))Jp‚àí1
œà‚Ñ¶‚àó (u, Œª)dHp‚àí1 (u)dŒª
N (‚àÇ‚Ñ¶‚àó ,Œ¥)

‚àíŒ¥

‚àÇ‚Ñ¶‚àó

76

for every Œ¥ ‚àà (0, ¬µ) and every function g : Rp ‚Üí R that is integrable on N (‚àÇ‚Ñ¶‚àó , Œ¥).
p

d

Below we show that Œ≤ÃÇ1 ‚àí‚Üí Œ≤1 if nŒ¥n ‚Üí ‚àû and Œ¥n ‚Üí 0 and that œÉÃÇ ‚àí1 (Œ≤ÃÇ1 ‚àí Œ≤1 ) ‚àí‚Üí N (0, 1) if
3
nŒ¥n ‚Üí 0 in addition. The proof proceeds in eight steps.
Step C.6.3.1. There exist Œ¥ÃÑ > 0 and a bounded function r : ‚àÇ‚Ñ¶‚àó ‚à© N (X , Œ¥ÃÑ) √ó (‚àí1, 1) √ó (0, Œ¥ÃÑ) ‚Üí R
such that
pM L (u + Œ¥vŒΩ‚Ñ¶‚àó (u); Œ¥) = k(v) + Œ¥r(u, v, Œ¥)
for every (u, v, Œ¥) ‚àà ‚àÇ‚Ñ¶‚àó ‚à© N (X , Œ¥ÃÑ) √ó (‚àí1, 1) √ó (0, Œ¥ÃÑ), where
k(v) =

(
1
1 ‚àí 21 I(1‚àív2 ) ( p+1
2 , 2)
p+1 1
1
2 I(1‚àív 2 ) ( 2 , 2 )

for v ‚àà [0, 1)
for v ‚àà (‚àí1, 0).

Here Ix (Œ±, Œ≤) is the regularized incomplete beta function (the cumulative distribution function of
the beta distribution with shape parameters Œ± and Œ≤).
Proof. By Assumption 3 (f) (ii), there exists Œ¥ÃÑ ‚àà (0, ¬µ2 ) such that M L(x) = 0 for almost every
x ‚àà N (X , 3Œ¥ÃÑ) \ ‚Ñ¶‚àó . By Taylor‚Äôs theorem, for every u ‚àà ‚àÇ‚Ñ¶‚àó ‚à© N (X , Œ¥ÃÑ) and a ‚àà B(0, 2Œ¥ÃÑ),
ds‚Ñ¶‚àó (u + a) = ds‚Ñ¶‚àó (u) + ‚àáds‚Ñ¶‚àó (u)0 a + a0 R(u, a)a,
where

Z
R(u, a) =

1

(1 ‚àí t)D2 ds‚Ñ¶‚àó (u + ta)dt.

0

Since D2 ds‚Ñ¶‚àó is
cl(N (‚àÇ‚Ñ¶‚àó , 2Œ¥ÃÑ)).

continuous and cl(N (‚àÇ‚Ñ¶‚àó , 2Œ¥ÃÑ)) is bounded and closed, D2 ds‚Ñ¶‚àó is bounded on
Therefore, R(¬∑, ¬∑) is bounded on ‚àÇ‚Ñ¶‚àó ‚à© N (X , Œ¥ÃÑ) √ó B(0, 2Œ¥ÃÑ). It also follows that
ds‚Ñ¶‚àó (u + a) = ŒΩ‚Ñ¶‚àó (u)0 a + a0 R(u, a)a,

since ds‚Ñ¶‚àó (u) = 0 and ‚àáds‚Ñ¶‚àó (u) = ŒΩ‚Ñ¶‚àó (u) for every u ‚àà ‚àÇ‚Ñ¶‚àó ‚à© N (X , 2Œ¥ÃÑ) by Lemma B.1. For
(u, v, Œ¥) ‚àà ‚àÇ‚Ñ¶‚àó ‚à© N (X , Œ¥ÃÑ) √ó (‚àí1, 1) √ó (0, Œ¥ÃÑ),
pM L (u + Œ¥vŒΩ‚Ñ¶‚àó (u); Œ¥)
R
‚àó
B(0,1) M L(u + Œ¥vŒΩ‚Ñ¶ (u) + Œ¥w)dw
R
=
B(0,1) dw
R
‚àó
‚àó
B(0,1) 1{u + Œ¥vŒΩ‚Ñ¶ (u) + Œ¥w ‚àà ‚Ñ¶ }dw
=
Volp
R
s
‚àó
B(0,1) 1{d‚Ñ¶‚àó (u + Œ¥(vŒΩ‚Ñ¶ (u) + w)) ‚â• 0)}dw
=
Volp
R
0
2
0
‚àó
‚àó
‚àó
‚àó
‚àó
B(0,1) 1{Œ¥ŒΩ‚Ñ¶ (u) (vŒΩ‚Ñ¶ (u) + w) + Œ¥ (vŒΩ‚Ñ¶ (u) + w) R(u, Œ¥(vŒΩ‚Ñ¶ (u) + w))(vŒΩ‚Ñ¶ (u) + w) ‚â• 0}dw
=
,
Volp
where Volp denotes the volume of the p-dimensional unit ball, and the second equality follows
since u + Œ¥vŒΩ‚Ñ¶‚àó (u) + Œ¥w ‚àà N (X , 3Œ¥ÃÑ) and hence M L(u + Œ¥vŒΩ‚Ñ¶‚àó (u) + Œ¥w) = 0 for almost every
77

w ‚àà B(0, 1) such that u + Œ¥vŒΩ‚Ñ¶‚àó (u) + Œ¥w ‚àà
/ ‚Ñ¶‚àó . Observe that
1{Œ¥ŒΩ‚Ñ¶‚àó (u)0 (vŒΩ‚Ñ¶‚àó (u) + w) + Œ¥ 2 (vŒΩ‚Ñ¶‚àó (u) + w)0 R(u, Œ¥(vŒΩ‚Ñ¶‚àó (u) + w))(vŒΩ‚Ñ¶‚àó (u) + w) ‚â• 0}
= 1{v + ŒΩ‚Ñ¶‚àó (u) ¬∑ w + Œ¥(vŒΩ‚Ñ¶‚àó (u) + w)0 R(u, Œ¥(vŒΩ‚Ñ¶‚àó (u) + w))(vŒΩ‚Ñ¶‚àó (u) + w) ‚â• 0}
= 1{v + ŒΩ‚Ñ¶‚àó (u) ¬∑ w ‚â• 0}
‚àí 1{v + ŒΩ‚Ñ¶‚àó (u) ¬∑ w ‚â• 0, v + ŒΩ‚Ñ¶‚àó (u) ¬∑ w + Œ¥(vŒΩ‚Ñ¶‚àó (u) + w)0 R(u, Œ¥(vŒΩ‚Ñ¶‚àó (u) + w))(vŒΩ‚Ñ¶‚àó (u) + w) < 0}
{z
}
|
=a(u,v,w,Œ¥)

+ 1{v + ŒΩ‚Ñ¶‚àó (u) ¬∑ w < 0, v + ŒΩ‚Ñ¶‚àó (u) ¬∑ w + Œ¥(vŒΩ‚Ñ¶‚àó (u) + w)0 R(u, Œ¥(vŒΩ‚Ñ¶‚àó (u) + w))(vŒΩ‚Ñ¶‚àó (u) + w) ‚â• 0} .
|
{z
}
=b(u,v,w,Œ¥)

Note that the set {w ‚àà B(0, 1) : v + ŒΩ(u) ¬∑ w ‚â• 0} is a region of the p-dimensional unit ball cut
off by the plane {w ‚àà Rp : v + ŒΩ(u) ¬∑ w = 0}. The distance from the center of the unit ball to
the plane is |v|. Using the formula for the volume of a hyperspherical cap (see e.g. Li (2011)),
we have
(
Z
1
Volp ‚àí 21 Volp I(2(1‚àív)‚àí(1‚àív)2 ) ( p+1
for v ‚àà [0, 1)
2 , 2)
1{v + ŒΩ(u) ¬∑ w ‚â• 0}dw =
p+1 1
1
for v ‚àà (‚àí1, 0).
B(0,1)
2 Volp I(2(1+v)‚àí(1+v)2 ) ( 2 , 2 )
Therefore, for every (u, v, Œ¥) ‚àà ‚àÇ‚Ñ¶‚àó ‚à© N (X , Œ¥ÃÑ) √ó (‚àí1, 1) √ó (0, Œ¥ÃÑ),
R
B(0,1) (‚àía(u, v, w, Œ¥) + b(u, v, w, Œ¥))dw
ML
p (u + Œ¥vŒΩ‚Ñ¶‚àó (u); Œ¥) = k(v) +
.
Volp
Now let r(u, v, Œ¥) = Œ¥ ‚àí1 (pM L (u + Œ¥vŒΩ‚Ñ¶‚àó (u); Œ¥) ‚àí k(v)). Since R(¬∑, ¬∑) is bounded on ‚àÇ‚Ñ¶‚àó ‚à©
N (X , Œ¥ÃÑ) √ó B(0, 2Œ¥ÃÑ) and kŒΩ‚Ñ¶‚àó (u)k = 1, there exists rÃÑ > 0 such that
|(vŒΩ‚Ñ¶‚àó (u) + w)0 R(u, Œ¥(vŒΩ‚Ñ¶‚àó (u) + w))(vŒΩ‚Ñ¶‚àó (u) + w)| ‚â§ rÃÑ
for every (u, v, w, Œ¥) ‚àà ‚àÇ‚Ñ¶‚àó ‚à© N (X , Œ¥ÃÑ) √ó (‚àí1, 1) √ó B(0, 1) √ó (0, Œ¥ÃÑ). Therefore,
0 ‚â§ a(u, v, w, Œ¥) ‚â§ 1{0 ‚â§ v + ŒΩ‚Ñ¶‚àó (u) ¬∑ w < Œ¥rÃÑ}
and
0 ‚â§ b(u, v, w, Œ¥) ‚â§ 1{‚àíŒ¥rÃÑ ‚â§ v + ŒΩ‚Ñ¶‚àó (u) ¬∑ w < 0}.
It then follows that
R
R
‚àó
B(0,1) (‚àía(u, v, w, Œ¥) + b(u, v, w, Œ¥))dw
B(0,1) 1{0 ‚â§ v + ŒΩ‚Ñ¶ (u) ¬∑ w < Œ¥rÃÑ}dw
‚â§
‚àí
Volp
Volp
R
‚àó
B(0,1) 1{‚àíŒ¥rÃÑ ‚â§ v + ŒΩ‚Ñ¶ (u) ¬∑ w < 0}dw
‚â§
.
Volp
The set {w ‚àà B(0, 1) : 0 ‚â§ v + ŒΩ‚Ñ¶‚àó (u) ¬∑ w < Œ¥rÃÑ} is a region of the p-dimensional unit ball cut
off by the two planes {w ‚àà Rp : v + ŒΩ‚Ñ¶‚àó (u) ¬∑ w = 0} and {w ‚àà Rp : v + ŒΩ‚Ñ¶‚àó (u) ¬∑ w = Œ¥rÃÑ}. Its

78

Lebesgue measure is at most the volume of the (p ‚àí 1)-dimensional unit ball times the distance
between the two planes, so
Z
1{0 ‚â§ v + ŒΩ‚Ñ¶‚àó (u) ¬∑ w < Œ¥rÃÑ}dw.
‚àíŒ¥Volp‚àí1 rÃÑ ‚â§ ‚àí
B(0,1)

Likewise,

Z
1{‚àíŒ¥rÃÑ ‚â§ v + ŒΩ‚Ñ¶‚àó (u) ¬∑ w < 0}dw ‚â§ Œ¥Volp‚àí1 rÃÑ.
B(0,1)

Therefore,
Œ¥Volp‚àí1 rÃÑ
‚àí
‚â§
Volp

R

B(0,1) (‚àía(u, v, w, Œ¥)

+ b(u, v, w, Œ¥))dw

Volp

‚â§

Œ¥Volp‚àí1 rÃÑ
.
Volp

It follows that
R
r(u, v, Œ¥) = Œ¥ ‚àí1
‚àà [‚àí

B(0,1) (‚àía(u, v, w, Œ¥)

+ b(u, v, w, Œ¥))dw

Volp

Volp‚àí1 rÃÑ Volp‚àí1 rÃÑ
,
],
Volp
Volp

and hence r is bounded on ‚àÇ‚Ñ¶‚àó ‚à© N (X , Œ¥ÃÑ) √ó (‚àí1, 1) √ó (0, Œ¥ÃÑ).
Step C.6.3.2. For every (u, v, Œ¥) ‚àà ‚àÇ‚Ñ¶‚àó ‚à©N (X , Œ¥ÃÑ)√ó(‚àí1, 1)√ó(0, Œ¥ÃÑ), pM L (u+Œ¥vŒΩ‚Ñ¶‚àó (u); Œ¥) ‚àà (0, 1).
Proof. Fix (u, v, Œ¥) ‚àà ‚àÇ‚Ñ¶‚àó ‚à© N (X , Œ¥ÃÑ) √ó (‚àí1, 1) √ó (0, Œ¥ÃÑ). Suppose v = 0. By Step C.6.3.1,
pM L (u) = limŒ¥0 ‚Üí0 pM L (u; Œ¥ 0 ) = k(0) = 21 . This implies that there exists Œ¥ 0 ‚àà (0, Œ¥) such that
pM L (u; Œ¥ 0 ) ‚àà (0, 1). It then follows that 0 < Lp (B(u, Œ¥ 0 ) ‚à© ‚Ñ¶‚àó ) ‚â§ Lp (B(x, Œ¥) ‚à© ‚Ñ¶‚àó ) and that
p
‚àó)
0 < Lp (B(x, Œ¥ 0 ) \ ‚Ñ¶‚àó ) ‚â§ Lp (B(x, Œ¥) \ ‚Ñ¶‚àó ). Therefore, pM L (u; Œ¥) = L L(B(u,Œ¥)‚à©‚Ñ¶
‚àà (0, 1).
p (B(u,Œ¥))
Suppose v 6= 0 and let  ‚àà (0, Œ¥(1 ‚àí |v|)). Note that B(u, ) ‚äÇ B(u + Œ¥vŒΩ‚Ñ¶‚àó (u), Œ¥), since for
any x ‚àà B(u, ), ku + Œ¥vŒΩ‚Ñ¶‚àó (u) ‚àí xk ‚â§ kŒ¥vŒΩ‚Ñ¶‚àó (u)k + ku ‚àí xk ‚â§ Œ¥|v| +  < Œ¥. Since pM L (u) = 12 ,
there exists 0 ‚àà (0, ) such that pM L (u; 0 ) ‚àà (0, 1). It then follows that 0 < Lp (B(u, 0 ) ‚à© ‚Ñ¶‚àó ) ‚â§
Lp (B(u, )‚à©‚Ñ¶‚àó ) ‚â§ Lp (B(u+Œ¥vŒΩ‚Ñ¶‚àó (u), Œ¥)‚à©‚Ñ¶‚àó ) and that 0 < Lp (B(x, 0 )\‚Ñ¶‚àó ) ‚â§ Lp (B(x, )\‚Ñ¶‚àó ) ‚â§
p
‚àó
‚Ñ¶‚àó (u),Œ¥)‚à©‚Ñ¶ )
Lp (B(u+Œ¥vŒΩ‚Ñ¶‚àó (u), Œ¥)\‚Ñ¶‚àó ). Therefore, pM L (u+Œ¥vŒΩ‚Ñ¶‚àó (u); Œ¥) = L L(B(u+Œ¥vŒΩ
‚àà (0, 1).
p (B(u+Œ¥vŒΩ ‚àó (u),Œ¥))
‚Ñ¶

N (‚àÇ‚Ñ¶‚àó , Œ¥ 0 )

Rp

Step C.6.3.3. Let g :
‚Üí R be a function that is bounded on
0
Œ¥ > 0. Then, for l ‚â• 0, there exist Œ¥ÃÉ > 0 and constant C > 0 such that

‚à© N (X , Œ¥ 0 ) for some

|Œ¥ ‚àí1 E[pM L (Xi ; Œ¥)l g(Xi )1{pM L (Xi ; Œ¥) ‚àà (0, 1)}]| ‚â§ C
for every Œ¥ ‚àà (0, Œ¥ÃÉ). If g is continuous on N (‚àÇ‚Ñ¶‚àó , Œ¥ 0 ) ‚à© N (X , Œ¥ 0 ) for some Œ¥ 0 > 0, then
Œ¥
Œ¥

‚àí1

‚àí1

E[p

E[Zi p

ML

ML

l

(Xi ; Œ¥) g(Xi )1{p
l

(Xi ; Œ¥) g(Xi )1{p

ML

ML

Z

1

(Xi ; Œ¥) ‚àà (0, 1)}] =

Z

k(v) dv
‚àí1
Z 1

(Xi ; Œ¥) ‚àà (0, 1)}] =

‚àÇ‚Ñ¶‚àó
l

Z

k(v) dv
0

79

l

‚àÇ‚Ñ¶‚àó

g(x)fX (x)dHp‚àí1 (x) + o(1)
g(x)fX (x)dHp‚àí1 (x) + o(1)

for l ‚â• 0. Furthermore, if g is continuously differentiable and ‚àág is bounded on N (‚àÇ‚Ñ¶‚àó , Œ¥ 0 ) ‚à©
N (X , Œ¥ 0 ) for some Œ¥ 0 > 0, then
Z 1
Z
‚àí1
ML
l
ML
l
Œ¥ E[p (Xi ; Œ¥) g(Xi )1{p (Xi ; Œ¥) ‚àà (0, 1)}] =
k(v) dv
g(x)fX (x)dHp‚àí1 (x) + O(Œ¥)
Œ¥ ‚àí1 E[Zi pM L (Xi ; Œ¥)l g(Xi )1{pM L (Xi ; Œ¥) ‚àà (0, 1)}] =

‚àÇ‚Ñ¶‚àó

‚àí1
1

Z

k(v)l dv

Z
‚àÇ‚Ñ¶‚àó

0

g(x)fX (x)dHp‚àí1 (x) + O(Œ¥)

for l ‚â• 0.
Proof. Let Œ¥ÃÑ be given in Step C.6.3.1. Under Assumption 3 (g), there exists Œ¥ÃÉ ‚àà (0, Œ¥ÃÑ) such that
fX is bounded, is continuously differentiable, and has bounded partial derivatives on N (‚àÇ‚Ñ¶‚àó , 2Œ¥ÃÉ)‚à©
N (X , 2Œ¥ÃÉ). Let Œ¥ÃÉ ‚àà (0, Œ¥ÃÑ) be such that both g and fX are bounded on N (‚àÇ‚Ñ¶‚àó , 2Œ¥ÃÉ) ‚à© N (X , 2Œ¥ÃÉ).
We first show that pM L (x; Œ¥) ‚àà {0, 1} for every x ‚àà X \ N (‚àÇ‚Ñ¶‚àó , Œ¥) for every Œ¥ ‚àà (0, Œ¥ÃÉ). Pick
x ‚àà X \ N (‚àÇ‚Ñ¶‚àó , Œ¥) and Œ¥ ‚àà (0, Œ¥ÃÉ). Since B(x, Œ¥) ‚à© ‚àÇ‚Ñ¶‚àó = ‚àÖ, either B(x, Œ¥) ‚äÇ int(‚Ñ¶‚àó ) or B(x, Œ¥) ‚äÇ
int(Rp \ ‚Ñ¶‚àó ). If B(x, Œ¥) ‚äÇ int(‚Ñ¶‚àó ), pM L (x; Œ¥) = 1. If B(x, Œ¥) ‚äÇ int(Rp \ ‚Ñ¶‚àó ), pM L (x; Œ¥) = 0,
since M L(x0 ) = 0 for almost every x0 ‚àà B(x, Œ¥) ‚äÇ N (X , 3Œ¥ÃÑ) \ ‚Ñ¶‚àó by the choice of Œ¥ÃÑ. Therefore,
{x ‚àà X : pM L (x; Œ¥) ‚àà (0, 1)} ‚äÇ N (‚àÇ‚Ñ¶‚àó , Œ¥) for every Œ¥ ‚àà (0, Œ¥ÃÉ). By this and Lemma B.5, for
Œ¥ ‚àà (0, Œ¥ÃÉ),
Œ¥ ‚àí1 E[pM L (Xi ; Œ¥)l g(Xi )1{pM L (Xi ; Œ¥) ‚àà (0, 1)}]
Z
‚àí1
=Œ¥
pM L (x; Œ¥)l g(x)1{pM L (x; Œ¥) ‚àà (0, 1)}fX (x)dx
Z
pM L (x; Œ¥)l g(x)1{pM L (x; Œ¥) ‚àà (0, 1)}fX (x)dx
= Œ¥ ‚àí1
= Œ¥ ‚àí1

N (‚àÇ‚Ñ¶‚àó ,Œ¥)
Œ¥ Z

Z

‚àÇ‚Ñ¶‚àó

‚àíŒ¥

pM L (u + ŒªŒΩ‚Ñ¶‚àó (u); Œ¥)l g(u + ŒªŒΩ‚Ñ¶‚àó (u))1{pM L (u + ŒªŒΩ‚Ñ¶‚àó (u); Œ¥) ‚àà (0, 1)}
‚àó

‚àÇ‚Ñ¶
√ó fX (u + ŒªŒΩ‚Ñ¶‚àó (u))Jp‚àí1
œà‚Ñ¶‚àó (u, Œª)dHp‚àí1 (u)dŒª.

With change of variables v = ŒªŒ¥ , we have
Œ¥ ‚àí1 E[pM L (Xi ; Œ¥)l g(Xi )1{pM L (Xi ; Œ¥) ‚àà (0, 1)}]
Z 1Z
pM L (u + Œ¥vŒΩ‚Ñ¶‚àó (u); Œ¥)l 1{pM L (u + Œ¥vŒΩ‚Ñ¶‚àó (u); Œ¥) ‚àà (0, 1)}
=
‚àí1

‚àÇ‚Ñ¶‚àó

‚àó

‚àÇ‚Ñ¶
√ó g(u + Œ¥vŒΩ‚Ñ¶‚àó (u))fX (u + Œ¥vŒΩ‚Ñ¶‚àó (u))Jp‚àí1
œà‚Ñ¶‚àó (u, Œ¥v)dHp‚àí1 (u)dv.

For every (u, v, Œ¥) ‚àà ‚àÇ‚Ñ¶‚àó \ N (X , Œ¥ÃÉ) √ó (‚àí1, 1) √ó (0, Œ¥ÃÉ), u + Œ¥vŒΩ‚Ñ¶‚àó (u) ‚àà
/ X , so
Œ¥ ‚àí1 E[pM L (Xi ; Œ¥)l g(Xi )1{pM L (Xi ; Œ¥) ‚àà (0, 1)}]
Z 1Z
=
pM L (u + Œ¥vŒΩ‚Ñ¶‚àó (u); Œ¥)l 1{pM L (u + Œ¥vŒΩ‚Ñ¶‚àó (u); Œ¥) ‚àà (0, 1)}
‚àí1

‚àÇ‚Ñ¶‚àó ‚à©N (X ,Œ¥ÃÉ)

‚àó

‚àÇ‚Ñ¶
√ó g(u + Œ¥vŒΩ‚Ñ¶‚àó (u))fX (u + Œ¥vŒΩ‚Ñ¶‚àó (u))Jp‚àí1
œà‚Ñ¶‚àó (u, Œ¥v)dHp‚àí1 (u)dv

Z

1

Z

=
‚àí1

‚àÇ‚Ñ¶‚àó ‚à©N (X ,Œ¥ÃÉ)

‚àó

‚àÇ‚Ñ¶
(k(v) + Œ¥r(u, v, Œ¥))l g(u + Œ¥vŒΩ‚Ñ¶‚àó (u))fX (u + Œ¥vŒΩ‚Ñ¶‚àó (u))Jp‚àí1
œà‚Ñ¶‚àó (u, Œ¥v)dHp‚àí1 (u)dv,

80

‚àó

‚àÇ‚Ñ¶ œà ‚àó (¬∑, ¬∑)
where the second equality follows from Steps C.6.3.1 and C.6.3.2. By Lemma B.5, Jp‚àí1
‚Ñ¶
‚àó
is bounded on ‚àÇ‚Ñ¶ √ó (‚àíŒ¥ÃÉ, Œ¥ÃÉ). Since r, g and fX are also bounded, for some constant C > 0,

|Œ¥ ‚àí1 E[pM L (Xi ; Œ¥)l g(Xi )1{pM L (Xi ; Œ¥) ‚àà (0, 1)}]| ‚â§ C

Z

1

‚àí1

Z

dHp‚àí1 (u)dv,

‚àÇ‚Ñ¶‚àó ‚à©N (X ,Œ¥ÃÉ)

which is finite by Assumption 3 (f) (i). Moreover, if g and fX are continuous on N (‚àÇ‚Ñ¶‚àó , 2Œ¥ÃÉ) ‚à©
N (X , 2Œ¥ÃÉ), by the Dominated Convergence Theorem,
Œ¥

‚àí1

ML

E[p

l

(Xi ; Œ¥) g(Xi )1{p

ML

Z

1

(Xi ; Œ¥) ‚àà (0, 1)}] ‚Üí

l

Z

k(v) dv
‚àÇ‚Ñ¶‚àó

‚àí1

g(u)fX (u)dHp‚àí1 (u),

‚àó

‚àó

‚àÇ‚Ñ¶ œà ‚àó (u, Œª) is continuous in Œª and J ‚àÇ‚Ñ¶ œà ‚àó (u, 0) =
where we use the fact from Lemma B.5 that Jp‚àí1
‚Ñ¶
p‚àí1 ‚Ñ¶
1.
Note that M L(x) = 1 for every x ‚àà ‚Ñ¶‚àó and M L(x) = 0 for almost every x ‚àà N (X , 2Œ¥ÃÉ) \ ‚Ñ¶‚àó .
Also, for every (u, v, Œ¥) ‚àà ‚àÇ‚Ñ¶‚àó ‚à© N (X , Œ¥ÃÉ) √ó (‚àí1, 1) √ó (0, Œ¥ÃÉ), u + Œ¥vŒΩ‚Ñ¶‚àó (u) ‚àà ‚Ñ¶‚àó if v ‚àà (0, 1) and
u + Œ¥vŒΩ‚Ñ¶‚àó (u) ‚àà N (X , 2Œ¥ÃÉ) \ ‚Ñ¶‚àó if v ‚àà (‚àí1, 0]. Therefore,

Œ¥ ‚àí1 E[Zi pM L (Xi ; Œ¥)l g(Xi )1{pM L (Xi ; Œ¥) ‚àà (0, 1)}]
= Œ¥ ‚àí1 E[M L(Xi )pM L (Xi ; Œ¥)l g(Xi )1{pM L (Xi ; Œ¥) ‚àà (0, 1)}]
Z 1Z
M L(u + Œ¥vŒΩ‚Ñ¶‚àó (u))(k(v) + Œ¥r(u, v, Œ¥))l g(u + Œ¥vŒΩ‚Ñ¶‚àó (u))
=
‚àÇ‚Ñ¶‚àó ‚à©N (X ,Œ¥ÃÉ)

‚àí1

‚àó

‚àÇ‚Ñ¶
√ó fX (u + Œ¥vŒΩ‚Ñ¶‚àó (u))Jp‚àí1
œà‚Ñ¶‚àó (u, Œ¥v)dHp‚àí1 (u)dv

Z

1Z

‚àó

=
‚àÇ‚Ñ¶‚àó ‚à©N (X ,Œ¥ÃÉ)

0

Z
‚Üí

1
l

‚àÇ‚Ñ¶
(k(v) + Œ¥r(u, v, Œ¥))l g(u + Œ¥vŒΩ‚Ñ¶‚àó (u))fX (u + Œ¥vŒΩ‚Ñ¶‚àó (u))Jp‚àí1
œà‚Ñ¶‚àó (u, Œ¥v)dHp‚àí1 (u)dv

Z

k(v) dv
0

‚àÇ‚Ñ¶‚àó

g(u)fX (u)dHp‚àí1 (u).

Now suppose that g and fX are continuously differentiable on N (‚àÇ‚Ñ¶‚àó , 2Œ¥ÃÉ) ‚à© N (X , 2Œ¥ÃÉ) and
that ‚àág and ‚àáf are bounded on N (‚àÇ‚Ñ¶‚àó , 2Œ¥ÃÉ) ‚à© N (X , 2Œ¥ÃÉ). Using the mean-value theorem, we
obtain that, for any (u, v, Œ¥) ‚àà ‚àÇ‚Ñ¶‚àó ‚à© N (X , Œ¥ÃÉ) √ó (‚àí1, 1) √ó (0, Œ¥ÃÉ),
g(u + Œ¥vŒΩ‚Ñ¶‚àó (u)) = g(u) + ‚àág(yg (u, Œ¥vŒΩ‚Ñ¶‚àó (u)))0 Œ¥vŒΩ‚Ñ¶‚àó (u),
fX (u + Œ¥vŒΩ‚Ñ¶‚àó (u)) = fX (u) + ‚àáfX (yf (u, Œ¥vŒΩ‚Ñ¶‚àó (u)))0 Œ¥vŒΩ‚Ñ¶‚àó (u)
for some yg (u, Œ¥vŒΩ‚Ñ¶‚àó (u)) and yf (u, Œ¥vŒΩ‚Ñ¶‚àó (u)) that are on the line segment connecting u and
u + Œ¥vŒΩ‚Ñ¶‚àó (u). In addition,
‚àó

‚àÇ‚Ñ¶‚àó
Jp‚àí1
œà‚Ñ¶‚àó (u, Œ¥v)

‚àÇ‚Ñ¶ œà ‚àó (u, y (u, Œ¥v))
‚àÇJp‚àí1
‚Ñ¶
J
Œ¥v
=
+
‚àÇŒª
‚àó
‚àÇ‚Ñ¶ œà ‚àó (u, y (u, Œ¥v))
‚àÇJp‚àí1
‚Ñ¶
J
=1+
Œ¥v
‚àÇŒª
‚àÇ‚Ñ¶‚àó
Jp‚àí1
œà‚Ñ¶‚àó (u, 0)

81

‚àó

for some yJ (u, Œ¥v) that is on the line segment connecting 0 and Œ¥v. By Lemma B.5,
is bounded on ‚àÇ‚Ñ¶‚àó √ó (‚àíŒ¥ÃÉ, Œ¥ÃÉ). We then have

‚àÇ‚Ñ¶ œà ‚àó (¬∑,¬∑)
‚àÇJp‚àí1
‚Ñ¶
‚àÇŒª

Œ¥ ‚àí1 E[pM L (Xi ; Œ¥)l g(Xi )1{pM L (Xi ; Œ¥) ‚àà (0, 1)}]
Z 1Z
(k(v) + Œ¥r(u, v, Œ¥))l (g(u) + ‚àág(yg (u, Œ¥vŒΩ‚Ñ¶‚àó (u)))0 Œ¥vŒΩ‚Ñ¶‚àó (u))
=
‚àí1

‚àÇ‚Ñ¶‚àó ‚à©N (X ,Œ¥ÃÉ)

‚àó

‚àÇJ ‚àÇ‚Ñ¶ œà‚Ñ¶‚àó (u,yJ (u,Œ¥v))
Œ¥v)dHp‚àí1 (u)dv
‚àÇŒª

Z

1

=
‚àí1
1

√ó (fX (u) + ‚àáfX (yf (u, Œ¥vŒΩ‚Ñ¶‚àó (u)))0 Œ¥vŒΩ‚Ñ¶‚àó (u))(1 + p‚àí1
Z
(k(v)l g(u)fX (u) + Œ¥h(u, v, Œ¥))dHp‚àí1 (u)dv
‚àÇ‚Ñ¶‚àó ‚à©N (X ,Œ¥ÃÉ)

Z

Z

l

=

k(v) dv
‚àÇ‚Ñ¶‚àó

‚àí1

g(u)fX (u)dH

p‚àí1

Z

1

Z

h(u, v, Œ¥)dHp‚àí1 (u)dv

(u) + Œ¥
‚àí1

‚àÇ‚Ñ¶‚àó ‚à©N (X ,Œ¥ÃÉ)

for some function h bounded on ‚àÇ‚Ñ¶‚àó ‚à© N (X , Œ¥ÃÉ) √ó (‚àí1, 1) √ó (0, Œ¥ÃÉ). It then follows that
Z 1
Z
l
‚àí1
ML
l
ML
k(v) dv
g(u)fX (u)dHp‚àí1 (u) + O(Œ¥).
Œ¥ E[p (Xi ; Œ¥) g(Xi )1{p (Xi ; Œ¥) ‚àà (0, 1)}] =
‚àÇ‚Ñ¶‚àó

‚àí1

Also,
Œ¥ ‚àí1 E[Zi pM L (Xi ; Œ¥)l g(Xi )1{pM L (Xi ; Œ¥) ‚àà (0, 1)}]
Z 1Z
‚àÇ‚Ñ¶‚àó
=
(k(v) + Œ¥r(u, v, Œ¥))l g(u + Œ¥vŒΩ‚Ñ¶‚àó (u))fX (u + Œ¥vŒΩ‚Ñ¶‚àó (u))Jp‚àí1
œà‚Ñ¶‚àó (u, Œ¥v)dHp‚àí1 (u)dv
‚àÇ‚Ñ¶‚àó ‚à©N (X ,Œ¥ÃÉ)

0

Z
=

1
l

Z

k(v) dv
‚àÇ‚Ñ¶‚àó

0

g(u)fX (u)dHp‚àí1 (u) + O(Œ¥).

Step C.6.3.4. Let SD = limŒ¥‚Üí0 Œ¥ ‚àí1 E[Zi D0i 1{pM L (Xi ; Œ¥) ‚àà (0, 1)}] and SY = limŒ¥‚Üí0 Œ¥ ‚àí1 E[Zi Yi 1{pM L (Xi ; Œ¥) ‚àà
‚àí1
SY is Œ≤1 .
(0, 1)}]. Then the second element of SD
Proof. Note that Di = Zi Di (1) + (1 ‚àí Zi )Di (0) and Yi = Zi Y1i + (1 ‚àí Zi )Y0i . By Step C.6.3.3,
SD
Ô£Æ

2f¬ØX
Ô£Ø
=Ô£∞
f¬ØX
R1
k(v)dv f¬ØX
‚àí1

where f¬ØX =

Ô£π
R1
E[Di (1) + Di (0)|Xi = x]fX (x)dHp‚àí1 (x)
k(v)dv f¬ØX
‚àí1
R1
Ô£∫
p‚àí1
(x)
k(v)dv f¬ØX Ô£ª ,
‚àó E[Di (1)|Xi = x]fX (x)dH
‚àÇ‚Ñ¶
0
R
R
R1
R0
1
( k(v)dvE[Di (1)|Xi = x] + ‚àí1 k(v)dvE[Di (0)|Xi = x])fX (x)dHp‚àí1 (x) ‚àí1 k(v)2 dv f¬ØX
‚àÇ‚Ñ¶‚àó 0
R

‚àÇ‚Ñ¶‚àóR

fX (x)dHp‚àí1 (x), and
Ô£Æ
Ô£π
R
p‚àí1 (x)
‚àÇ‚Ñ¶R‚àó E[Y1i + Y0i |Xi = x]fX (x)dH
Ô£Ø
Ô£∫
p‚àí1 (x)
SY = Ô£∞
Ô£ª.
‚àÇ‚Ñ¶‚àó E[Y1i |XRi = x]fX (x)dH
R1
R
0
p‚àí1
(x)
‚àÇ‚Ñ¶‚àó ( 0 k(v)dvE[Y1i |Xi = x] + ‚àí1 k(v)dvE[Y0i |Xi = x])fX (x)dH
R

‚àÇ‚Ñ¶‚àó

After a few lines of algebra, we have
Z
‚àí2
¬Ø
det(SD ) =fX
E[Di (1) ‚àí Di (0)|Xi = x]fX (x)dHp‚àí1 (x)
‚àÇ‚Ñ¶‚àó
Z 0
Z 0
Z 1
Z 1
√ó(
(k(v) ‚àí
k(s)ds)2 dv +
(k(v) ‚àí
k(s)ds)2 dv),
‚àí1

‚àí1

0

82

0

which is nonzero under Assumption 3 (b) and (f) (i). After another few lines of algebra, we
‚àí1
obtain that the second element of SD
SY is
R
(1) ‚àí Di (0))(Yi (1) ‚àí Yi (0))|Xi = x]fX (x)dHp‚àí1 (x)
‚àÇ‚Ñ¶‚àó E[(D
Ri
.
p‚àí1 (x)
‚àÇ‚Ñ¶‚àó E[Di (1) ‚àí Di (0)|Xi = x]fX (x)dH
On the other hand, by Step C.6.3.3,
Œ≤1 = lim E[œâi (Œ¥)(Yi (1) ‚àí Yi (0))]
Œ¥‚Üí0

Œ¥ ‚àí1 E[pM L (Xi ; Œ¥)(1 ‚àí pM L (Xi ; Œ¥))(Di (1) ‚àí Di (0))(Yi (1) ‚àí Yi (0))1{pM L (Xi ; Œ¥) ‚àà (0, 1)}]
Œ¥ ‚àí1 E[pM L (Xi ; Œ¥)(1 ‚àí pM L (Xi ; Œ¥))(Di (1) ‚àí Di (0))1{pM L (Xi ; Œ¥) ‚àà (0, 1)}]
R
p‚àí1 (x)
‚àí1 k(v)(1 ‚àí k(v))dv ‚àÇ‚Ñ¶‚àó E[(Di (1) ‚àí Di (0))(Yi (1) ‚àí Yi (0))|Xi = x]fX (x)dH
=
R1
R
p‚àí1 (x)
‚àí1 k(v)(1 ‚àí k(v))dv ‚àÇ‚Ñ¶‚àó E[Di (1) ‚àí Di (0)|Xi = x]fX (x)dH
R
p‚àí1 (x)
‚àó E[(Di (1) ‚àí Di (0))(Yi (1) ‚àí Yi (0))|Xi = x]fX (x)dH
R
= ‚àÇ‚Ñ¶
.
p‚àí1 (x)
‚àÇ‚Ñ¶‚àó E[Di (1) ‚àí Di (0)|Xi = x]fX (x)dH
= lim
Œ¥‚Üí0
R1

p

Step C.6.3.5. If nŒ¥n ‚Üí ‚àû as n ‚Üí ‚àû, then Œ≤ÃÇ1 ‚àí‚Üí Œ≤1 .
P
P
Proof. It suffices to verify that the variance of each element of nŒ¥1n ni=1 Zi D0i Ii and nŒ¥1n ni=1 Zi Y Ii
P
is o(1). Here, we only verify that Var( nŒ¥1n ni=1 pM L (Xi ; Œ¥n )Yi Ii ) = o(1). Note that
E[Yi2 |Xi ] = E[Zi Y1i2 + (1 ‚àí Zi )Y0i2 |Xi ] ‚â§ E[Y1i2 + Y0i2 |Xi ].
Under Assumption 3 (g), there exists Œ¥ 0 > 0 such that E[Y1i2 +Y0i2 |Xi ] is continuous on N (‚àÇ‚Ñ¶‚àó , Œ¥ 0 ).
Since cl(N (‚àÇ‚Ñ¶‚àó , 12 Œ¥ 0 )) is closed and bounded, E[Y1i2 + Y0i2 |Xi ] is bounded on cl(N (‚àÇ‚Ñ¶‚àó , 12 Œ¥ 0 )). We
have
Var(

n
1 X ML
1 ‚àí1
p (Xi ; Œ¥n )Yi Ii ) ‚â§
Œ¥ E[pM L (Xi ; Œ¥n )2 Yi2 Ii ]
nŒ¥n
nŒ¥n n
i=1

1 ‚àí1
Œ¥ E[pM L (Xi ; Œ¥n )2 E[Yi2 |Xi ]Ii ]
nŒ¥n n
1
‚â§
C
nŒ¥n
=

for some C > 0, where the last inequality follows from Step C.6.3.3. The conclusion follows since
nŒ¥n ‚Üí ‚àû.
‚àí1
Now let Œ≤ = (Œ≤0 , Œ≤1 , Œ≤2 )0 = SD
SY and let i = Yi ‚àí D0i Œ≤. We can write
n
n
p
1 X
1 X
0
‚àí1
nŒ¥n (Œ≤ÃÇ ‚àí Œ≤) = (
Zi D i I i ) ‚àö
Zi i Ii
nŒ¥n
nŒ¥n
i=1

i=1

n
n
1 X
1 X
0
‚àí1
=(
Zi D i I i ) ‚àö
{(Zi i Ii ‚àí E[Zi i Ii ]) + E[Zi i Ii ]}.
nŒ¥n
nŒ¥n
i=1

i=1

83

Step C.6.3.6.

n
1 X
d
‚àö
(Zi i Ii ‚àí E[Zi i Ii ]) ‚àí‚Üí N (0, V),
nŒ¥n i=1

where V = limn‚Üí‚àû Œ¥n‚àí1 E[2i Zi Zi Ii ].
Proof. We use the triangular-array Lyapunov CLT and the Cram√©r-Wold device. Pick a nonzero
1
Œª0 (Zi i Ii ‚àí E[Zi i Ii ]). First,
Œª ‚àà Rp , and let Vi,n = ‚àönŒ¥
n

n
X

2
E[Vi,n
] = Œ¥n‚àí1 Œª0 (E[2i Zi Z0i Ii ] ‚àí E[Zi i Ii ]E[Z0i i Ii ])Œª.

i=1

By Step C.6.3.3,
E[Zi i Ii ] = E[Zi (Yi ‚àí D0i Œ≤)Ii ] = O(Œ¥n ),
so
Œ¥n‚àí1 E[Zi i Ii ]E[Z0i i Ii ] = o(1).
We have
E[2i Zi Z0i Ii ] = E[(Yi ‚àí Œ≤0 ‚àí Œ≤1 Di ‚àí Œ≤2 pM L (Xi ; Œ¥n ))2 Zi Z0i Ii ]
= E[Zi (Y1i ‚àí Œ≤0 ‚àí Œ≤1 Di (1) ‚àí Œ≤2 pM L (Xi ; Œ¥n ))2 Zi Z0i Ii ]
+ E[(1 ‚àí Zi )(Y0i ‚àí Œ≤0 ‚àí Œ≤1 Di (0) ‚àí Œ≤2 pM L (Xi ; Œ¥n ))2 Zi Z0i Ii ].
Since E[Y1i |Xi ], E[Y0i |Xi ], E[Di (1)|Xi ], E[Di (0)|Xi ], E[Y1i2 |Xi ], E[Y0i2 |Xi ], E[Y1i Di (1)|Xi ] and
E[Y0i Di (0)|Xi ] are continuous on N (‚àÇ‚Ñ¶‚àó , Œ¥ 0 ) for some Œ¥ 0 > 0 under Assumption 3 (g), limn‚Üí‚àû Œ¥n‚àí1 E[2i Zi Z0i Ii ]
exists and finite. Therefore,
n
X
2
E[Vi,n
] ‚Üí Œª0 VŒª < 0.
i=1

We next verify the Lyapunov condition: for some t > 0,
n
X

E[|Vi,n |2+t ] ‚Üí 0.

i=1

We have
n
X

E[|Vi,n |4 ] =

i=1

‚â§

1 ‚àí1
Œ¥ E[|Œª0 (Zi i Ii ‚àí E[Zi i Ii ])|4 ]
nŒ¥n n
1 3c ‚àí1
2 Œ¥n {E[|Œª0 Zi i Ii |4 ] + |Œª0 E[Zi i Ii ]|4 }
nŒ¥n

by the cr -inequality. Repeating using the cr -inequality gives
Œ¥n‚àí1 E[|Œª0 Zi i Ii |4 ] = Œ¥n‚àí1 E[|Œª0 Zi (Yi ‚àí Œ≤0 ‚àí Œ≤1 Di ‚àí Œ≤2 pM L (Xi ; Œ¥n ))|4 Ii ]
‚â§ 23c Œ¥n‚àí1 E[(|Œª0 Zi |4 )(|Yi |4 + |Œ≤0 |4 + |Œ≤1 |4 Di + |Œ≤2 |4 pM L (Xi ; Œ¥n )4 )Ii ]
‚â§ 23c (Œª1 + Œª2 + Œª3 )4 Œ¥n‚àí1 E[(Yi4 + Œ≤04 + Œ≤14 + Œ≤24 )Ii ]
= 23c O(1)
84

for some finite constant c, where the last equality holds by Step C.6.3.3 under Assumption 3 (g).
Moreover,
Œ¥n‚àí1 |Œª0 E[Zi i Ii ]|4 = Œ¥n3 |Œª0 Œ¥n‚àí1 E[Zi i Ii ]|4
= Œ¥n3 O(1)
= o(1).
Therefore, when nŒ¥n ‚Üí ‚àû,

n
X

E[|Vi,n |4 ] ‚Üí 0,

i=1

and the conclusion follows from the Lyapunov CLT and the Cram√©r-Wold device.
p

‚àí1
0 )‚àí1 .
Step C.6.3.7. nŒ¥n Œ£ÃÇ ‚àí‚Üí SD
V(SD

Proof. We have
n
n
1 X 2
1 X
0
ÀÜi Zi Zi Ii =
(Yi ‚àí D0i Œ≤ÃÇ)2 Zi Z0i Ii
nŒ¥n
nŒ¥n
i=1

i=1

n
1 X
=
(i ‚àí D0i (Œ≤ÃÇ ‚àí Œ≤))2 Zi Z0i Ii
nŒ¥n
i=1

n
1 X 2
=
i Zi Z0i Ii
nŒ¥n
i=1

‚àí

n
2 X
(Yi ‚àí D0i Œ≤)((Œ≤ÃÇ0 ‚àí Œ≤0 ) + Di (Œ≤ÃÇ1 ‚àí Œ≤1 ) + pM L (Xi ; Œ¥n )(Œ≤ÃÇ2 ‚àí Œ≤2 ))Zi Z0i Ii
nŒ¥n
i=1

n
1 X
+
((Œ≤ÃÇ0 ‚àí Œ≤0 ) + Di (Œ≤ÃÇ1 ‚àí Œ≤1 ) + pM L (Xi ; Œ¥n )(Œ≤ÃÇ2 ‚àí Œ≤2 ))2 Zi Z0i Ii
nŒ¥n

i=1
n
1 X 2
=
i Zi Z0i Ii
nŒ¥n
i=1

+ op (1)Op (1),

where the last equality follows from the result that Œ≤ÃÇ ‚àí Œ≤ = op (1) and from application of
P
p
Step C.6.3.3 as in Steps C.6.3.5 and C.6.3.6. To show nŒ¥1n ni=1 2i Zi Z0i Ii ‚àí‚Üí V, it suffices
P
to verify that the variance of each element of nŒ¥1n ni=1 2i Zi Z0i Ii is o(1). We only verify that
P
Var( nŒ¥1n ni=1 2i pM L (Xi ; Œ¥n )2 Ii ) = o(1). Using the cr -inequality, we have that for some constant

85

c,
n
1 X 2 ML
1 ‚àí1
Œ¥ E[4i Ii ]
Var(
i p (Xi ; Œ¥n )2 Ii ) ‚â§
nŒ¥n
nŒ¥n n
i=1

1 ‚àí1
Œ¥ E[(Yi ‚àí Œ≤0 ‚àí Œ≤1 Di ‚àí Œ≤2 pM L (Xi ))4 Ii ]
nŒ¥n n
1 3c ‚àí1
‚â§
2 Œ¥n E[(Yi4 + Œ≤04 + Œ≤14 Di + Œ≤24 pM L (Xi )4 )Ii ]
nŒ¥n
1 3c ‚àí1
‚â§
2 Œ¥n E[(Yi4 + Œ≤04 + Œ≤14 + Œ≤24 )Ii ]
nŒ¥n
1 3c
=
2 O(1)
nŒ¥n
= o(1),
=

where the second last equality holds by Step C.6.3.3 under Assumption 3 (g). Therefore,
n
1 X 2
p
ÀÜi Zi Z0i Ii ‚àí‚Üí V.
nŒ¥n
i=1

It follows that
n
n
n
X
1 X
1 X
p
‚àí1
0 ‚àí1
0
‚àí1 1
2
0
nŒ¥n Œ£ÃÇ = (
V(SD
) .
Zi Di Ii ) (
ÀÜi Zi Zi Ii )(
Di Z0i Ii )‚àí1 ‚àí‚Üí SD
nŒ¥n
nŒ¥n
nŒ¥n
i=1

i=1

i=1

d

Step C.6.3.8. œÉÃÇ ‚àí1 (Œ≤ÃÇ1 ‚àí Œ≤1 ) ‚àí‚Üí N (0, 1).
‚àí1 ‚àí1
Œ¥n E[Zi Yi Ii ]. We then have
Proof. Let Œ≤n = SD

‚àö

n
p
1 X
E[Zi i Ii ] = nŒ¥n Œ¥n‚àí1 E[Zi (Yi ‚àí D0 Œ≤)Ii ]
nŒ¥n i=1
p
= nŒ¥n Œ¥n‚àí1 E[Zi (Yi ‚àí D0i Œ≤n + D0i (Œ≤n ‚àí Œ≤))Ii ]
p
= nŒ¥n Œ¥n‚àí1 {E[Zi Yi Ii ] ‚àí E[Zi D0i Ii ]Œ≤n + E[Zi D0i Ii ](Œ≤n ‚àí Œ≤)}
p
‚àí1 ‚àí1
= nŒ¥n {(SD ‚àí Œ¥n‚àí1 E[Zi D0i Ii ])SD
Œ¥n E[Zi Yi Ii ]
‚àí1 ‚àí1
+ Œ¥n‚àí1 E[Zi D0i Ii ]SD
(Œ¥n E[Zi Yi Ii ] ‚àí SY )}
p
= nŒ¥n (O(Œ¥n )O(1) + O(1)O(Œ¥n ))
p
= O( nŒ¥n Œ¥n ),

where we use Step C.6.3.3 for the second last equality. Thus, when nŒ¥n3 ‚Üí 0,
n
n
p
1 X
1 X
nŒ¥n (Œ≤ÃÇ ‚àí Œ≤) = (
Zi D0i Ii )‚àí1 ‚àö
{(Zi i Ii ‚àí E[Zi i Ii ]) + E[Zi i Ii ]}
nŒ¥n
nŒ¥n
i=1

i=1

d

‚àí1
0 ‚àí1
‚àí‚Üí N (0, SD
V(SD
) ).

The conclusion then follows from Step C.6.3.7.

86

C.6.4

Consistency and Asymptotic Normality of Œ≤ÃÇ1s When Pr(M L(Xi ) ‚àà (0, 1)) = 0

Let Iis = 1{ps (Xi ; Œ¥n ) ‚àà (0, 1)}, Dsi = (1, Di , ps (Xi ; Œ¥n ))0 and Zsi = (1, Zi , ps (Xi ; Œ¥n ))0 . Œ≤ÃÇ s and Œ£ÃÇs
are given by
n
n
X
X
Œ≤ÃÇ s = (
Zsi (Dsi )0 Iis )‚àí1
Zsi Yi Iis .
i=1

and

i=1

n
n
n
X
X
X
s
s 0 s ‚àí1
s 2 s
s 0 s
Œ£ÃÇ = (
Zi (Di ) Ii ) ( (ÀÜ
i ) Zi (Zi ) Ii )(
Dsi (Zsi )0 Iis )‚àí1 ,
s

i=1

i=1

i=1

where ÀÜsi = Yi ‚àí (Dsi )0 Œ≤ÃÇ s . It is sufficient to show that
Œ≤ÃÇ s ‚àí Œ≤ÃÇ = op (1),
if Sn ‚Üí ‚àû and that
p
nŒ¥n (Œ≤ÃÇ s ‚àí Œ≤ÃÇ) = op (1),
p

‚àí1
0 ‚àí1
nŒ¥n Œ£ÃÇs ‚àí‚Üí SD
V(SD
)

if Assumption 5 holds.
2
Step C.6.4.1. Let {Vi }‚àû
i=1 be i.i.d. random variables. If E[Vi |Xi ] and E[Vi |Xi ] are bounded on
N (‚àÇ‚Ñ¶‚àó , Œ¥ 0 ) ‚à© N (X , Œ¥ 0 ) for some Œ¥ 0 > 0, and Sn ‚Üí ‚àû, then
n
n
1 X
1 X
Vi ps (Xi ; Œ¥n )l Iis ‚àí
Vi pM L (Xi ; Œ¥n )l Ii = op (1)
nŒ¥n
nŒ¥n
i=1

i=1

for l = 0, 1, 2, 3, 4. If, in addition, Assumption 5 holds, then
n
n
1 X
1 X
s
l s
‚àö
Vi p (Xi ; Œ¥n ) Ii ‚àí ‚àö
Vi pM L (Xi ; Œ¥n )l Ii = op (1)
nŒ¥n i=1
nŒ¥n i=1

for l = 0, 1, 2.
Proof. We have
n
n
1 X
1 X
Vi ps (Xi ; Œ¥n )l Iis ‚àí
Vi pM L (Xi ; Œ¥n )l Ii
nŒ¥n
nŒ¥n
i=1

i=1

n
n
1 X
1 X
=
Vi ps (Xi ; Œ¥n )l (Iis ‚àí Ii ) +
Vi (ps (Xi ; Œ¥n )l ‚àí pM L (Xi ; Œ¥n )l )Ii .
nŒ¥n
nŒ¥n
i=1

i=1

87

P
We first consider nŒ¥1n ni=1 Vi (ps (Xi ; Œ¥n )l ‚àí pM L (Xi ; Œ¥n )l )Ii . By using the argument in the proof
of Step C.6.3.3 in Section C.6.3, we have
|E[
=
‚â§

n
1 X
Vi (ps (Xi ; Œ¥n )l ‚àí pM L (Xi ; Œ¥n )l )Ii ]|
nŒ¥n

i=1
‚àí1
Œ¥n |E[E[Vi |Xi ]E[ps (Xi ; Œ¥n )l ‚àí pM L (Xi ; Œ¥n )l |Xi ]Ii ]|
Œ¥n‚àí1 E[|E[Vi |Xi ]||E[ps (Xi ; Œ¥n )l ‚àí pM L (Xi ; Œ¥n )l |Xi ]|Ii ]

Z

1

Z

=
‚àÇ‚Ñ¶‚àó ‚à©N (X ,Œ¥ÃÉ)

‚àí1

|E[Vi |Xi = u + Œ¥n vŒΩ‚Ñ¶‚àó (u)]||E[ps (u + Œ¥n vŒΩ‚Ñ¶‚àó (u); Œ¥n )l ‚àí pM L (u + Œ¥n vŒΩ‚Ñ¶‚àó (u); Œ¥n )l ]|
‚àó

‚àÇ‚Ñ¶
√ó fX (u + Œ¥n vŒΩ‚Ñ¶‚àó (u))Jp‚àí1
œà‚Ñ¶‚àó (u, Œ¥n v)dHp‚àí1 (u)dv,

where the choice of Œ¥ÃÉ is as in the proof of Step C.6.3.3. By Lemma B.7, for l = 0, 1, 2,
n
1 X
Vi (ps (Xi ; Œ¥n )l ‚àí pM L (Xi ; Œ¥n )l )Ii ]|
nŒ¥n
i=1
Z 1Z
1
‚àÇ‚Ñ¶‚àó
|E[Vi |Xi = u + Œ¥n vŒΩ‚Ñ¶‚àó (u)]|fX (u + Œ¥n vŒΩ‚Ñ¶‚àó (u))Jp‚àí1
‚â§
œà‚Ñ¶‚àó (u, Œ¥n v)dHp‚àí1 (u)dv
Sn ‚àí1 ‚àÇ‚Ñ¶‚àó ‚à©N (X ,Œ¥ÃÉ)

|E[

= O(Sn‚àí1 ).
Also, by Lemma B.7,
|E[

n
1 X
Vi (ps (Xi ; Œ¥n )3 ‚àí pM L (Xi ; Œ¥n )3 )Ii ]|
nŒ¥n

i=1
‚àí1
= |Œ¥n E[Vi (ps (Xi ; Œ¥n ) ‚àí pM L (Xi ; Œ¥n ))(ps (Xi ; Œ¥n )2 + ps (Xi ; Œ¥n )pM L (Xi ; Œ¥n ) + pM L (Xi ; Œ¥n )2 )Ii ]|
‚â§ Œ¥n‚àí1 E[|E[Vi |Xi ]||E[(ps (Xi ; Œ¥n ) ‚àí pM L (Xi ; Œ¥n ))(ps (Xi ; Œ¥n )2 + ps (Xi ; Œ¥n )pM L (Xi ; Œ¥n ) + pM L (Xi ; Œ¥n )2 )|Xi ]|Ii ]
‚â§ 3Œ¥n‚àí1 E[|E[Vi |Xi ]|E[|ps (Xi ; Œ¥n ) ‚àí pM L (Xi ; Œ¥n )||Xi ]Ii ]
Z 1Z
=
|E[Vi |Xi = u + Œ¥n vŒΩ‚Ñ¶‚àó (u)]|E[|ps (u + Œ¥n vŒΩ‚Ñ¶‚àó (u); Œ¥n ) ‚àí pM L (u + Œ¥n vŒΩ‚Ñ¶‚àó (u); Œ¥n )|]
‚àó
‚àí1 ‚àÇ‚Ñ¶ ‚à©N (X ,Œ¥ÃÉ)
‚àÇ‚Ñ¶‚àó
√ó fX (u + Œ¥n vŒΩ‚Ñ¶‚àó (u))Jp‚àí1
œà‚Ñ¶‚àó (u, Œ¥n v)dHp‚àí1 (u)dv

‚â§(

1
+ )O(1)
Sn 2

for every  > 0. We can make the right-hand side arbitrarily close to zero by taking suffiP
ciently small  > 0 and sufficiently large Sn , which implies that |E[ nŒ¥1n ni=1 Vi (ps (Xi ; Œ¥n )3 ‚àí
pM L (Xi ; Œ¥n )3 )Ii ]| = o(1) if Sn ‚Üí ‚àû. Likewise,
|E[
=
‚â§
‚â§

n
1 X
Vi (ps (Xi ; Œ¥n )4 ‚àí pM L (Xi ; Œ¥n )4 )Ii ]|
nŒ¥n

i=1
‚àí1
|Œ¥n E[Vi (ps (Xi ; Œ¥n )2 + pM L (Xi ; Œ¥n )2 )(ps (Xi ; Œ¥n ) + pM L (Xi ; Œ¥n ))(ps (Xi ; Œ¥n ) ‚àí pM L (Xi ; Œ¥n ))Ii ]|
Œ¥n‚àí1 E[|E[Vi |Xi ]||E[(ps (Xi ; Œ¥n )2 + pM L (Xi ; Œ¥n )2 )(ps (Xi ; Œ¥n ) + pM L (Xi ; Œ¥n ))(ps (Xi ; Œ¥n ) ‚àí pM L (Xi ; Œ¥n ))|Xi ]|Ii ]
8Œ¥n‚àí1 E[|E[Vi |Xi ]|E[|ps (Xi ; Œ¥n ) ‚àí pM L (Xi ; Œ¥n )||Xi ]Ii ]

= o(1).
88

As for variance, for l = 0, 1, 2,
Var(

n
1 X
1 ‚àí1
Œ¥ E[Vi2 (ps (Xi ; Œ¥n )l ‚àí pM L (Xi ; Œ¥n )l )2 Ii ]
Vi (ps (Xi ; Œ¥n )l ‚àí pM L (Xi ; Œ¥n )l )Ii ) ‚â§
nŒ¥n
nŒ¥n n
i=1

1 ‚àí1
Œ¥ E[E[Vi2 |Xi ]E[(ps (Xi ; Œ¥n )l ‚àí pM L (Xi ; Œ¥n )l )2 |Xi ]Ii ]
nŒ¥n n
4
‚â§
Œ¥ ‚àí1 E[E[Vi2 |Xi ]Ii ]
nŒ¥n Sn n
= O((nŒ¥n Sn )‚àí1 ),
‚â§

and for l = 3, 4,
Var(

n
1 X
1 ‚àí1
Œ¥ E[Vi2 (ps (Xi ; Œ¥n )l ‚àí pM L (Xi ; Œ¥n )l )2 Ii ]
Vi (ps (Xi ; Œ¥n )l ‚àí pM L (Xi ; Œ¥n )l )Ii ) ‚â§
nŒ¥n
nŒ¥n n
i=1

1 ‚àí1
Œ¥ E[Vi2 Ii ]
nŒ¥n n
= o(1).

‚â§

‚àö
nŒ¥n

We

Pn

s
l
M L (X ; Œ¥ )l )I = o (1) if S ‚Üí ‚àû for l = 0, 1, 2, 3, 4, and
i n
i
p
n
i=1 Vi (p (Xi ; Œ¥n ) ‚àí p
s
l
M
L
l )I = o (1) if n‚àí1/2 S ‚Üí ‚àû for l = 0, 1, 2.
V
(p
(X
;
Œ¥
)
‚àí
p
(X
;
Œ¥
)
i n
i n
i
p
n
i=1 i
1 Pn
s
l
s
next show that nŒ¥n i=1 Vi p (Xi ; Œ¥n ) (Ii ‚àí Ii ) = op (1) if Sn ‚Üí ‚àû for l ‚â• 0. We have

Therefore,
1 Pn

|E[

1
nŒ¥n

n
1 X
Vi ps (Xi ; Œ¥n )l (Iis ‚àí Ii )]| = Œ¥n‚àí1 |E[Vi ps (Xi ; Œ¥n )l (Iis ‚àí Ii )]|
nŒ¥n
i=1

‚â§ Œ¥n‚àí1 E[|E[Vi |Xi ]||E[ps (Xi ; Œ¥n )l (Iis ‚àí Ii )|Xi ]|]
= Œ¥n‚àí1 E[|E[Vi |Xi ]|E[|Iis ‚àí Ii ||Xi ]].
Since Iis ‚àí Ii ‚â§ 0 with strict inequality only if Ii = 1,
E[|Iis ‚àí Ii ||Xi ] = ‚àíE[Iis ‚àí Ii |Xi ]Ii = (1 ‚àí E[Iis |Xi ])Ii = Pr(ps (Xi ; Œ¥n ) ‚àà {0, 1}|Xi )Ii .
We then have
n
1 X
|E[
Vi ps (Xi ; Œ¥n )l (Iis ‚àí Ii )]|
nŒ¥n
‚â§
‚â§

i=1
‚àí1
Œ¥n E[|E[Vi |Xi ]| Pr(ps (Xi ; Œ¥n ) ‚àà {0, 1}|Xi )Ii ]
Œ¥n‚àí1 E[|E[Vi |Xi ]|((1 ‚àí pM L (Xi ; Œ¥n ))Sn + pM L (Xi ; Œ¥n )Sn )Ii ]

Z

1

Z

|E[Vi |Xi = u + Œ¥n vŒΩ‚Ñ¶‚àó (u)]|{(1 ‚àí pM L (u + Œ¥n vŒΩ‚Ñ¶‚àó (u); Œ¥n ))Sn

‚â§
‚àí1

‚àÇ‚Ñ¶‚àó ‚à©N (X ,Œ¥ÃÉ)
ML

+p

‚àó

‚àÇ‚Ñ¶
(u + Œ¥n vŒΩ‚Ñ¶‚àó (u); Œ¥n )Sn }fX (u + Œ¥n vŒΩ‚Ñ¶‚àó (u))Jp‚àí1
œà‚Ñ¶‚àó (u, Œ¥n v)dHp‚àí1 (u)dv,

where the second inequality follows from Lemma B.7. Note that for every (u, v) ‚àà ‚àÇ‚Ñ¶‚àó ‚à©N (X , Œ¥ÃÉ)√ó
(‚àí1, 1), limŒ¥‚Üí0 pM L (u + Œ¥n vŒΩ‚Ñ¶‚àó (u); Œ¥n ) = k(v) ‚àà (0, 1) by Step C.6.3.1 in Section C.6.3. Since
‚àÇ‚Ñ¶‚àó œà ‚àó are bounded, by the Bounded Convergence Theorem,
E[Vi |Xi ], fX and Jp‚àí1
‚Ñ¶
n
1 X
Vi ps (Xi ; Œ¥n )l (Iis ‚àí Ii )]| = o(1)
|E[
nŒ¥n
i=1

89

if Sn ‚Üí ‚àû.
As for variance,
Var(

n
1 X
1 ‚àí1
Œ¥ E[Vi2 ps (Xi ; Œ¥n )2l (Iis ‚àí Ii )2 ]
Vi ps (Xi ; Œ¥n )l (Iis ‚àí Ii )) ‚â§
nŒ¥n
nŒ¥n n
i=1

1 ‚àí1
Œ¥ E[Vi2 |Iis ‚àí Ii |]
nŒ¥n n
1 ‚àí1
=
Œ¥ E[E[Vi2 |Xi ]E[|Iis ‚àí Ii ||Xi ]]
nŒ¥n n
= o(1).
‚â§

Lastly, we show that, for l ‚â• 0,
holds. Let Œ∑n =

n
Œ≥ log
Sn ,

‚àö1
nŒ¥n

Pn

i=1 Vi p

s (X ; Œ¥ )l (I s
i n
i

‚àí Ii ) = op (1) if Assumption 5

where Œ≥ is the one satisfying Assumption 5. We have

n

1 X
|E[ ‚àö
Vi ps (Xi ; Œ¥n )l (Iis ‚àí Ii )]|
nŒ¥n i=1
q
‚â§ nŒ¥n‚àí1 E[|E[Vi |Xi ]|((1 ‚àí pM L (Xi ; Œ¥n ))Sn + pM L (Xi ; Œ¥n )Sn )Ii ]
q
= nŒ¥n‚àí1 E[|E[Vi |Xi ]|((1 ‚àí pM L (Xi ; Œ¥n ))Sn + pM L (Xi ; Œ¥n )Sn ))1{pM L (Xi ; Œ¥n ) ‚àà (0, Œ∑n ) ‚à™ (1 ‚àí Œ∑n , 1)}]
q
+ nŒ¥n‚àí1 E[|E[Vi |Xi ]|((1 ‚àí pM L (Xi ; Œ¥n ))Sn + pM L (Xi ; Œ¥n )Sn ))1{pM L (Xi ; Œ¥n ) ‚àà (Œ∑n , 1 ‚àí Œ∑n )}]
q
‚â§(
sup
|E[Vi |Xi = x]|)( nŒ¥n‚àí1 Pr(pM L (Xi ; Œ¥n ) ‚àà (0, Œ∑n ) ‚à™ (1 ‚àí Œ∑n , 1))
x‚ààN (‚àÇ‚Ñ¶‚àó ,2Œ¥ÃÉ)‚à©N (X ,2Œ¥ÃÉ)

+2

p
nŒ¥n (1 ‚àí Œ∑n )Sn Œ¥n‚àí1 E[1{pM L (Xi ; Œ¥n ) ‚àà (Œ∑n , 1 ‚àí Œ∑n )}]).

p
By Assumption 5, nŒ¥n‚àí1 Pr(pM L (Xi ; Œ¥n ) ‚àà (0, Œ∑n ) ‚à™ (1 ‚àí Œ∑n , 1)) = o(1). For the second term,
p
p
2 nŒ¥n (1 ‚àí Œ∑n )Sn Œ¥n‚àí1 E[1{pM L (Xi ; Œ¥n ) ‚àà (Œ∑n , 1 ‚àí Œ∑n )}] ‚â§ 2 nŒ¥n (1 ‚àí Œ∑n )Sn Œ¥n‚àí1 E[Ii ]
p
= 2 nŒ¥n (1 ‚àí Œ∑n )Sn O(1).
n
log n
1
‚àí1/2 S ‚Üí ‚àû and
Observe that Œ∑n = Œ≥ log
n
Sn = Œ≥ n1/2 n‚àí1/2 Sn ‚Üí 0, since n
t
that e ‚â• 1 + t for every t ‚àà R, we have
p
p
nŒ¥n (1 ‚àí Œ∑n )Sn ‚â§ nŒ¥n (e‚àíŒ∑n )Sn
p
= nŒ¥n e‚àíŒ∑n Sn
p
= nŒ¥n e‚àíŒ≥ log n
p
= nŒ¥n n‚àíŒ≥

= n1/2‚àíŒ≥ Œ¥n1/2
‚Üí 0,

90

log n
n1/2

‚Üí 0. Using the fact

since Œ≥ > 1/2. As for variance,
n
1 X
Var( ‚àö
Vi ps (Xi ; Œ¥n )l (Iis ‚àí Ii )) ‚â§ Œ¥n‚àí1 E[Vi2 ps (Xi ; Œ¥n )2l (Iis ‚àí Ii )2 ]
nŒ¥n i=1

‚â§ Œ¥n‚àí1 E[E[Vi2 |Xi ]E[|Iis ‚àí Ii ||Xi ]Ii ]
= o(1).

We have
Œ≤ÃÇ s ‚àí Œ≤ÃÇ
n
n
n
n
1 X s s 0 s ‚àí1 1 X s s
1 X
1 X
=(
Zi (Di ) Ii )
Zi Yi Ii ‚àí (
Zi D0i Ii )‚àí1
Zi Yi Ii
nŒ¥n
nŒ¥n
nŒ¥n
nŒ¥n
i=1

i=1

i=1

i=1

n
n
n
1 X s s 0 s ‚àí1 1 X s s
1 X
=(
Zi (Di ) Ii ) (
Zi Yi Ii ‚àí
Zi Yi Ii )
nŒ¥n
nŒ¥n
nŒ¥n
i=1

i=1

i=1

n
n
n
n
n
X
1 X s s 0 s ‚àí1 1 X s s 0 s
1 X
1 X
0
0
‚àí1 1
‚àí(
Zi (Di ) Ii ) (
Zi (Di ) Ii ‚àí
Zi Di Ii )(
Zi Di Ii )
Zi Yi Ii .
nŒ¥n
nŒ¥n
nŒ¥n
nŒ¥n
nŒ¥n
i=1
i=1
i=1
i=1
i=1
‚àö
By Step C.6.4.1, Œ≤ÃÇ s ‚àí Œ≤ÃÇ = op (1) if Sn ‚Üí ‚àû, and nŒ¥n (Œ≤ÃÇ s ‚àí Œ≤ÃÇ) = op (1) if Assumption 5 holds.
By proceeding as in Step C.6.3.7 in Section C.6.3, we have
n
n
1 X s 2 s s 0 s
1 X s 2 s s 0 s
(ÀÜ
i ) Zi (Zi ) Ii =
(i ) Zi (Zi ) Ii + op (1),
nŒ¥n
nŒ¥n
i=1

where

si

= Yi ‚àí

(Dsi )0 Œ≤.

i=1

Then, by Step C.6.4.1,

n
n
1 X 2
1 X s 2 s s 0 s
(ÀÜ
i ) Zi (Zi ) Ii ‚àí
i Zi Z0i Ii
nŒ¥n
nŒ¥n

=

1
nŒ¥n

i=1
n
X

i=1

(Yi2 ‚àí 2Yi (Dsi )0 Œ≤ + Œ≤ 0 Dsi (Dsi )0 Œ≤)Zsi (Zsi )0 Iis ‚àí

i=1

n
1 X 2
(Yi ‚àí 2Yi D0i Œ≤ + Œ≤ 0 Di D0i Œ≤)Zi Z0i Ii + op (1)
nŒ¥n
i=1

= op (1)
so that

n
1 X s 2 s s 0 s p
(ÀÜ
i ) Zi (Zi ) Ii ‚àí‚Üí V.
nŒ¥n
i=1

Also,

C.7

1
nŒ¥n

Pn

s
s 0 s
i=1 Zi (Di ) Ii

p

‚àí‚Üí SD by using Step C.6.4.1. The conclusion then follows.

Proof of Proposition A.2

We can prove Part (a) using the same argument in the proof of Proposition 1 (a). For Part (b),
suppose to the contrary that there exists xd ‚àà XdA such that Lpc ({xc ‚àà XcA (xd ) : pM L (xd , xc ) ‚àà
{0, 1}}) > 0. Without loss of generality, assume Lpc ({xc ‚àà XcA (xd ) : pM L (xd , xc ) = 1}) > 0.
The proof proceeds in five steps.
91

Step C.7.1. Lpc (XcA (xd ) ‚à© Xc,1 (xd )) > 0.
Step C.7.2. XcA (xd ) ‚à© int(Xc,1 (xd )) 6= ‚àÖ.
Step C.7.3. pM L (xd , xc ) = 1 for any xc ‚àà int(Xc,1 (xd )).
Step C.7.4. For every x‚àóc ‚àà XcA (xd ) ‚à© int(Xc,1 (xd )), there exists Œ¥ > 0 such that B(x‚àóc , Œ¥) ‚äÇ
XcA (xd ) ‚à© int(Xc,1 (xd )).
Step C.7.5. E[Y1i ‚àí Y0i |Xi ‚àà A] is not identified.
Following the argument in the proof of Proposition 1 (b), we can prove Steps C.7.1‚ÄìC.7.3. Once
Step C.7.4 is established, we prove Step C.7.5 by following the proof of Step C.1.4 in Proposition
1 (b) with B(x‚àóc , Œ¥) and B(x‚àóc , ) in place of B(x‚àó , Œ¥) and B(x‚àó , ), respectively, using the fact
that Pr(Xci ‚àà B(x‚àóc , )|Xdi = xd ) > 0 by the definition of support. Here, we provide the proof
of Step C.7.4.
Proof of Step C.7.4. Pick an x‚àóc ‚àà XcA (xd ) ‚à© int(Xc,1 ). Then, x‚àó = (xd , x‚àóc ) ‚àà A. Since A
is open relative to X , there exists an open set U ‚àà Rp such that A = U ‚à© X . This implies that
for any sufficiently small Œ¥ > 0, B(x‚àó , Œ¥) ‚à© X ‚äÇ U ‚à© X = A. It then follows that {xc ‚àà Rpc :
(xd , xc ) ‚àà B(x‚àó , Œ¥) ‚à© X } ‚äÇ {xc ‚àà Rpc : (xd , xc ) ‚àà A}, equivalently, B(x‚àóc , Œ¥) ‚à© Xc (xd ) ‚äÇ XcA (xd ).
By choosing a sufficiently small Œ¥ > 0 so that B(x‚àóc , Œ¥) ‚äÇ int(Xc,1 (xd )) ‚äÇ Xc (xd ), we have
B(x‚àóc , Œ¥) ‚äÇ XcA (xd ) ‚à© int(Xc,1 (xd )).

C.8

Proof of Theorem A.1

The proof is analogous to the proof of Theorem 1. The only difference is that, when we prove the
convergence of expectations, we show the convergence of the expectations conditional on Xdi ,
and then take the expectations over Xdi .

D

Machine Learning Simulation: Details

Parameter Choice. For the variance-covariance matrix Œ£ of Xi , we first create a 100 √ó 100
symmetric matrix V such that the diagonal elements are one, Vij is nonzero and equal to
Vji for (i, j) ‚àà {2, 3, 4, 5, 6} √ó {35, 66, 78}, and everything else is zero. We draw values from
Unif(‚àí0.5, 0.5) independently for the nonzero off-diagonal elements of V. We then create matrix
Œ£ = V √ó V, which is a positive semidefinite matrix.
For Œ±0 and Œ±1 , we first draw Œ±ÃÉ0j , j = 51, ..., 100, from Unif(‚àí100, 100) independently
across j, and draw Œ±ÃÉ1j , j = 1, ..., 100, from Unif(‚àí150, 200) independently across j. We then
set Œ±ÃÉ0j = Œ±ÃÉ1j for j = 1, ..., 50, and calculate Œ±0 and Œ±1 by normalizing Œ±ÃÉ0 and Œ±ÃÉ1 so that
Var(Xi0 Œ±0 ) = Var(Xi0 Œ±1 ) = 1.
Training of Prediction Model. We first randomly split the sample {(YÃÉi , XÃÉi , DÃÉi , ZÃÉi )}nÃÉi=1 into
train (80%) and test datasets (20%). We use random forests on the training sample to obtain
92

the prediction model ¬µz and validate its performance on the test sample. The trained algorithm
has an accuracy of 97% on the test data.

E
E.1

Empirical Policy Application: Details
Hospital Cost Data

We use publicly available Healthcare Cost Report Information System (HCRIS) data,40 to
project41 safety net eligibility and funding amounts for all hospitals in the dataset. This data
set contains information on various hospital characteristics including utilization, number of employees, medicare cost data and financial statement data.
The data is available from financial year 1996 to 2019. As the coverage is higher for 2018
(compared to 2019), we utilize the data corresponding to the 2018 financial year. Hospitals are
uniquely identified in a financial year by their CMS (Center for Medicaid and Medicare Services)
Certification Number. We have data for 4,705 providers for the 2018 financial year. We focus
on 4,648 acute care and critical access hospitals that are either located in one of the 50 states or
Washington DC.

Disproportionate patient percentage
Disproportionate patient percentage is equal to the percentage of Medicare inpatient days attributable to patients eligible for both Medicare Part A and Supplemental Security Income (SSI)
summed with the percentage of total inpatient days attributable to patients eligible for Medicaid
but not Medicare Part A.42 In the data, this variable is missing for 1560 hospitals. We impute
the disproportionate patient percentage to 0 when it is missing.

Uncompensated care per bed
Cost of uncompensated care refers to the care provided by the hospital for which no compensation
was received from the patient or the insurer. It is the sum of a hospital‚Äôs bad debt and the
financial assistance it provides.43 The cost of uncompensated care is missing for 86 hospitals,
which we impute to 0. We divide the cost of uncompensated care by the number of beds in the
hospital to obtain the cost per bed. The data on bed count is missing for 15 hospitals, which we
drop from the analysis, leaving us with 4,633 hospitals in 2,473 counties.
40

We use the RAND cleaned version of this dataset, which can be accessed https://www.hospitaldatasets.
org/
41
We use the methodology detailed in the CARE ACT website to project funding based on 2018 financial year
cost reports.
42
For the precise definition, see https://www.cms.gov/Medicare/Medicare-Fee-for-Service-Payment/
AcuteInpatientPPS/dsh.
43
The
precise
definition
can
be
found
at
https://www.aha.org/fact-sheets/
2020-01-06-fact-sheet-uncompensated-hospital-care-cost.

93

Profit Margin
Hospital profit margins are indicative of the financial health of the hospitals. We calculate profit
margins as the ratio of net income to total revenue where total revenue is the sum of net patient
revenue and total other income. After the calculation, profit margins are missing for 92 hospitals,
which we impute to 0.

Funding
We calculate the projected funding using the formula on the CARES ACT website. Hospitals that
do not qualify on any of the three dimensions are not given any funding. Each eligible hospital
is assigned an individual facility score, which is calculated as the product of disproportionate
patient percentage and number of beds in that hospital. We calculate cumulative facility score
as the sum of all individual facility scores in the dataset. Each hospital receives a share of $10
billion, where the share is determined by the ratio of individual facility score of that hospital to
the cumulative facility score. The amount of funding received by hospitals is bounded below at
$5 million and capped above at $50 million.

E.2

Hospital Utilization Data

We use the publicly available COVID-19 Reported Patient Impact and Hospital Capacity by
Facility dataset for our outcome variables. This provides facility level data on hospital utilization
aggregated on a weekly basis, from July 31st onwards. These reports are derived from two
main sources ‚Äì (1) HHS TeleTracking and (2) reporting provided directly to HHS Protect by
state/territorial health departments on behalf of health care facilities.44
The hospitals are uniquely identified for a given collection week (which goes from Friday to
Thursday) by their CMS Certification number. All hospitals that are registered with CMS by
June 1st 2020 are included in the population. We merge the hospital cost report data with the
utilization data using the CMS certification number. According to the terms and conditions of
the CARES Health Care Act, the recipients may use the relief funds only to ‚Äúprevent, prepare
for, and respond to coronavirus‚Äù and for ‚Äúhealth care related expenses or lost revenues that
are attributable to coronavirus‚Äù. Therefore, for our analysis we focus on 4 outcomes that were
directly affected by COVID-19, for the week spanning July 31st to August 6th 2020. The outcome
measures are described below.
1. Total reports of patients currently hospitalized in an adult inpatient bed who have laboratoryconfirmed or suspected COVID-19, including those in observation beds reported during the
7-day period.
2. Total reports of patients currently hospitalized in an adult inpatient bed who have laboratoryconfirmed COVID-19 or influenza, including those in observation beds. Including patients
who have both laboratory-confirmed COVID-19 and laboratory confirmed influenza during
the 7-day period.
44

Source:
anag-cw7u.

https://healthdata.gov/Hospital/COVID-19-Reported-Patient-Impact-and-Hospital-Capa/

94

3. Total reports of patients currently hospitalized in a designated adult ICU bed who have
suspected or laboratory-confirmed COVID-19.
4. Total reports of patients currently hospitalized in a designated adult ICU bed who have
laboratory-confirmed COVID-19 or influenza, including patients who have both laboratoryconfirmed COVID-19 and laboratory-confirmed influenza.
In the dataset, when the values of the 7 day sum are reported to be <4, they are replaced with
-999,999. We recode these values to missing.

E.3

Computing Quasi Propensity Score

As the three determinants of safety net eligibility are continuous variables, we can think of this
setting as a multi-dimensional regression discontinuity design and a suitable setting to apply our
method. In this setting, the Xi are disproportionate patient percentage, uncompensated care
per bed and profit margin. Funding eligibility (Zi ) is determined algorithmically using these 3
dimensions. Di is the amount of funding received by hospital i, which depends on both safety
net eligibility status Zi , number of beds in the hospital, and disproportionate patient percentage.
Before calculating QPS, we normalize each characteristic of Xi to have mean 0 and variance 1.
For each hospital and every Œ¥ ‚àà {0.01, 0.025, 0.05, 0.075, 0.1, 0.25, 0.5}, we draw 1000 times from
a Œ¥-ball around the normalized covariate space and calculate QPS by averaging funding eligibility
Zi over these draws.

95

