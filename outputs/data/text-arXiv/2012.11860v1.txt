Efficient and Visualizable Convolutional Neural Networks for COVID-19 Classification
Using Chest CT
Aksh Garga (akshgarg@gmail.com), Sana Salehib (sana.salehi@med.usc.edu), Marianna La
Roccaa (marianna.larocca@loni.usc.edu), Rachael Garnera (rachael.garner@loni.usc.edu),
Dominique Duncana (dominique.duncan@loni.usc.edu)
a

Laboratory of Neuro Imaging, USC Stevens Neuroimaging and Informatics Institute, Keck
School of Medicine, University of Southern California, 2025 Zonal Avenue, Los Angeles,
California, USA
b

Department of Radiology, Keck School of Medicine, University of Southern California, 1975
Zonal Avenue, Los Angeles, CA, USA
ABSTRACT
The novel 2019 coronavirus disease (COVID-19) has infected over 65 million people worldwide
as of December 4, 2020, pushing the world to the brink of social and economic collapse. With
cases rising rapidly, deep learning has emerged as a promising diagnosis technique. However,
identifying the most accurate models to characterize COVID-19 patients is challenging because
comparing results obtained with different types of data and acquisition processes is non-trivial.
In this paper, we evaluated and compared 40 different convolutional neural network architectures
for COVID-19 diagnosis, serving as the first to consider the EfficientNet family for COVID-19
diagnosis. EfficientNet-B5 is identified as the best model with an accuracy of 0.9931Â±0.0021, F1
score of 0.9931Â±0.0020, sensitivity of 0.9952Â±0.0020, and specificity of 0.9912Â±0.0048.
Intermediate activation maps and Gradient-weighted Class Activation Mappings offer humaninterpretable evidence of the modelâ€™s perception of ground-class opacities and consolidations,
hinting towards a promising use-case of artificial intelligence-assisted radiology tools.
Keywords: Computed Tomography, Convolutional Neural Networks, COVID-19, Deep
Learning, EfficientNets, Gradient-weighted Class Activation Maps, Intermediate Activation
Maps

1

INTRODUCTION
The novel 2019 coronavirus disease, COVID-19, has infected over 65 million people
worldwide and claimed over 1.5 million lives as of December 4, 2020 [1]. The universal
transmission of COVID-19 has put a large portion of the worldâ€™s population in quarantine,
disrupted economies, and ravaged traditional educational architectures, pushing the world into a
social and financial crisis. With common symptoms ranging from fever, dry cough, shortness of
breath, fatigue, dizziness, chills, headaches, loss of appetite, loss of smell, and body ache to more
severe manifestations like delirium, seizures, sepsis and stroke, COVID-19 has quickly proven to
be one of the most dangerous diseases to plague human civilization [2]. Moreover, the lack of
clinical vaccines and precise drug prescription practices makes effectively fighting the disease
extremely challenging, forcing us to rely solely upon social distancing to stop its spread.
The reverse transcription-polymerase chain reaction, RT-PCR, is currently considered to
be the gold-standard for COVID-19 diagnosis. However, the rapid increase in COVID-19 cases,
delay in obtaining PCR results, and strict requirements for testing environments make the fast
and effective screening of suspected cases challenging [3]. Moreover, PCRâ€™s low sensitivity, or
high false-negative rate, results in many COVID-19 positive patients incorrectly being diagnosed
as negative, further exacerbating the disease spread. In particular, a recent study by Feng et al.
revealed a specificity of 71% for PCR tests, much lower than targeted specificity for effectively
containing the spread of the virus [4].
Radiological imaging via X-ray and computed tomography (CT) has emerged as a
promising alternative form of diagnosis due to its ability to visualize lung structures. Imaging
already serves as a quintessential factor by triaging confirmed COVID-19 cases on the basis of
the severity of lung involvement [5]. However, manual readings of scans are prone to error and

2

time-consuming. The use of machine learning (ML) and artificial intelligence (AI) algorithms
that can learn from data without the need for explicit programming offers a promising avenue for
meeting the high costs and radiologist shortages surrounding CT imaging. While human readings
of CT scans can take upwards of 15 minutes, ML-based algorithms can analyze images within a
few seconds. Moreover, with developments in computer vision and computational resources,
state-of-the-art convolutional neural network (CNN) architectures may reach specificities,
sensitivities, and accuracies of as high as 0.992, 1.00, and 0.995, respectively in distinguishing
between COVID-19 and Non-COVID-19 lung CT images [3].
Since the introduction of deep learning-based techniques for COVID-19 in a work by Wu
et al. [6], several works have been dedicated to evaluating their efficacy. For example, Butt et al.
[7] considered the use of ResNet18 attaining an accuracy of 0.867, sensitivity of 0.815, precision
of 0.808, and F1 score of 81.1. In [8] and [9], Jin et al. trained and evaluated ResNet152, DPN92, Inception-v3, ResNet50, and Attention ResNet-50 with U-Net++, reaching accuracies and
sensitivities as high as 94.98 and 94.06, respectively. Works like those by Yousefzadeh et al.
[10] and Ardakani et al. [11] extended these efforts further by collectively training DenseNets,
Xception, EfficientNetB0, AlexNet, VGG-16, VGG-19, SqueezeNet, GoogleNet, and
MobileNet-V2 for COVID-19 diagnosis, reaching sensitivities as high as 1.00 and accuracies as
high as 0.9951 for diagnosis.
With the diverse array of available models for diagnosis, identifying the most optimal has
become a valued yet incredibly challenging task. Although several literature reviews consider the
use of ML and AI for COVID-19 diagnosis and severity assessment, they presented models
trained on different datasets, evaluated with varying metrics, and incomparable standard errors
[3,12â€“16]. In contrast, our paper presents 40 ML models trained on a fixed dataset, evaluates

3

their performance through metrics such as specificity, sensitivity, accuracy, F-1 scores, and
applies visualization techniques such as Gradient-weighted Class Activation Mappings
(GradCAMs) and intermediate activation maps to highlight core features such as ground-glass
opacities, consolidations, crazy paving patterns, and linear opacities in the input CT images that
the model used for making predictions.
EfficientNets, with their markedly smaller network sizes and extremely high accuracies
in the ImageNet dataset, [17] have rapidly become a go-to choice for image-recognition tasks
with ML. However, to the best of our knowledge, this paper is the first to consider the entire
EfficientNet family of CNN architectures for diagnosis on CT images. Although a limited
number of studies have directed their attention to this nascent CNN architecture, they restrict
their consideration to X-ray images [18â€“20]. While X-ray radiography is cheaper and more
universally accessible, CT imaging is preferred over X-ray for diagnoses because of its detailed
cross-sectional images [21,22]. Moreover, the yet fewer studies which train EfficientNets on CT
Scan images limit their study to EfficientNet B0, EfficientNet B1, EfficientNet B4 due to
computational limitations and a focus on other network architectures, leaving the remaining
models EfficientNet B2, EfficientNet B3, EfficientNet B5, EfficientNet B6, and EfficientNet B7
mostly unexplored in terms of their COVID-19 diagnosing abilities [12,23]. Given that many of
the larger EfficientNet architectures acquire the highest accuracy on the ImageNet dataset [24],
this study includes them for comparative purposes and hopes of attaining higher performance.
In addition to the broad base of CNN architectures considered, this paper is the first to
visualize intermediate activation maps for COVID-19 diagnosis. Although the conventional
visualization framework â€”GradCAMsâ€” are useful for localizing abnormalities in input images,
they do not offer insight into the modelâ€™s learning process. In contrast, intermediate activations

4

help understand how successive CNN filters transform their inputs and get a more thorough
understanding of individual CNN filters and the model learning behavior [24,25].
A final goal of this paper is to note whether certain model families, such as EfficientNet,
ResNet, Xception, Inception, DenseNet, VGG, and their characteristics such as residual linkages,
depthwise and pointwise convolutions are better suited to the classification task.
2. PROCEDURES
2.1 Dataset
A dataset containing 1252 CT Scan images of 60 COVID-positive patients and 1230 CT Scan
images of 60 COVID-negative patients was obtained from the SARS-COV-2 CT Scan Dataset
on Kaggle [26,27]. The images were collected from patients in Sao Paulo, Brazil, and made
freely accessible through Kaggle by Soares E. et al. [26] The authors reported a pre-obtained F-1
score of 97.31% on the validation set with a [80,20] split using a custom-designed Explainable
Deep Neural Network (xDNN) architecture, which serves as the benchmark of comparison to the
models we trained. The data may also be found on COVID-ARC (https://covid-arc.loni.usc.edu/)
[28], a data archive of multimodal and longitudinal data related to COVID-19.
2.2 Data Preprocessing
Data preprocessing is an essential step in ML because a model learns to recognize patterns based
on the data that it receives. Therefore, the quality of data significantly impacts the quality of the
results obtained. First, to uniformize the ML pipeline in this study, all input images to the CNN
were reshaped into (380, 380, 1) images. Next, the model was separated into training and testing
sets using a [80,20] split in 5-fold cross-validation. Finally, several augmentation techniques
such as random rotations, skews, sheers, flips, and gaussian noise were applied to the training
images to prevent the model from overfitting and ensure that the CNN can generalize results.

5

2.3 Model Development and Training
A total of 40 models were trained and evaluated for the purposes of this study. These models
were derived from the following base models: EfficientNet B0 [29], EfficientNet B1 [29],
EfficientNet B2 [29], EfficientNet B3 [29], EfficientNet B4 [29], EfficientNet B5 [29],
EfficientNet B6 [29], EfficientNet B7 [29], ResNet 50 [30], ResNet 50V2 [31], ResNet 101V2
[30], ResNet 152V2 [30], InceptionV3 [32], InceptionResNetV2 [33], Xception [34], DenseNet
121 [35], DenseNet 169 [35], DenseNet 201 [35], VGG16 [36], and VGG19 [36]. All of these
models have previously been trained on the ImageNet dataset [17,37]. The models used in this
work can be subdivided into two major categories: Base Models and Modified Networks.
2.3.1 CATEGORY 1: BASE MODELS
For the base models, the model architecture from the state-of-the-art architectures outlined above
was retained, i.e., the number and type of layers were kept the same except that the final output
SoftMax layer was changed to a 2-dimensional versus the 1000-dimensional node. Even though
the network architecture was kept the same, the entire model weights were trained from scratch,
differentiating our procedure from the several existing studies that employ transfer learning. It is
worth specifying that, for convenience sake, these base models are indicated with the name of
the parent model from which they were derived. For instance, a base model derived from its
parent model ResNet50 will simply be indicated with ResNet50.
2.3.1.1 EfficientNets: Proposed by Mingxing Tan and Quoc V. Le [29], EfficientNets have
quickly revolutionized the current standing of computer vision, providing not only high accuracy
results, but attaining them with computational complexities orders of magnitude (8.4x smaller
and 6.1 x faster) lower than the best ConvNets. They proposed a novel ConvNet scaling
framework, adjusting the width (the number of channels in network layers), depth (number of

6

layers in the CNN), and resolution (the input image size into the model) systematically.
Particularly, if
ğ‘‘ğ‘’ğ‘ğ‘¡â„, ğ‘‘ = ğ›¼ ğœ™ , ğ‘¤ğ‘–ğ‘‘ğ‘¡â„ ğ‘¤ = ğ›½ ğœ™ ; ğ‘ğ‘›ğ‘‘ ğ‘Ÿğ‘’ğ‘ ğ‘œğ‘™ğ‘¢ğ‘¡ğ‘–ğ‘œğ‘› ğ‘Ÿ = ğ›¾ ğœ™ , ğ‘ . ğ‘¡. ğ›¼ âˆ— ğ›½ 2 âˆ— ğ›¾ 2 â‰ˆ 2 ,
then the model may be scaled by adjusting based on the available computational resources.
Given the need for rapid diagnosis and limited computational capacities for obtaining them, we
hypothesized EfficientNets would be the most promising form of diagnosis.
2.3.1.2 ResNets: Proposed by Kaiming He et al. [30], ResNets are arguably the most popular
CNN architectures for image recognition tasks today. Their strength, the residual learning
framework capable of transmitting gradients despite great depths by skip connections and batchnormalization, has rapidly been applied into numerous modern architectures today. This paper
considers four types of ResNet architectures: ResNet50, ResNet50V2, ResNet101V2, and
ResNet152V2 (Note: for this paper, InceptionResNetV2 is grouped in the InceptionNet category
along with InceptionV3 and Xception)
2.3.1.3 DenseNets: Proposed by Huang et al., [35] DenseNets introduce the idea of connecting
every layer to every other layer in a feed-forward fashion. This, in turn, allows them to avoid the
vanishing-gradient problem, reduce training parameters, and improve feature transmission [35].
Given their clever design, they are yet another popular CNN of choice. This study considers 3
DenseNet models: DenseNet121, DenseNet169, and DenseNet201 in order of increasing
parameter size.
2.3.1.4 InceptionNets & Xception: Proposed by Svegedy et al. [38], InceptionNets advance the
concept of building CNNs using blocks instead of just convolutional layers, a framework most
modern networks utilize. Moreover, they decomposed convolutional operations into spatially
separable ones for improved computational resources utilization, increasing both the depth and
width of the model while keeping computational costs static. In a subsequent study by Svegedy
7

et al. [32], they jointly capitalized on InceptionNetâ€™s module-based architecture and the residual
connections from ResNets to propose InceptionResNetV2, a powerful model combining the best
features from ResNet and InceptionNet. Finally, FranÃ§ois Chollet [34] expanded upon
InceptionNets in his work Xception: Deep Learning with Depthwise Separable Convolutions,
replacing inception modules with depthwise separable convolutions (a depthwise convolution
and then a pointwise convolution). This work examines two models from the InceptionNet class
(InceptionNetV2, InceptionResNetV2) and Xception.
2.3.1.5 VGG: Proposed by Karen Simonyan and Andrew Zisserman [36], VGGâ€™s primary
contribution was to experiment with increasing model depth and seeing its impact on model
performance. We trained two forms of VGGs: VGG16 and VGG19.
2.3.2 CATEGORY 2: MODIFIED NETS
For the Modified Nets, the base model architectures were augmented by removing the final
SoftMax layer and appending a short multi-layer perceptron near the end. Specifically, the
Dense-2 layer was replaced with a block composed of Dense-128, Dropout, Dense-64, Dense-32,
and Dense-2 layers so that the increasingly dense network complexity could improve data
description and model performance. As with category 1, these models were retrained using the
CT images. For convenience sake, the modified networks are referred to by the stem â€œModified-â€
in front of the parent model name, i.e., a modified model developed from EfficientNetB0 will
have the name Modified-EfficientNetB0.
2.4 Model Evaluation
Each model was trained and validated by running ten rounds of 5-fold cross-validation. The
accuracy, specificity, sensitivity or recall, precision, F-1 scores were subsequently found, and
confusion matrices were generated. These metrics were averaged over all the rounds, and their

8

expected values were presented within a 95% confidence interval. A detailed description of each
of the metrics follows below:
ğ´ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¦ =

ğ‘‡ğ‘ƒ + ğ‘‡ğ‘
ğ‘‡ğ‘ƒ + ğ‘‡ğ‘ + ğ¹ğ‘ƒ + ğ¹ğ‘

ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› =
ğ‘†ğ‘ğ‘’ğ‘ğ‘–ğ‘“ğ‘–ğ‘ğ‘–ğ‘¡ğ‘¦ =

ğ‘‡ğ‘ƒ
ğ‘‡ğ‘ƒ + ğ¹ğ‘ƒ
ğ‘‡ğ‘
ğ‘‡ğ‘ + ğ¹ğ‘ƒ

ğ‘†ğ‘’ğ‘›ğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘–ğ‘¡ğ‘¦ ğ‘œğ‘Ÿ ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™ =
ğ¹1 ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’ = 2 âˆ—

ğ‘‡ğ‘ƒ
ğ‘‡ğ‘ƒ + ğ¹ğ‘

ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› âˆ— ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™
ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› + ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™

Where,
1) True Positive (TP) represents a scan from a COVID-19 positive patient being correctly
classified by the network as being COVID-19 positive.
2) True Negative (TN) represents a scan from a COVID-19 negative patient being correctly
classified by the network as being COVID-19 negative.
3) False Positive (FP) represents a scan from a COVID-19 negative patient being incorrectly
classified by the network as being COVID-19 positive.
4) False Negative (FN) represents a scan from a COVID-19 positive patient being
incorrectly classified by the network as being COVID-19 negative.
Several factors were considered when identifying the more appropriate metrics by which to rank
the model performance. In the case of COVID-19 diagnosis, failing to classify a COVID-19
patient as having the disease allows the disease to spread rapidly, exposing a greater number of
patients at risk. In contrast, if a COVID-19 negative patient is classified as positive, the error,
albeit time and cost-invasive, may easily be corrected in subsequent testing through PCR.

9

Therefore, while all results are presented, this paper attributes the greatest emphasis on the
sensitivity, i.e., the modelâ€™s ability to diagnose a COVID-19 positive patient as having the
disease correctly.
2.5 Visualization
Intermediate Activation Maps and GradCAMs [39] were used to identify which portions of
the images the model is using to make diagnoses. These visualizations become especially
important when considering the general stigma against ML and CNNâ€™s black-box nature. By
offering human-interpretable insight into the procedures, the model performs while making
diagnoses, they effectively increase the chances of being received favorably by human
evaluators. Figure 1 summarizes the overall ML pipeline adopted in this paper.

Figure 1: A summary of the overall ML Pipeline. 1250 COVID-19 and 1230 Normal Images are used to train 40 different
CNNs. The results are generated in 5-fold cross-validation with a [80,20] split and averaged over all rounds to obtain network
performance. Finally, GradCAMs and intermediate activate maps are generated.

2.5.1 GradCAMs
The GradCAMs were computed using the process outlined in [39]. First, we found the neuron
importance weights:
10

ğœ”ğ‘˜ğ‘ =

1
ğœ•ğ‘Œ ğ‘
âˆ‘âˆ‘ ğ‘˜
ğ‘
ğœ•ğ´ğ‘–,ğ‘—
ğ‘–

ğ‘—

Where ğ´ğ‘˜ğ‘–,ğ‘— represents the activation map of the ğ‘˜th filter of a convolutional layer and ğ‘Œ ğ‘
represents the probability of classifying class c. These weights were then combined with the
forward activation maps in a weighted manner and then passed through a ReLU filter to obtain
the class discriminative saliency maps for the targeted image c.
ğ¿ğ‘ğ‘–.ğ‘— = ğ‘…ğ‘’ğ¿ğ‘ˆ âˆ— âˆ‘ ğœ”ğ‘˜ğ‘ ğ´ğ‘˜ğ‘–,ğ‘—
ğ‘˜

After generating the GradCAMs, we computed the mean intensities of the RGB-pixels of the
generated heatmaps corresponding to regions associated with high neuron importance weights
that we found by calculating the mean pixel intensity value for areas in the entire heatmap image
with intensities one standard deviation above the mean as part of the mask.
The masks were first found using the base modelâ€™s heatmap image as a template. The
generated masks were then applied to both the base model and the modified-network to compare
localization and visualization abilities between the base models and modified-networks.
2.5.2 Feature Map and Filter Visualization (Intermediate Activation Maps)
To visualize intermediate feature maps for the network, we simply iterated through layers in
between the input and output layer of the CNN and extracted the pixel values from each filterâ€™s
outputs. Over time the model learned more specific features from the images, moving from
output representations like the input image to gradually towards drastically different ones.
Intermediate activation maps operated from the principle that each filter in a CNN learns
different features. For example, the first filter in the opening layer may be detecting vertical
edges, horizontal edges, or color gradients. As one moves deeper into the network, the results
from applying the filter from the preliminary layer generated a new array of pixels representing a

11

new â€œintermediateâ€ image. This is passed to the filter of the next layer, and over time the model
can learn more complex embeddings. However, the filters themselves were not manually
designed but instead learned by the model through the training process. Thus, they offered a
tremendous amount of insight into how the steps undertaken by a model from going from an
input image to its final classification.
3. RESULTS
3.1 Category 1: Base Networks. A detailed summary report, including tables and histograms for
each model family, is included in the supplementary material. This section presents the most
optimal networks for accuracy, sensitivity, specificity, precision, and F-1 scores for each model
family.
3.1.1 EfficientNets: EfficientNet-B5 was found to have the highest F-1 scores, accuracy, and
sensitivity of all EfficientNet models, with a F1 score of 0.9909 (95% CI: (0.9880, 0.9938)),
accuracy of 0.9911 (95% CI: (0.9883, 0.9939)), and sensitivity of 0.9890 (95% CI: (0.9833,
0.9947)). On the other hand, EfficientNet-B4 with significantly less parameters (17,677,402
parameters) obtained the greatest precision (0.9930; 95% CI: (0.9892, 0.9967)) and specificity
(0.9932; 95% CI: (0.9897, 0.9968)).
3.1.2 ResNets: ResNet50 was seen to outperform all other models from this family in all metrics,
achieving a F1 score of 0.9894 (95% CI: (0.9859, 0.9928)), accuracy of 0.9895 (95% CI:
(0.9861, 0.9929)), sensitivity of 0.9905 (95% CI: (0.9848, 0.9963)), precision of 0.9883 (95%
CI: (0.9839, 0.9928)), and specificity of 0.9886 (95% CI: (0.9842, 0.9930)).
3.1.3 DenseNet: DenseNet201 was seen to outperform all other models from the family,
achieving a F1 score of 0.9928 (95% CI: (0.9862, 0.9994)), accuracy of 0.9930 (95% CI:

12

(0.9869, 0.9991)), sensitivity of 0.9918 (95% CI: (0.9861, 0.9975)), precision of 0.9883 (95%
CI: (0.9851, 1.0025)), and specificity of 0.9941 (95% CI: (0.9862, 1.002)).
3.1.4 InceptionNet: Xception outperformed both InceptionV3 and InceptionResNetV2 of every
metric, obtaining a F1 score of 0.9898 (95% CI: (0.9862, 0.9994)), accuracy of 0.9899 (95% CI:
(0.9868, 0.9991)), sensitivity of 0.9874 (95% CI: (0.9861, 0.9975)), precision of 0.9923 (95%
CI: (0.9851, 1.0025)), and specificity of 0.9924 (95% CI: (0.9862, 1.002)).
3.1.5 VGG: VGG-16 outperformed VGG-19 in all metrics obtaining a F1 score of F1 score of
0.9659 (95% CI: (0.9596, 0.9722)), accuracy of 0.9663 (95% CI: (0.9599, 0.9727)), sensitivity of
0.9691 (95% CI: (0.9569, 0.9814)), precision of 0.9631 (95% CI: (0.9521, 0.9742)), and
specificity of 0.9642 (95% CI: (0.9534, 0.9749)).
3.1.6 CATEGORY 1 SUMMARY
Table 1 presents the evaluation metrics for all the models, and figure 2 shows a comparison of
the best models from each family. We note that EfficientNetB5 emerged as the top-performing
model regarding the F1 score, accuracy, and sensitivity. On the other hand, we see that
DenseNet201 claimed the 1st place spot in the case of specificity and precision.
#

Model

F1-Score

Accuracy

Sensitivity

Precision

Specificity

# of
Params

0

DenseNet121

0.9894 Â± 0.0035

0.9895 Â± 0.0036

0.9894 Â± 0.0036

0.9895 Â± 0.0061

0.9895 Â± 0.0061

7.0M

1

DenseNet169

0.9905 Â± 0.0065

0.9907 Â± 0.0063

0.9917 Â± 0.0104

0.9894 Â± 0.0091

0.9897 Â± 0.0089

12.6M

2

DenseNet201

0.9928 Â± 0.0066

0.9930 Â± 0.0061

0.9918 Â± 0.0057

0.9938 Â± 0.0087

0.9941 Â± 0.0079

18.3M

3

EfficientNetB0

0.9861 Â± 0.0053

0.9864 Â± 0.005

0.9836 Â± 0.0045

0.9887 Â± 0.0080

0.9893 Â± 0.0073

4.0M

4

EfficientNetB1

0.9886 Â± 0.0051

0.9888 Â± 0.0049

0.9851 Â± 0.0062

0.9921 Â± 0.0060

0.9924 Â± 0.0057

6.5M

5

EfficientNetB2

0.9902 Â± 0.0025

0.9903 Â± 0.0025

0.9886 Â± 0.0029

0.9918 Â± 0.0033

0.9921 Â± 0.0031

7.7M

6

EfficientNetB3

0.9894 Â± 0.0028

0.9895 Â± 0.0028

0.9878 Â± 0.0044

0.9910 Â± 0.0035

0.9912 Â± 0.0033

10.8M

7

EfficientNetB4

0.9909 Â± 0.0029

0.9911 Â± 0.0028

0.9890 Â± 0.0057

0.9930 Â± 0.0038

0.9932 Â± 0.0036

17.7M

8

EfficientNetB5

0.9931 Â± 0.0020

0.9931 Â± 0.0021

0.9952 Â± 0.0035

0.9911 Â± 0.0048

0.9912 Â± 0.0048

28.5M

9

EfficientNetB6

0.9878 Â± 0.0051

0.9881 Â± 0.0046

0.9895 Â± 0.0049

0.9862 Â± 0.0086

0.9871 Â± 0.0072

41M

10

EfficientNetB7

0.9904 Â± 0.0050

0.9904 Â± 0.0050

0.9942 Â± 0.0032

0.9867 Â± 0.0104

0.9867 Â± 0.0103

64.1M

11

InceptionResNetV2

0.9870 Â± 0.0038

0.9869 Â± 0.0039

0.9849 Â± 0.0042

0.9892 Â± 0.0054

0.9888 Â± 0.0059

54.3M

12

InceptionV3

0.9885 Â± 0.0023

0.9887 Â± 0.0024

0.9857 Â± 0.0047

0.9914 Â± 0.0039

0.9913 Â± 0.0042

21.8M

13

ResNet101V2

0.9861 Â± 0.0028

0.9861 Â± 0.0029

0.9883 Â± 0.0034

0.9839 Â± 0.0047

0.9840 Â± 0.0048

42.6M

13

14

ResNet152V2

0.9823 Â± 0.0026

0.9824 Â± 0.0026

0.9799 Â± 0.0060

0.9849 Â± 0.0037

0.9850 Â± 0.0037

58.3M

15

ResNet50

0.9894 Â± 0.0035

0.9895 Â± 0.0034

0.9905 Â± 0.0057

0.9883 Â± 0.0044

0.9886 Â± 0.0044

23.6M

16

ResNet50V2

0.9862 Â± 0.0020

0.9863 Â± 0.0021

0.9895 Â± 0.0025

0.9831 Â± 0.0037

0.9832 Â± 0.0037

23.6M

17

VGG16

0.9659 Â± 0.0063

0.9663 Â± 0.0064

0.9691 Â± 0.0123

0.9631 Â± 0.0110

0.9642 Â± 0.0107

14.7M

18

VGG19

0.9407 Â± 0.0066

0.9406 Â± 0.0081

0.9513 Â± 0.0097

0.9307 Â± 0.0161

0.9301 Â± 0.018

20.0M

19

Xception

0.9898 Â± 0.0022

0.9899 Â± 0.0022

0.9874 Â± 0.0036

0.9923 Â± 0.0018

0.9924 Â± 0.0018

20.9M

Table 1: Summary of results from all base models. The best performing model in each metric is highlighted in green. We note
that of all the models trained in the first category for 25 epochs, EfficientNet-B5 obtained the highest F1 score, accuracy, and
specificity, whereas DenseNet attains the highest precision and specificity. Given that the models are trained for COVID-19
diagnosis, for which the authors believe sensitivity is the most critical metric, EfficientNet-B5 was deemed the best performing
model.

Figure 2: Comparison of the best models from each model family, presented in order of increasing sensitivity. We note that
EfficientNet provides the highest sensitivities, followed by DenseNet201, ResNet50, Xception, and VGG16.

3.2 CATEGORY 2: Modified Networks
This section presents the results from each modified network:
3.2.1 MODIFIED EFFICIENTNETS: Modified-EfficientNet-B5 performed best in terms of F1

scores, accuracy, and sensitivity, obtaining a F1 score of 0.9905 (95% CI: (0.9886, 0.9924)),
accuracy of 0.9906 (95% CI: (0.9888, 0.9925)), sensitivity of 0.9913 (95% CI: (0.9883, 0.9942)),
precision of 0.9898 (95% CI: (0.9855, 0.9942)), and specificity of 0.99 (95% CI: (0.986,

14

0.9941)). On the other hand, Modified-EfficientNetB4 performed best in terms of precision and
specificity, attaining F1 score of 0.99 (95% CI: (0.987, 0.993)), accuracy of 0.9901 (95% CI:
(0.9871, 0.9931)), sensitivity of 0.9871 (95% CI: (0.9834, 0.9908)), precision of 0.9929 (95%
CI: (0.9896, 0.9963)), and specificity of 0.993 (95% CI: (0.9896, 0.9964)).
3.2.2 MODIFIED-RESNETS: Modified ResNet50 outperformed other models in the family on
all metrics, obtaining a F1 score of 0.9875 (95% CI: (0.9824, 0.9927)), accuracy of 0.9875 (95%
CI: (0.9823, 0.9928)), sensitivity of 0.9875 (95% CI: (0.9812, 0.9937)), precision of 0.9877
(95% CI: (0.9812, 0.9941)), and specificity of 0.9873 (95% CI: (0.9802, 0.9945)).
3.2.3 MODIFIED-DENSENETS: Modified DenseNet-121 performed best in terms of F1 score,
accuracy, sensitivity, precision, and specificity, obtaining a F1 score of 0.9893 (95% CI: (0.9853,
0.9932)), accuracy of 0.9893 (95% CI: (0.9853, 0.9933)), sensitivityof 0.9887 (95% CI: (0.9803,
0.9971)), precision of 0.9899 (95% CI: (0.9844, 0.9954)), and specificity of 0.99 (95% CI:
(0.9846, 0.9954)).
3.2.4 MODIFIED INCEPTION NETS: Modified-InceptionResNetV2 attained the best F1
score, accuracy, Sensitivity, precision and specificity. obtaining a F1 score of 0.9898 (95% CI:
(0.9875, 0.992)), accuracy of 0.9898 (95% CI: (0.9876, 0.9921)), sensitivity of 0.9858 (95% CI:
(0.9815, 0.99)), precision of 0.9939 (95% CI: (0.9918, 0.996)), and specificity of 0.994 (95% CI:
(0.9918, 0.9962)).
3.2.5 MODIFIED VGG: Modified VGG-16 attained the best F1 score, accuracy, sensitivity,
precision and specificity, obtaining a F1 score of 0.9287 (95% CI: (0.9187, 0.9387)), accuracy of
0.9255 (95% CI: (0.9128, 0.9381)), sensitivity of 0.9601 (95% CI: (0.9497, 0.9706)), precision
of 0.8994 (95% CI: (0.8849, 0.9139)), and specificity of 0.8898 (95% CI: (0.8664, 0.9133)).
3.2.6 CATEGORY 2 SUMMARY

15

Table 2 presents the evaluation metrics for all the modified-models, and figure 3 presents a
comparison of the best models from each family. We note that once again EfficientNet-B5
attained the best F1 score (0.9905 Â± 0.0019), accuracy (0.9906 Â± 0.0019), and sensitivity (0.9913
Â± 0.003). However, the best performing model in terms of sensitivity and precision changed from
DenseNet201 to Modified InceptionResNetV2.
#

Model

F1 Score

Accuracy

Sensitivity

Precision

Specificity

# of
Params

0

Modified DenseNet121

0.9893 Â± 0.004

0.9893 Â± 0.0040

0.9887 Â± 0.0084

0.9899 Â± 0.0055

0.9900 Â± 0.0054

7.2M

1

Modified DenseNet169

0.9878 Â± 0.0024

0.9879 Â± 0.0023

0.9886 Â± 0.0033

0.9870 Â± 0.0048

0.9873 Â± 0.0045

12.9M

2

Modified DenseNet201

0.9885 Â± 0.0028

0.9886 Â± 0.0027

0.9874 Â± 0.0040

0.9897 Â± 0.0045

0.9897 Â± 0.0044

18.6M

3

Modified EfficientNetB0

0.9891 Â± 0.0039

0.9894 Â± 0.0038

0.9881 Â± 0.0051

0.9902 Â± 0.0051

0.9907 Â± 0.0046

4.2M

4

Modified EfficientNetB1

0.9874 Â± 0.0035

0.9874 Â± 0.0035

0.9864 Â± 0.0040

0.9884 Â± 0.0046

0.9884 Â± 0.0046

6.7M

5

Modified EfficientNetB2

0.9887 Â± 0.0033

0.9888 Â± 0.0033

0.9872 Â± 0.0051

0.9903 Â± 0.0037

0.9904 Â± 0.0036

8.0M

6

Modified EfficientNetB3

0.9889 Â± 0.0021

0.9890 Â± 0.0021

0.9876 Â± 0.0030

0.9903 Â± 0.0035

0.9904 Â± 0.0035

11.0M

7

Modified EfficientNetB4

0.9900 Â± 0.0030

0.9901 Â± 0.0030

0.9871 Â± 0.0037

0.9929 Â± 0.0034

0.9930 Â± 0.0034

17.9M

8

Modified EfficientNetB5

0.9905 Â± 0.0019

0.9906 Â± 0.0019

0.9913 Â± 0.003

0.9898 Â± 0.0043

0.9900 Â± 0.0041

28.8M

9

Modified EfficientNetB6

0.9883 Â± 0.0034

0.9884 Â± 0.0034

0.9892 Â± 0.0034

0.9875 Â± 0.0052

0.9874 Â± 0.0054

41.3M

10

Modified EfficientNetB7

0.9858 Â± 0.0036

0.9859 Â± 0.0037

0.9858 Â± 0.0043

0.9859 Â± 0.0050

0.9859 Â± 0.0052

64.4M

11

Modified InceptionResNetV2

0.9898 Â± 0.0023

0.9898 Â± 0.0022

0.9858 Â± 0.0042

0.9939 Â± 0.0021

0.9940 Â± 0.0022

54.5M

12

Modified InceptionV3

0.9838 Â± 0.0031

0.9841 Â± 0.0029

0.9816 Â± 0.0049

0.9862 Â± 0.0057

0.9865 Â± 0.0055

22.1M

13

Modified ResNet101V2

0.9781 Â± 0.0062

0.9781 Â± 0.0060

0.9760 Â± 0.0070

0.9804 Â± 0.0095

0.9802 Â± 0.0094

42.9M

14

Modified ResNet152V2

0.9760 Â± 0.0050

0.9760 Â± 0.0052

0.9750 Â± 0.0075

0.9772 Â± 0.0067

0.9771 Â± 0.0067

58.6M

15

Modified ResNet50

0.9875 Â± 0.0051

0.9875 Â± 0.0053

0.9875 Â± 0.0063

0.9877 Â± 0.0065

0.9873 Â± 0.0072

23.9M

16

Modified ResNet50V2

0.9775 Â± 0.0088

0.9775 Â± 0.0094

0.9791 Â± 0.005

0.9763 Â± 0.0149

0.9757 Â± 0.0168

23.8M

17

Modified VGG16

0.9287 Â± 0.0100

0.9255 Â± 0.0127

0.9601 Â± 0.0104

0.8994 Â± 0.0145

0.8898 Â± 0.0234

14.8M

18

Modified VGG19

0.9174 Â± 0.0130

0.9150 Â± 0.0114

0.9669 Â± 0.0105

0.8738 Â± 0.0279

0.8671 Â± 0.0236

20.1M

19

Modified Xception

0.9887 Â± 0.0043

0.9888 Â± 0.0042

0.9852 Â± 0.0060

0.9922 Â± 0.0046

0.9922 Â± 0.0047

21.1M

Table 2: Summary of results obtained for all Modified Networks. We see that the Modified EfficientNet-B5 obtained the
highest F1-scores, a, and sensitivity, while the Modified InceptionResNetV2 attains the highest precision and specificity
amongst the modified networks.

16

Figure 3: Comparison of the best models from each model family, presented in order of increasing sensitivity. We note that
EfficientNet provideed the highest sensitivities, followed by DenseNet121, ResNet50, InceptionResNetV2, and VGG16.

3.3 COMPARISON OF BASE MODELS VS. MODIFIED NETWORKS
Model

F1 Score

Sensitivity

Model

F1-Score

Sensitivity

0

Modified DenseNet121

0.9893 Â± 0.0040

0.9887 Â± 0.0084

DenseNet121

0.9894 Â± 0.0035

0.9894 Â± 0.0036

1

Modified DenseNet169

0.9878 Â± 0.0024

0.9886 Â± 0.0033

DenseNet169

0.9905 Â± 0.0065

0.9917 Â± 0.0104

2

Modified DenseNet201

0.9885 Â± 0.0028

0.9874 Â± 0.0040

DenseNet201

0.9928 Â± 0.0066

0.9918 Â± 0.0057

3

Modified EfficientNetB0

0.9891 Â± 0.0039

0.9881 Â± 0.0051

EfficientNetB0

0.9861 Â± 0.0053

0.9836 Â± 0.0045

4

Modified EfficientNetB1

0.9874 Â± 0.0035

0.9864 Â± 0.004

EfficientNetB1

0.9886 Â± 0.0051

0.9851 Â± 0.0062

5

Modified EfficientNetB2

0.9887 Â± 0.0033

0.9872 Â± 0.0051

EfficientNetB2

0.9902 Â± 0.0025

0.9886 Â± 0.0029

6

Modified EfficientNetB3

0.9889 Â± 0.0021

0.9876 Â± 0.0030

EfficientNetB3

0.9894 Â± 0.0028

0.9878 Â± 0.0044

7

Modified EfficientNetB4

0.9900 Â± 0.0030

0.9871 Â± 0.0037

EfficientNetB4

0.9909 Â± 0.0029

0.989 Â± 0.0057

8

Modified EfficientNetB5

0.9905 Â± 0.0019

0.9913 Â± 0.003

EfficientNetB5

0.9931 Â± 0.002

0.9952 Â± 0.0035

9

Modified EfficientNetB6

0.9883 Â± 0.0034

0.9892 Â± 0.0034

EfficientNetB6

0.9878 Â± 0.0051

0.9895 Â± 0.0049

10

Modified EfficientNetB7

0.9858 Â± 0.0036

0.9858 Â± 0.0043

EfficientNetB7

0.9904 Â± 0.005

0.9942 Â± 0.0032

11

Modified InceptionResNetV2

0.9898 Â± 0.0023

0.9858 Â± 0.0042

InceptionResNetV2

0.9870 Â± 0.0038

0.9849 Â± 0.0042

12

Modified InceptionV3

0.9838 Â± 0.0031

0.9816 Â± 0.0049

InceptionV3

0.9885 Â± 0.0023

0.9857 Â± 0.0047

13

Modified ResNet101V2

0.9781 Â± 0.0062

0.9760 Â± 0.0070

ResNet101V2

0.9861 Â± 0.0028

0.9883 Â± 0.0034

17

14

Modified ResNet152V2

0.976 Â± 0.005

0.975 Â± 0.0075

ResNet152V2

0.9823 Â± 0.0026

0.9799 Â± 0.006

15

Modified ResNet50

0.9875 Â± 0.0051

0.9875 Â± 0.0063

ResNet50

0.9894 Â± 0.0035

0.9905 Â± 0.0057

16

Modified ResNet50V2

0.9775 Â± 0.0088

0.9791 Â± 0.005

ResNet50V2

0.9862 Â± 0.002

0.9895 Â± 0.0025

17

Modified VGG16

0.9287 Â± 0.01

0.9601 Â± 0.0104

VGG16

0.9659 Â± 0.0063

0.9691 Â± 0.0123

18

Modified VGG19

0.9174 Â± 0.013

0.9669 Â± 0.0105

VGG19

0.9407 Â± 0.0066

0.9513 Â± 0.0097

19

Modified Xception

0.9887 Â± 0.0043

0.9852 Â± 0.006

Xception

0.9898 Â± 0.0022

0.9874 Â± 0.0036

Table 3: Comparison of the performance of the Base models and Modified Networks. Green highlights represent the model
that performed better. We noted that in 80% (16/20) of the cases, the original model performed better, suggesting that the
imposed architectural changes had no noticeable improvement in network performance.

3.4 GradCAM VISUALIZATIONS
This section visualizes the GradCAMs, comparing them first between COVID-19 positive and
COVID-19 negative patients and closely examining the impact of modifying network
architectures on GradCAM efficacy. It is evident from figure 4 that the networks indeed focused
upon features radiologically recognized as being suggestive of lung involvement in COVID-19
in cases with high pre-test probabilities for making classifications with high accuracy.
3.4.1 COVID-19 vs. Non-COVID-19: These images were generated using the EfficientNet-B5,
EfficientNet-B4, ResNet50, Xception-Net, InceptionResNetV4, DenseNet201, DenseNet121,
VGG16, and VGG-19, the best models from each class. We note that while the generated
heatmaps were specific and localized when visualizing COVID-19, they tended to become more
diffused and spread out for almost all models when visualizing non-COVID-19 images. This
emphasized the effectiveness of the proposed ML architectures, for they not only accurately
diagnosed the disease but localized regions in the lungs used for diagnoses. Patients and
radiologists may further study this representation to gain certainty in diagnosis and prognosis.

18

COVID-19 Positive

COVID-19 Negative

Fig. 4a: EfficientNet-B5

Fig. 4b: EfficientNet-B5

19

Fig. 4c: EfficientNet-B4

Fig. 4d: EfficientNet-B4

Fig. 4e: ResNet50

Fig. 4f: ResNet50

Fig. 4g: Xception

Fig. 4h: Xception

Fig. 4i: InceptionResNetV2

Fig. 4j: InceptionResNetV2

Fig. 4k: DenseNet201

Fig. 4l: DenseNet201

Fig. 4m: DenseNet121

Fig. 4n: DenseNet121

Fig. 4o: VGG16

Fig. 4p: VGG16

Fig. 4q: VGG19

Fig. 4r: VGG19

Figure 4: Comparison of GradCAM Visualizations between COVID-19 Positive and COVID-19 negative images. We see that
the saliency maps are much more diffuse and spread out throughout the image in the case of Non-COVID images, which
suggests that the model was unable to pinpoint regions that would hint towards the presence of COVID-19.

3.4.2 Comparison between Modified and Base Networks
This section presents the variations in the produced heatmaps for the original models compared
to the Modified-Networks, focusing its attention on the top-performing models from each class.
First, we note in figure 5 that for the best performing modelsâ€”EfficientNetB5, ResNet50,
DenseNet201, and Xceptionâ€”, both the base and modified models used roughly the same
regions for making final diagnoses. The consistency in the areas perceived hinted towards how
these networks maintained a greater confidence level in making predictions. In contrast, for
VGG16, the visualized regions were in different areas. This suggests that VGG16 makes
predictions with a lower confidence-level, which are significantly skewed due to minor
alterations in the network architecture. We see similar patterns reflected in their lower
performance on the same dataset as well.

20

Secondly, while there are no clear delineators suggesting the effectiveness of one
visualization model over another, we note that in general, modified networks produced heatmaps
that were most spread out and less vibrant in intensity. On average, we note that the mean pixel
intensities tend to be higher for base-models compared to their modified counterparts.
Particularly, EfficientNetB5 attains a mean pixel intensity of 106.2554, whereas the modified
EfficientNetB5 attains a slightly lower mean intensity of 104.8093 pixels; p-value < 0.01. Such
trends continue to re-emphasize the greater performance the base models tend to deliver
compared to their modified counterparts. We see these trends continued in the case of Xception
(112.2687 vs. 109.8970, p-value < 0.01), DenseNet201 (127.3185 vs. 114.0104, p-value < 0.01),
ResNet50 (118.5625 vs. 112.8382, p-value < 0.01), and VGG16 (123.1306 vs. 78.5967, p-value
< 0.01). We attribute the decreased pixel intensities to the greater depths that the gradients must
be transmitted in the modified networks.
Finally, we note that for some models, such as VGG16, the base networks and modified
models generate complementary results and heatmaps. This complementary nature may, in turn,
be used to design ensemble networks in the future, where a combination of the features
highlighted by the two networks is used to improve network results. Moreover, even for models
for which the base and modified networks present nearly the same emphasized regions, such as
EfficientNetB5, we see that the maps still contain slight differences. For such models, the
ensemble technique would allow for a more exhaustive identification of features related to
COVID-19.

21

Fig. 5a: EfficientNet B5: 106.2554

Fig. 5b: Modified EfficientNet B5: 104.8093

Fig. 5c: Xception: 112.2687

Fig. 5d: Modified Xception: 109.8970

Fig. 5e: DenseNet201: 127.3185

Fig. 5f: Modified DenseNet201: 114.0104

Fig. 5g: VGG-16: 123.1306

Fig. 5h: Modified VGG-16: 78.5967

Fig. 5i: ResNet50: 118.5625

Fig. 5j: Modified ResNet50: 112.8382

Figure 5: Comparison of GradCAM Visualizations between some of the original and modified networks along with the mean
pixel intensities. We can note that although the differences are minute, the original saliency maps were more focused and
vibrant, an artifact which suggests that the gradients fade out as they pass through the increased number of model layers. We
propose the use of Batch Normalization and Skip Connections to alleviate this deficit.

3.5 VISUALIZING INTERMEDIATE ACTIVATION MAPS
Figure 6 shows the complexity of these maps that evolve with time. In the early stages of the
network (Fig. 6b, 6c), the model starts to learn basic feature maps from the image, such as their
22

edges, color gradients, etc. In layers near the middle of these models (Fig. 6d, 6e, 6f), these maps
get significantly more complicated, picking up on ground-glass opacities, consolidations, and
crazy paving patterns as noted in the highlighted sections of greater vibrance. Gradually, these
embeddings become more and more complex, veering from being human interpretable towards
features only understandable by a computer (Fig 6g, 6h). These maps with their invaluable
insight show how the model learns, playing an equally important role as GradCAMs, the current
visualization scheme of choice.

23

Input Image
Fig. 6a.

Conv2_block1_out
Fig. 6b. Layer 17

Conv3_block2_out
Fig. 6c. Layer 63

Conv4_block2_out
Fig. 6d. Layer 109

conv4_block8_preact_relu
Fig. 6e. Layer 166

conv4_block12_out
Fig. 6f .Layer 219

conv4_block16_out
Fig. 6g. Layer 263

conv4_block_20_out
Fig. 6h. Layer 307

dense_2
Fig. 6l. Layer 379 (Final Layer)

Figure 6: Intermediate Activation Maps: From 4-image sub slices from the intermediate activation maps, we can see the
progression of the modelâ€™s learning behavior. The model progresses from maps similar to input images during its early layers
(Fig 6b.) and progress towards maps of increasing complexity as the layer depth increases (Fig 6f, 6g, 6h). Since layer 109
(fig 6d), we start noticing patterns of the model zeroing in on ground-glass opacities in the image. Fig 6.l represents the final
activation map used before the model prediction. We can note the model ability to close-in upon and depict small pixels and
voxels in the input image

3.5.1 COMPARISON BETWEEN COVID-19 POSITIVE AND NEGATIVE IMAGES
This section presents a comparison between activation maps for COVID-19 positive and
COVID-19 negative images. Although difficult to interpret without radiological expertise, we
note that the activation maps for COVID-positive images tend to focus on more intricate lung
patterns like paving patterns and consolidations, whereas the feature maps for COVID-19 cases
are less detailed and have a more uniform pattern. The maps become increasingly localized at
later layers, indicating that the model closely examines each minute pattern, opacity, and
coagulation on the long before classifying a patient.
Opening Layer (Conv1):

Intermediate Layer (54)

block6_sepconv3
COVID-19
Positive

24

Near-Ending Layer (119)
Block13_sepcovnv1_bn

COVID-19
Negative

Figure 7: Comparison of intermediate activate maps for COVID-19 positive versus COVID-19 negative images. We note that
while the activation patterns for COVID-19 positive images tend to capture nodules, paving patterns, and opacities, the
patterns for Non-COVID images are a bit simpler and uniform. Once again, near the later layers of the model, the feature
maps become increasingly uninterpretable.

4. DISCUSSION
The CNNs showed a large increase in the sensitivity of COVID-19 diagnosis using CT
scans (0.9952) in comparison to RT-PCR (0.71) [4]. As expected, the Efficient-Net family
performed exceedingly well for diagnosis, attaining 7 out of the 11 top sensitivities for all base
networks trained, with EfficientNet-B5, EfficientNet-B6, EfficientNet-B0, EfficientNet-B3
achieving the 1st, 2nd, 4th, and 5th highest sensitivities respectively. Similarly, the ModifiedEfficientNet class claimed 8 out of the 11 top sensitivities of all the modified models trained,
with Modified-EfficientNetB5, Modified-EfficientNetB6, and Modified-EfficientNetB0 attaining
1st, 2nd, and 5th place, respectively.
We noted that the EfficientNet-B5 class obtained the best F1 scores, accuracy, and
sensitivity amongst all models trained and evaluated, whereas the modified-InceptionResNet-V2
obtained the highest specificity and precision. However, considering that sensitivity may be more
important than specificity in the case of COVID-19 diagnosis, we identified EfficientNet-B5 as
the model of choice for COVID-19 diagnosis, well in line with the proposed hypothesis that
EfficientNets would perform better in diagnosis.
On average, models unaltered with the modified network structure outperformed those
with a modified architecture. This may suggest that adding additional layers allows the

25

performance on the training sets to increase. On the other hand, it decreases generalizability,
limiting the modelâ€™s prediction capabilities to images similar to those in the training set.
Additionally, the addition of dense layers near the modelâ€™s end tends to cause the GradCAMS
generated to be less accurate. This is likely because the gradients from the final 4-Dimensional
image must be transmitted deeper. The addition of batch normalization and dense skip
connections may be a possible solution to alleviate the diminishing gradient problem. We plan to
explore these adjustments for the modified network in subsequent studies.
As predicted, the addition of intermediate activation maps significantly increased
interpretability in the models, indicating each step performed by the CNN before its final
classification. Additionally, when deployed in augmented reality formats, the visualization
techniques can accelerate the image-classification process for radiologists by rapidly identifying
regions of interest.
LIMITATIONS AND FUTURE WORK
Although the dataset was obtained using CT imaging, it simply included the 2D slices that
are used to reconstruct 3D scans. However, it did not provide any additional directory structure
to reconstruct the 3D images. In addition, this dataset did not indicate which 2D slices were
associated with the same patients. Therefore, in this work, we decided to focus on image
diagnosis rather than patient diagnosis, which cannot be fully addressed using the presented
dataset.
To meet this gap, we propose the following methodology for meeting this deficiency: lung
segmentation for extracting lung shape, identifying the similarity between lungs by computing
intersection over union scores, a popular benchmark for object detection, as outlined in [40], and
grouping images into categories based off the lung anatomy. After grouping, we suggest

26

extracting 2D slices from a 3D array representing all images available for a given patient,
predicting the probability of that scan indicating COVID-19, and averaging the obtained
probabilities through all slices to calculate an overall metric of COVID-19 likelihood. We
believe that considering a patient-based diagnosis compared with an image-based diagnosis can
give a more exhaustive clinical picture.
Finally, visualizations obtained through GradCAMs and intermediate activation maps greatly
improved the human interpretability of the CNNs predictions. If deployed in an AI aided
augmented reality tool, these visualizations may, in turn, accelerate the rate at which radiologists
process reports by directing them to the key pathological features in the images.
5. CONCLUSION
This study successfully presented a thorough analysis of the use of traditional and custom
ML techniques, specifically CNN architectures, for COVID-19 detection based on chest CT. As
predicted, the EfficientNet family performed exceptionally well for diagnoses, consistently
performing best in terms of F1 scores, sensitivity, and accuracy. DenseNet201 and the ModifiedInceptionResNetV2 were found to have the highest sensitivities and precision. Contrary to our
expectations, modifying network architectures through the addition of dense layers had no
statistical significance and, on average, appeared to worsen performance. We suggest developing
model add-ons using skip connections and batch normalization to increase the gradients passed
through the network.
Both the adopted visualization schemes were helpful in providing human interpretability to
the model. While the GradCAMs successfully highlighted malignant portions of lungs, the
intermediate activation maps offered invaluable information about the networkâ€™s training and
learning process. With the backlog of PCR tests, causing results to take anywhere from 6 hours

27

to 4 days to reach patients, CT technology could identify potentially COVID 19 positive patients
and rapidly accelerate the rate of testing, helping control the spread of the disease, and converge
towards curbing the financial and social collapse it has created.
6. FUNDING
This work was supported by the National Science Foundation under Award Number 2027456
(COVID-ARC).
7. REFERENCES
[1]

Covid World Map: Tracking the Global Outbreak - The New York Times, (n.d.).
https://www.nytimes.com/interactive/2020/world/coronavirus-maps.html (accessed
December 4, 2020).

[2]

COVID-19 basics - Harvard Health, (n.d.). https://www.health.harvard.edu/diseases-andconditions/covid-19-basics (accessed December 4, 2020).

[3]

M.M. Islam, F. Karray, R. Alhajj, J. Zeng, A Review on Deep Learning Techniques for
the Diagnosis of Novel Coronavirus (COVID-19), (2020). http://arxiv.org/abs/2008.04815
(accessed October 14, 2020).

[4]

Y. Fang, H. Zhang, J. Xie, M. Lin, L. Ying, P. Pang, W. Ji, Sensitivity of chest CT for
COVID-19: Comparison to RT-PCR, Radiology. 296 (2020) E115â€“E117.
https://doi.org/10.1148/radiol.2020200432.

[5]

D. Dong, Z. Tang, S. Wang, H. Hui, L. Gong, Y. Lu, Z. Xue, H. Liao, F. Chen, F. Yang,
R. Jin, K. Wang, Z. Liu, J. Wei, W. Mu, H. Zhang, J. Jiang, J. Tian, H. Li, The role of
imaging in the detection and management of COVID-19: a review, IEEE Rev. Biomed.
Eng. PP (2020). https://doi.org/10.1109/RBME.2020.2990959.

28

[6]

X. Wu, H. Hui, M. Niu, L. Li, L. Wang, B. He, X. Yang, L. Li, H. Li, J. Tian, Y. Zha,
Deep learning-based multi-view fusion model for screening 2019 novel coronavirus
pneumonia: A multicentre study, Eur. J. Radiol. 128 (2020) 109041.
https://doi.org/10.1016/j.ejrad.2020.109041.

[7]

C. Butt, J. Gill, D. Chun, B.A. Babu, Deep learning system to screen coronavirus disease
2019 pneumonia, Appl. Intell. (2020). https://doi.org/10.1007/s10489-020-01714-3.

[8]

S. Jin, B. Wang, H. Xu, C. Luo, L. Wei, W. Zhao, X. Hou, W. Ma, Z. Xu, Z. Zheng, W.
Sun, L. Lan, W. Zhang, X.-A. Mu, C. Shi, Z. Wang, J. Lee, Z. Jin, M. Lin, H. Jin, L.
Zhang, J. Guo, B. Zhao, Z. Ren, S. Wang, Z. You, J. Dong, X. Wang, J. Wang, W. Xu,
AI-assisted CT imaging analysis for COVID-19 screening: Building and deploying a
medical AI system in four weeks, MedRxiv. (2020) 2020.03.19.20039354.
https://doi.org/10.1101/2020.03.19.20039354.

[9]

C. Jin, W. Chen, Y. Cao, Z. Xu, Z. Tan, X. Zhang, L. Deng, C. Zheng, J. Zhou, H. Shi, J.
Feng, Development and evaluation of an artificial intelligence system for COVID-19
diagnosis, Nat. Commun. 11 (2020) 5088. https://doi.org/10.1038/s41467-020-18685-1.

[10]

M. Yousefzadeh, P. Esfahanian, S.M.S. Movahed, S. Gorgin, R. Lashgari, D. Rahmati, A.
Kiani, S. Kahkouee, S.A. Nadji, S. Haseli, M. Hoseinyazdi, J. Roshandel, N. Bandegani,
A. Danesh, M. Bakhshayesh Karam, A. Abedini, ai-corona: Radiologist-Assistant Deep
Learning Framework for COVID-19 Diagnosis in Chest CT Scans, MedRxiv. (2020)
2020.05.04.20082081. https://doi.org/10.1101/2020.05.04.20082081.

[11]

A.A. Ardakani, A.R. Kanafi, U.R. Acharya, N. Khadem, A. Mohammadi, Application of
deep learning technique to manage COVID-19 in routine clinical practice using CT

29

images: Results of 10 convolutional neural networks, Comput. Biol. Med. 121 (2020)
103795. https://doi.org/10.1016/j.compbiomed.2020.103795.
[12]

Z. Xiong, R. Wang, H.X. Bai, K. Halsey, J. Mei, Y.H. Li, M.K. Atalay, X.L. Jiang, F.X.
Fu, L.T. Thi, R.Y. Huang, W.H. Liao, I. Pan, J.W. Choi, Q.H. Zeng, B. Hsieh, D.
CuiWang, R. Sebro, P.F. Hu, K. Chang, L.B. Shi, Z.Y. Qi, Artificial Intelligence
Augmentation of Radiologist Performance in Distinguishing COVID-19 from Pneumonia
of Other Origin at Chest CT, Radiology. 296 (2020) E156â€“E165.
https://doi.org/10.1148/radiol.2020201491.

[13]

A. Waleed Salehi, P. Baglat, G. Gupta, Review on Machine and Deep Learning Models
for the Detection and Prediction of Coronavirus, Mater. Today Proc. (2020).
https://doi.org/10.1016/j.matpr.2020.06.245.

[14]

S. Lalmuanawma, J. Hussain, L. Chhakchhuak, Applications of machine learning and
artificial intelligence for Covid-19 (SARS-CoV-2) pandemic: A review, Chaos, Solitons
and Fractals. 139 (2020) 110059. https://doi.org/10.1016/j.chaos.2020.110059.

[15]

T. Ozturk, M. Talo, E.A. Yildirim, U.B. Baloglu, O. Yildirim, U. Rajendra Acharya,
Automated detection of COVID-19 cases using deep neural networks with X-ray images,
Comput. Biol. Med. 121 (2020). https://doi.org/10.1016/j.compbiomed.2020.103792.

[16]

N.S. Punn, S.K. Sonbhadra, S. Agarwal, COVID-19 Epidemic Analysis using Machine
Learning and Deep Learning Algorithms, MedRxiv. (2020) 2020.04.08.20057679.
https://doi.org/10.1101/2020.04.08.20057679.

[17]

J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, L. Fei-Fei, ImageNet: A Large-Scale
Hierarchical Image Database, n.d. http://www.image-net.org. (accessed November 19,

30

2020).
[18]

N.K. Chowdhury, M.M. Rahman, N. Rezoana, M.A. Kabir, ECOVNet: An Ensemble of
Deep Convolutional Neural Networks Based on EfficientNet to Detect COVID-19 From
Chest X-rays, (2020). http://arxiv.org/abs/2009.11850.

[19]

G. Marques, D. Agarwal, I. de la Torre DÃ­ez, Automated medical diagnosis of COVID-19
through EfficientNet convolutional neural network, Appl. Soft Comput. J. 96 (2020).
https://doi.org/10.1016/j.asoc.2020.106691.

[20]

Z. Muftuoglu, M.A. Kizrak, T. Yildlnm, Differential Privacy Practice on Diagnosis of
COVID-19 Radiology Imaging Using EfficientNet, in: INISTA 2020 - 2020 Int. Conf.
Innov. Intell. Syst. Appl. Proc., Institute of Electrical and Electronics Engineers Inc.,
2020. https://doi.org/10.1109/INISTA49547.2020.9194651.

[21]

H. Kim, H. Hong, S. Ho Yoon, Diagnostic performance of ct and reverse transcriptase
polymerase chain reaction for coronavirus disease 2019: A meta-analysis, Radiology. 296
(2020) E145â€“E155. https://doi.org/10.1148/radiol.2020201343.

[22]

Z. Ye, Y. Zhang, Y. Wang, Z. Huang, B. Song, Chest CT manifestations of new
coronavirus disease 2019 (COVID-19): a pictorial review, Eur. Radiol. 30 (2020) 4381â€“
4389. https://doi.org/10.1007/s00330-020-06801-0.

[23]

X. He, X. Yang, S. Zhang, J. Zhao, Y. Zhang, E. Xing, P. Xie, Sample-Efficient Deep
Learning for COVID-19 Diagnosis Based on CT Scans, IEEE Trans. Med. Imaging. XX
(n.d.) 1. https://doi.org/10.1101/2020.04.13.20063941.

[24]

31

Convolutional Neural Network: Feature Map and Filter Visualization | by Renu

Khandelwal | Towards Data Science, (n.d.). https://towardsdatascience.com/convolutionalneural-network-feature-map-and-filter-visualization-f75012a5a49c (accessed November
19, 2020).
[25]

F. Chollet, Visualizing convnet filters, in: Deep Learn. with Python, 1st ed., Manning
Publications Co., Shelter Island, 2017: pp. 160â€“172.

[26]

E. Soares, P. Angelov, S. Biaso, M. Higa Froes, D. Kanda Abe, SARS-CoV-2 CT-scan
dataset: A large dataset of real patients CT scans for SARS-CoV-2 identification, (2020).
https://doi.org/10.1101/2020.04.24.20078584.

[27]

SARS-COV-2 Ct-Scan Dataset | Kaggle, (n.d.).
https://www.kaggle.com/plameneduardo/sarscov2-ctscan-dataset (accessed November 19,
2020).

[28]

COVID-ARC, (n.d.). https://covid-arc.loni.usc.edu/ (accessed December 5, 2020).

[29]

M. Tan, Q. V. Le, EfficientNet: Rethinking Model Scaling for Convolutional Neural
Networks, 36th Int. Conf. Mach. Learn. ICML 2019. 2019-June (2019) 10691â€“10700.
http://arxiv.org/abs/1905.11946 (accessed November 19, 2020).

[30]

K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recognition, in: Proc.
IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit., IEEE Computer Society, 2016:
pp. 770â€“778. https://doi.org/10.1109/CVPR.2016.90.

[31]

K. He, X. Zhang, S. Ren, J. Sun, Identity Mappings in Deep Residual Networks, Lect.
Notes Comput. Sci. (Including Subser. Lect. Notes Artif. Intell. Lect. Notes
Bioinformatics). 9908 LNCS (2016) 630â€“645. http://arxiv.org/abs/1603.05027 (accessed

32

November 19, 2020).
[32]

C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, Z. Wojna, Rethinking the Inception
Architecture for Computer Vision, in: Proc. IEEE Comput. Soc. Conf. Comput. Vis.
Pattern Recognit., IEEE Computer Society, 2016: pp. 2818â€“2826.
https://doi.org/10.1109/CVPR.2016.308.

[33]

C. Szegedy, S. Ioffe, V. Vanhoucke, A.A. Alemi, Inception-v4, inception-ResNet and the
impact of residual connections on learning, in: 31st AAAI Conf. Artif. Intell. AAAI 2017,
AAAI press, 2017: pp. 4278â€“4284. https://arxiv.org/abs/1602.07261v2 (accessed
November 19, 2020).

[34]

F. Chollet, Xception: Deep learning with depthwise separable convolutions, in: Proc. 30th IEEE Conf. Comput. Vis. Pattern Recognition, CVPR 2017, Institute of Electrical
and Electronics Engineers Inc., 2017: pp. 1800â€“1807.
https://doi.org/10.1109/CVPR.2017.195.

[35]

G. Huang, Z. Liu, L. van der Maaten, K.Q. Weinberger, Densely Connected
Convolutional Networks, Proc. - 30th IEEE Conf. Comput. Vis. Pattern Recognition,
CVPR 2017. 2017-January (2016) 2261â€“2269. http://arxiv.org/abs/1608.06993 (accessed
November 19, 2020).

[36]

Karen Simonyanâˆ— & Andrew Zisserman+, VERY DEEP CONVOLUTIONAL
NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION Karen, Am. J. Heal.
Pharm. 75 (2018) 398â€“406.

[37]

33

ImageNet, (n.d.). http://www.image-net.org/ (accessed November 19, 2020).

[38]

C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke,
A. Rabinovich, Going deeper with convolutions, in: Proc. IEEE Comput. Soc. Conf.
Comput. Vis. Pattern Recognit., IEEE Computer Society, 2015: pp. 1â€“9.
https://doi.org/10.1109/CVPR.2015.7298594.

[39]

R.R. Selvaraju, Â· Michael Cogswell, A. Das, Â· Ramakrishna Vedantam, Â· Devi Parikh, Â·
Dhruv Batra, M. Cogswell, R. Vedantam, D. Parikh, Grad-CAM: Visual Explanations
from Deep Networks via Gradient-Based Localization, Int. J. Comput. Vis. 128 (2020)
336â€“359. https://doi.org/10.1007/s11263-019-01228-7.

[40]

H. Rezatofighi, N. Tsoi, J. Gwak, A. Sadeghian, I. Reid, S. Savarese, Generalized
Intersection over Union: A Metric and A Loss for Bounding Box Regression, n.d.

8. AUTHOR BIOGRPAHIES
Aksh Garg is a Researcher at the USC Stevens Neuroimaging and Informatics Institute in the
Laboratory of Neuro Imaging (LONI). His research interests include machine learning,
optimization, signal processing, and stochastic modeling applied towards creating generalizable
and scalable computer-aided disease diagnostics, enhancing wireless communication networks,
and building brain-machine interfaces.
Sana Salehi is a Research Scholar in the field of Radiology at the Keck School of Medicine of
USC. Her research interests include global health, cardiothoracic imaging, neuroimaging, and
Artificial Intelligence in Radiology. Her recent research has been focused on diagnostic and
prognostic role of imaging in COVID-19.
Marianna La Rocca is a Postdoctoral Scholar-Research Associate at the USC Stevens
Neuroimaging and Informatics Institute in the Laboratory of Neuro Imaging (LONI). Her

34

research interests include the use of complex networks, machine learning techniques and
quantitative methods to study neurodegenerative diseases and COVID-19 using
electrophysiology and imaging data.
Rachael Garner is a Project Specialist at the Laboratory of Neuro Imaging at the USC Stevens
Neuroimaging and Informatics Institute. Her research interests include multimodal data analysis
related to neurodegenerative disorders, epilepsy, cognitive disorders, and COVID-19.
Dominique Duncan is an Assistant Professor of Neurology at the USC Stevens Neuroimaging
and Informatics Institute in the Laboratory of Neuro Imaging (LONI). Her interests lie at the
intersection of multimodal data analysis, neuroinformatics, signal processing, and machine
learning, particularly in the areas of traumatic brain injury, epilepsy, and COVID-19.

35

