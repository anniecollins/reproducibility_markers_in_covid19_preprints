arXiv:2007.14229v1 [stat.ME] 28 Jul 2020

ROBUST PARAMETER ESTIMATION IN DYNAMICAL
SYSTEMS VIA STATISTICAL LEARNING WITH AN
APPLICATION TO EPIDEMIOLOGICAL MODELS
DIEGO MARCONDES

Abstract. We propose a robust parameter estimation method for dynamical systems based on Statistical Learning techniques which aims to
estimate a set of parameters that well fit the dynamics in order to obtain
robust evidences about the qualitative behaviour of its trajectory. The
method is quite general and flexible, since it dos not rely on any specific property of the dynamical system, and represents a mathematical
formalisation of the procedure consisting of sampling and testing parameters, in which evolutions generated by candidate parameters are tested
against observed data to assess goodness-of-fit. The Statistical Learning
framework introduces a mathematically rigorous scheme to this general
approach for parameter estimation, adding to the great field of parameter estimation in dynamical systems. The method is specially useful for
estimating parameters in epidemiological compartmental models. We illustrate it in simulated and real data about COVID-19 spread in the US
in order to assess qualitatively the peak of deaths by the disease.
Keywords: parameter estimation; dynamical systems; statistical learning; epidemiological models; COVID-19.

1. Introduction
Dynamical systems are important tools in Applied Mathematics for modelling phenomena studied by many branches of science, such as physics, cosmology, biology, epidemiology, medicine, chemistry and engineering [6, 15,
16, 19, 21], being applied to describe the deterministic evolution in time of
certain processes, when starting from a fixed initial condition. The equations
which govern this deterministic evolution are specific to each application,
and depend on some unknown parameters related to the phenomena being
modelled. For example, dynamical systems applied to physics and cosmology usually depend on universal constants, while compartmental models in
epidemiology depend on rates related to the spread of infectious diseases, as
1

PARAMETER ESTIMATION IN DYNAMICAL SYSTEMS VIA STATISTICAL LEARNING

2

their basic reproductive number [4, 31]. Hence, in order to model phenomena
by dynamical systems, one should find parameters which represent reality.
Some parameters, such as universal constants, are known, while others
can be measured, but some are, at principle, completely unknown and need
to be estimated. Such estimation is carried out from available data on the
evolution of the process in a time interval, when one observes a partial trajectory of a system and try to determine the parameters which generate an
evolution which fit the observed one. This problem may not have an unique
solution, since the evolution in an interval may not be enough to identify an
unique vector of parameters, and, even when the parameters are identifiable,
there may not exist a computable algorithm capable of finding them (see
[3, 13] for a discussion on identifiability of parameters).
In this scenario, there are three features usually present in methods for
parameter estimation in dynamical systems: closed formulae which relates
the parameters to observed values, as is the case of the basic reproduction
number in epidemiological models [32]; an algorithm which randomly selects
parameters from a class of candidates; and some measure of goodness-offit which evaluates if the evolution generated by each parameter well fit
the observed one. There are countless methods for parameter estimation
in dynamical system which present these, and other features. We refer, for
instance, [10, 11, 12, 18, 34] and the references therein.
In this paper we propose a robust approach for parameter estimation in
dynamical systems based on Statistical Learning techniques. The method
does not look for the vector of parameters which best fit the observed evolution, but rather seeks to find a set of parameters which well fit, in some
sense, such evolution. This method is suitable when one wants to identify
qualitative properties of the observed dynamics, rather than know it exactly,
or when the evolution is only approximated by a dynamical system. Furthermore, since the dynamics may be sensitive to the choice of parameters,
considering a set of parameters may mitigate such sensitivity, since properties satisfied by all of them may also be satisfied by the observed dynamics
(see for example the role of parameter s in [17]). Hence, considering a set
of parameters and looking for common properties shared by the evolutions
they generated account for the robustness of the method.
In the proposed approach, one chooses a set of candidate parameters,
based on all prior knowledge about the modelled dynamics, and fix a binary
rule which, given a candidate parameter, determine if its evolution well fit, or
not, the observed dynamics. We say that a parameter is good if its evolution

PARAMETER ESTIMATION IN DYNAMICAL SYSTEMS VIA STATISTICAL LEARNING

3

well fit the dynamics, and is bad otherwise. The method aims to find good
parameters among candidate ones by sampling and testing parameters. To
this end, we apply Statistical Learning techniques to determine an upper
bound for the sample size needed to find a proportion at least 1âˆ’c, 0 < c < 1,
of the good parameters with high probability, which is a measure of the
computational complexity of the approach.
The method is quite general, since it does not rely on any specific property
of the dynamical system, being at principle applicable to multidimensional
systems with real-valued parameters. Observe that the agenda the method
seeks to carry is much more humble than trying to identify exactly the value
of the optimal parameter, since its aim is only to find good parameters in a
pre-defined set. This is more plausible to be achieved in practice, specially
in processes which are only approximated by dynamical systems or whose
parameters change from time to time. Also, there is no need to fix a likelihood
function for the process, nor prior distributions for the parameters, as is done
in common Bayesian approaches to parameter estimation [5, 23].
The Statistical Learning approach is specially suitable when modelling
disease spread by compartmental epidemiological models. We illustrate in
an example with real data about the spread of COVID-19 in the US how this
method can be quite useful in obtaining qualitative information about the
evolution of the process, which in the case of infectious disease may be its
peak of deaths, in quite complex models. Indeed, we will see how the parameters of a SEIR model change from time to time due to the enforcement of
measures to slow the disease spread, demanding a constant fit of them which
in itself can provide evidences about the disease spread and the effectiveness
of measures to overcome it.
In Section 2 we discuss the problem of robustly estimating parameters in
dynamical systems from the evolution of the system in an interval. In Section
3 we propose a method based on Statistical Learning techniques to perform
such robust estimation. In Section 4 we present a couple of examples of the
proposed method for a SIR model on a simulated dataset, and for a SEIR
model for the spread of COVID-19 in the US. In Section 5 we give our final
remarks.
2. Robust parameter estimation in dynamical systems
Let {{XÎ¸ (t)}tâˆˆN : Î¸ âˆˆ Î˜} be a family of discrete time dynamical systems,
with a same metric phase space (â„¦, d), indexed by parameters Î¸ âˆˆ Î˜. The

PARAMETER ESTIMATION IN DYNAMICAL SYSTEMS VIA STATISTICAL LEARNING

4

parametric space Î˜ may be such that Î˜ âŠ† Rd , d â‰¥ 1, when the parameters are
time independent, or Î˜ âŠ† N Ã— Rd , when the parameters are time dependent.
We assume that the initial condition is the same for all systems in the family:
XÎ¸ (0) = x0 âˆˆ â„¦ for all Î¸ âˆˆ Î˜.
The problem of parameter estimation in dynamical systems we consider
in this paper is characterized when one observes the time evolution of a
process XÎ¸? (t) for t âˆˆ {1, . . . , T }, with a fixed T â‰¥ 1, and wants to identify
the parameter Î¸? âˆˆ Î˜ which generated such evolution. On the one hand, for
T âˆˆ N, the map
Ï†T : Î˜ â†’ â„¦T
Ï†T (Î¸) = {XÎ¸ (t)}Tt=1
which maps each parameter Î¸ âˆˆ Î˜ to the evolution of XÎ¸ (t) until t = T , is in
general not invertible, i.e., Î¸ is not identifiable, so the evolution until a time T
T
does not define uniquely the system. Furthermore, the set Ï†âˆ’1
T ({XÎ¸? (t)}t=1 )
may not be computable in real problems, so one cannot identify a subset
of candidate parameters by inverting such a map. On the other hand, it
is not computationally feasible to test all parameters in Î˜, in case it has
infinite elements, or to perform an efficient grid search of Î˜, when it is
multidimensional, to find candidate parameters.
Precisely estimating parameters of a dynamical system from the time
evolution in an interval may have an intrinsic lack of robustness due to the
sensitivity of the evolution on parameter Î¸ (see [33] for an example of sensitivity analysis for compartmental models). This implies that, even if we
estimate Î¸? by a Î¸Ì‚ close to Î¸? , d (XÎ¸? (t), XÎ¸Ì‚ (t)) may be too great for t > T ,
rendering the estimative useless for the problem at hand, specially when the
aim of estimating parameters of a dynamical system is predicting exactly its
trajectory.
A manner of increasing the robustness of the estimation method is, rather
than estimating Î¸? precisely by a Î¸Ì‚, identifying a set Î˜Ì‚ âŠ† Î˜ of possible values
for Î¸? which will generate evolutions of the system: {{XÎ¸ (t)}tâˆˆN : Î¸ âˆˆ Î˜Ì‚}.
Establishing common properties of these evolutions, one may obtain evidences about the behaviour of XÎ¸? (t) for t > T . Even though the chosen
models are still sensible to the parameters, there may be some qualitative
behaviour, common to all of them, which may also be shared with the evolution generated by Î¸? , so this method adds to one understanding of such
trajectory.

PARAMETER ESTIMATION IN DYNAMICAL SYSTEMS VIA STATISTICAL LEARNING

5

The chosen models should well fit the observed evolution until time T .
The definition of well fit is problem dependent and shall be given by a fitness
map
F : Î˜ Ã— â„¦T â†’ {0, 1}
which, for a Î¸ âˆˆ Î˜ and observed evolution in â„¦T , attributes 1 if the system
generated by Î¸ well fit the observed evolution, and attributes 0 otherwise.
An example of fitness map is
F (Î¸, {XÎ¸? (t)}Tt=1 )

=

n
o
min 1 d(XÎ¸ (t), XÎ¸? (t)) â‰¤ Î´(t, XÎ¸? (t))

tâˆˆ{1,...,T }

(1)

for Î´(t, XÎ¸? (t)) > 0, dependent on t and XÎ¸? (t). By (1), a model well fits
{XÎ¸? }Tt=1 if its time evolution is relatively close to the observed one for all
t â‰¤ T . The fitness map is problem dependent and should be chosen according
to the purpose of the dynamical system.
The estimation method for dynamical systems parameters proposed in
this paper aims to find a subset Î˜Ì‚ âŠ† Î˜ of parameters such that F (Î¸, {XÎ¸? (t)}Tt=1 ) =
1 for all Î¸ âˆˆ Î˜Ì‚. As an alternative to invert map Ï†T or perform a grid search
on Î˜, we will propose a probabilistic consistent method to randomly select
parameters from Î˜ and take Î˜Ì‚ as the ones with goodness-of-fit according to
a fitness map F . This method is based on Statistical Learning techniques
[8, 25, 26].
Remark 2.1. Goodness-of-fit, as defined above, is a dichotomous concept:
a model either does, or does not, fit the observed evolution. Even if we considered some likelihood function or distance between the observed and generated evolution as numerical measures of goodness-of-fit, we would have to
choose a threshold for it in order to generate Î˜Ì‚. Hence, the class of fitness
maps contemplate these numerical goodness-of-fit measures, since they may
be composed by such measures, as in example (1). Therefore, dichotomous
goodness-of-fit measures are enough for our purposes.
3. Parameter estimation via Statistical Learning
3.1. Statistical Learning. The Statistical Learning method under the Empirical Risk Minimization (ERM) paradigm, restricted to classification problems, may be stated as follows. Let Z be a random vector, and Y a random
variable, defined on a same probability space (Î›, S, P), with ranges Z âŠ‚
Rd , d â‰¥ 1, and {0, 1}, respectively. Denote P (z, y) := P(Z â‰¤ z, Y â‰¤ y) as the

PARAMETER ESTIMATION IN DYNAMICAL SYSTEMS VIA STATISTICAL LEARNING

6

joint probability distribution of (Z, Y ) at point (z, y) âˆˆ Z Ã— {0, 1}, which we
assume unknown, but fixed. Define a sample DN = {(Z1 , Y1 ), . . . , (ZN , YN )}
as a sequence of independent and identically distributed random vectors,
defined on (Î›, S, P), with joint distribution P .
Let H be a set of functions with domain Z and image {0, 1}, whose typical
element we denote by h : Z â†’ {0, 1}. We call H Hypotheses Space. For
each hypothesis h in H, we assign a value indicating the error incurred by
the use of such hypothesis to predict Y from the value of Z, i.e., how well
h(Z) predicts Y . To obtain such an error measure, we consider the simple
loss function ` : Z Ã— {0, 1} Ã— H 7â†’ R given by
(
1, if h(z) 6= y
l(z, y; h) =
0, if h(z) = y
for (z, y, h) âˆˆ Z Ã— {0, 1} Ã— H. In this framework, the loss of predicting y
by h(z) is either zero if h(z) = y, or one if h(z) 6= y. The prediction error,
known in the literature as out-of-sample error, risk or loss [1, 8, 25, 26], of a
hypothesis h âˆˆ H is defined as
L(h) := E[`(Z, Y ; h)] = P (h(Z) 6= Y )
in which E means expectation under P. This is the probability of error
incurred when hypothesis h is used to predict Y from Z.
The out-of-sample error is fixed, but unknown, as is P . Therefore, in
order to asses the out-of-sample error of a hypothesis, one needs to estimate
it. An estimator for L may be obtained by the empirical error on sample
DN , called in-sample error and defined as
N
1 X
1 {h(Zi ) 6= Yi }
LDN (h) :=
N i=1

for h âˆˆ H. This is simply the classification error of h when applied to classify
the points in sample DN .
The main goal of Statistical Learning in this context is to approximate
target hypotheses, which are minimizers of the out-of-sample error in H.
These hypotheses are in set
h? := arg min L(h)
hâˆˆH

PARAMETER ESTIMATION IN DYNAMICAL SYSTEMS VIA STATISTICAL LEARNING

7

and satisfy L(h? ) â‰¤ L(h), âˆ€h âˆˆ H. We assume throughout this paper that
L(h? ) = 0, i.e., there is a hypothesis in H which predicts Y from the values of Z with probability one. Under the ERM principle, which proposes
the minimization of the in-sample error as a method to approximate target
hypotheses, we estimate them by
hÌ‚ := arg min LDN (h),
hâˆˆH

the hypotheses which minimize the classification error in sample DN . Observe
that hÌ‚ = hÌ‚DN is actually dependent on DN , but we drop the subscript to ease
notation.
In the ERM paradigm we are interested in considering Hypotheses Spaces
H which are Probably Approximately Correct (PAC)-Learnable [24]. We say
that Hypotheses Space H is PAC-Learnable1 if


lim P L(hÌ‚) >  = 0
(2)
N â†’âˆž

for all  > 0. This means that, if the sample size N tends to infinity, the
out-of-sample error of the estimated hypotheses hÌ‚ is arbitrarily close to the
out-of-sample error of the target hypotheses, i.e., zero, with high probability.
In Probability Theory [2], it means that L(hÌ‚) converges in probability to
zero. In a PAC-Learnable Hypotheses Space we may estimate h? arbitrarily
well, with great confidence, if the sample size is great enough.
In this paper, we will be interested in Hypotheses Spaces with a finite
number of elements, i.e., |H| < âˆž. Such Hypotheses Spaces are not only
PAC-Learnable, but we can establish a distribution-free rate of convergence
to zero of limit (2). Distribution-free in this context means that such a rate
is true for any possible joint distribution P of (Z, Y ). Theorem 3.1 presents
such convergence rate. Its elementary proof, deduced for the first time by
[27] and presented in [8, Theorem 12.1], is in the Appendix.
Theorem 3.1. Assume that |H| < âˆž and L(h? ) = 0. Then, for every N
and  > 0,


P L(hÌ‚) >  â‰¤ |H|eâˆ’N  .
(3)
From Theorem 3.1 we obtain as a corollary an upper bound for the sample
size needed to estimate a hypotheses hÌ‚ such that L(hÌ‚) <  with probability
1See

[20, Definition 3.3] for a more general definition of (Agnostic) PAC-learnability.

PARAMETER ESTIMATION IN DYNAMICAL SYSTEMS VIA STATISTICAL LEARNING

8

at least 1 âˆ’ Î´. This bound is obtained by solving |H|eâˆ’N  = Î´ for N , fixed 
and Î´, and is stated below.
Corollary 3.2. Define mH : (0, 1)2 â†’ Z+ as
mH (, Î´) =

1
|H|
log
.

Î´

For all , Î´ âˆˆ (0, 1), if N â‰¥ mH (, Î´), then


P L(hÌ‚) <  â‰¥ 1 âˆ’ Î´.

(4)

Remark 3.3. The concept of PAC-Learnability is related to Vapnik-Chervonenkis
(VC) Theory [25, 26, 27, 28, 29, 30]. Bounds analogous to (3) may be obtained
from VC Theory for Hypotheses Spaces with infinite cardinality or such that
L(h? ) > 0. For more details see [8, 20, 25, 26].
3.2. Parameter estimation. Robust parameter estimation in dynamical
systems may be achieved via Statistical Learning with the framework above.
Let Z âŠ† Î˜ be a finite subset of candidate parameters. It can be, for example,
a grid of Î˜ âŠ† Rd . The joint distribution of (Z, Y ) is such that, for Î¸ âˆˆ Z
and y âˆˆ {0, 1},

P (Z = Î¸, Y = y) = q(Î¸)1 F (Î¸, {XÎ¸? (t)}Tt=1 ) = y
P
in which 0 < q(Î¸) < 1 and Î¸âˆˆZ q(Î¸) =, i.e., Z has a discrete probability
distribution q in Z, and Y is equal to F (Z, {XÎ¸? (t)}Tt=1 ) with probability
one. Hence, Z represents a parameter chosen randomly from Z according to
probability distribution q and Y is equal to one if the evolution generated by
this parameter well fit the observed evolution, and zero otherwise. Therefore,
independently of the Hypotheses Spaces H, the loss function is given by
(
1, if h(Î¸) 6= F (Î¸, {XÎ¸? }Tt=1 )
`(Î¸, y; h) =
.
0, if h(Î¸) = F (Î¸, {XÎ¸? }Tt=1 )
The loss function tells us what the hypothesis h should do: point out
what are the good (well fit) and what are the bad parameters. Let p âˆˆ N be
the number of good parameters in Z:
p := {Î¸ âˆˆ Z : F (Î¸, {XÎ¸? }Tt=1 ) = 1} .

PARAMETER ESTIMATION IN DYNAMICAL SYSTEMS VIA STATISTICAL LEARNING

9

The Hypotheses Space considered in this scenario is

H := h : Z â†’ {0, 1} : h(Î¸) = 0 for all Î¸ s.t. F (Î¸, {XÎ¸? }Tt=1 ) = 0
so a hypothesis h should be interpreted as the characteristic function of a
subset of {Î¸ âˆˆ Z : F (Î¸, {XÎ¸? }Tt=1 ) = 1}. Indeed, if Î¸ is such that h(Î¸) = 1,
then it is a good parameter, hence {Î¸ : h(Î¸) = 1} is a subset of the good
parameters, for all h âˆˆ H. In this scenario, h? is the subset of the p good
parameters in Z and L(h? ) = 0. Observe that
|H| = 2p ,
so H has finite cardinality.
The number p of good parameters in Z is unknown, but strictly related
to the choice of fitness map F . On the one hand, if the concept of well fit
is too restrict, then less parameters will be good, hence one has a small p.
In this case, one may not be able to obtain robust evidences about XÎ¸? (t)
due to the small number of candidate evolutions. On the other hand, if the
concept of well fit is too loose, then more parameters may meet the criteria
of a good one, so greater is p. As we want a robust estimation, p should
be much greater than one, so there are a handful of evolutions available
from which to obtain evidences about the evolution of XÎ¸? (t). Nevertheless,
if p is too great, then probably our definition of well fit is too loose, so
the candidate systems may not be homogeneously related to XÎ¸? , rendering
useless evidences about XÎ¸? (t). Hence, both Z and F should be chosen based
on all prior information available about the problem at hand, so p  1, but
the good parameters actually generate meaningful trajectories.
After one fixes Z and F , the ERM principle is then applied to estimate
a set of at most p good parameters. The estimated sets are
n
o
hÌ‚ = h âˆˆ H : h(Zi ) = F (Zi , {XÎ¸? }Tt=1 ), âˆ€i âˆˆ {1, . . . , N }
i.e., the subsets of good parameters which contain all of the observed in the
sample, which generates the following set of good parameters:
n
o
Î˜Ì‚ = Zi : F (Zi , {XÎ¸? }Tt=1 ) = 1 .
Therefore, the set de facto estimated by the ERM principle is hÌ‚ such that
(
1, if Î¸ = Zi s.t. F (Zi , {XÎ¸? }Tt=1 ) = 1
hÌ‚(Î¸) =
0, if Î¸ âˆˆ Z \ {Zi : F (Zi , {XÎ¸? }Tt=1 ) = 1}

PARAMETER ESTIMATION IN DYNAMICAL SYSTEMS VIA STATISTICAL LEARNING 10

that is, the set containing only the good parameters presented in the sample.
From now on when we denote hÌ‚ we mean the set above.
Observe that
X
X
L(hÌ‚) =
(1 âˆ’ hÌ‚(Î¸)) q(Î¸) =
q(Î¸),
Î¸âˆˆZ
F (Î¸,{XÎ¸? }T
t=1 )=1

Î¸âˆˆZ
F (Î¸,{XÎ¸? }T
t=1 )=1
Î¸âˆˆ{Z
/ 1 ,...,ZN }

the measure of the good parameters not present in the sample. Since
G({XÎ¸? }Tt=1 ) := P(Î¸ : F (Î¸, {XÎ¸? }Tt=1 ) = 1),
an upper bound for the quantity above, may be very small, simply having
L(hÌ‚) <  is not meaningful if such probability is lesser than . Hence, in
order to assess if L(hÌ‚) is small, one should compare it with G({XÎ¸? }Tt=1 ), to
establish if it is small relatively to G({XÎ¸? }Tt=1 ).
In order to perform such a comparison, we apply Theorem 3, which
state that the probability of having an out-of-sample error greater than
cG({XÎ¸? }Tt=1 ), which means that a proportion at least 0 < c < 1 of the
measure of the set of all good parameters is not observed in the sample, is
such that


T
T
?
P LDN (hÌ‚) > cG({XÎ¸ }t=1 ) â‰¤ 2p eâˆ’cG({XÎ¸? }t=1 )N
(5)
so it follows from Corollary 3.2 that, in order to obtain a proportion2 at least
(1 âˆ’ c) of the good parameters in the sample, with a probability greater than
1 âˆ’ Î´, one needs a sample of size at least
mH (c, Î´) =

1
[p log 2 âˆ’ log Î´] .
cG({XÎ¸? }Tt=1 )

(6)

Assuming that p âˆ¼ G({XÎ¸? }Tt=1 )|Z|, the sample size above is meaningful only
if
c â‰¥ log 2 âˆ’

log Î´
> 0.69,
p

for otherwise mH (c, Î´) > |Z|. This results is reasonable since, as N â†’ |Z|,
the cardinality of {Z1 , . . . , ZN } does not converge to |Z| with probability one
for there will be points which are sampled more than once. Hence, to observe
2In

the sense of measure, not necessarily a proportion of the number of good parameters.

PARAMETER ESTIMATION IN DYNAMICAL SYSTEMS VIA STATISTICAL LEARNING 11

a proportion of the good parameters one needs to sample more models than
there are in Z, so this method is better than an exhaustive search of Z only if
one wants a small number of good parameters when compared to the number
of good parameters in Z.
The quantities on the right-hand side of (5) and (6) are not known, since
p and G({XÎ¸? }Tt=1 ) are unknown. However, the sample size above is an increasing function of p, therefore, given an upper bound to p, one can obtain a
greater sample size that guarantees (4). Also, G({XÎ¸? }Tt=1 ) may be estimated
by a pre-sample as the proportion of good parameters in it.
If q(Î¸) = G({XÎ¸? }Tt=1 )/p for all Î¸ such that F (Î¸, {XÎ¸? }Tt=1 ) = 1 then the
bounds in (5) and the sample size (6) may be improved. The bound below
is much better than (6), especially for c close to 1.
Proposition 3.4. If the measure q conditioned on F (Î¸, {XÎ¸? }Tt=1 ) = 1 is
uniform then


P LDN (hÌ‚) >



cG({XÎ¸? }Tt=1 )

(1âˆ’c)p 

â‰¤

X
k=0


p âˆ’cG({XÎ¸? }Tt=1 )N
e
k

from which follows
 




p
(1 âˆ’ c)2 p2
1
log
+ log 1 +
âˆ’ log Î´
mH (c, Î´) â‰¤
(1 âˆ’ c)p
cp + 1
cG({XÎ¸? }Tt=1 )
(7)
if c > 1/2.
Figure 1 presents mH (c, Î´) given by (6) (dashed) and its upper bound
when q conditioned on the good parameters is uniform (solid line), as a
function of c, with Î´ = 0.1, G({XÎ¸? }Tt=1 ) = 0.001, |Z| = 106 and p = 1, 000.
We see that, for example, to obtain 10% of the good parameters one has to
sample a proportion around 0.35 and 0.8 when q conditioned is uniform and
in general, respectively. Therefore, taking q close to the uniform distribution
when conditioned on the good parameters is better since less samples are
needed to find a given proportion of the good parameters.
Since the sample size above is distribution-free, i.e., true for any distribution, it will be an overestimated number in most cases. Even the sample
size when q conditioned on the good parameters is uniform tends to be overestimated. For example, if q is the uniform distribution on Z, in order to
obtain a proportion 1 âˆ’ c of the good parameters one should have a sample of

PARAMETER ESTIMATION IN DYNAMICAL SYSTEMS VIA STATISTICAL LEARNING 12

1.0
0.9
0.8
0.7
0.6

m
|Z| 0.5
0.4
0.3
0.2
0.1
0.0
0.70

0.75

0.80

0.85

0.90

0.95

1.00

c

Figure 1. Sample size mH (c, Î´) given by (6) (dashed) and
its upper bound (7) when q conditioned on the good parameters is uniform (solid line), as a function of c, with Î´ = 0.1,
G({XÎ¸? }Tt=1 ) = 0.001, |Z| = 106 and p = 1, 000.
the order (1 âˆ’ c)|Z| which is lesser than the one calculated above. However,
when q is far from the uniform distribution, the researcher does not have
control over q, i.e., he cannot choose it, or it is unknown, the sample size
above may be a tool to assess beforehand the computational complexity of
the estimation process, rather than a number that must be achieved.

4. Examples
4.1. SIR. The spread of infectious diseases within a population is generally studied via compartmental epidemiological models [4, 31], in which an
individual is in either one of a handful of states related to the disease (Susceptible, Exposed, Infected, Recovered, etc...), and changes states according to
some rates. In these models, the rates, which have epidemiological meaning,
are the parameters, which should be estimated for each disease and different
populations.

PARAMETER ESTIMATION IN DYNAMICAL SYSTEMS VIA STATISTICAL LEARNING 13

We consider the family of Susceptible-Infected-Recovered (SIR) dynamical systems generated by the following difference equations
ï£±
ï£´
ï£²S(t + 1) âˆ’ S(t) = âˆ’Î² I(t)S(t)
(8)
I(t + 1) âˆ’ I(t) = Î² I(t)S(t) âˆ’ Î³ I(t)
ï£´
ï£³
R(t + 1) âˆ’ R(t) = Î³ I(t)
with (S(0), I(0), R(0)) = c(0.95, 0.05, 0). In this case, â„¦ = {x âˆˆ [0, 1]3 :
x1 + x2 + x3 = 1}, Î¸ = (Î², Î³) and Î˜ = R2+ . The observed evolution will be
that generated by Î¸? = (0.25, 1/21), for t â‰¤ 10, and the fitness map will be
n
o
? (t))i | â‰¤ r (XÎ¸ ? (t))i
F (Î¸, {XÎ¸? (t)}10
)
=
min
min
1
|(X
(t))
âˆ’
(X
Î¸
i
Î¸
t=1
tâˆˆ{1,...,T } iâˆˆ{1,2,3}

so a parameter Î¸ is good if the evolution {XÎ¸ (t)}10
t=1 is within 100r% of
10
{XÎ¸ (t)}t=1 for all three compartments, 0 < r < 1.
In the simulations, we consider as candidate models a grid of squares with
side 0.001 of the following three subsets of Î˜
Z1 = (0, 1] Ã— (0, 0.5]

Z1 = (0, 1] Ã— (0, 0.2]

Z3 = [0.1, 0.5] Ã— (0, 0.2]

which represent distinct prior knowledge about parameters Î² and Î³. If one
does not known much about the parameters, he may assume that Î² is lesser
than 1 and Î³ lesser than 0.5, considering Z1 . If one has prior information
about Î³, that it is no more than 0.2, then he may consider Z2 . Finally, if
one has also prior knowledge about Î², that it is between 0.1 and 0.5, then
he may consider Z3 . The cardinality of these sets are, respectively, 5 Ã— 105 ,
2 Ã— 105 and 80, 200. We take q(Î¸) as the uniform measure in the respective
set Zi .
Two values of r, namely, 0.05 and 0.1, are considered, and c = 0.9 is
chosen to obtain 10% of the good models. To illustrate the method, an
exhaustive search of all sets was performed in order to calculate G({XÎ¸? }Tt=1 )
and p, even though it will not be performed when applying the method
on practice. The value of p is the same in all sets, although G({XÎ¸? }Tt=1 )
is greater in the smallest set Z3 , that is the one which incorporated more
prior information about the problem. Also, an upper bound for the sample
size needed to obtain a same proportion of these good parameters, 10% in
this case, is much lesser in the smallest set, illustrating that as more prior
information about the problem at hand is incorporated in the estimation

PARAMETER ESTIMATION IN DYNAMICAL SYSTEMS VIA STATISTICAL LEARNING 14

process, less samples are needed. Observe that the fitness map with r = 0.1
is a more loose definition of goodness-of-fit, hence there are almost four times
the number of good models when r = 0.05.
Table 1. Values of G({XÎ¸? }Tt=1 ), p and m(0.9, 0.01) for each
set and fitness map.
Z G({XÎ¸? }Tt=1 ) p m(0.9, 0.01)
Z1
0.000136
68
211,219
0.05 Z2
0.00034
68
84,487
Z3
0.00084
68
33,879
Z1
0.000526
263
186,534
0.10 Z2
0.001315
263
74,613
Z3
0.003279
263
29,920
r

Figure 2 shows the evolution of the good models obtained in each scenario,
and Table 2 presents descriptive statistics of the good parameters and the
peak of infected, i.e., the day with more simultaneously infected individuals,
simulated by the good parameters. The evolution generated by the good
parameters is close to the evolution of the SIR for all times, not only the
first ten which were used in the estimation. Also, Table 2 shows that the
values of Î² and Î³ of the good parameters are close to the real ones. Finally,
the peak of infected individuals, which for the observed model occurs at day
24, was predicted within an error of no more than two days by the evolution
generated by the good parameters in all scenarios. We conclude that, by
applying the method developed in this paper, we may obtain informations
about the parameters and qualitative behaviour of the SIR model even when
we do not have much prior information of the parameters (Z1 ) or when we
choose a more loose fitness map (r = 0.1).
4.2. SEIR for COVID-19. In order to mitigate the effects of COVID-19
pandemic, the greatest world health crisis in a century, governments and the
population have sought reliable information about its spread. More than
quantitative predictions about cases and deaths tolls, the main questions
regarding the disease spread are about certain milestones such as its peak,
when it will be over and if or when the available intensive care units will
be full. In this section we illustrate, with data about its spread in the US,
how the method developed in this paper may be useful in answering such
questions about COVID-19.

PARAMETER ESTIMATION IN DYNAMICAL SYSTEMS VIA STATISTICAL LEARNING 15

Table 2. Sample size, number of good parameters in the sample and descriptive statistics of Î², Î³ and the peak of the disease
for the evolution generated by the good parameters.

Z1
Z2
Z3

r

N

0.05 211,219
0.1 186,534
0.05 84,488
0.1 74,614
0.05 33,880
0.1 29,920

|Î˜Ì‚|
25
86
30
85
25
90

Î²
Î³
Peak
Mean Median Min Max Mean Median Min Max Mean Median
0.249 0.248 0.242 0.256 0.047 0.047 0.046 0.049
24
24
0.249 0.248 0.232 0.266 0.048 0.048 0.043 0.052 23.988
24
0.250 0.249 0.241 0.258 0.047 0.047 0.046 0.049 23.933
24
0.251 0.251 0.232 0.268 0.047 0.047 0.043 0.052 23.918
24
0.250 0.249 0.242 0.257 0.048 0.048 0.046 0.049 23.880
24
0.250 0.251 0.232 0.268 0.047 0.048 0.043 0.052 23.978
24
Z2 and r = 0.05

0.75

Proportion of population

Proportion of population

Z1 and r = 0.05

0.50

0.25

0.00

0.75

0.50

0.25

0.00
0

20

40

60

20

40

60

0

0.25

0.00
60

40

60

Time

0.75

0.50

0.25

0.00
40

20

Z3 and r = 0.1

Proportion of population

Proportion of population

Proportion of population

0.25

Z2 and r = 0.1

0.50

20

0.50

Time

0.75

0

0.75

0.00
0

Time
Z1 and r = 0.1

Min Max
23
25
23
26
23
25
22
26
23
25
22
26

Z3 and r = 0.05

Proportion of population

Z

0.75

0.50

0.25

0.00
0

20

40

Time

60

Time
State

S

I

0

20

40

60

Time
R

Figure 2. The SIR evolution (solid line), and the mean evolution generated by the good parameters (dashed line) in each
scenario. The ribbon refers to the area between the minimum
and the maximum evolution generated by the good parameters.
In order to account for features of COVID-19, we add a compartment
to the usual SEIR model (see [14] for a review of compartmental models).
After being exposed to the disease, an individual becomes Infected with
rate Î³I and is able to infect others. However, once infected, an individual

PARAMETER ESTIMATION IN DYNAMICAL SYSTEMS VIA STATISTICAL LEARNING 16

either recovers with rate Î½R or is accounted by the official statistics and
becomes Infected in Statistics with rate Î³S , and is not able to infect others.
Hence, we are assuming that a parcel of the infected is never accounted by
official statistics and that, once diagnosed with COVID-19, an individual
will not infect others, as it will be either admitted to a hospital or be in
quarantine. These assumptions account for features of the disease such as
the under-notification of cases. We also add a compartment for Deaths by
the disease and assume that natural deaths and births balance out, so they
are not considered to simplify the model, which is illustrated in the diagram
of Figure 3.
Susceptible

Î²

Exposed

Î³I

Infected

Î³S

Infected in Statistics

Î½R

Î½R

Recovered

S

Î´

Death

Figure 3. Diagram of the SEIR model for COVID-19.
The evolution of the Susceptible (S), Exposed (E), Infected (I), Infected
in Statistics (Is ), Recovered (R) and Deaths (D) by COVID-19 is ruled by
the difference equations
Î²S(t)
I(t)
(N âˆ’ D(t))
Î²S(t)
E(t + 1) âˆ’ E(t) = âˆ’Î³I E(t) +
I(t)
(N âˆ’ D(t))
I(t + 1) âˆ’ I(t) = âˆ’ (Î½R + Î³S ) I(t) + Î³I E(t)
S(t + 1) âˆ’ S(t) = âˆ’

IS (t + 1) âˆ’ IS (t) = âˆ’(Î½RS + Î´)IS (t) + Î³S I(t)
R(t + 1) âˆ’ R(t) = Î½R I(t) + Î½RS IS (t)
D(t + 1) âˆ’ D(t) = Î´IS (t)
in which N is the population size. We assume that the time scale is that of
days, so t + 1 is the day after t, and consider the absolute number of individuals in each compartment, rather than the proportion of the population.

PARAMETER ESTIMATION IN DYNAMICAL SYSTEMS VIA STATISTICAL LEARNING 17

This model has a nice biological meaning and its parameters are related
to characteristics of the disease. Indeed, apart from the force of infection,
they are functions of the disease average exposed time Ï„E , average time infected before getting into statistics Ï„S , average time infected before recovering
without being accounted by statistics Ï„R , average time before recovering after
being accounted by statistics Ï„RS , average time until death once accounted
by statistics Ï„D , and the proportions of infected individuals accounted by
statistics pS , and of deaths among them pD :
Î³I =

1
Ï„E

Î³S =

pS
Ï„S

Î½R =

(1 âˆ’ pS )
Ï„R

Î½RS =

(1 âˆ’ pD )
Ï„RS

Î´=

pD
.
Ï„D

Hence, after being exposed, an individual become infected after a time with
mean Ï„E . Among the infected, a proportion pS will be accounted by official
statistics after a time with mean Ï„S , and not be able to infect any more,
while a proportion 1 âˆ’ pS will recover after a time with mean Ï„R without ever
being accounted by official statistics. Then, a proportion pD of the Infected
in Statistics will die after a time with mean Ï„D and a proportion 1 âˆ’ pD will
recover after a time with mean Ï„RS . Once recovered, an individual cannot
become infected again.
Apart from pD , which can be estimated directly from data as the proportion of deaths among infected accounted by statistics, the other parameters
need to be estimated. Since the rates of the model may be calculated from
the mean average time in each compartment and the proportion of infected
accounted by statistics, we consider
Î¸ := (Î², Ï„E , Ï„R , Ï„S , Ï„RS , Ï„D , pS ) âˆˆ Î˜ = R6+ Ã— [0, 1]
as the free parameters to be estimated. The candidate values for each parameter are presented in Table 3. The number of candidate models is
|Z| = 116, 121, 600.
In order to illustrate the method, we will estimate the parameters multiple times considering goodness-of-fit during consecutive periods of 7 days
starting on March 20th 2020, i.e., we will apply the method for multiple t0 ,
with 7 days apart from one to the next, starting on March 20th until midJuly. Goodness-of-fit will be tested against data about COVID-19 spread in
the US compiled by Johns Hopkins Coronavirus Resource Center [9]. The
observed data is smoothed by taking a centred seven day average of the

PARAMETER ESTIMATION IN DYNAMICAL SYSTEMS VIA STATISTICAL LEARNING 18

Table 3. Candidate parameters of the model
Parameter
Candidates
Î²
0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 1.1, 1.2, 1.3, 1.4, 1.5
Ï„E
4, 5, 6, 7
Ï„R
5, 6, 7, . . . , 14
Ï„S
3, 4, 5, . . . , 14
Ï„RS
5, 6, 7, . . . , 28
Ï„D
1, 2, 3, . . . , 28
pS
0.01, 0.025, 0.05, 0.075, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99

incidence of confirmed cases, deaths and recovered, and the prevalences calculated by summing the smoothed incidences. Since only two compartments,
namely Infected in Statistics and Deaths, are fully observed, the goodnessof-fit of a candidate model should be defined comparing the predicted with
their observed values in these compartments. Denote X(t) = (Is (t), D(t))
and XÎ¸ (t) = (Is (t; Î¸), D(t; Î¸)) the observed and predicted by model with parameter Î¸, respectively, Infected in Statistics and Deaths. With this notation,
the fitness map considered is


XÎ¸ (t) âˆ’ X(t)
t0 +6
â‰¤ r(t0 )
F (Î¸, {X(t)}t=t0 ) =
min
1
tâˆˆ{t0 ,...,t0 +6}
X(t)
âˆž
for each t0 considered and r(t0 ) âˆˆ (0, 1), in which division by X(t) is componentwise. Hence, a parameter Î¸ well fit the observed data if the predicted value
of Infected in Statistics and Deaths is within 100r(t0 )% of the respective observed value for everyday in the week starting on t0 . The value of r must
depend on t0 since there are moments when the disease spread is better explained by a SEIR model, as when few measures are implemented to slow its
evolution, as will be seen below.
We estimated the model every seven days in order to asses its peak if
the disease were to keep spreading as in the week starting on t0 . Due to the
enforcement of measures to flatten the epidemiological curve, the parameters
may be actually time-dependent, specially the force of infection Î². Hence,
estimating the peak assessing goodness-of-fit only against the first weeks of
the disease spread is doomed to fail, since the force of infection changes from
time to time. On the other hand, fitting the model every week, one can see
changes on the peak estimate over time, which will reflect the enforcement
or looseness of measures to slow the spread, providing a robust and better
picture of the future. Observe that the method provides evidences about the

PARAMETER ESTIMATION IN DYNAMICAL SYSTEMS VIA STATISTICAL LEARNING 19

disease spread if it were to continue as in the week when goodness-of-fit was
assessed, not providing forecasts of distinct scenarios.
To estimate the parameters and then simulate the chosen models starting
on t0 , we need to estimate the initial condition for compartments E and I,
which are not observed, and R which is only partially observed. Assume that
we fixed a vector Î¸ of candidate parameters. Denoting Rs as the recovered
among the accounted by statistics, we take


(1 âˆ’ pS )
(1 âˆ’ pS )
Is (t)
R(t) =
+ 1 Rs (t)
I(t) =
pS
pS
for t âˆˆ {t0 , . . . , t0 + 9}. Proceeding in this manner, we have that among all
infected (I +Is ) and recovered (R), a proportion pS is accounted by statistics.
To estimate the exposed, we obtain from the equations ruling the evolution
of the disease that
I(t + 1) = âˆ’ (Î½R + Î³S âˆ’ 1) I(t) + Î³I E(t)
hence
E(t) =

1
[I(t + 1) + (Î½R + Î³S âˆ’ 1)I(t)]
Î³I

so we calculate E(t0 ) from I(t0 ) and I(t0 + 1). Observe that each candidate
model has a distinct initial condition on the unobserved compartments, dependent on its parameters. We take pD as the moving death rate considering
only the confirmed cases and deaths according to the smoothed data of the
week ending on t0 , so it express the death rate around t0 .
The error r(t0 ) was calculated by sampling 100, 000 models and observing
the minimum difference
min
tâˆˆ{t0 ,...,t0 +6}

XÎ¸ (t) âˆ’ X(t)
X(t)

âˆž

over the sampled Î¸, which gives an estimative how how well the SEIR model
approximates the evolution on the week starting on t0 . The error r(t0 ) was
then taken as 1.1 times this minimum difference. We could not choose an
absolute r beforehand since there is not a model approximating arbitrarily
well the evolution for all t0 as it does not follow exactly a SEIR model. The
values of r(t0 ) were around 0.03 until April 10th when it dropped to around
0.01. On May and until mid-June the values of r(t0 ) were more or less 0.07,

PARAMETER ESTIMATION IN DYNAMICAL SYSTEMS VIA STATISTICAL LEARNING 20

dropped to 0.03 on the second half of June and then to 0.01 on the first half
of July. Fixed r(t0 ), for each t0 there were sampled 500, 000 models.
Figure 4 shows the box-plot of the parameters of the good models for each
t0 , and the number of good models (m) among the sampled ones. Figure 5
shows the Infected in Statistics and Deaths according to the smoothed data
and simulated by the good models for the seven days starting on their t0 .
We first see that the distribution of Ï„E , Ï„R and Ï„RS , which are mainly related
to features of the disease, does not vary very much over time. Also, we see
that the good models better fit observed data when implemented measures
to slow its spread were not effective, namely, until the end of April, when
it was spreading mainly in the Northeast region, and from mid-June when
it started spreading rapidly in other states, specially in the South. This is
reasonable, since SEIR models assume that every individual which has not
ever been infected is susceptible to the disease and that any contact between
an infected and a susceptible has a same probability of infection, which is
not true if part of the population is on lockdown or if social distance and
other measures, such as mask wearing, are implemented. Hence it is expected
a poor performance of the model when such measures are implemented, at
least in what regards the number of confirmed cases and deaths.
The behaviour of the estimated parameters over time evidences interesting qualitative features of the disease spread in the US. The moving rate of
death pD started low, but more than tripled in the following weeks, slowly
decreasing over time, what may be due to the increase in testing or the fact
that deaths among the new infected is yet to occur [7]. The number of good
models was great when the disease was spreading more slowly, what is due
to the employment of a more loose fitness map, since a SEIR model could
not well fit data, in a absolute sense, in this period, as can be seem in the
simulated and observed curves of Figure 5.
The values of Ï„S and Ï„D started small and then increased until an equilibrium around 11 and 27 days, respectively. This may be due to the fact that
early on there have been a lot of people dying at home, specially on nursing
homes [22], so they may have died rapidly due to underlying health problems
not having the time to procure health assistance, so their time infected before getting into statistics was small, and they were accounted as infected in
statistics and deaths at the same time, so their time until death after being
accounted by statistics is zero; these cases cause the mean times Ï„S and Ï„D
to be small.

PARAMETER ESTIMATION IN DYNAMICAL SYSTEMS VIA STATISTICAL LEARNING 21

Î²

m

pD

1.5

2500
2000

0.06
1.0

1500

0.04

1000
0.5
500

0.02
0

0.0
3/20 4/03 4/17 5/01 5/15 5/29 6/12 6/26 7/10

3/20 4/03 4/17 5/01 5/15 5/29 6/12 6/26 7/10

pS

Ï„D

3/20 4/03 4/17 5/01 5/15 5/29 6/12 6/26 7/10
Ï„E
7

0.75

Value

20

6

0.50
5

10

0.25

4

0.00
3/20 4/03 4/17 5/01 5/15 5/29 6/12 6/26 7/10

3/20 4/03 4/17 5/01 5/15 5/29 6/12 6/26 7/10

3/20 4/03 4/17 5/01 5/15 5/29 6/12 6/26 7/10

Ï„R

Ï„S

Ï„RS

12.5

12.5

20

10.0
10.0
7.5
7.5

10

5.0
5.0
3/20 4/03 4/17 5/01 5/15 5/29 6/12 6/26 7/10

2.5

3/20 4/03 4/17 5/01 5/15 5/29 6/12 6/26 7/10

3/20 4/03 4/17 5/01 5/15 5/29 6/12 6/26 7/10

t0

Figure 4. Number of good parameters among the 500, 000
sampled (m) and box-plot of the set of good parameters sampled for each t0 . The value of pD is calculated directly from
data hence is the same for all models.

Observe that, when the model was not well fitting data, the range of
most estimated parameters contemplated all the candidates, what tells us
that, with a right combination of parameters, any given value of a parameter
can generate an evolution that fit data, according to the loose definition of
well fit. On the other hand, when the model was absolutely well fitting the
data, until the end of April and from mid-June, the parameters are more
meaningful. This is specially true for pD , which started low until mid-April,
and when the disease spread increased again at mid-June it was again low,
but greater than on March and April, illustrating the increase on testing.
Finally, the parameter Î² tended to be greater on mid-June than on the time

PARAMETER ESTIMATION IN DYNAMICAL SYSTEMS VIA STATISTICAL LEARNING 22

3e+06
1e+05

Infected in Statistica

Accumulated Deaths

1e+06

1e+04

3e+05

1e+05

1e+03
3e+04

3/20 4/03 4/17 5/01 5/15 5/29 6/12 6/26 7/10

Date

3/20 4/03 4/17 5/01 5/15 5/29 6/12 6/26 7/10

Date

Figure 5. Infected in Statistics and Deaths according to the
smoothed data (solid line) and the good models (blue lines).
The good models were simulated only on the seven days starting on their t0 .

until the end of April, evidencing that the force of infection in the second
moment of fast spread is greater; the values when the model was not fitting
data are not meaningful since vary over the range of candidate parameters.
Figure 6 presents the median and selected percentiles of the peak of
deaths, i.e., the day with more deaths after t0 , estimated by the good models
in each t0 . The models fitted between March 20th and April 3rd predicted
that the the peak could happen around April 15th, which was the peak observed on the smoothed data, since the percentile 2.5% of them was on this
day. Also, for t0 equal to April 10th, the median of the models was pointing
to a close peak, although it got it wrong by one week. After the first peak,
for some t0 it was predicted as a good scenario no more peaks (when the peak
is in t0 ), but then, for the months in which the disease was spreading more
slowly, the peak was predicted for a sequence of t0 as around one month after
it. This was the behavior until the end of June when the peak started to be
predicted as more distant, evidencing the future increase on the incidence of
deaths in the mid-term before it start decreasing.
This example illustrates how this simple estimation method may offer
many insights into qualitative properties of the disease spread and aid in

PARAMETER ESTIMATION IN DYNAMICAL SYSTEMS VIA STATISTICAL LEARNING 23

10/16

Estimated Peak

9/16
8/17
7/18
6/18
5/19
4/19
3/20
3/20

4/03

4/17

5/01

5/15

5/29

6/12

6/26

7/10

t0

Figure 6. Median estimate for the peak of deaths among the
good models sampled for each t0 (dots), with the percentiles 2.5
and 97.5% represented in red. The horizontal line represents
the first peak of deaths according to the smoothed data. The
solid line is the identity function.
predicting its future spread. By fitting this model every week during an
outbreak, one may analyse its predictions and estimated parameters over
time to assess the effectiveness of measures implemented to slow the spread,
and some qualitative features of present and future evolution.
5. Final Remarks
The estimation method for dynamical systems parameters proposed in
this paper aims to find a subset Î˜Ì‚ âŠ† Î˜ of parameters that well fit the observed evolution in a time interval. This is a more humble agenda when compared with methods that try to find exactly the parameter which generated
the evolution, and is quite robust when evidencing qualitative behaviour of
evolution, specially ones that are only approximated by dynamical systems,
as was illustrated by the study of the COVID-19 spread on the US. In essence,
the proposed method is a mathematical formalization of the procedure consisting of trial and error for parameter estimation, in which the evolution
generated by candidate parameters are tested against observed data to determine which well fit it. The Statistical Learning framework introduces a

PARAMETER ESTIMATION IN DYNAMICAL SYSTEMS VIA STATISTICAL LEARNING 24

mathematically rigorous scheme to this general approach for parameter estimation, adding to the great field of parameter estimation in dynamical
systems.
The method is quite general, as it does not rely on any specific property
of the system, and highly flexible, since one may define the set of candidate
parameters Z, the fitness map F and select any sampling algorithm on Z to
obtain distinct methods for parameter estimation. An interesting topic for
future research is to compare the methodâ€™s results and computation complexity with established ones in the literature, such as Bayesian methods, which
can also be computational demanding. Another topic for future research
would be to consider Z with infinity cardinality, but choose H and F in a
way such that H has finite VC-dimension, so results analogous to Theorem
3.1 are true. With such a result, one could consider sampling parameters
from a set with infinity cardinality.
The method may be quite useful for disease spread models which have
the properties it was developed to address: disease spread does not exactly
follow a compartmental model and there is usually an interest in the qualitative behaviour of the evolution, rather than in its exact trajectory. These
qualitative properties may aid the population and government officials in the
decision making process during an outbreak, hence, as illustrated with the
COVID-19 spread in the US, fitting a compartmental model weekly may be
a rich source of information about the disease spread, even if it cannot give
reliable predictions on the number of cases and deaths for the long-term.
The shortcomings of the method are the need for high computational
power to simulate the evolution generated by thousands of parameters, and
the need to carefully define Z beforehand, properly identifying the set of
candidate parameters. Nevertheless, it may be a good option to robustly
estimate parameters in complex models, specially when there is interest in
the qualitative behaviour of the trajectories.

Acknowledgements
The author has received financial support from CNPq during the development of this paper. I thank C. Peixoto, P. Peixoto and S. Oliva for fruitful
discussions about the modelling of disease spread, specially COVID-19, and
about the development of compartmental models to address it.

PARAMETER ESTIMATION IN DYNAMICAL SYSTEMS VIA STATISTICAL LEARNING 25

References
[1] Yaser S Abu-Mostafa, Malik Magdon-Ismail, and Hsuan-Tien Lin. Learning from
data, volume 4. AMLBook New York, NY, USA:, 2012.
[2] Patrick Billingsley. Probability and measure. John Wiley & Sons, 2008.
[3] BegoÃ±a CantÃ³, Carmen Coll, and Elena SÃ¡nchez. Estimation of parameters in a structured sir model. Advances in Difference Equations, 2017(1):33, 2017.
[4] Vincenzo Capasso. Mathematical structures of epidemic systems, volume 97. Springer
Science & Business Media, 2008.
[5] Jianye Ching, James L Beck, and Keith A Porter. Bayesian state and parameter estimation of uncertain dynamical systems. Probabilistic engineering mechanics,
21(1):81â€“96, 2006.
[6] Alan A Coley. Dynamical systems and cosmology, volume 291. Springer Science &
Business Media, 2013.
[7] Matthew Collen. The rise in testing is not driving the rise in u.s. virus cases. The
New York Times.
[8] Luc Devroye, LÃ¡szlÃ³ GyÃ¶rfi, and GÃ¡bor Lugosi. A probabilistic theory of pattern recognition, volume 31. Springer, 1996.
[9] Ensheng Dong, Hongru Du, and Lauren Gardner. An interactive web-based dashboard
to track covid-19 in real time. The Lancet infectious diseases, 20(5):533â€“534, 2020.
[10] Zoubin Ghahramani and Geoffrey E Hinton. Parameter estimation for linear dynamical systems. Technical report, Technical Report CRG-TR-96-2, University of
Totronto, Dept. of Computer Science, 1996.
[11] Zoubin Ghahramani and Sam T Roweis. Learning nonlinear dynamical systems using
an em algorithm. In Advances in neural information processing systems, pages 431â€“
437, 1999.
[12] PL Green and K Worden. Bayesian and markov chain monte carlo methods for
identifying nonlinear systems in the presence of uncertainty. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences,
373(2051):20140405, 2015.
[13] Ya Gu and Rui Ding. A least squares identification algorithm for a state space model
with multi-state delays. Applied Mathematics Letters, 26(7):748â€“753, 2013.
[14] Herbert W Hethcote. The mathematics of infectious diseases. SIAM review, 42(4):599â€“
653, 2000.
[15] Trachette Jackson and Ami Radunskaya. Applications of Dynamical Systems in Biology and Medicine, volume 158. Springer, 2015.
[16] Zhien Ma. Dynamical modeling and analysis of epidemics. World Scientific, 2009.

PARAMETER ESTIMATION IN DYNAMICAL SYSTEMS VIA STATISTICAL LEARNING 26

[17] Pedro S Peixoto, Diego Marcondes, ClÃ¡udia Peixoto, and SÃ©rgio M Oliva. Modeling
future spread of infections via mobile geolocation data and population dynamics. an
application to covid-19 in brazil. PloS one, 15(7):e0235732, 2020.
[18] Andreas Raue, Bernhard Steiert, Max Schelker, Clemens Kreutz, Tim Maiwald, Helge
Hass, Joep Vanlier, C TÃ¶nsing, L Adlung, R Engesser, et al. Data2dynamics: a modeling environment tailored to parameter estimation in dynamical systems. Bioinformatics, 31(21):3558â€“3560, 2015.
[19] Robert Rosen. Dynamical system theory in biology, Volume I. Stability theory and its
applications. New York: Wiley, 1970.
[20] Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From
theory to algorithms. Cambridge university press, 2014.
[21] Stephen Strogatz. Nonlinear dynamics and chaos: with applications to physics, biology, chemistry, and engineering (studies in nonlinearity). 2001.
[22] The New York Times. More than 40% of u.s. coronavirus deaths are linked to nursing
homes. The New York Times.
[23] Tina Toni, David Welch, Natalja Strelkowa, Andreas Ipsen, and Michael PH Stumpf.
Approximate bayesian computation scheme for parameter inference and model selection in dynamical systems. Journal of the Royal Society Interface, 6(31):187â€“202,
2009.
[24] Leslie G Valiant. A theory of the learnable. Communications of the ACM,
27(11):1134â€“1142, 1984.
[25] Vladimir Vapnik. Statistical learning theory. 1998, volume 3. Wiley, New York, 1998.
[26] Vladimir Vapnik. The nature of statistical learning theory. Springer science & business
media, 2000.
[27] Vladimir N Vapnik and Alexey J Chervonenkis. Theory of pattern recognition. 1974.
[28] Vladimir N Vapnik and Alexey Ya Chervonenkis. Oordered risk minimization ii. Automation and Remote Control, 35(9):1403â€“1412, 1974.
[29] Vladimir N Vapnik and Alexey Ya Chervonenkis. Ordered risk minimization i. Automation and Remote Control, 35(8):1226â€“1235, 1974.
[30] VN Vapnik and A Ya Chervonenkis. On the uniform convergence of relative frequencies of events to their probabilities. Theory of Probability and its Applications,
16:264â€“280, 1971.
[31] Emilia Vynnycky and Richard White. An introduction to infectious disease modelling.
OUP oxford, 2010.
[32] Jacco Wallinga and Marc Lipsitch. How generation intervals shape the relationship
between growth rates and reproductive numbers. Proceedings of the Royal Society B:
Biological Sciences, 274(1609):599â€“604, 2007.

PARAMETER ESTIMATION IN DYNAMICAL SYSTEMS VIA STATISTICAL LEARNING 27

[33] Jianyong Wu, Radhika Dhingra, Manoj Gambhir, and Justin V Remais. Sensitivity analysis of infectious disease models: methods, advances and their application.
Journal of The Royal Society Interface, 10(86):20121018, 2013.
[34] Ling Xu. Application of the newton iteration algorithm to the parameter estimation
for dynamical systems. Journal of Computational and Applied Mathematics, 288:33â€“
43, 2015.

Appendix A. Proof of results
Proof of Theorem 3.1. Observe that LDN (hÌ‚) = 0, since at least LDN (h? ) = 0.
Now, if L(hÌ‚) > , then
max
L(h) > 
hâˆˆH:LDN (h)=0



hence P L(hÌ‚) >  is lesser or equal to

P

max
hâˆˆH:LDN (h)=0




L(h) >  = E max 1 {LDN (h) = 0} 1 {L(h) > } .
hâˆˆH

Applying the union bound on the last expectation above we obtain that


X
P L(hÌ‚) >  â‰¤
P (LDN (h) = 0) â‰¤ |H|(1 âˆ’ )N
hâˆˆH:L(h)>

since the probability that no pair (Zi , Yi ), i = 1, . . . , N, falls in the set {(z, y) :
h(z) 6= y} is less than (1 âˆ’ )N , as the probability of this set is L(h) >  and
the pairs are independent. The result follows since (1 âˆ’ ) â‰¤ eâˆ’ .

Proof of Proposition 3.4. From the proof of Theorem 3.1 we have that


T
P L(hÌ‚) > cG({XÎ¸? }Tt=1 ) â‰¤ {h âˆˆ H : L(h) > cG({XÎ¸? }Tt=1 )} eâˆ’cG({XÎ¸? }t=1 )N .
Now, if L(h) > cG({XÎ¸? }Tt=1 ) then at least cp of the good parameters are not
in the subset generated by h since measure G({XÎ¸? }Tt=1 ) is uniformly spread
among the good parameters. As there are
(1âˆ’c)p 

X
k=0

p
k



PARAMETER ESTIMATION IN DYNAMICAL SYSTEMS VIA STATISTICAL LEARNING 28

subsets with no more than (1âˆ’c)p good parameters the first assertion follows.
To show the second assertion, by applying Corollary 3.2 we have
ï£¹
ï£®
(1âˆ’c)p  
X p
1
ï£°log
âˆ’ log Î´ ï£» .
(9)
m(c, Î´) =
T
k
cG({XÎ¸? }t=1 )
k=0
Since the left-hand side of (9) is lesser than
 




1
p
(1 âˆ’ c)2 p2
log
+ log 1 +
âˆ’ log Î´
(1 âˆ’ c)p
cp + 1
cG({XÎ¸? }Tt=1 )


by Proposition B.1, we have the result.
Appendix B. Auxiliary results
Proposition B.1. For n â‰¥ 2p integers
 


p  
X
n
n
p2
log
â‰¤ log
+ log 1 +
k
p
nâˆ’p+1
k=0
Proof. By multiplying and dividing each term of the sum by
that the expression in left-hand side of (10) is equal to

 
p
n
X
n
k
log
+ log
n .
p
k=0 p
Since for every 0 â‰¤ k â‰¤ p âˆ’ 1

n
k
n â‰¤
p

n
pâˆ’1

n
p


=

the result follows from inequality

p
n
X
k
n â‰¤ 1+
k=0

p

(10)
n
p



we obtain

p
nâˆ’p+1

p2
.
nâˆ’p+1


Institute of Mathematics and Statistics, Universidade de SÃ£o Paulo,
R. do MatÃ£o, 1010 - ButantÃ£, SÃ£o Paulo - SP, 05508-090, Brazil., e-mail:
dmarcondes@ime.usp.br

