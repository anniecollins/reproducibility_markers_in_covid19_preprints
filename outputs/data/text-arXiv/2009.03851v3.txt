arXiv:2009.03851v3 [stat.AP] 7 Jan 2021

Referenced Thermodynamic Integration for Bayesian Model
Selection: Application to COVID-19 Model Selection
Iwona Hawrylukâˆ— , Swapnil Mishra, Seth Flaxman, Samir Bhattâ€  , and Thomas A.
Mellanâ€ âˆ—
MRC Centre for Global Infectious Disease Analysis, Department of Infectious
Disease Epidemiology, Imperial College London
Department of Mathematics, Imperial College London
â€ 

Correspondence: t.mellan@imperial.ac.uk,
s.bhatt@imperial.ac.uk
âˆ—

Contributed equally

Abstract
Model selection is a fundamental part of the applied Bayesian statistical methodology.
Metrics such as the Akaike Information Criterion are commonly used in practice to select
models but do not incorporate the uncertainty of the modelsâ€™ parameters and can give misleading choices. One approach that uses the full posterior distribution is to compute the ratio
of two modelsâ€™ normalising constants, known as the Bayes factor. Often in realistic problems,
this involves the integration of analytically intractable, high-dimensional distributions, and
therefore requires the use of stochastic methods such as thermodynamic integration (TI). In
this paper we apply a variation of the TI method, referred to as referenced TI, which computes a single modelâ€™s normalising constant in an efficient way by using a judiciously chosen
reference density. The advantages of the approach and theoretical considerations are set out,
along with explicit pedagogical 1 and 2D examples. Benchmarking is presented with comparable methods and we find favourable convergence performance. The approach is shown to be
useful in practice when applied to a real problem â€” to perform model selection for a semimechanistic hierarchical Bayesian model of COVID-19 transmission in South Korea involving
the integration of a 200D density.

1

1

Introduction

Bayesian computation has been in the limelight in recent months. Modern techniques of statistical machine learning, in particular those based on stochastic inference, have been central to
current epidemiological models [1, 2], with statistical inference underpinning estimates of pathogen
transmission being used to make informed public health policy. With advances in both computing
power and availability of data, it is possible to build more complex, reliable and accurate models,
while the recent increased focus on epidemiological models emphasises a need for synthesis with
rigorous statistical methods. This synthesis is required to robustly estimate necessary parameters,
quantify uncertainty in predictions, and to test hypotheses [3].
In practice the decision to choose a model is often based on heuristics, relying on the knowledge
and experience of the modeller, rather than through a systematic selection process [4, 5]. A
number of model selection methods are available, but those methods often come with a tradeoff between accuracy and computational complexity. For example, widely used in epidemiology
Akaike Information Criterion (AIC) or Bayesian Information Criterion (BIC) are easy to compute,
but come with certain limitations [3]. Specifically, they do not take into account the parametersâ€™
uncertainty or the prior probabilities, and might favour excessively complex models.
The ratio of two normalising constants â€” the Bayes factor (BF) â€” is a popular and general
method used for model selection in the Bayesian setting [6]. This approach integrates out the
parameters of the model to find the probability of the getting the data from a given model hypothesis. But in general a normalising constant cannot be computed analytically or through direct
quadrature methods, due to the difficulty of integrating arbitrary high-dimensional distributions.
Typically approximations are applied such as Laplaceâ€™s method, variational approximations, or
probabilistic algorithms such as bridge sampling [7, 8], stochastic density of states based methods
[9, 10] or thermodynamic integration [11, 12, 13, 14].
Thermodynamic integration (TI) provides a useful way estimate the log ratio of the normalising
constants of two densities. Instead of marginalising the densities explicitly in terms of the highdimensional integrals, by using TI we only need to evaluate a 1-dimensional integral, where the
integrand can easily be sampled with Markov Chain Monte Carlo (MCMC). To see how this works,
consider two models with the pair of normalising constants z1 and z2 ,
Z
zi = qi (Î¸)dÎ¸ , i âˆˆ {1, 2} ,
(1)
where qi is a density for model Mi with parameters Î¸, that can be normalised to give the modelâ€™s
Bayesian posterior density
qi (Î¸)
, i âˆˆ {1, 2} .
pi (Î¸) =
zi
To apply thermodynamic integration we introduce the concept of a path between q1 (Î¸) and q2 (Î¸),
linking the two densities via a series of intermediate ones. This family of densities is denoted
q(Î»; Î¸). An example path in Î» is shown in Figure 1. Note â€” we use the symbol Î» but often the
coupling parameter is denoted Î² (or t) in reference to a physical thermodynamic integration in
inverse temperature (or temperature). In learning terms the temperature represents a Lagrange
multiplier regularising the loss, and in many instances the tempering analogy is useful. Here
however the approach is a more generic procedure and we prefer to consider it simply as a path
Code available at https://github.com/mrc-ide/referenced-TI

2

integration between two distributions coupled by an arbitrary but common switching parameter
Î».
The parametric density q(Î»; Î¸), linking q1 to q2 and defining the intermediate densities, can be
chosen to have an optimal or in some way convenient path, but a common choice is the geometric
one
q(Î»; Î¸) = q2Î» (Î¸)q11âˆ’Î» (Î¸) , Î» âˆˆ [0, 1] .
The important point to note is that for Î» = 0, q(Î»; Î¸) returns the first density q(0; Î¸) = q1 (Î¸), for
Î» = 1 it gives q(1; Î¸) = q2 (Î¸), and for in-between Î» values a log-linear mixture of the endpoint
densities. Just as we have defined a family of densities, there is an associated normalising constant
for any point along the path, that for any value of Î» is given by
Z
z(Î») =
q(Î»; Î¸)dÎ¸ .
â„¦(Î»)

A further small but important point to avoid complications is to have densities that have common
support, for example â„¦(1) = â„¦(0). Hereafter support is denoted â„¦.
Having set up the definitions of q(Î»; Î¸) and z(Î»), the TI expression can be derived, to compute
the log ratio of z1 = z(0) and z2 = z(1), while avoiding explicit integrals over the modelsâ€™
paramR
eters Î¸. By the fundamental theorem of calculus, assuming that the order of âˆ‚Î» and dÎ¸ integral
can be exchanged, and by the rules of differentiating logarithms we get:
Z 1
z2
=
âˆ‚Î» log z(Î») dÎ»
log
z1
0
Z 1
1
âˆ‚Î» z(Î») dÎ»
=
0 z(Î»)
Z 1
Z
1
=
âˆ‚Î»
q(Î»; Î¸) dÎ¸ dÎ»
â„¦
0 z(Î»)
Z 1
Z
1
=
(âˆ‚Î» log q(Î»; Î¸)) q(Î»; Î¸) dÎ¸ dÎ»
0 z(Î») â„¦
Z 1
=
Ep(Î»;Î¸) [âˆ‚Î» log q(Î»; Î¸)] dÎ»
0


Z 1
q2 (Î¸)
=
Ep(Î»;Î¸) log
dÎ»
q1 (Î¸)
0


Z 1
q2 (Î¸)
=
Eq(Î»;Î¸) log
dÎ» .
(2)
q1 (Î¸)
0
The notation Eq(Î»;Î¸) is for an expectation with respect to the sampling distribution q(Î»; Î¸). The
final line in the expression summarises the usefulness of TIâ€”instead of having to work with the
complicated high-dimensional integrals of Equation 1 to find the log-Bayes factor log zz12 , we only
need consider a 1-dimensional integral of an expectation, and the expectation can be readily produced by MCMC.
Here we set out in detail some variations on the TI theme that we have found to be useful in
practice, in particular with regard to evaluating evidence for epidemiological models. The variation
is to work primarily in terms of appropriately chosen reference normalising constants. The approach
3

of using exactly integrate-able references has provided us with a particularly efficient method of
selection between different hierarchical Bayesian models models, and we hope the approach will be
useful to others working on similar problems, for example in phylogenetic model selection where
TI is already a popular established method [15, 14].
In the following, we begin with introducing the reference density, then go on to illustrate
different practical choices of a reference normalising constant, along with theoretical and practical
considerations. Next, the mechanics of applying the method are set out for two simple pedagogical
examples. Performance benchmarks are discussed for a well-known problem in the statistical
literature [16], which shows the method performs favourably in terms of accuracy and the number
of iterations to convergence. Finally the technique is applied to a hierarchical Bayesian time-series
model describing the COVID-19 epidemic in South Korea. In the COVID-19 infections model we
show how the approach may be used to select technical parameters that specify the reproduction
number (Rt ) model, such as the autoregression window size and AR(k) lag, as well epidemiologically
meaningful parameters such as the serial interval distribution time for generating infections.

2

Referenced-TI

An efficient approach to compute Bayes factors, or more generally to marginalise a given density
for any application, is to introduce a reference as
z

z
zref

=

zref

=

zref exp

Z
0

1



q(Î¸)
dÎ» .
Eq(Î»;Î¸) log
qref (Î¸)

(3)

To clarify notation, z is the normalising constant of interest with density q, zref is a reference
normalising constant with associated density qref . The second line replaces the ratio z/zref with a
thermodynamic integral as per the identity derived in Equation 2.
The introduction of a reference naturally facilitates the marginalisation of a single density,
rather than requiring pairwise model comparisons
 by direct application of Equation 2. This is
useful when comparing multiple models as n > n2 for n â‰¥ 3. Another motivation to reference the
TI is the MCMC computational efficiency of converging the TI expectation. In Equation 3, with
judicious choice of qref , the reference normalising constant zref can be evaluated analytically and
q(Î¸)
account for most of z. In this case log qref
(Î¸) tends to have a small expectation and variance and
converges quickly.
The idea of using an exactly solvable reference, to aid in the solution of an otherwise intractable
problem, is a recurrent and perennial theme in the computational and mathematical sciences in
general [17, 18, 19, 20], and variations on this approach have been used to compute normalising
constants in various guises in the statistical literature [21, 13, 22, 14, 15, 23, 24, 25, 26]. For
example, in the generalised stepping stone method a reference is introduced to speed up convergence
of the importance sampling at each temperature rung [25, 26]. In the work of Lefebvre et al.
[24] a theoretical discussion has been presented that shows the error budget of thermodynamic
integration depends on the J-divergence of the densities being marginalised. Noting this, Cameron
and Pettitt [23] illustrate in their work on recursive pathways to marginal likelihood estimation,
that a "data driven" auxiliary density reduces standard errors for a 2D banana-shaped likelihood
posterior. While, in the power posteriors (PP) approach, the reference in Equation 3 is the prior
4

distribution of q and thus zref = 1 [21]. This approach is elegant as the reference need not be
chosen â€” it is simply the prior â€” however the downside of the simplicity is that for poorly chosen
or uninformative priors, the thermodynamic integral will be slow to converge and susceptible to
instability. In particular for complex hierarchical models with weakly informative priors this is
found to be an issue.
The reference density in Equation 3 can be chosen at convenience, but the main desirable
features are that it should be easily formed without special consideration or adjustments, similar
to the power posteriors method, and that zref should be analytically integratable and account for
as much of z as possible. Such a choice of zref ensures the part with expensive sampling is small
and convergence is fast. An obvious choice in this regard is the Laplace-type reference, where the
log density is approximated with a second-order one, for example a multivariate Gaussian. For
densities with a single concentration, Laplace-type approximations are ubiquitous, and an excellent
natural choice for many problems. In the following section we consider three approaches that can
be used to formulate a reference normalising constant zref from a second-order log density (though
more generally other tractable references are possible). In each referenced TI scenario, we note
that even if the reference approximation is poor, the estimate of the normalising constant based on
Equation 3 remains asymptotically exactâ€”only the speed of convergence may be reduced (subject
to the assumptions of matching support of endpoint densities).

2.1

Taylor Expansion at the Mode Laplace Reference

The most straightforward way to generate a reference density is to Taylor expand log q(Î¸) to second
order about a mode. Noting there is no linear term, we see the reference density is


1
(4)
qref (Î¸) = exp log q(Î¸0 ) + (Î¸ âˆ’ Î¸0 )T H (Î¸ âˆ’ Î¸0 ) ,
2
where H is the hessian matrix and Î¸0 is the vector of mode parameters. The associated normalising
constant is
Z âˆž
zref =
qref (Î¸)dÎ¸
âˆ’âˆž


Z âˆž
1
T
=
exp log q(Î¸0 ) + (Î¸ âˆ’ Î¸0 ) H (Î¸ âˆ’ Î¸0 ) dÎ¸
2
âˆ’âˆž


Z âˆž
1
T
= q(Î¸0 )
exp
(Î¸ âˆ’ Î¸0 ) H (Î¸ âˆ’ Î¸0 ) dÎ¸
2
âˆ’âˆž
p
= q(Î¸0 ) det(2Ï€Hâˆ’1 ) .
(5)
The Taylor expansion method at the mode with analytic or finite difference gradients tends to
produce a reference density that works well in the neighbourhood of Î¸0 but can be less suitable
if density is asymmetric, has long or short tails, or if the derivatives at the mode are poorly
approximated for example due to cusps or conversely very flat curvature at the mode. In many
instances a more reliable choice of reference can found by using MCMC samples from the whole
posterior density.

5

2.2

Sampled Covariance Laplace Reference

The second straightforward but often more robust approach is to form a reference density by
drawing samples from the true density q(Î¸), to estimate the mean parameters Î¸Ì‚ and covariance
matrix Î£Ì‚, such that


1
T âˆ’1
qref (Î¸) = q(Î¸Ì‚) exp âˆ’ (Î¸ âˆ’ Î¸Ì‚) Î£Ì‚ (Î¸ âˆ’ Î¸Ì‚) .
(6)
2
q
The reference normalising constant is zref = q(Î¸Ì‚) det(2Ï€ Î£Ì‚) .
This method of generating a reference is simple and reliable. It requires sampling from the
posterior q(Î¸) so is more expensive than derivative methods, but the cost associated with drawing
enough samples to generate a sufficiently good reference tends to be quite low. In the primary
application discussed later, regarding relatively complex high-dimensional Bayesian hierarchical
models, we use this approach to generate a reference density and normalising constant.
The sampled covariance reference is typically a good approach, but it is not in general optimal
within the Laplace-type family of approaches â€” typically another Gaussian reference exists with
different parameters that can generate a normalising constant closer to the true one, thus potentially leading to overall faster convergence of the thermodynamic integral to the exact value. Such
an optimal reference can be identified variationally.

2.3

Variational Laplace Reference

The conditions to identify an optimal reference normalising constant can be derived by considering
a Taylor expansion of the log normalising constant log z(Î») about Î» = 0:
1
log z(Î») â‰ˆ log z(0) + Î» âˆ‚Î» log z(0) + Î»2 âˆ‚Î»2 log z(0) .
2
The first derivative gives the expectation


q(Î¸)
âˆ‚Î» log z(Î») = Eq(Î»;Î¸) log
,
qref (Î¸)
as per the derivation in Equation 2, and the second derivative is a variance
2

ï£«R 
ï£¶2
q(Î¸)
q(Î¸)
log qref
log qref
q(Î»; Î¸)dÎ¸
q(Î»; Î¸)dÎ¸
(Î¸)
(Î¸)
ï£¸
R
R
âˆ’ï£­
q(Î»; Î¸)dÎ¸
q(Î»; Î¸)dÎ¸
(
"
2 #

2 )
q(Î¸)
q(Î¸)
Eq(Î»;Î¸)
log
âˆ’ Eq(Î»;Î¸) log
qref (Î¸)
qref (Î¸)
R

âˆ‚Î»2 log z(Î»)

=

=
â‰¥

0.

As the curvature of log z(Î») is increasing, to first order we see


q(Î¸)
log z(Î») â‰¥ log z(0)+Î»Eq(0;Î¸) log
,
q0 (Î¸)
6

and for the specific case of Î» = 1,


q(Î¸)
log z â‰¥ log zref +Eqref (Î¸) log
.
qref (Î¸)
This inequality establishes bounds that can be maximised with respect to the position (Âµ) and
scale (S) parameters of a reference density such as


1
qref (Âµ, S; Î¸) = q(Âµ)exp âˆ’ (Î¸ âˆ’ Âµ)T S (Î¸ âˆ’ Âµ) .
2
Thus the parameters that optimise


max log zref + Eqref (Î¸) log
Âµ,S

q(Î¸)
qref (Âµ, S; Î¸)


,

provide a reference density that is variationally optimal. We note this is an application of what is
known as the Gibbs-Feynman-Bogoliubov inequality from other fields [27, 28, 29], and that finding
approximations of this type to the true density is a well-studied problem in machine learning, with
well-documented approaches that can be used to determine qref variationally [30, 31]. In itself the
existence of a variational bound provides no guarantee of being a good approximation to the true
normalising constant, and is thus alone not a satisfactory general approach. However as a point
of reference from which to estimate the true normalising constant, it provides a first-order optimal
density within the family of trial reference functions considered, therefore improving convergence
to the MCMC normalising constant in referenced TI.

2.4

Multi-reference TI

Having set out three approaches to find a single reference for the TI expression in Equation 3, a
natural generalisation is the telescopic expansion
z

=

=

nâˆ’1
Y

zi+1 z
zi zn
i=0



 !
Z 1 nâˆ’1
X
qi+1 (Î¸)
q(Î¸)
Eqi (Î»;Î¸) log
z0 exp
+ Eqn (Î»;Î¸) log
dÎ» .
qi (Î¸)
qn (Î¸)
0 i=0
z0

(7)

Note, here the analytic reference is denoted z0 rather than zref to generalise the indexing. In cases
where q0 differs substantially from q, the telescopic generalisation can improve numerical stability.
(Î¸)
By bridging the endpoints in terms of intermediate density pairs, qi+1
qi (Î¸) , we can form a series of
lower variance MCMC simulations with favourable convergence properties. A reasonable choice for
th
generating intermediate densities is for the qith density to be the 2 (i + 1) order Taylor expansion
of q(Î¸).

2.5

Reference Support

If a model has parameters with limits, for example Î¸1 âˆˆ [0, âˆž), Î¸2 âˆˆ (âˆ’1, âˆž) etc., in referenced
TI the exact analytic integration for the reference density should be commensurately limited.
7

However, the calculation of arbitrary probability density function orthants, even for well-known
analytic functions such as the multivariate Gaussian, is in general a difficult problem. Computing
high-dimensional orthants usually requires advanced techniques, the use of approximations, or
sampling methods [32, 33, 34, 35, 36, 37]. Fortunately we can simplify our reference density to
create a reference with tractable analytic integration for limits by using a diagonal approximation
to the sampled covariance or hessian matrix. For example the orthant of a diagonal multivariate
Gaussian can be given in terms of the error function [38], leading to the expression
ï£«
ï£«
ï£¶ï£¶
q
Y
Î¸Ì‚
âˆ’
a
i
i
ï£­1 + erf ï£­ q
ï£¸ï£¸ ,
(8)
zref = q(Î¸Ì‚) det(2Ï€Î£diag )
diag
iâˆˆK
2Î£i
where K denotes the set of indices of the parameters with lower limits ai . Î£diag is a diagonal
covariance matrix, that is one containing only the variance of each of the parameters, without the
covariance terms and Î£diag
denotes the ith element of the diagonal. Restricting our density to a
i
diagonal one is a poorer approximation than using the full covariance matrix. In practice however
this has not been particularly detrimental to the convergence of the thermodynamic integralâ€”
and again we note that the quality of the reference only affects convergence rather than eventual
theoretical Monte Carlo accuracy of the normalising constant. This behaviour is observed in the
practical examples later considered, though the distinction between accuracy and convergence and
matters of asymptotic consistency using an MCMC estimator with finite iterations are naturally
less clear cut.

2.6

Technical Implementation

The referenced TI algorithm was implemented in Python and Stan programming languages. Using
Stan enables fast MCMC simulations, using Hamiltonian Monte Carlo (HMC) and No-U-Turn
samples (NUTS) algorithm [39, 40], and portability between other statistical languages, such as
R or Julia. Additionally it is familiar to many epidemiologists using Bayesian statistics [41].
The code for all examples shown in this paper is available at https://github.com/mrc-ide/
referenced-TI. In the examples shown in Section 3, we used 4 chains with 20,000 iterations per
chain for the pedagogical examples, and 4 chains with 2,000 iterations for the other applications.
In all cases, half of iterations were used for the burn-in. Mixing of the MCMC chains and the
sampling convergence was checked in each case, by ensuring that the RÌ‚ value was â‰¤ 1.05 for each
parameter in every model.
In all examples in the remaining part of this paper, the integral
h given in
i Equation 2 was
q1 (Î¸)
discretised to allow computer simulations. Each expectation Eq(Î»;Î¸) log q0 (Î¸) was evaluated at
Î» = 0.0, 0.1, 0.2, ...., 0.9, 1.0, unless stated otherwise. To obtain the value of the integral in Equation
2, we interpolated a curve linking the expectations using a cubic spline, which was then integrated
numerically. The pseudo-code of the algorithm with sampled covariance Laplace reference is shown
in Algorithm 1.

3

Applications

In this section we present an application of the referenced TI algorithm to 1- and 2-dimensional
pedagogical examples, a linear regression model, and a model selection task for a model of COVID8

Algorithm 1 Referenced thermodynamic integration algorithm
Input q - un-normalised density, qref - un-normalised reference density, Î› - set of coupling
parameters Î», N - number of MCMC iterations
Output z - normalising constant of the density q
1: Define un-normalised density q and the reference density qref
2: Calculate zref analytically by using the determinant of the covariance matrix.
3: for Î» âˆˆ Î› do
1âˆ’Î»
4:
Sample N values Î¸n from q Î» qref
5:
for n = 1, 2, . . . , N do
q(Î¸n )
6:
Calculate log qref
(Î¸n )
7:
end for
q(Î¸n )
8:
Compute the mean, EÎ» = N1 Î£N
n=1 log qref (Î¸n )
9: end for
10: Interpolate between the consecutive EÎ» values to obtain a curve âˆ‚Î» log(z(Î»))
11: Integrate âˆ‚Î» log(z(Î»)) over Î» âˆˆ [0, 1] to get log z z
ref
12: Calculate z = zref Â· exp{log z z }
ref
19 epidemic in South Korea.

3.1

1D Pedagogical Example

To illustrate the technique consider the 1-dimensional density


1p
1
4
q(Î¸) = exp âˆ’
|Î¸ âˆ’ 4| âˆ’ (Î¸ âˆ’ 4)
, Î¸ âˆˆ R,
(9)
2
2
Râˆž
with normalising constant z = âˆ’âˆž q(Î¸)dÎ¸. This density has a cusp â€” one of the more awkward
pathologies of some naturally occurring densities [42, 43] â€” and it does not have an analytical
integral that easily generalises to multiple dimensions, but is otherwise a made-up 1-dimensional
example that could be interchanged with an other.
In this instance the Laplace approximation based on the second-order Taylor expansion at the
mode will fail due to the cusp, so we can use the more robust covariance sampling method. Sampling
from the 1d density q(Î¸) we find a variance of ÏƒÌ‚ 2 = 0.424, giving a Gaussian reference density qref (Î¸)
z
with normalising constant of zref = 1.559. The full normalising constant, z = zref zref
, is evaluated
(1âˆ’Î»)

by Equation 3, by setting
up aithermodynamic integration along the sampling path q Î» qref . The
h
q(Î¸)
expectation, Eq(Î»;Î¸) log qref (Î¸) , is evaluated at 5 points along the coupling parameter path Î» =
0.0, 0.2, 0.5, 0.8, 1.0, shown in Figure 1. In this simple example, the integral can be easily evaluated
to high accuracy using quadrature [44, 45], giving a value of 1.523. Referenced thermodynamic
integration reproduces this value, with convergence of z shown in Figure 1, converging to 1% of z
with 500 iterations and 0.1% within 17, 000 iterations.
This example illustrates notable characteristic features of referenced thermodynamic integration. Here the reference qref (Î¸) is a good approximation to q(Î¸), with zrefh accounting
i for most
(102%) of z. Consequently

z
zref

q(Î¸)
is close to 1, and the expectations, Eq(Î»;Î¸) log qref
(Î¸) , evaluated

9

   

qref      

   

    
    

 ( [ S H F W D W L R Q  S H U 

   

 ' H Q V L W \

    

     
     
     
q      

   
   

    
    
    
    
    

   

     
     

    

   

   

   

   

   

   

   

   

   

 

    

    

(a)

     
     

     

                                  

 , W H U D W L R Q

(b)
    
     

    

    
    
    
   

   

   

   

   

     
     

 ( Y L G H Q F H

log(zref)
log(z)

    

 / R J  H Y L G H Q F H

    

 ( [ S H F W D W L R Q

zref

    

 ]

zTI
z Â± 1%
z Â± 0.1%

     

    
    
    

     

    

     

    

   

(c)

 

    

    

                                  

 , W H U D W L R Q

(d)

(1âˆ’Î»)

Figure 1: a) q Î» qref

for
density (Equation 9) at selected Î» values along the path.
h the 1d example
i
q(Î¸)
b) Expectation Eq(Î»;Î¸) log qref (Î¸) vs MCMC iteration, shown at each value of Î» sampled. c) Î»dependence of the TI contribution to the log-evidence d) Convergence of the evidence z, with 1%
convergence after 500 iterations and 0.1% after 17, 000 iterations per Î».

10

q( )

 

    

qref( )   I X O O  F R Y D U L D Q F H

 

    

    

 

    

    
 

    

    
 

    

 

    

 

 
1

 

 

    

    
 

    

 

    

 

    
 

 

    

2

    

2

2

    

 

 

    

 

 

    

    

 

 

qref( )   G L D J R Q D O  F R Y D U L D Q F H

 

    
 

 

 

 

 

 

 

    

1

    
 

 

 

 

 

 

 

    

1

Figure 2: Contour plots of the un-normalised density q and its two reference densities qref , one
using a full covariance matrix and another using a diagonal covariance matrix. The red line shows
the lower boundary Î¸1 = 0 and the shaded Î¸1 < 0 region to the left of the line is outside of the
support of the density q.
by MCMC for the remaining part of the integral are small. For the same reasons the variance at
each Î» his small, leading
to favourable convergence within a small number of iterations. And finally
i
q(Î¸)
Eq(Î»;Î¸) log qref (Î¸) weakly depends on Î», so there is no need to use a very fine grid of Î» values or
consider optimal pathsâ€”satisfactory convergence is easily achieved using a simple geometric path
with 4 Î» intervals.

3.2

2D Pedagogical Example with Constrained Parameters

As a second example, consider a 2-dimensional un-normalised density with a constrained parameter
space:



1 4
1 2
1 4 1
1 2
1
2
(10)
q(Î¸1 , Î¸2 ) = exp âˆ’ (Î¸1 + ) + (Î¸1 + ) + (Î¸2 + ) + (Î¸2 + ) + Î¸1 Î¸2 ,
4
2
2
2
2
2
where

Î¸1 âˆˆ [0, +âˆž) and Î¸2 âˆˆ (âˆ’âˆž, +âˆž) .

A reference density qref (Î¸) can be constructed from the Hessian at the mode of q(Î¸). Notice, that
because parameter Î¸1 is constrained to be â‰¥ 0, integrating the Gaussian approximations qref (Î¸)
using the formula given in Equation 5 will give an overestimate. To account for this we use the
diag
reference density qref
(Î¸), based on a diagonal Hessian, that has an exact and easy to calculate
orthant. All densities are shown in Figure 2.
To obtain the log-evidence of the model, we calculated the exact value numerically [46, 44],
using the full covariance Laplace method as per Equation 6 and using the diagonal covariance with
correction added to take into account the lower bound of the parameter Î¸1 , as per Equation 8. The
Gaussian reference densities were then used to carry out referenced thermodynamic integration.
Results of all methods are given in Table 1. As expected, without applying the correction the value
of the evidence is overestimated.
11

Method

Evidence

Exact
Laplace with full covariance
Laplace with diagonal covariance + constraint correction
Ref TI with full covariance
Ref TI with diagonal covariance + constraint correction

3.31
5.55
3.81
4.79
3.33

âˆ—

Table 1: Evidence calculated with different methods. Constraint correction refers to imposing the
integration limits on the reference as per Equation 8. âˆ— obtained numerically with [46, 44].

3.3

Benchmarksâ€”Radiata Pine

To benchmark the application of the referenced TI in the model selection task, two non-nested
linear regression models are compared for the radiata pine data set [16]. This example has been
widely used for testing normalising constant calculating methods, since in this instance the exact
value of the model evidence can be computed. The data consists of 42 3-dimensional data-points,
expressed as yi - maximum compression strength, xi - density and zi - density adjusted for resin
content. In this example, we follow the approach of Friel and Wyse [22], and test which of the two
models M1 and M2 provides better predictions for the compression strength:
M1 : yi = Î± + Î²(xi âˆ’ xÌ„) + i , i âˆ¼ N (0, Ï„ âˆ’1 ), i = 1, ..., n) ,
M2 : yi = Î³ + Î´(zi âˆ’ zÌ„) + Î·i , Î·i âˆ¼ N (0, Ïâˆ’1 ), i = 1, ..., n) .
In other words, we want to know, whether density or density adjusted allows to predict the compression strength better. The priors for the models were selected in a way which enables obtaining
an exact solution and can be found in Friel and Wyse [22].
Five methods of estimating the model evidence were used in this example: Laplace approximation using a sampled covariance matrix, model switch TI along a path directly connecting the
models [14, 47], referenced TI, power posteriors with equidistant 11 Î»-placements (labelled here as
PP11 ) and power posteriors with 100 Î»-s (PP100 ) as in [22]. For the model switch TI, referenced
TI and PP11 we used Î» âˆˆ {0.0, 0.1, ..., 1.0}.
The expectation from MCMC sampling per each Î» for model switch TI, referenced TI, PP11
and PP100 and fitted cubic splines between the expectations are shown in Figure 3. Immediately we
notice that both TI methods eliminate the problem of divergence of expectation for Î» = 0, which
is observed with the power posteriors, where samples for Î» = 0 come from the prior distribution.
The PP11 method failed to estimate the log-evidence correctly.
The 1-dimensional lines estimated by fitting a cubic spline to the expectation were integrated
for each of the models to obtain the log-evidence for M1 and M2 , and the log ratio of the two
modelsâ€™ evidences for the model switch TI. Rolling mean of the integral over 1500 iterations for
referenced TI and PP100 are shown in Figure 4 a-b. We can see from the plots, that referenced
TI presents excellent convergence to the exact value, whereas PP100 oscillates around it. In the
same Figure 4, plots c-d show the distribution of log-evidence for each model generated by 15 runs
of the three algorithms: Laplace approximation with sampled covariance matrix, referenced TI
and PP100 . Although all three methods resulted in a log-evidence satisfactorily close to the exact
solution, referenced TI was the most accurate and importantly, converged fastest.
12

   

    

   

   

 ( [ S H F W D W L R Q

 ( [ S H F W D W L R Q

   

   
   
   

   

   

   

   

   

   

M1
M2

   

 S D W K  I U R P M1  W R M2

   

   

   

   

   

   

   

   

(b) Referenced TI

   

   

   

   

 ( [ S H F W D W L R Q

 ( [ S H F W D W L R Q

(a) Model switch TI

   

   

   
   

   

M1
M2

   
   

   

   

   

   

M1
M2

   

   

   

(c) PP11

   

   

   

   

   

(d) PP100

h
i
q(Î¸)
Figure 3: The HMC-evaluated expectation of Eq(Î»;Î¸) log qref
(Î¸) vs coupling parameter Î» is given
for models M1 and M2 for four methods of calculating the model evidence: model switch TI (a),
referenced TI (b), power posteriors with 11 Î»-placements (c) and power posteriors with 100 Î»
placements (d). Model switch TI (a) creates the path directly between two competing densities,
therefore only one line is shown (see Equation 2).

13

      
      
      
      
      
      
      

 U H I  7 , M2
 3 3100 M2
M2  H [ D F W

      

 / R J  H Y L G H Q F H

 / R J  H Y L G H Q F H

      

 U H I  7 , M1
 3 3100 M1
M1  H [ D F W

      
      
      
      

 

   

   

   

   

 , W H U D W L R Q

    

    

      

    

 

   

(a) M1

   

   

   

 , W H U D W L R Q

    

    

    

(b) M2

      

      
      

      

 / R J  H Y L G H Q F H

 / R J  H Y L G H Q F H

      
      
      
      

      
      
      
      
      

      

      
 / D S O D F H

 5 H I  7 ,

 $ S S U R [ L P D W L R Q  P H W K R G

 3 3100

 / D S O D F H

 5 H I  7 ,

 $ S S U R [ L P D W L R Q  P H W K R G

(c) M1

 3 3100

(d) M2

Figure 4: Log-evidence of M1 and M2 for the three algorithms. (a) and (b) show the rolling mean
of log-evidence of M1 and M2 over 1500 iterations per Î» obtained by referenced TI (blue line) and
PP100 (orange line) methods. The exact value is shown with red dashed line. (c) and (d) show the
mean log-evidence of the two models evaluated over 15 runs of the three algorithms. The exact
value of the log-evidence is shown with the dotted line.

14

The BFs calculated to assess whether model M2 fits the data better than model M1 and the
number of iterations needed to achieve standard error of 0.5%, excluding the iterations needed for
the MCMC burn-in, are presented in Table 2. Notably, both TI methods gave a BF very close
to the exact value. Referenced TI performed the best out of tested methodsâ€”it converged faster
than all other methods, requiring only 308 MCMC draws compared to 55,000 draws needed for the
power posterior method or over 2,000 for the model switch TI. Referenced TI also showed excellent
accuracy both in estimating individual modelâ€™s evidence and the BF.
Method
Exact
Laplace approximationâˆ—
Model switch TI
Referenced TI
PP11
PP100

BF21

MCMC steps

4552.35
6309.10
4557.63
4558.71
4463.71
4757.82

2,365
308
41,514
55,000

Table 2: Comparison of Bayes factors for radiata pine models for each method. Here we show
2
BF21 = M
M1 to determine whether model M2 is better than model M1 . Both TI and referenced
TI methods used 11 equidistant Î»-s. Power posteriors method was used with 11 (PP11 ) and 100
(PP100 ) Î»-s. Third column shows the total number of MCMC steps required to achieve standard
error of 0.5%, excluding the burn-in steps. âˆ— - using sampled covariance matrix.

3.4

Model Selection for the COVID-19 Epidemic in South Korea

The final example of using referenced TI for calculating model evidence is fitting a renewal model
to COVID-19 case data from South Korea. The data were obtained from https://opendata.ecdc.
europa.eu/covid19/casedistribution/csv (accessed 19-07-2020) and contained time-series data
of confirmed cases from 31-12-2019 to 18-07-2020. The model is based on the Bellman-Harris
branching process whose expectation is the renewal equation. Its derivation and details are explained in Mishra et al. [48] and a short explanation of the model is provided in the Appendix.
Briefly, the model is fitted to the time-series case data and estimates a number of parameters, including serial interval and the effective reproduction number, Rt . Number of cases for each day are
modelled by a negative binomial distribution, with shape and overdispersion parameters estimated
by a renewal equation. Three modification of the original model were tested:
â€¢ GI = k, k = 5, 6, 6.5, 7, 8, 9, 10, 20â€”fixing the mean of the GI parameter, denoting the mean
of Rayleigh-distributed generation interval (which we assume to be the same as the serial
interval),
â€¢ AR(k), k = 2, 3, 4â€”autoregressive model with k days lag,
â€¢ W = k, k = 1, 2, 3, 4, 7â€”changing the length k of the sliding window W .
Within each group of models, GI, AR and W , we want to select the best model through the
highest evidence method. For example, we want to check whether GI = 6 fits the data better
than GI = 10, etc. The dimension of each model was dependent on the modifications applied, but
15

in all the cases the normalising constant was a 40- to 200-dimensional integral. The log-evidence
of each model was calculated using the Laplace approximation with a sampled covariance matrix,
and then correction to the estimate was obtained using referenced TI method. Values of the logevidence for each model calculated by both Laplace and referenced TI methods are given in Table
3. Interestingly, the favoured model in each group, that is the model with the highest log-evidence,
was different when the evidence was evaluated using the Laplace approximation than when it was
evaluated with referenced TI. For example, using the Laplace method, sliding window of length 7
was incorrectly identified as the best model, whereas with referenced TI window of length 2 was
chosen to be the best among the tested sliding windows models, which agrees with the previous
studies of the window-length selection in H1N1 influenza and SARS outbreaks [49]. This exposes
how essential it is to accurately determine the evidence, even good approximations can result in
misleading results. Log-Bayes factors for all model pairs within each of the three groups are shown
in Figure 7 in the Appendix.
Model

Log-evidence (Laplace)

Correction

Log-evidence (ref TI)

GI=5
GI=6
GI=6.5
GI=7
GI=8
GI=9
GI=10
GI=20
AR(2)
AR(3)
AR(4)
W=1
W=2
W=3
W=4
W=7

-1274
-1274
-1269
-1255
-1245
-1310
-1313
-1170
-1207
-1293
-2166
-1260
-1069
-1003
-940
-875

558
572
530
522
561
507
508
385
496
589
1346
458
278
196
129
62

-716 [-715.6, -715.2]
-703 [-703.3, -702.7]
-739 [-738.6, -738.3]
-732 [-732.4, -731.8]
-685 [-685.5, -684.7]
-803 [-802.8, -802.3]
-805 [-805.1, -805.3]
-796 [-796.3, -795.5]
-711 [-711.2, -710.6]
-704 [-704.7, -703.7]
-821 [-820.6, -819.2.]
-802 [-802.1, -801.6]
-791 [-791.2, -790.7]
-807 [-807.5, -807.2]
-811 [-811.1, -810.7]
-814 [-813.7, -813.5]

Table 3: Log-evidence estimated by Laplace approximation, added referenced TI correction and
total log-evidence from referenced TI, with 95% credible interval given in brackets. In each section,
model with the highest log-evidence estimated by Laplace or referenced TI method is indicated in
bold.

3.5

Interpretation of the COVID-19 model selection

The importance of performing model selection in a rigorous way is clear from Figure 5, where
the posterior densities of parameters Ï† and Ïƒ and the generated Rt time-series are plotted for
the models favoured by Laplace and referenced TI methods (meaning of the parameters is given
in the Appendix). The differences in the densities and time-series show the pitfalls of selecting
an incorrect model. For example, the parameter Ïƒ was overestimated by the models selected
16

by Laplace approximation in comparison to these selected by the referenced TI. The differences
between the two favoured models were most extreme for the GI = 8 and GI = 20 models. While
a GI = 8 is plausible, even likely for COVID-19, GI = 20 is implausible given observed data [50].
This is further supported by observing that for GI = 20, favoured by the Laplace method, Rt
reached the value of over 125 in the first peakâ€”around 100 more than for the GI = 8. The second
peak was also largely overestimated, where Rt reached a value of 75.
We find it interesting to note that all models present a similar fit to the confirmed COVID-19
cases data, as shown by Figure 8 in the Appendix. This makes it impossible to select the best
model through visual inspection and comparison of the model fits, or by using model selection
methods that do not take the full posterior distributions into account. Although the models
might fit the data well, other quantities generated, which are often of interest to the modeller,
might be completely incorrect. Moreover, it emphasises the need to test multiple models before
any conclusion or inference is undertaken, especially with the complex, hierarchical models. In
epidemiology this is important as the modellers can be tempted to pick arbitrary parameters for
their model, as long as the predictions fit the data. Although the fit might be accurate, other
inferred parameters or uncertainty related to the predictions might be completely inappropriate
for making any meaningful predictions.
In the radiata pine example, the contribution of TI to the marginalised likelihood estimated by
the Laplace approximation was not substantial and using the Laplace approximation would suffice
to make an informed model choice. However in this example, we see that the TI contribution
is relatively large, and that it changes the decision to be made based on model evidence relative
to the Laplace method. Moreover, from Table 3 we see that the evidence was the highest for the
"boundary" models when Laplace approximation was applied. For example, for the sliding window
length models, when the Gaussian approximation was applied, the log-evidence was monotonically
increasing with the value of W within the range of values that seem reasonable (W = 1 âˆ’ 7). In
contrast, with referenced TI, the log-evidence is concave within the range of a priori reasonable
parameters.

4

Discussion

The examples shown in Section 3 illustrate the applicability of the referenced TI algorithm for
calculating model evidence. In the radiata pine example, referenced TI performed better than the
other tested methods in terms of accuracy and speed. The power posteriors method required a
much denser placement of the coupling parameters around Î» = 0, where the values are sampled
purely from the prior distribution. In the case of referenced TI, at Î» = 0 values are sampled from the
reference density, which should be closer to the original density (in the sense of Kullbackâ€“Leibler or
Jensen-Shannon divergence), which results not only in a more accurate estimate of the normalising
constant, but also much faster convergence of the MCMC samples. It also worth noting that
referenced TI even performed better than the model switch TI method. A detailed theoretical
characterisation of rates of convergence is beyond the scope of this article, nonetheless the empirical
tests presented have consistently shown faster convergence than with comparative approaches. This
is useful to know in the context of evaluating model evidence in complex hierarchical models where
where each MCMC iteration is computationally demanding.
Although referenced thermodynamic integration and other methods using path-sampling have
theoretical asymptotically exact Monte Carlo estimator limits, in practice a number of consider17

W=2
W=7

AR(3)
AR(2)

   

GI = 8
GI = 20

   
   
   

 

 

  

  

    

 

  

  

    

 

  

  

  

(a) Posterior distributions for overdispersion parameter Ï†
W=2
W=7

AR(3)
AR(2)

 

GI = 8
GI = 20

 
 
 
 
                                                                                                        

(b) Posterior distributions for Ïƒ parameter
AR(3)
AR(2)

  

Rt

  

  

  

  

  

  

  

  

 

 

 

 

  

   

 ' D \ V

   

W=2
W=7

  

   

 

GI = 8
GI = 20

   
   
  
  
  
 

 

  

   

 ' D \ V

   

   

 

  

   

 ' D \ V

   

   

(c) Rt generated by the favoured models

Figure 5: Posterior distributions for modelsâ€™ parameters for models favoured by BFs using the
Laplace approximation (orange lines) and referenced TI (blue lines).

18

1.0
0.8

l

q

p

b

Î©/Î©ref â‰ˆ1

q
qref

0.6
0.4
0.2

8
6
4
2

0.0

1.0

c

Î©/Î©ref â‰ˆ 0.3

Systematic Error at Î»=0

Density

10
Variance at Î»=0

a

0.5
0.0
- 0.5
- 1.0
- 1.5
-4

-2

0

2

4

Î¸

0

0.25
0.20
0.15
0.10
0.05
0.00
0.0

0.5

1.0

1.5

2.0

2.5

Î©/Î©ref

Figure 6: a) 1D examples to illustrate the bias and variance introduced with finite MCMC samples
when q and qref are mismatched. In these examples â„¦ and â„¦ref denote the domain of the 99%
quartiles of q and qref . b) A mismatch between q and qref (â„¦ and â„¦ref ) causes the variance of
q
log qref
to increase, requiring more iterations to convergence. c) Similarly the mismatch causes
q
the mass of the distribution for the expectation of log qref
(evaluated with respect to the reference
distribution) to increase beyond the parameter range effectively sampled with finite iterations, in
this example corresponding to the 99% quartile of the sampling distribution, thus introducing a
bias in the expectation.
ations affect accuracy. For example, biases will be introduced to the referenced TI estimate in
practice if one endpoint density substantially differs from another. Then the volume of parameter
space that must be explored to produce an unbiased estimate of the expectation cannot be sampled
based on the reference density generating proposals within a practical number of iterations. The
point is shown for a simple 1D example in Figure 6. Similarly, the larger the mismatch, the higher
the variance and slower the expectation is to converge. This illustrates the advantage of using a
reference that matches the posterior as closely as possible, as opposed to a typically wide reference
like the prior distribution, that gives the characteristic divergence at Î» = 0 with power posteriors.
Measures of density similarity in path sampling have been discussed by [24], however in practical
terms there remains much scope for analysis of reference performance in terms of scaling with
distribution dimension and type, which should be considered in detail in future work.
Furthermore, the discretisation of the coupling parameter path in Î» can introduce a discretisation bias. For the power posteriors method, Friel et al. (2017) propose an iterative way of

19

selecting the Î»-placements to reduce the discretisation error [13]. Calderhead and Girolami (2009)
test multiple Î»-placements for 2 and 20 dimensional regression models, and report relative bias for
each tested scenario [51]. In the referenced TI algorithm discretisation bias is however negligible
â€” the use of the reference density results in TI expectations that are both small and have low
variance, and therefore curvature with respect to Î». In our framework we use geometric paths with
equidistant coupling parameters Î» between the un-normalised posterior densities, but there are
other possible choices of the path constructions, for example a harmonic [12] or hypergeometric
path [47]. This optimisation might be worthwhile exploring, however, as illustrated in Fig. 3b, the
expectations evaluated vs Î» are typically near-linear with referenced TI suggesting limited gains,
although the extent of this will differ from problem to problem.
In the application to the renewal model for the COVID-19 epidemic in South Korea, we showed
that for a complex structured model, hypothesis selection by Laplace approximation of the normalising constant can give misleading results. Using referenced TI, we calculated model evidence
for 16 models, which enabled a quick comparison between chosen pairs of competing models. Importantly, the evidence given by the referenced TI was not monotonic with the increase of one
of the parameters, which was the case for the Laplace approximation in the models tested. The
referenced TI presented here will similarly be useful in other situations particularly where the
high-dimensional posterior distribution is uni-modal but non-Gaussian.

5

Conclusions

Normalising constants are fundamental in Bayesian statistics and allow the best model to be
selected for given data. In this paper we give an account of referenced thermodynamic integration
(TI), in terms of theoretical consideration regarding the choice of reference, and show how it can
be applied to realistic practical problems. We show referenced TI allows efficient calculation of a
single modelâ€™s evidence by sampling from geometric paths between the un-normalised density of
the model and a judiciously chosen reference density â€” here, a sampled multivariate normal that
can be generated and integrated with ease. The referenced TI approach was applied to several
examples, in which normalising constants over 1 to 200 dimensional integrals were calculated. In
the examples, the referenced TI approach had better convergence performance in terms of iterations
to a cut-off convergence and minimum bias achievable compared to similar methods such as power
posteriors or model switch thermodynamic integration. We showed the referenced TI method
has practical utility for substantially challenging problems of model selection â€” in this instance
concerning the epidemiology of infectious diseases â€” and suggest similar applicability in other fields
of applied machine learning that rely on high-dimensional Bayesian models with non-inferential
hyper-parameters.

20

References
[1]

Mrinank Sharma, SÃ¶ren Mindermann, Jan Brauner, Gavin Leech, Anna Stephenson, TomÃ¡Å¡
GavenÄiak, Jan Kulveit, Yee Whye Teh, Leonid Chindelevitch, and Yarin Gal. â€œHow Robust are the Estimated Effects of Nonpharmaceutical Interventions against COVID-19?â€ In:
Advances in Neural Information Processing Systems 33 (2020).

[2]

Seth Flaxman, Swapnil Mishra, Axel Gandy, H Juliette T Unwin, Thomas A Mellan, Helen
Coupland, Charles Whittaker, Harrison Zhu, Tresnia Berah, Jeffrey W Eaton, et al. â€œEstimating the effects of non-pharmaceutical interventions on COVID-19 in Europeâ€. In: Nature
584.7820 (2020), pp. 257â€“261.

[3]

Nicholas C. Grassly and Christophe Fraser. â€œMathematical models of infectious disease transmissionâ€. In: Nature Reviews Microbiology 6 (2008), pp. 477â€“487. doi: 10.1038/nrmicro1845.
url: https://doi.org/10.1038/nrmicro1845.

[4]

Libo Sun, Chihoon Lee, and Jennifer A. Hoeting. â€œParameter inference and model selection
in deterministic and stochastic dynamical models via approximate Bayesian computation:
modeling a wildlife epidemicâ€. In: Environmetrics 26.7 (2015), pp. 451â€“462. doi: 10.1002/
env.2353.

[5]

Theresa Stocks, Tom Britton, and Michael HÃ¶hle. â€œModel selection and parameter estimation
for dynamic epidemic models via iterated filtering: application to rotavirus in Germanyâ€. In:
Biostatistics 21.3 (2018), pp. 400â€“416. issn: 1465-4644. doi: 10 . 1093 / biostatistics /
kxy057.

[6]

Robert E. Kass and Adrian E. Raftery. â€œBayes Factorsâ€. In: Journal of the American Statistical Association 90.430 (1995), pp. 773â€“795. eprint: https://www.stat.cmu.edu/~kass/
papers/bayesfactors.pdf.

[7]

Charles H Bennett. â€œEfficient estimation of free energy differences from Monte Carlo dataâ€.
In: Journal of Computational Physics 22.2 (1976), pp. 245â€“268.

[8]

Xiao-Li Meng and Wing Hung Wong. â€œSimulating ratios of normalizing constants via a simple
identity: a theoretical explorationâ€. In: Statistica Sinica (1996), pp. 831â€“860.

[9]

John Skilling. â€œNested Sampling for General Bayesian Computationâ€. In: Bayesian Analysis
1.4 (2006), pp. 833â€“860. url: https://projecteuclid.org/download/pdf_1/euclid.ba/
1340370944.

[10]

Michael Habeck. â€œEvaluation of marginal likelihoods via the density of statesâ€. In: Artificial
Intelligence and Statistics. 2012, pp. 486â€“494.

[11]

John G. Kirkwood. â€œStatistical Mechanics of Fluid Mixturesâ€. In: The Journal of Chemical
Physics 3.5 (1935), pp. 300â€“313. doi: 10.1063/1.1749657. eprint: https://doi.org/10.
1063/1.1749657. url: https://doi.org/10.1063/1.1749657.

[12]

Andrew Gelman and Xiao-Li Meng. â€œSimulating normalizing constants: From importance
sampling to bridge sampling to path samplingâ€. In: Statistical science (1998), pp. 163â€“185.

[13]

Nial Friel, Merrilee Hurn, and Jason Wyse. â€œImproving power posterior estimation of statistical evidenceâ€. In: Statistics and Computing (27 2017), pp. 1165â€“1180. doi: https://doi.
org/10.1007/s11222-016-9678-6.

21

[14]

Nicolas Lartillot and HervÃ© Philippe. â€œComputing Bayes Factors Using Thermodynamic Integrationâ€. In: Systematic Biology 55.2 (2006), pp. 195â€“207. issn: 1063-5157. doi: 10.1080/
10635150500433722.

[15]

Wangang Xie, Paul O. Lewis, Yu Fan, Lynn Kuo, and Ming-Hui Chen. â€œImproving Marginal
Likelihood Estimation for Bayesian Phylogenetic Model Selectionâ€. In: Systematic Biology
60.2 (2010), pp. 150â€“160. issn: 1063-5157. doi: 10.1093/sysbio/syq085.

[16]

Evan J. Williams. Regression Analysis. Wiley, 1959.

[17]

Paul Adrien Maurice Dirac. â€œThe quantum theory of the emission and absorption of radiationâ€. In: Proceedings of the Royal Society of London. Series A, Containing Papers of a
Mathematical and Physical Character 114.767 (1927), pp. 243â€“265.

[18]

Murray Gell-Mann and Keith A Brueckner. â€œCorrelation energy of an electron gas at high
densityâ€. In: Physical Review 106.2 (1957), p. 364.

[19]

Lidunka Vocadlo and Dario Alfe. â€œAb initio melting curve of the fcc phase of aluminumâ€. In:
Phys Rev B 65.21 (2002).

[20]

Andrew Ian Duff, Theresa Davey, Dominique Korbmacher, Albert Glensk, Blazej Grabowski,
JÃ¶rg Neugebauer, and Michael W Finnis. â€œImproved method of calculating ab initio hightemperature thermodynamic properties with application to ZrCâ€. In: Physical Review B 91.21
(2015), p. 214311.

[21]

Nial Friel and Anthony N. Pettitt. â€œMarginal likelihood estimation via power posteriorsâ€.
In: Journal of the Royal Statistical Society: Series B (Statistical Methodology) 70.3 (2008),
pp. 589â€“607. doi: 10.1111/j.1467-9868.2007.00650.x.

[22]

Nial Friel and Jason Wyse. â€œEstimating the evidence: a reviewâ€. In: Statistica Neerlandica
66.3 (2012), pp. 288â€“308. doi: 10.1111/j.1467-9574.2011.00515.x.

[23]

Ewan Cameron and Anthony Pettitt. â€œRecursive pathways to marginal likelihood estimation
with prior-sensitivity analysisâ€. In: Statistical Science 29.3 (2014), pp. 397â€“419.

[24]

GeneviÃ¨ve Lefebvre, Russell Steele, and Alain C Vandal. â€œA path sampling identity for computing the Kullbackâ€“Leibler and J divergencesâ€. In: Computational statistics & data analysis
54.7 (2010), pp. 1719â€“1731.

[25]

Yu Fan, Rui Wu, Ming-Hui Chen, Lynn Kuo, and Paul O Lewis. â€œChoosing among partition
models in Bayesian phylogeneticsâ€. In: Molecular Biology and Evolution 28(1) (2011), pp. 523â€“
532. doi: 10.1093/molbev/msq224.

[26]

Guy Baele, Philippe Lemey, and Marc A Suchard. â€œGenealogical Working Distributions for
Bayesian Model Testing with Phylogenetic Uncertaintyâ€. In: Systematic Biology 65(2) (2016),
pp. 250â€“264. doi: 10.1093/sysbio/syv083.

[27]

Nikolai N. Bogolubov Jr. â€œOn model dynamical systems in statistical mechanicsâ€. In: Physica
32.5 (1966), pp. 933â€“944.

[28]

Alexander L. Kuzemsky. â€œVariational principle of Bogoliubov and generalized mean fields
in many-particle interacting systemsâ€. In: International Journal of Modern Physics B 29.18
(2015), p. 1530010.

[29]

Jun Zhang. â€œThe application of the Gibbs-Bogoliubov-Feynman inequality in mean field
calculations for Markov random fieldsâ€. In: IEEE Transactions on Image Processing 5.7
(1996), pp. 1208â€“1214.
22

[30]

Radford M. Neal and Geoffrey E. Hinton. â€œA view of the EM algorithm that justifies incremental, sparse, and other variantsâ€. In: Learning in graphical models. Springer, 1998, pp. 355â€“
368.

[31]

Michael I. Jordan, Zoubin Ghahramani, Tommi S. Jaakkola, and Lawrence K. Saul. â€œAn
introduction to variational methods for graphical modelsâ€. In: Machine learning 37.2 (1999),
pp. 183â€“233.

[32]

James Ridgway. â€œComputation of Gaussian orthant probabilities in high dimensionâ€. In:
Statistics and computing 26.4 (2016), pp. 899â€“916.

[33]

Dario Azzimonti and David Ginsbourger. â€œEstimating orthant probabilities of high-dimensional
Gaussian vectors with an application to set estimationâ€. In: Journal of Computational and
Graphical Statistics 27.2 (2018), pp. 255â€“267.

[34]

Donald B. Owen. â€œOrthant probabilitiesâ€. In: Wiley StatsRef: Statistics Reference Online
(2014).

[35]

Tetsuhisa Miwa, AJ Hayter, and Satoshi Kuriki. â€œThe evaluation of general non-centred orthant probabilitiesâ€. In: Journal of the Royal Statistical Society: Series B (Statistical Methodology) 65.1 (2003), pp. 223â€“234.

[36]

Robert N. Curnow and Charles W. Dunnett. â€œThe numerical evaluation of certain multivariate normal integralsâ€. In: The Annals of Mathematical Statistics (1962), pp. 571â€“579.

[37]

Harold Ruben. â€œAn Asymptotic Expansion for the Multivariateâ€. In: Journal of Research of
the National Bureau of Standards: Mathematics and mathematical physics. B 68 (1964), p. 3.

[38]

M. Brown. â€œA generalized error function in n dimensionsâ€. In: U.S. Naval Missile Center,
Theorethical Analysis Division (1963). url: https://apps.dtic.mil/dtic/tr/fulltext/
u2/401722.pdf.

[39]

Matthew D Hoffman and Andrew Gelman. â€œThe No-U-Turn sampler: adaptively setting path
lengths in Hamiltonian Monte Carloâ€. In: J. Mach. Learn. Res. 15.1 (2014), pp. 1593â€“1623.

[40]

Bob Carpenter, Andrew Gelman, Matthew Hoffman, Daniel Lee, Ben Goodrich, Michael Betancourt, Marcus Brubaker, Jiqiang Guo, Peter Li, and Allen Riddell. â€œStan: A Probabilistic
Programming Languageâ€. In: Journal of Statistical Software, Articles 76.1 (2017), pp. 1â€“32.
issn: 1548-7660. doi: 10.18637/jss.v076.i01. url: https://www.jstatsoft.org/v076/
i01.

[41]

Stan Development Team. Stan for epidemiology. url: https : / / epidemiology - stan .
github.io/ (visited on 08/04/2020).

[42]

John N. Bahcall and R. A. Wolf. â€œStar distribution around a massive black hole in a globular
clusterâ€. In: The Astrophysical Journal 209 (1976), pp. 214â€“232.

[43]

Tosio Kato. â€œOn the eigenfunctions of many-particle systems in quantum mechanicsâ€. In:
Communications on Pure and Applied Mathematics 10.2 (1957), pp. 151â€“177.

[44]

Robert Piessens, Elise deDoncker Kapenga, Christian Ueberhuber, and David Kahaner.
QUADPACK: A Subroutine Package for Automatic Integration. Springer, 1983. isbn: 3540125531.

[45]

SciPy.org. SciPy: Open Source Scientific Tools for Python: scipy.integrate. url: https://
docs.scipy.org/doc/scipy/reference/integrate.html (visited on 08/06/2020).

23

[46]

Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David
Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, StÃ©fan
J. van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov,
Andrew R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, CJ Carey, Ä°lhan Polat, Yu
Feng, Eric W. Moore, Jake Vand erPlas, Denis Laxalde, Josef Perktold, Robert Cimrman,
Ian Henriksen, E. A. Quintero, Charles R Harris, Anne M. Archibald, AntÃ´nio H. Ribeiro,
Fabian Pedregosa, Paul van Mulbregt, and SciPy 1. 0 Contributors. â€œSciPy 1.0: Fundamental
Algorithms for Scientific Computing in Pythonâ€. In: Nature Methods 17 (2020), pp. 261â€“272.
doi: https://doi.org/10.1038/s41592-019-0686-2.

[47]

Silia Vitoratou and Ioannis Ntzoufras. â€œThermodynamic Bayesian model comparisonâ€. In:
Statistics and Computing (27 2017), pp. 1165â€“1180. doi: https : / / doi . org / 10 . 1007 /
s11222-016-9678-6.

[48]

Swapnil Mishra, Tresnia Berah, Thomas A. Mellan, H. Juliette T. Unwin, Michaela A.
Vollmer, Kris V. Parag, Axel Gandy, Seth Flaxman, and Samir Bhatt. â€œOn the derivation
of the renewal equation from an age-dependent branching process: an epidemic modelling
perspectiveâ€. In: arXiv preprint arXiv:2006.16487 (2020).

[49]

Kris V. Parag and Christl A. Donnelly. â€œUsing information theory to optimise epidemic
models for real-time prediction and estimationâ€. In: PLOS Computational Biology 16.7 (July
2020), pp. 1â€“20. doi: 10.1371/journal.pcbi.1007990.

[50]

Qifang Bi, Yongsheng Wu, Shujiang Mei, Chenfei Ye, Xuan Zou, Zhen Zhang, Xiaojian Liu,
Lan Wei, Shaun A Truelove, Tong Zhang, Wei Gao, Cong Cheng, Xiujuan Tang, Xiaoliang
Wu, Yu Wu, Binbin Sun, Suli Huang, Yu Sun, Juncen Zhang, Ting Ma, Justin Lessler, and
Tiejian Feng. â€œEpidemiology and transmission of COVID-19 in 391 cases and 1286 of their
close contacts in Shenzhen, China: a retrospective cohort studyâ€. In: The Lancet Infectious
Disease 20 (8 2020), pp. 911â€“919. doi: doi:10.1016/S1473-3099(20)30287-5.

[51]

Ben Calderhead and Mark Girolami. â€œEstimating Bayes factors via thermodynamic integration and population MCMCâ€. In: Computational Statistics & Data Analysis 53.12 (2009),
pp. 4028 â€“4045. issn: 0167-9473. doi: https://doi.org/10.1016/j.csda.2009.07.025.
url: http://www.sciencedirect.com/science/article/pii/S0167947309002722.

24

Appendix A. COVID-19 Model
The COVID-19 model shown is based on the renewal equation derived from the Bellman-Harris
process. The details of the model and its derivation are provided in Mishra et al. [48]. Here, we
give a short overview of the AR(2) model. The model has a Bayesian hierarchical structure and
is fitted to the time-series data containing a number of new confirmed COVID-19 cases per day in
South Korea, obtained from https://opendata.ecdc.europa.eu/covid19/casedistribution/
csv. New infections y(t) are modelled by a negative binomial distribution, with a mean parameter
in a form of a renewal equation. The number of confirmed cases y(t) is modelled as:
y âˆ¼ NegBin(f (t), Ï†) ,
where Ï† is an overdispersion or variance parameter and the mean of the negative binomial distribution is denoted as f (t) and represents the daily case data through:
Z t
f (t) = R0
f (t âˆ’ Ï„ )g(Ï„ )dÏ„ .
Ï„ =0

As the case data is not continuous but is reported per day, f (t) can be represented in a discretised,
binned form as:
X
f (t) = Rt
f (t âˆ’ Ï„ )g(Ï„ ) .
Ï„ <t

Here, g(Ï„ ) is a Raleigh-distributed serial interval with mean GI, which is discretised as
Z s+0.5
Z 1.5
gs =
g(Ï„ )dÏ„ for s = 2, 3, ... and g1 =
g(Ï„ )dÏ„ .
sâˆ’0.5

0

Rt , the effective reproduction number, is parametrised as Rt = exp(t ), with exponent ensuring
positivity. t is an autoregressive process with two-days lag, that is AR(2), with 1 âˆ¼ N (âˆ’1, 0.1),
2 âˆ¼ N (âˆ’1, Ïƒ) and
t âˆ¼ N (Ï1 tâˆ’1 + Ï2 tâˆ’2 , Ïƒt ) for t = {3, 4, 5, ...}.
The modelâ€™s priors are:
Ïƒ âˆ¼ N + (0, 0.2) ,
Ï1 âˆ¼ N + (0.8, 0.05) ,
Ï2 âˆ¼ N + (0.1, 0.05) ,
Ï† âˆ¼ N + (0, 5) ,
GI âˆ¼ N + (0.01, 001) .
Modification were applied to this basic model, to obtain the different variants of the model
as described in Section 3. First group of models analysed was the AR(2) model described above,
but with the GI parameter fixed to a certain value instead of inferring that parameter from the
data. AR(3) and AR(4) models had additional parameters Ï3 and Ï4 , which allow to model the
autoregressive process with a longer lag (3- and 4- days respectively). Finally, models W = k,
k = 1, .., 7 were similar to the AR(2) model, but the underlying assumption of these models is that
the Rt stays constant for the duration of the length of the sliding window W = k.
25

GI = 5

 

   

  

  

   

  

  

  

GI = 6

  

 

  

  

   

  

   

  

 

  

   

  

  

  

  

  

  

GI = 6.5        
GI = 7

       

 

 

   

GI = 8

  

  

  

 

GI = 9

                    

                        

 

  

GI = 20

                    

 

 

GI = 20

 

GI = 9

GI = 8

GI = 10

GI = 7

  

GI = 6.5

 

GI = 6

 

GI = 5

  
 

           

GI = 10

  

   

  
   

(a)

 

 
  

 
  

  
  

  

   

  
  

AR(2)

  

   

 

AR(3)

W=2

 

 

 

   

AR(4)

W=1

  

   

    

    

 

  

 

W=3

  

   

 

 

 

 
 

 

W=7

   

   

  

  

 

  

  

  
  

W=7

 

W=4

  

W=3

   

W=2

  

W=1

W=4

 

   

AR(2)

(b)

AR(3)

AR(4)

(c)

Figure 7: Logarithms of Bayes factors for the analysed COVID-19 renewal models, evaluated using
the normalising constants ratios obtained by referenced TI. In each cell, the colour indicates the
value of the BF1,2 for models M1 (row) and M2 (column). Higher values, that is a brighter orange
colour, suggest that M1 is strongly better than M2 , and values below 0 in blue palette indicate that
M1 is worse than M2 . GI = 8 performed best out of fixed GI models, W = 2 best out of sliding
window models, and AR(3) performed better than AR(2) and AR(4). For the interpretation of
the BF values see [6].

26

    

 3 U H G L F W L R Q V

W=2
W=7

AR(3)
AR(2)

   

GI = 8
GI = 20

   
   
   
 

 

  

   

 ' D \ V

   

     

  

   

 ' D \ V

   

     

  

   

 ' D \ V

   

   

Figure 8: Cases of SARS-CoV-2 infections in South Korea from the data (shown with bars) and the
cases predicted by different models. On each graph, predictions made by the model favoured by the
Laplace approximation is shown with a blue dashed line, and predictions made by the referenced
TI favoured models are shown with an orange dashed line. The lines in all three subplots are
largely overlapping, revealing that all models fitted the case data similarly well.

27

