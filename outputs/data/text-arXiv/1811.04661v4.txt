Noname manuscript No.
(will be inserted by the editor)

RelDenClu: A Relative Density based Biclustering
Method for identifying non-linear feature relations with
an Application to identify factors affecting spread of
COVID-19

arXiv:1811.04661v4 [cs.CV] 28 May 2020

Namita Jain ¬∑ Susmita Ghosh ¬∑ C A Murthy

the date of receipt and acceptance should be inserted later

Abstract The existing biclustering algorithms for finding feature relation based biclusters often depend on
assumptions like monotonicity or linearity. Though a
few algorithms overcome this problem by using densitybased methods, they tend to miss out many biclusters
because they use global criteria for identifying dense
regions. The proposed method, RelDenClu uses the local variations in marginal and joint densities for each
pair of features to find the subset of observations, which
forms the bases of the relation between them. It then
finds the set of features connected by a common set of
observations, resulting in a bicluster.
To show the effectiveness of the proposed methodology, experimentation has been carried out on fifteen
types of simulated data. It has been used for unsupervised learning using three real-life datasets. Applicability of the algorithm is also explored on three real-life
datasets to see its utility as an aid to supervised learning. For all the cases the performance of the proposed
method is compared with that of seven different stateof-the-art algorithms. The proposed algorithm produces
better results. The efficacy of the algorithm is shown on
COVID-19 dataset and found out that some features
(genetic, demographics and others) are likely to affect
the spread of COVID-19.
Namita Jain
Department of Computer Science and Engineering, Jadavpur University, Kolkata 700032, India E-mail: namita.saket@gmail.com
Susmita Ghosh
Department of Computer Science and Engineering, Jadavpur University, Kolkata 700032, India E-mail: susmitaghoshju@gmail.com
C A Murthy
Machine Intelligence Unit, Indian Statistical Institute, 203
Barrackpore Trunk Road,Kolkata 700108, India

Keywords Biclustering ¬∑ Relative density ¬∑ Marginal
density ¬∑ Non-linear relationship
.

1 Introduction
Similarity within a bicluster can be interpreted in different ways like spatial closeness, the relation between features for selected observations, etc. Most of the existing
algorithms focus on spatial proximity between observations in the selected subspace. Relation-based biclustering methods mostly search for biclusters based on
specific relations. While a few algorithms exist for finding non-linear relation-based biclusters, they search for
biclusters with some constraint e.g., UniBic by Wang
et al [35] finds monotonous relations and CBSC by
Jain and Murthy [20] does not adjust for variations in
marginal distributions, thereby limiting their ability to
find many types of relation based biclusters. For example, UniBic will not be able to find a relation based
periodic wave. CBSC finds more general relations, but
if marginal densities have sparse and dense regions, the
actual bicluster in the data may be not be obtained due
to fragmentation. In this article, we have tried to overcome these problems. We propose a methodology for
finding biclusters based on non-linear continuous feature relation. For each feature pair, we identify observations forming relationships with each other by comparing joint density and marginal densities. Then, we check
whether there are more features pairs connected with
same set of observations forming related dense regions.
Once this information is available, we have an adjacency
matrix which allows us to find connected components.
Thus, we find all the features connected with same set

2

of observations. The resulting pair of sets (of observations and features) is called a bicluster. We have named
this method RelDenClu, and it is seen to have several
desirable properties like invariance to different permutations, scaling and translation. This allows results of
RelDenClu to be consistent in spite of changes in data
representation. The proposed technique of identifying
related dense regions is the main contribution of this
article.
We have discussed some of the existing biclustering methods in Section 2. In Section 3 we present the
method of finding dense related regions using marginal
and joint densities. Section 4 presents entire algorithm
in detail. Section 5 describes datasets, method used
for comparison and evaluation criterion. The results on
simulated and real-life datasets are presented in Section
6. We have applied RelDenClu to analyse the effects of
World Development Indicators on spread of COVID-19
in different regions of world, and presented the results
in Section 7, and conclude with Section 8.
2 Related Work and Contributions
2.1 Discussion on density based and relationship based
biclustering methods
Hartigan [17] pioneered biclustering by proposing a biclustering method based on constant value of elements.
Subsequently, many biclustering algorithms were proposed with varying objectives and approaches. These
include linear algebra based ones proposed by CarmonaSaez et al [5] and Costeira and Kanade [9]. Some authors used linear algebra to find biclusters in transformed space [24]. Mitra and Banka [26] used evolutionary computing, Getz et al [16] used iterative approach,
while Tanay et al [32] used graph-based approach for
biclustering. Detailed surveys on biclustering and subspace clustering have been done by Kriegel et al [23],
PrelicÃÅ et al [31], Parsons et al [29], Madeira and Oliveira
[25].
In this article, we use a density-based approach for
biclustering. Density-based biclustering algorithms such
as SubClu by Kailing et al [21] and CLIQUE by Agrawal
et al [3] find high-density regions in corresponding subspaces. These methods start by finding dense regions
in one-dimensional space and grow biclusters using the
apriori approach [2]. The performance of these methods
depends on the parameters used by the algorithm, while
the proposed method, RelDenClu automatically calculates the parameters for density estimation for each feature pair. Thus, the user need not bother about underlying density estimation process and only needs to
provide parameters directly related to biclustering.

Namita Jain et al.

The main objective of RelDenClu is to find biclusters formed by features related to each other with a
subset of observations as a base. Existing algorithms
with this objective include the algorithm proposed by
Cheng and Church [7]. Multiplicative algorithm FABIA
has been proposed by Hochreiter et al [19]. Here a row
can be obtained by multiplying another with a constant. Unlike our proposed method, these methods assume that the relation between features is linear or multiplicative.
More generalized biclustering methods based on an
arbitrary relationship between features have been proposed. These include algorithms like UniBic by [35],
based on longest common subsequence. UniBic can find
biclusters based on monotonous relationships between
features. Curler by Tung et al [34] finds such non-linear
clusters by combining a density-based approach with
the principal component method. The method finds
high-density Gaussian regions using Expectation Maximization and combines them using the relation between
directional information of resultant biclusters. OPSM
proposed by Cheung et al [8] finds order-preserving submatrices.
A more recent algorithm named CBSC proposed by
Jain and Murthy [20] identifies non-linear relationships
between pairs of dimensions using a density-based approach. However, CBSC tends to produce fragmented
biclusters when marginal densities are highly variable.
The proposed algorithm, RelDenClu overcomes the
problem of fragmentation by adapting to local variations in density along each dimension. It finds nonlinear continuous relationship based biclusters and does
not make assumptions of monotonicity, linearity or fixed
relationship between features.

2.2 Contributions
The proposed method, RelDenClu finds biclusters based
on continuous non-linear relationships using marginal
and joint density, information derived from two-dimensional
spaces corresponding to each pair of features. The advantages of this approach are listed below. The first
point in this list is discussed in Section 3.4 and the last
two points have been elaborated using a toy example
in Section 3.2.3.
1. It does not expect the user to provide parameters
for finding high-density regions.
2. It does not assume that the relationship between
features has a particular form like linear or multiplicative.
3. It does not assume that the relationship between
features is monotonous.

Relative density-based biclustering

4. It avoids the fragmentation of biclusters by adapting
for variations in marginal densities.

3 Proposed Biclustering Method: RelDenClu
In this section, we define the objective of RelDenClu
and provide relevant definitions. We also present the
procedure for finding dense sets for each pair of features,
which is the main contribution of the present article.

3.1 Objective of the proposed biclustering method
The bicluster obtained by RelDenClu is a submatrix
in which columns are related directly or indirectly. Let
the dataset be denoted by a matrix D containing N
rows each corresponding to observation and M columns
each corresponding to a feature. In this article, a direct
relationship between two columns of a matrix means
the following: Dependence exists between features corresponding to these two columns when the subset of
observations contained in the bicluster is used as the
base. Two columns are connected indirectly if there exists a chain of columns connecting them, such that each
consecutive pair of columns in this chain is directly connected. This has been stated in the definitions below.
Definition 1 For a matrix A let the ith column be denoted by A‚àó,i . In this article, we call the ith and the j th
columns of a matrix to be directly connected if there is a
dependence between these two columns. Perfect dependence occurs when either A‚àó,i can be used to determine
A‚àó,j , or vice-versa.
Imagine each direct relationship between features to be
an edge connecting two vertices, each representing a
column. Then, two columns are indirectly connected if
they are connected by a path. Note that the direct or
indirect connection between the columns exists only on
the base of rows contained in the submatrix.
Definition 2 We call the ith and the j th columns of a
matrix A to be connected if these two columns are directly connected or there exist columns A‚àó,i1 , ¬∑ ¬∑ ¬∑ , A‚àó,ik ,
such that 1) each column in this list is directly connected to the consecutive column in the list (e.g. A‚àó,i2
is directly connected to A‚àó,i3 ), 2) A‚àó,i is directly connected to A‚àó,i1 , and 3) A‚àó,ik is directly connected to
A‚àó,j .
Using the definition of connected features we now
define a bicluster as follows:
Definition 3 A bicluster in the dataset D is a pair
of two sets given by O = {oi1 , oi2 , ¬∑ ¬∑ ¬∑ , oin } and F =

3

{f i1 , f i2 , ¬∑ ¬∑ ¬∑ , f im }. The matrix D restricted to observations in O and features in F , gives us a submatrix A
corresponding to the bicluster < O, F >. More specifically, the submatrix corresponding to < O, F > is given
by A where the q th element of the pth row is given by,
A[p, q] = D[oip , f iq ]. A bicluster is said to be relationbased if all the columns of the corresponding submatrix
are connected, either directly or indirectly.
It may be noted that the proposed method finds biclusters based on the relation between feature pairs, and so
it can result in biclusters with disconnected regions.
Thus, the proposed method can be seen as grouping
related features based on observations.
To find biclusters based on definitions given above,
we need to check whether a pair of features are related
to each other for any set of observations. A method to
find related subsets of observations for a given feature
pair is presented in the following section. In this section,
we define the objective of RelDenClu and provide the
relevant definitions. We also present the procedure for
finding dense sets for each pair of features.

3.2 Proposed technique of finding a set of observations
having dependence for a given feature pair
In this section, a method has been proposed to identify
subsets of observations which form the base for a relationship between two features. The novelty of the proposed algorithm lies in its way of finding these related
subsets by comparing joint distribution to marginal distribution using histogram technique. The dense regions
obtained form the basis of a direct connection between
features or columns as discussed in Section 3.1.
The data in the Euclidean space is normalized to
space [0, 1] √ó [0, 1]. Either a grid or rolling window is
used to calculate the density. Data is analyzed by considering small rectangular regions called cells. The terminology used in this section is as follows: The region
given by (vx ‚àí xlen/2, vx + xlen/2] √ó (vy ‚àí ylen/2, vy +
ylen/2] is said to be a cell of size (xlen, ylen) centred
at point(vx , vy ). The phrase ‚Äúcorresponding marginal
cells‚Äù refers to rectangular regions given by (vx ‚àíxlen/2, vx +
xlen/2]√ó[0, 1] and [0, 1]√ó(vy ‚àíylen/2, vy +ylen/2]. Here
we use the term ‚Äòdensity‚Äô in a casual way i.e., density is
given by the number of observations in a cell divided by
its area. A cell is said to be dense if its density is higher
than the average density of the corresponding marginal
cells.
As examining cells centered at each observation may
not be computationally feasible for large datasets, we
use slightly different schemes for small and large datasets.
This allows us to achieve a balance between accuracy

4

and ease of computation. Whether we consider a dataset
to be large or small depends on computational power.
3.2.1 Finding related dense regions for small datasets
For small datasets, cell centered around each observation is examined. Note that, we will have many overlapping cells. For each observation with coordinates (vx , vy )
we find observations lying in the cell centred on it. More
precisely, we check the cell given by (vx ‚àí xlen/2, vx +
xlen/2]√ó(vy ‚àíylen/2, vy +ylen/2] to see if its density is
greater than the average density of both the marginal
cells, which are the cells given by (vx ‚àí xlen/2, vx +
xlen/2] √ó (0, 1] and (0, 1] √ó (vy ‚àí ylen/2, vy + ylen/2].
If this condition is found to be true and the density of
the cell is also higher than the average density of the
entire region, we mark the cell centred at (vx , vy ) as
‚Äòdense‚Äô. Two dense cells are merged only if the center
of each cell lies within the other cell and the average
density of the overlapping region between two cells is
high. Density is considered high if it is not less than
the density of each of the marginal cells corresponding to the two overlapping cells and also not less than
the average density of the entire space. Repeating these
steps iteratively allows us to find larger connected dense
regions. Algorithm 1 provides pseudo-code for finding
dense regions as described here.
The size of a cell along a dimension X is calculated
using maximal separation along X. For each pair of
dimensions X and Y let the maximal separations be
sx and sy , respectively. The intervals used to find the
dense regions along X and Y are given by xlen = sx c
and ylen = sy c , where c < 0.5 and is close to 0.5. We
have taken c = 0.4999. The reason behind choosing this
value has been discussed in Section 3.4.
As the maximal separation does not go to zero for
datasets having non-continuous distribution this method
cannot be used for such datasets. Therefore, for such
datasets and for large datasets we present a simpler
method which does not use maximal separation. This
is discussed in the following section.
3.2.2 Finding related dense regions for large datasets
For larger datasets having non-continuous distribution,
using the method elaborated in Section 3.2.1 would not
be feasible, as number of observations in neighbourhood
of each observation has to be calculated. To overcome
this problem we present a simplified method for identifying dense regions in large datasets. For large datasets,
we simply take 3 log(N ) equal intervals along each axis,
where N is the number of observations in the dataset.
Since only disjoint cells are examined, execution time

Namita Jain et al.

is reduced. We decide whether a cell is dense using the
same criterion as small datasets i.e., the density of the
cell should be greater than densities of marginal cells
as well as the average density of the entire space. Two
dense regions are merged if they are adjacent to each
other, horizontally, vertically or diagonally. A dense region can have at most 8 dense regions connected to it.
After finding dense regions in two-dimensional space,
noise is removed by using the overlap between three
two-dimensional spaces for every set of three features.
A detailed description of this step can be found in Section 4.3.
3.2.3 The proposed method on a toy example

4

4

3.5

3.5

3

3

2.5

2.5

2

2

1.5

1.5

1

1

0.5

0.5

0

0
0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

1

1

0.9

0.9

0.8

0.8

0.7

0.7

0.6

0.6

0.5

0.5

0.4

0.4

0.3

0.3

0.2

0.2

0.1

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

0.1

0

0
0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Fig. 1: Left column shows two-dimensional scatter plots
for two pairs of features. The right column shows the
scatter plot for the same data where the related observations identified by the proposed method are marked
in red
The working of the proposed scheme has been shown,
using two toy examples in Figure 1. The top figure in
the left column shows the scatter plot for two variables.
For a subset of data, the two features are related by the
function f (x) = 4x.2 , where x lies in the range (0, 1).
For the rest of the observations, both variables are unrelated and have a uniform distribution in (0, 1). If the
density of each cell is compared to the average density
of the scatter plot, the unrelated observations will also
get identified as biclusters as they are dense in comparison to the empty region lying in the upper part of the
scatter plot. However, the proposed method can identify the observations related by the square function, as
it compares the density of each cell with marginal den-

Relative density-based biclustering

5

sity, as can be seen from the top image in the right
column.
Similarly, the bottom left image gives a scatter plot
for two variables. For a subset of data, the two variables
are related by the function f (x) = 0.5 sin(2œÄx) + 0.5,
where x lies in the range (0, 1). For the rest of the observations, both variables are unrelated and have a uniform distribution in (0, 1). From the bottom image in
the right column, we can see that the proposed method
can identify the set of related observations (marked in
red). This is a non-monotonous relation, so UniBic will
not be able to detect biclusters based on such relations.
The relationships used in first and second images have
been respectively used in generating simulated datasets
for which results have been reported in first and second
rows in Table 2. These images have been presented here
to clarify the motivation behind the proposed method.

j th partition along Y axis with probability one, 2)If an
observation is in the j th partition along Y axis it implies that it will be in the ith partition along X axis
with probability one. This means that there is total dependence between two variables.
Lower bound for joint probability is given by the
product of marginal probabilities and can be written
as PXY (i, j) ‚â• PX (i) √ó PY (j). The equality is attained
when the variables are independent. In case of any dependence PXY (i, j) > PX (i) √ó PY (j).
Recall that, we have normalized the data so that
it lies in [0, 1] √ó [0, 1]. If marginal distribution for X
is uniform, the probability PX (i) = 1/nX . Similarly, if
marginal distribution for Y is uniform, the probability PY (i) = 1/nY . Since we want to select only highdensity regions we would like to choose partitions having more than average density. Thus we select observations which fulfil each of the following conditions:

3.3 Relation between proposed method and Mutual
Information

PXY (i, j) >

PX (i)
nY

(1)

PXY (i, j) >

PY (j)
nX

(2)

PXY (i, j) >

1
nX nY

(3)

Unlike algorithms like ITL [12], the proposed method
does not use mutual information for finding biclusters.
However, it compares joint density to marginal densities
establishing an implicit connection to mutual information which is discussed in this section. This discussion
is done in terms of data partition for simplicity, similar
reasoning can be applied to the rolling window scheme
used for small datasets. The entire dataset is scaled to
the range [0, 1] along each axis. Suppose, the [0, 1] intervals along X and Y axis are divided into nX and nY
equal partitions, respectively.
We use following notations:
‚Äì PXY (i, j) denotes the probability of an observation
lying in the ith partition along X axis and the j th
partition along Y axis
‚Äì PX (i) denotes the probability of an observation lying in the ith partition along X axis
‚Äì PY (j) denotes the probability of an observation lying in the j th partition along Y axis
Let, the probability that an observation belongs to the
ith partition along X axis and the j th partition along Y
axis, be denoted by PXY (i, j). Similarly, let the probability that an observation belongs to the ith partition
along X axis be denoted by PX (i) and that it belongs
to the j th partition alongPY axis be denoted by PY (j).
nY
Obviously, PX (i) =
j=1 PXY (i, j) and PY (j) =
PnX
P
(i,
j).
i=1 XY
Note that, PXY (i, j) ‚â§ min(PX (i), PY (j)). The equality is attained when at least one of the following two
conditions are fulfilled: 1) If an observation is in the ith
partition along X axis, it implies that it will be in the

These are the three conditions we have used to select
dense cells in 2-dimensional space. Note that the inequality PXY (i, j) > PX (i) √ó PY (j) is equivalent to
PXY (i,j)
PX (i)√óPY (j) > 1. But the Expectation of log of this
XY (i,j)
term i.e., EXY [log( PXP(i)√óP
)] is the mutual informaY (j)
tion for X and Y . In this sense, we are trying to identify
observations having high mutual information. However,
the uniform distribution assumption used in equations
given above may not be true, so the proposed method
will be biased towards detecting regions having high
marginal densities. Thus, the proposed method will obtain subsets of observations, using a trade-off between
high density and high dependence.

3.4 Choosing parameters for density estimates and
convergence
The performance of histogram-based density methods
is dependent on appropriate bin length (size of partitions used for density estimation ). If the bin length
reduces too quickly, the estimated density will be incorrect as some of the bins (partitions) may remain empty.
On the other hand, if bin length decreases too slowly it
will not converge to true density as it will be averaging

6

Namita Jain et al.

over a large area. Parzen [30] has given the criterion for
convergence of density estimates as follows: The area
of each bin AN should converge to zero as the number
of observations N goes to infinity. Also, the product
of the number of observations N and the area of each
cell should go to infinity as N goes to infinity. We can
write the condition as limN ‚Üí‚àû AN ‚Üí 0 and the product limN ‚Üí‚àû (N √ó AN ) ‚Üí ‚àû.
In this article, we use the separation distance based
bin length. As discussed in Subsection 3.2.1, for small
datasets, we use the bin lengths xlen = scx and ylen =
scy where sx and sy are maximal separations along X
and Y axes, respectively and c is the constant which
should have a value close to 0.5, but also less than 0.5.
We will now discuss how this choice leads to consistent
estimates.
Suppose N observations are distributed on the interval [0, 1] along a particular axis and the maximal
separation along this axis be given by s. The work done
by Deheuvels [11] shows that if certain regularity conditions are fulfilled, and probability density fulfils following conditions: 1) probability density function is defined
on a bounded interval and 2) probability density has
a non-zero minima on this interval, then limit inferior
and superior are given by limN ‚Üí‚àû s = C1 log(N )/N
and limN ‚Üí‚àû s = C2 log(N )/N , where C1 and C2 are
different constants.
For small datasets, the area of each cell is given by
AN = scx √ó scy . Since c < 0.5, we see that upper limit is
given by:
lim AN = lim (C2 log(N )/N ))

N ‚Üí‚àû

N ‚Üí‚àû

2c

=0

1

1

0.9

0.9

0.8

0.8

0.7

0.7

0.6

0.6

0.5

0.5

0.4

0.4

0.3

0.3

0.2

0.2

0.1

0.1

0

0
0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

0

1

1

0.9

0.9

0.8

0.8

0.7

0.7

0.6

0.6

0.5

0.5

0.4

0.4

0.3

0.3

0.2

0.2

0.1

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

0.1

0

0
0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Fig. 2: Performance of the proposed method on small
and large datasets. The left column shows all data in
blue. The right column shows related observations identified by the proposed method in red and remaining
observations in blue

4 Stepwise Description of RelDenClu

(4)

Parzen‚Äôs second condition is also fulfilled as product of
area and number of observations goes to infinity. We
find that lower limit is given by:
lim N √ó AN = lim N √ó (C1 log(N )/N ))2c
N ‚Üí‚àû

of observations in both cases. We can see that the proposed method can find the related set of observations
in both cases, as seen by observations marked in red.

N ‚Üí‚àû

= lim N (1‚àí2c) (C1 log(N )))2c = ‚àû (5)
N ‚Üí‚àû

as 1 ‚àí 2c > 0
Thus, estimated density for small datasets is consistent, according to Parzen density estimates. For larger
datasets we simply divide the [0, 1] interval along each
axis into 3 log(N ) partitions. Thus, for a two-dimensional
space normalized to [0, 1] √ó [0, 1] the area of each cell
is limN ‚Üí‚àû 9 log12 (N ) . Note that, limN ‚Üí‚àû 9 log12 (N ) = 0
and limN ‚Üí‚àû 9 logN2 (N ) = ‚àû. Thus, we obtain consistent
density estimates according to Parzen‚Äôs rules. Figure 2
shows the performance of the proposed method on a
small dataset containing only 100 observations and a
larger dataset containing 1000 observations. The relation f (x) = x2 , where x lies in (0, 1), exists for a subset

The previous section discussed the novel part of RelDenClu, while in this section we present the entire biclustering procedure. While CBSC [20] compares the density of cell to average density, RelDenClu compares the
cell density to marginal densities. Both algorithms use
similar preprocessing and a similar procedure to obtain
biclusters from dense related regions. Many algorithms
like [21], [19], [22] use the apriori method to obtain biclusters from information obtained in low dimensional
spaces. The procedure used by RelDenClu and CBSC is
given in this section and it is different from the apriori
method.

4.1 Data preprocessing
For finding dense regions, we need to normalize the data
to [0, 1] √ó [0, 1]. We use a common procedure to normalize the data, which has also been used by Jain and
Murthy [20]. It is to be mentioned that, the data may
come from a distribution with an unbounded base or a
bounded base with an unknown range. As mentioned

Relative density-based biclustering

7

earlier, let data be represented by matrix D. The element in ith row and jth column is denoted by ei,j . To
normalize data from different distributions to the range
[0, 1] we have applied one of the following two transformations to data:

ei,j ‚àí min (ek,j )
norm1 (ei,j ) =

1‚â§k‚â§n

max (ek,j ) ‚àí min (ek,j )

1‚â§k‚â§n

(6)

1‚â§k‚â§n

norm2 (ei,j ) = norm1 (tan‚àí1 (ei,j )/œÄ + 0.5)

(7)

The transformation norm1 (x) (x represents elements
of input data matrix) is commonly used for mapping
data from a bounded base to [0, 1] interval while, transformation norm2 (x) maps the interval (‚àí‚àû, ‚àû) to interval (0, 1), which is a subset of [0, 1]. The latter transformation is capable of mapping points from an unbounded region to a bounded interval. Thus it can be
applied preferably if data follows distributions like Gaussian, which have non-compact base.

4.3.1 Finding seed biclusters
To identify the set of observations related to each other
we find relative density-based subsets in two-dimensional
Euclidean space given by each pair of features. However, because the background noise can be distributed
in arbitrary ways, this procedure of finding relations can
result in a lot of noise. Therefore, to weed out noise, we
search for a set of three features, for which a given set
of observations form dense regions for each pair of features belonging to the set. Thus for a set of features
{f1 , f2 , f3 }, we find dense regions for pairs < f1 , f2 >,
< f2 , f3 >, and < f3 , f1 >. The intersection of observations forming dense regions for these three pairs is
found. If the resulting set of observations is significant
i.e., the number of observations is larger than a userdefined parameter named ObsInMinBase we consider it
to be a seed bicluster. How ObsInMinBase and other
user-defined parameters can be decided, is discussed in
Section 4.5. For clarity, we have used the typewriter font
for all such parameters. Once the set of observations has
been identified, we treat it as a seed bicluster and add
other feature-triplets which have significant overlap in
their corresponding observation subsets. The following
subsection describes the procedure for doing this.
4.3.2 Using seed biclusters to get larger biclusters

4.2 Finding dense regions in two-dimensional space
In this section we will discuss in brief the procedure
of finding dense regions. The data is normalized and
dense regions are obtained using rolling window estimates or histogram-based estimates depending on the
size of data. As mentioned earlier, a cell is called dense
if its density is higher than the densities of the corresponding marginal cells. When the number of observations in the dataset is small, cell centered around
each observation is considered. Overlapping cells are
merged if the common region belonging to both the
cells, has a density higher than the average density of
marginal cells. For large datasets, a grid of disjoint cells
is taken and neighbouring dense cells are merged to obtain larger dense regions.

4.3 Obtaining biclusters from dense regions found
with different dimension pairs
Once we have obtained dense regions in two-dimensional
spaces, this information is used to obtain biclusters in
higher dimensions. The pseudo-code for the procedure
described in this section is given as Algorithm 2.

We first arrange seed biclusters in descending order by
the number of observations present in each seed bicluster. We iteratively use these seed biclusters as a base
for finding biclusters. When we use a particular seed
bicluster for building a larger bicluster, we call it base
seed bicluster. We start by initializing a bicluster B to
base seed bicluster S. We compare this bicluster B with
all other seed biclusters (except S). If the length of S
in terms of the number of observations is denoted by
length(S), we find seed biclusters which have at least
length(S) √ó Sim2Seed observations common with B,
where Sim2Seed is a user-defined parameter. As we keep
finding seed bicluster matching with B we update B
by adding all observations in matching seed bicluster.
This is done repeatedly till no more additions can be
made to B. Then, we find observations which occur in
at least ObsInMinBase number of seed biclusters, where
ObsInMinBase is a user-defined parameter. These observations and features corresponding to the matching
seed biclusters form an output bicluster.
The user also has an option of ignoring seed biclusters which have very high overlap with some other seed
bicluster already used as the base. To use this option
one must set the user-defined parameter ReuseAllSeeds
to false. In this case, the user-defined parameter named

8

Namita Jain et al.

Algorithm 1: DenseRegions()
/*
/*
/*
/*
/*
/*
1

2
3

4
5
6
7
8
9

10
11

12

13

14
15

16
17

18
19
20
21

22
23
24

25

---------------------------------------------------This is an algorithm for finding dense regions in
two-dimensional space defined by dimensions i and j,
for small dataset method described in Section 3.2.1
Input is list of all observations in two-dimensional space defined by i and j
-----------------------------------------------------

/* For dimensions i sepi is the maximum separation in dimension i
for each point pa do
/* neigh(pa ) is the set of points representing neighbourhood of pa
/* Initialize neighbourhood of each point
neigh(pa ) = ‚àÖ
for each pair of points pa , pb do
/* di denotes distance in dimension i and dj denotes distance in dimension j
if ( di (pa , pb ) < sepi and dj (pa , pb ) < sepj ) then
connection(pa , pb ) = 1
Add pb to neigh(pa )
Add pa to neigh(pb )

*/
*/
*/
*/
*/
*/
*/
*/
*/

*/

else
connection(pa , pb ) = 0
f inset = ‚àÖ
for each point pa do
/* M arginalN eighi (pa ) is set of points lying in the region given by (pa ‚àí sepi /2, pa + sepi /2) in dimension i
and (0, 1) in dimension j
*/
/* # denotes cardinality of a set
*/
/* Here, AverageDensity is given by ‚Äònumber of observations in dataset‚Äô ‚àó sepi ‚àó sepj , as data is scaled to
[0, 1] √ó [0, 1]
*/
if #(neigh(pa )) > AverageDensity and #(neigh(pa ))/(sepi ‚àó sepj ) > #(M arginalN eighi (pa ))/sepi and
#(neigh(pa ))/(sepi ‚àó sepj ) > #(M arginalN eighj (pa ))/sepj then
f inset = f inset ‚à™ pa
for each pair of points pa , pb in f inset do
if #(neigh(pa ) ‚à© neigh(pb )) ‚àó 4/(sepi ‚àó sepj ) >
max(M arginalN eighi (pa )/sepi , M arginalN eighj (pa )/sepj , M arginalN eighi (pb )/sepi , M arginalN eighj (pb )/sepj )
then
/* Merge the two cells i.e., cell connected to one is automatically connected to other
/* connection(pa , :) denotes all connections of pa
connection(pa , :) = connection(pa , :)‚à™ connection(pb , :)
connection(pb , :) = connection(pa , :)

*/
*/

/* Perform connected components according to new connections
while new connections are formed do
for each pair of points pa , pb connected to each other do
connection(pa , :) = connection(pa , :) ‚à™ connection(pb , :)
connection(pb , :) = connection(pa , :)

*/

/* Retain only those points from f inset which have merged with other points
f inset=f inset[sum(connection(pa , :)) > 0]
npc = #(f inset)
Find connected components in f inset using radius rad = log(npc)/npc
/* kth connected component is denoted by P ts(i, j, k)
/* Let total number of connected components for given pair of dimensions be mcc
Return points < P ts(fi , fj , 1), P ts(fi , fj , 2) ¬∑ ¬∑ ¬∑ P ts(fi , fj , mcc ) >

*/

ReuseSeedSim gives the threshold for the amount of
overlap between seed biclusters. If the overlap between
a base seed bicluster S and other seed bicluster is greater
than ReuseSeedSim √ó Sim2Seed √ó length(S), we do not
use the matching seed bicluster as base seed bicluster in
future iterations. On the other hand, if the parameter
ReuseAllSeeds is set to true we will not ignore seed

*/
*/

biclusters and they will be used as a base in subsequent
iterations. These two parameters control the amount of
overlap between biclusters in terms of observations.

Relative density-based biclustering

9

Algorithm 2: GetBiClusters
/*
/*
/*
/*
/*
/*
1

2

3

4

5
6
7
8
9
10
11

12

13
14
15
16
17
18
19
20

21

22
23
24
25

26

27
28

This algorithm gives the process of obtaining biclusters from dense regions obtained by Algorithm 1
The description of this algorithm is provided in Section 4.3
All the input parameters accept data matrix D are described in Section 4.5
Let the data be D
Normalize the data using norm1 if data seems to be
coming from a bounded distribution otherwise use norm2

*/
*/
*/
*/
*/
*/

Normalize data using the procedure given in Section 4.1
/* Each pair of dimensions may contain several sets representing dense regions
for each pair of features < fi , fj > do
/* This is described in Section 3.2.1
P ts(fi , fj , 1), P ts(fi , fj , 2), ¬∑ ¬∑ ¬∑=denseregions(Df i , Df j )
/* Initialize list of seed biclusters as given in first paragraph of Section 4.3
T list = ‚àÖ F list = ‚àÖ for each set of 3 features fi , fj , fk and each combination of u, v, w do
/* Check if a seed bicluster is formed by each dense region for a set of three features
/* dense regions numbered u, v and w in dimension pairs < fi , fj >, < fj , fk >
/* and < fk , fi > are checked for each combination between dense regions
Tx = P ts(fi , fj , u) ‚à© P ts(fj , fk , v ) ‚à© P ts(fk , fi , w)
if length(Tx < MinSeedSize) then
Delete (Tx )

*/
*/
*/
*/
*/
*/

else
Fx =< u, v, w >
Add Tx to Tlist
Add Fx to Flist

Sort T list in descending order of length
/* Remaining part of algorithm corresponds to Section 4.3 starting from second paragraph
Clusterlist = ‚àÖ
for each Tx in Tlist not marked as ignore do
Clf = Fx
SelectedT list = Tx
for each Ty T
in Tlist do
T
if (Clf Fy ) =
6 ‚àÖ ‚àß length(Tx Ty ) > Sim2Seed √ó length(Tx ) then
Clf = Clf ‚à™ Fy

*/

T

V

if ReuseAllSeeds == F ALSE length(Tx Ty ) > ReuseSeedSim √ó Sim2Seed √ó length(Tx ) then
/* This seed bicluster will not be used as base
Mark Ty as ignore

*/

Goto 17 if seed biclusters have been added
Cl is list of observations which occur in at least ObsInMinBase bases present in SelectedT list
if Cl 6= ‚àÖ then
Clusterlist = Clusterlist ‚à™ < Cl, Clf >
for each pair of biclusters in Clusterlist < Cli , Clfi >, < Clj , Clfj > do
/* Formula for cosine similarity is given in Section 4.3
if Cosine Similarity between < Cli , Clfi >, < Clj , Clfj >> ClusSim then

*/

Delete the smaller bicluster

4.3.3 Weeding out similar biclusters

note the cardinality of a set. If the similarity between
the two biclusters is found to be greater than ClusSim,
we discard the smaller ones.

The parameter ClusSim is used to weed out overlapping biclusters, as it controls the maximum amount
of similarity allowed between biclusters. For each pair
of biclusters we calculate the
T similarity between
T them
using the formula: ‚àö #(O1 ‚àöO2 )
√ó ‚àö #(F1 ‚àöF2 ) .
#(O1 )√ó

#(O2 )

#(F1 )√ó

#(F2 )

Here O1 and O2 denote the sets of observations in the
first and the second biclusters, respectively. Similarly,
F1 and F2 , respectively denote the sets of features in
the first and the second biclusters. #() is used to de-

4.4 RelDenClu from perspective of graphs
The process of finding larger biclusters from seed biclusters can also be understood in terms of graphs. By
identifying a pair of features f1 , f2 for which the Euclidean space has regions having a density higher than
marginal density, we get a probable set of observations

10

connecting these two features. By adding the constraint
that, a set of observations is said to connect features f1
and f2 only if there exists some feature f3 such that the
set of observations forms dense region in space given by
f1 , f3 and f2 , f3 , as described in Section 4.3.1 we reduce
the effect of noise. So, we get all pairs of connected features and corresponding observations. In the Section
4.3.2, each pair of connected feature f1 and f2 is considered as seed to grow a bicluster. After initializing a
bicluster using the seed we add other new features if
they are connected to the biclusters through a similar
set of observations. Thus we follow a connected component like approach in terms of features.

4.5 Choice of parameters for the proposed method
In this section, we provide a guideline to choose the
range of each parameter. The values of various parameters used for different experiments are reported in Section 6.
MinSeedSize: While choosing seed biclusters in Step
6 in Algorithm 2, this parameter is used. We make a list
of all seed biclusters longer than MinSeedSize and use
only these biclusters for further processing. Other seed
biclusters are deleted. Setting lower values will allow
the algorithm to detect biclusters with fewer observations. Setting a larger value will leave out smaller biclusters and report only large ones. If we want all small
and large biclusters, it can be set to three, as we have
defined connectedness based common dense region for
three pairs in Section 4.3.
Sim2Seed: In Section 4.3, a base seed bicluster is
compared to other seed biclusters to see if the latter can
be included in bicluster, based on the similarity between
the two. This similarity between base seed bicluster S
and other seed biclusters S 0 is judged using the number
of observations present in both S and S 0 . If this number
is greater than Sim2Seed √ó length(S), they are said to
be similar. The range (0.6, 0.95) is seen to give stable
results.
ReuseAllSeeds: Each seed bicluster can be used as
a base, for finding a bicluster. For using each seed bicluster as a base this parameter should be set to TRUE.
Otherwise, seeds previously found to have a high overlap with base seed bicluster are not used as bases in
further calculations.
ReuseSeedSim: If we ignore some seed biclusters
while choosing the bases, we set ReuseAllSeeds to FALSE.
In this case, we need to set the parameter ReuseSeedSim.
If the number of common observations between base
seed bicluster S and other seed bicluster S 0 is greater
than ReuseSeedSim √ó Sim2Seed √ó length(S), S 0 is not

Namita Jain et al.

used as base. The value of this parameter lies in (0, 1).
Setting a higher value of this parameter allows the algorithm to detect biclusters with a greater amount of
overlap. If the parameter is set to a lower value, less
overlap occurs and computation time is also less.
ObsInMinBase: If an observation is present in at
least ObsInMinBase number of dense regions forming
a bicluster, it is included in the set of observations corresponding to the bicluster. In our experiments, we have
used values in range 3 to 15.
ClusSim: If cosine similarity defined in Section 4.3
between biclusters, is found to be larger than ClusSim,
smaller bicluster is ignored. Unlike ReuseSeedSim, here
we compare biclusters in terms of both the number of
observations and the number of features. Also, it weeds
out biclusters once they are found and thus provides
finer control.
Methodology and results of the experiments is reported in Sections 5 and 6. Guidelines to choose the
parameter value were given in this section. Exact values
of parameters used in experiments have been reported
in Section 6.
5 Methodology for Experimental results
To show the effectiveness of the proposed algorithm we
have generated several simulated datasets, as described
in Section 5.2. The results for these datasets have been
reported in Section 6.1. In Section 6.2, the usefulness of
the proposed algorithm is demonstrated using real-life
datasets. Section 6.3 reports the execution time needed
for the proposed algorithm and other algorithms for
various simulated datasets. In our experiments, we have
considered is a dataset to be small if it has less than
750 observations, else it is considered to be large.
5.1 Algorithms used for comparison
The performance of the proposed algorithm is compared with seven other algorithms (which are described
below in brief) namely CLIQUE, Proclus, ITL, Subclu,
P3C, UniBic, CBSC for both simulated and real-life
datasets. For CBSC and UniBic, implementations, as
provided by the, respective authors, have been used. For
CBSC, we have used same parameter values as used in
[20]. For UniBic, the default values of parameters have
been used. For all other algorithms, implementations
available in R package named subspace by Hassani and
Hansen [18] have been used, with the parameters given
in the help page.
‚Äì CLIQUE: This algorithm by Agrawal et al [3] uses
a grid-based approach to find dense regions in low

Relative density-based biclustering

‚Äì

‚Äì

‚Äì

‚Äì

‚Äì

‚Äì

dimension space. Candidate biclusters generated are
analyzed in higher dimensions after adding features
gradually. Thus, it uses the monotonicity property
of high-density regions i.e., a set of points forming a
high-density region in high dimensional space must
form a high-density region in lower dimensions.
Proclus: This algorithm by Aggarwal et al [1], uses a
k-medoid like approach for identifying biclusters in
different subspaces. The biclusters obtained tend to
be spherical. It is known to be a robust biclustering
technique.
ITL: The objective of this algorithm, proposed by
Dhillon et al [12], is to minimize within-cluster divergence and maximize between-cluster divergence
for features.
Subclu: Subclu by Kailing et al [21] is an example of
a procedure where density connected regions form a
bicluster. A region is said to be dense if it is composed of connected open discs of radius r each having at least  observations. Thus density becomes
the effective criterion function, and its threshold
value is decided by  and r. Dense regions found
in smaller subspaces are combined using the apriori
approach, and density techniques are used again to
obtain actual biclusters from the set of candidate
biclusters.
P3C: This is also a grid-based method proposed
by Moise et al [27], which identifies dense cells using a statistical approach. Once probable biclusters are generated in lower dimension space, the
expectation-maximization algorithm is used. Initially,
the observations are assigned using a fuzzy membership criterion.
UniBic: This algorithm by Wang et al [35], finds
trend preserving biclusters. The seed biclusters are
identified using the Longest Common Subsequence
framework. Rows are added to these seed biclusters
to obtain final biclusters.
CBSC: CBSC by Jain and Murthy [20] is a method
where density estimation is done in two-dimensional
space using windows of radius calculated using the
length of the Minimal Spanning tree. This procedure implicitly identifies a sharp change in density
in two-dimensional space. Since the radius is calculated separately for each pair of dimensions, the
procedure takes care of specific density variations in
each two-dimensional subspace. Afterwards, results
from different two-dimensional spaces are combined.

CLIQUE, P3C, Subclu have been chosen for comparison as they are density-based methods. Proclus has
been chosen as it is robust in the presence of noise.
UniBic is a recent algorithm, which provides good results in comparison to many existing methods like FABIA

11

[19] and ISA [4]. ITL finds biclusters based on feature similarity using mutual information. CBSC also
searches for features leading to relation-based biclusters using a density-based approach.
5.2 Generating simulated datasets
To check whether the proposed biclustering algorithm
can detect biclusters based on linear and non-linear relationships, we have experimented with several simulated datasets. We also want to see the effect of applying different data transforms on the performance of the
proposed algorithm. This section outlines our method
of generating datasets for this purpose. Each type of
dataset corresponds to one row in Table 2, which reports the mean and standard deviation of accuracy obtained by different algorithms (For each type of dataset
ten dataset instances are used). The method for generating the datasets for each row is discussed in this
section. The datasets for first and second rows require
several functions, so these functions are reported in Table 1. Rest of the datasets are generated using a single
function.
In Table 2, the first row shows the results on datasets
generated in the following manner. A random matrix of
size 1000√ó20 is generated. Now we will replace 500√ó10
submatrix of data in a way so that the ith column of
this submatrix is given by hi (x), where x is the value in
first column and the definitions of hi for i = 1, 2, ¬∑ ¬∑ ¬∑ , 10
are given in Table 1. The row and column numbers that
form the bicluster submatrix are chosen randomly. Thus
we embed a bicluster of size 500 √ó 10 in the data. Note
that for some columns, the range in which the bicluster
elements fall is different from the range of remaining
data. Thus the background for the biclusters is highly
variable. One such example was presented in Figure 1
in Section 3.2.3.
Datasets in used for results shown in the second row
of Table 2 are generated using a set of functions, which
are modifications of the functions used in the first row.
The modifications are made so that data in bicluster
lies in the same range as the rest of the data. The modified functions have been reported in the second column
of Table 1. It may be noted that the function x2 occurs
both as h3 and h8 in the second column and x3 occurs both as h7 and h10 , amounting to the repetition
of a column in the bicluster. However, we have retained
the repeated columns to retain the similar structure of
bicluster as in datasets for the first row.
The third row presents the results on datasets of
size 1000 √ó 20 drawn from uniform distribution. It has
a bicluster of size 500 √ó 10 generated using functions
h1 (x) = I(x) = x, where I is the identity function

12

and hi (x) = ai ‚àó x with i = 2, 3 ¬∑ ¬∑ ¬∑ , 10. ai with i =
2, 3, ¬∑ ¬∑ ¬∑ , 10 are different random values lying in interval (0, 1). This is our base data for Uniform distribution
datasets. Different transformations have been applied
to this data for analysing properties of biclustering algorithms and corresponding results have been reported
in following rows.
The fourth row contains results for datasets obtained by scaling each column of data generated for
the third row with a random number lying in the interval (0, 1). The fifth row contains results for datasets
obtained by adding a random number lying in (0, 1) to
each column of the data generated for the third row.
These are used to analyze the scaling and translation
properties of the algorithms. The sixth row contains results for datasets obtained by linear transform to each
column of data generated for the third row. In a way,
this is a combination of scaling and translation. Two
random numbers r1 and r2 are generated for each column and the transform r1 x + r2 is applied. Scaling,
translation and linear transforms are special cases of
distance preserving transforms.
The seventh row and the eighth row contain results for datasets obtained by applying square transform f (x) = x2 and exponential transform f (x) =
exp(x), to each observation and each feature of data
generated for the third row. Since data used here lies in
the range (0, 1) both of these are monotone transforms,
but not necessarily distance preserving.
The ninth row contains results for datasets obtained
by duplicating each observation of datasets used for reporting the results in the third row. Thus these datasets
contain 2000 observations. The tenth row contains results for datasets obtained by duplicating each observation of bicluster in datasets generated for the third
row. Thus these datasets contain 1500 observations.
The eleventh row contains results for datasets obtained
by adding a random number in range (0,0.1) to each
element of data matrices. The random noise is drawn
from a uniform distribution. The robustness of the algorithms regarding noise is checked through this. The
twelfth row contains results for datasets obtained by
randomly shuffling rows and columns of the data generated for the third row.
The thirteenth row presents the results on datasets
of size 1000√ó20 drawn from Gaussian distribution. The
bicluster submatrix of size 500 √ó 10 is generated in the
same way as the third row. The fourteenth row contains
results for datasets obtained by adding noise to each
element of the data matrix generated in the thirteenth
row. Noise is generated using random numbers drawn
from Gaussian distribution with the standard deviation

Namita Jain et al.

0.1. Thus we test the robustness of the algorithms for
Gaussian data.
The fifteenth and the sixteenth rows present the results on data of size 1000 √ó 20 drawn from uniform
distribution. It has 2 biclusters of sizes 500 √ó 10 and
300 √ó 8. The fifteenth and the sixteenth rows give the
results for the first and second biclusters, respectively.
These biclusters are generated by adding a randomly
chosen value to its elements. Thus elements of biclusters lie in a different range from the elements not lying
in the bicluster. These biclusters have an overlapping
area of 300 √ó 3. This is done to see if the algorithm is
capable of finding overlapping biclusters.

5.3 Evaluating the performance on simulated datasets
In this section, we discuss how the performance of proposed and seven other algorithms are evaluated. The
datasets used for comparison are generated using the
procedure described in Section 5.2. As seen in Table 2,
there are thirteen type of datasets i.e., ‚ÄúBase‚Äù, ‚ÄúScaled‚Äù,
‚ÄúTranslated‚Äù etc. The rows ‚ÄúOverlap 1‚Äù and ‚ÄúOverlap
2‚Äù correspond to two overlapping biclusters in the same
dataset. For data given in an N √ó M matrix, accuracy
is calculated as follows: generate a matrix mB such that
mB (i, j) = 1 if observation i and feature j is a member
of actual bicluster in data. Similarly, generate a matrix
mE for estimated bicluster. Accuracy is given by the
ratio where mB matches mE to the size of the data matrix N √ó M . For each bicluster present in data, the best
match is reported for each algorithm. Significance of the
results is analyzed using pairwise right-tailed t ‚àí test,
and corresponding p ‚àí values are also reported.

5.4 Real-life datasets used for comparison
In this section we briefly describe the real-life datasets
used for comparison in Section 6.2.
5.4.1 Real-life datasets used for discovering biclusters
We have used three datasets from UCI ML repository
to discover classes existing in the dataset as biclusters
(reported in Section 6.2.1). These datasets are given
below.
‚Äì Magic: It is ‚ÄòMAGIC Gamma Telescope Data Set‚Äô
of size 19020 √ó 10 and contains observations representing gamma and hadron particles.
‚Äì Cancer: It is known as ‚ÄòBreast Cancer Wisconsin
(Original) Data Set‚Äô of size 683√ó9. The observations
represent the conditions ‚Äôbenign‚Äô and ‚Äômalignant‚Äô.

Relative density-based biclustering

13

Non-Linear 1
I (x) = x
sin(x)

Non-Linear 2
I (x) = x
sin(x)

x2
x10
sin(œÄx)
sin(2œÄx)
x3
4x2
sin(4œÄx)
4x3

x2
x10
0.5 sin(œÄx)
0.5 sin(2œÄx) + 0.5
x3
x2
0.5 sin(4œÄx) + 0.5
x3

‚Äì Ionosphere: Known as the ‚ÄòIonosphere Data Set‚Äô it
has size 351 √ó 34. The observations represent good
and bad radar signals.
5.4.2 Real-life datasets used for classification using
additional features generated by biclustering methods
We have also used biclustering algorithms to improve
the classification accuracy of three datasets listed below. These results are reported in Section 6.2.2.

6.1 Results on simulated datasets

Fig. 3: Accuracy obtained using various algorithms for
Non-Linear datasets
1

Proposed

hi
h1 (x)
h2 (x)
h3 (x)
h4 (x)
h5 (x)
h6 (x)
h7 (x)
h8 (x)
h9 (x)
h10 (x)

known classes with greater accuracy. Note that, the label information has not been used. These results have
been reported in Section 6.2.1. As an application to
supervised learning, the biclustering algorithms have
been used to generate new features for three UCI-ML
datasets (as described in Section 5.4.2). From the results (Table 9) it is found that RelDenClu provides
greater improvement in classification accuracy compared
to seven other methods for all three datasets thereby
leading to better identification of credit card defaulters, improved understanding of chemical bioconcentration in tissues, and better image segmentation results.
These results have been reported in Section 6.2.2.

Proposed

Table 1: Functions used to generate datasets of type
Non-Linear 1 and Non-Linear 2.

0.9

Unibic

CBSC

Subclu
P3C

CLIQUE
Proclus

ITL

Unibic

ITL

CBSC

Subclu
P3C

CLIQUE
Proclus

‚Äì ‚ÄòCredit Card: The Default of credit card clients Data
0.8
Set‚Äô is of size 30000 √ó 23. It contains observations
representing credit card defaulters and customers
0.7
who make regular payments.
‚Äì Image Segmentation: The ‚ÄòImage Segmentation Dataset‚Äô
0.6
is of size 2310 √ó 19. It contains data from seven different classes. Each instance represents 3 √ó 3 pixel
0.5
region, drawn randomly from a database of 7 outdoor images.
0.4
Non-Linear 1
Non-Linear 2
‚Äì Bioconcentration: It is named ‚ÄòQSAR Bioconcentration classes dataset Data Set‚Äô and has a size of
779 √ó 14. It is a dataset of chemicals. The chemFrom Table 2 we see that the proposed method proicals belong to thee classes depending on whether
vides better accuracy for all simulated datasets. The
a chemical: (1) is mainly stored within lipid tissues,
reported accuracy in each row is the average accuracy
(2) has additional storage sites (e.g. proteins), or (3)
for ten instances of data generated using a particular
is eliminated.
method. The Standard deviation of accuracy is also reported (denoted as ‚ÄòDeviation‚Äô). To understand the significance of results in Table 2, we have also reported
6 Results
the results of paired t ‚àí test in Table 3. The proposed
In this section we report the results of experiments with
method is compared with each of the seven other methsimulated and real-life datasets. Simulated datasets have ods and corresponding p‚àívalues have been reported in
been used to verify the performance of the proposed
Table 3. Here we also notice that the proposed method
method on datasets with varying properties. Paired t ‚àí
provides significantly better accuracy (p‚àívalue < 0.01)
test has been used for this comparison. Three UCI-ML
as compared to other methods for all types of simulated
datasets (as described in Section 5.4.1), each containdatasets used in the experiments.
ing two classes, have been used to compare the perIt is seen from Table 2, that the proposed algoformance of the proposed and other biclustering alrithm provides much better results in terms of accugorithms. RelDenClu finds biclusters corresponding to
racy for two types of non-linear datasets. Unlike CBSC

14

Namita Jain et al.

Table 2: Accuracy on Simulated datasets (Mean and Standard Deviation for 10 datasets each type are reported)
Dataset
Non-Linear 1
Non-Linear 2
Base
Scaled
Translated
Linear transform
Square
Exponential
Point proportion
Cluster proportion
Noisy uniform
Permutations
Normal
Noisy normal
Overlap 1
Overlap 2

Accuracy
Mean
Deviation
Mean
Deviation
Mean
Deviation
Mean
Deviation
Mean
Deviation
Mean
Deviation
Mean
Deviation
Mean
Deviation
Mean
Deviation
Mean
Deviation
Mean
Deviation
Mean
Deviation
Mean
Deviation
Mean
Deviation
Mean
Deviation
Mean
Deviation

CLIQUE
0.784
0.002
0.782
0.002
0.702
0.247
0.702
0.247
0.702
0.247
0.702
0.247
0.827
0.018
0.793
0.026
0.702
0.247
0.643
0.227
0.696
0.245
0.695
0.245
0.799
0.011
0.787
0.015
0.779
0.031
0.856
0.025

Proclus
0.781
0.014
0.792
0.007
0.785
0.013
0.768
0.015
0.803
0.023
0.77
0.028
0.805
0.021
0.791
0.016
0.8
0.024
0.711
0.024
0.792
0.01
0.785
0.015
0.783
0.017
0.772
0.011
0.744
0.005
0.836
0.008

ITL
0.587
0.024
0.445
0.119
0.701
0.355
0.701
0.355
0.701
0.355
0.701
0.355
0.75
0.021
0.729
0.023
0.701
0.355
0.586
0.029
0.71
0.032
0.6
0.189
0.504
0.013
0.512
0.011
0.847
0.005
0.759
0.086

Methods used
Subclu
P3C
0.785
0.765
0.017
0.004
0.793
0.764
0.016
0.003
0.794
0.781
0.023
0.04
0.759
0.782
0.016
0.04
0.788
0.779
0.019
0.041
0.765
0.779
0.025
0.041
0.8
0.771
0.024
0.009
0.811
0.848
0.025
0.055
0.802
0.787
0.03
0.022
0.724
0.802
0.02
0.092
0.795
0.772
0.017
0.032
0.798
0.783
0.022
0.015
0.783
0.767
0.01
0.017
0.78
0.758
0.015
0.047
0.754
0.831
0.014
0.007
0.833
0.856
0.006
0.021

UniBic
0.610
0.142
0.694
0.107
0.839
0.034
0.684
0.211
0.582
0.159
0.839
0.034
0.847
0.038
0.861
0.046
0.804
0.046
0.8
0.078
0.592
0.124
0.839
0.034
0.898
0.025
0.522
0.098
0.446
0.064
0.534
0.054

CBSC
0.669
0.049
0.752
0.022
0.834
0.063
0.834
0.063
0.834
0.063
0.834
0.063
0.696
0.15
0.866
0.062
0.834
0.063
0.937
0.02
0.851
0.049
0.834
0.063
0.805
0.075
0.769
0.074
0.85
0.087
0.818
0.098

Proposed
0.913
0.022
0.883
0.006
0.989
0.006
0.989
0.006
0.989
0.006
0.989
0.006
0.981
0.005
0.978
0.020
0.992
0.006
0.996
0.003
0.939
0.029
0.989
0.006
0.991
0.003
0.901
0.031
0.963
0.062
0.975
0.033

and other algorithms, the proposed method finds the
biclusters accurately in-spite of the highly variable distribution of background observations as seen from the
row ‚ÄúNon-Linear 1‚Äù. Its accuracy for datasets containing bicluster based on highly non-monotonous data is
also higher in comparison with other algorithms as seen
from row ‚ÄúNon-Linear 2‚Äù. The performance (in terms
of accuracy) of different algorithms on these datasets is
presented in Figure 3 as bar graphs.

0.8, ReuseAllSeeds = FALSE, ReuseSeedSim = 0.5,
MinSeedSize = 100, ObsInMinBase = 3, ClusSim = 1.
Transformation norm2 () given in Section 4.1 is applied
to datasets ‚ÄôNormal‚Äô and ‚ÄôNoisy normal‚Äô, while transformation norm1 () given in Section 4.1 is applied to all
other datasets. Large dataset method has been used for
all simulated datasets, as they have thousand or more
observations.
The results for simulated datasets suggest some properties
of the proposed method, which are discussed in
From Figure 4 and Table 2, we find that the profollowing section.
posed method has better accuracy for ‚ÄúBase‚Äù dataset
and all its transforms (‚ÄúScaled‚Äù, ..,‚ÄúPermutations‚Äù datasets).
The proposed dataset also performs better than other
algorithms on ‚ÄúNormal‚Äù and ‚ÄúNoisy Normal‚Äù data as
6.1.1 A discussion on properties of RelDenClu as seen
seen from Figure 5. It works better than other algofrom experiments on Simulated datasets
rithms on datasets containing two overlapping clusters
as seen in rows ‚ÄúOverlap1‚Äù and ‚ÄúOverlap2‚Äù of Table 2
It is already seen that the proposed algorithm can find
and also in Figure 6.
relation-based biclusters with better accuracy as comResults for simulated datasets have been found using the following values for parameters: Sim2Seed =

pared to other algorithms for the datasets as it is seen
from Table 2 and Figures 3-6. In this section, we dis-

Relative density-based biclustering

15

Table 3: Paired t ‚àí test statistics and p ‚àí values on datasets to compare the performance of the proposed method
with other methods
Dataset
Non-Linear 1
Non-Linear 2
Base
Scaled
Translated
Linear transform
Square
Exponential
Point proportion
Cluster proportion
Noisy uniform
Permutations
Normal
Noisy normal
Overlap 1
Overlap 2

Statistics
t
p ‚àí value

t
p ‚àí value

t
p ‚àí value

t
p ‚àí value

t
p ‚àí value

t
p ‚àí value

t
p ‚àí value

t
p ‚àí value

t
p ‚àí value

t
p ‚àí value

t
p ‚àí value

t
p ‚àí value

t
p ‚àí value

t
p ‚àí value

t
p ‚àí value1

t
p ‚àí value

CLIQUE
18.71
8.15e-09
40.78
7.99e-12
3.71
2.43e-03
3.71
2.43e-03
3.71
2.43e-03
3.71
2.43e-03
27.79
2.46e-10
16.68
2.24e-08
3.74
2.32e-03
4.91
4.15e-04
3.14
5.97e-03
3.84
1.99e-03
50.63
1.15e-12
10.37
1.32e-06
9.30
3.27e-06
11.66
4.91e-07

Methods used for comparing with
Proclus
ITL
Subclu
18.71
30.90
13.85
8.17e-09 9.53e-11 1.13e-07
24.34
11.47
15.64
7.99e-10 5.67e-07 3.93e-08
47.12
25.65
24.31
2.19e-12 5.02e-10 8.07e-10
51.05
25.65
41.87
1.07e-12 5.02e-10 6.31e-12
24.13
25.65
30.32
8.63e-10 5.02e-10 1.13e-10
23.78
25.65
26.74
9.83e-10 5.02e-10 3.46e-10
31.54
34.39
26.84
7.95e-11 3.67e-11 3.35e-10
21.10
22.77
15.16
2.83e-09 1.44e-09 5.14e-08
23.64
26.23
19.28
1.03e-09 4.12e-10 6.27e-09
37.66
46.93
45.52
1.63e-11 2.27e-12 2.98e-12
13.47
14.19
11.52
1.43e-07 9.11e-08 5.44e-07
35.37
6.53
25.11
2.85e-11 5.37e-05 6.06e-10
44.11
110.85
64.68
3.95e-12 1.00e-15 1.27e-13
14.11
39.07
9.72
9.61e-08 1.17e-11 2.26e-06
10.65
5.85
11.28
1.06e-06 1.22e-04 6.53e-07
11.70
8.07
13.96
4.76e-07 1.04e-05 1.05e-07

cuss some of the observed properties of the proposed
algorithm.
The proposed algorithm is invariant to scaling, translation and linear transforms because neither bin length
nor estimated densities are affected by these transforms.
Theoretically, the procedure is not invariant to arbitrary order-preserving transforms, as such transforms
may result in changed density. We compare the accuracy for ‚ÄúBase‚Äù dataset and its various transforms by
applying two-tailed t-test. We find that there is no significant difference (p‚àívalue threshold is 0.01) in performance of RelDenClu for ‚ÄúScaled‚Äù, ‚ÄúTranslated‚Äù, ‚ÄúLinear Transform‚Äù,‚ÄúSquare‚Äù, ‚ÄúExponential‚Äù, ‚ÄúPoint Proportion‚Äù and ‚ÄúPermutations‚Äù datasets.
As compared to ‚ÄúBase‚Äù dataset the performance
has improved for ‚ÄúCluster Proportion‚Äù dataset because
repetition of observations in the bicluster increases the
probability of its identification as high density region.
Both for ‚ÄúBase‚Äù and ‚ÄúNormal‚Äù datasets adding noise

the proposed method
P3C
UniBic
19.81
6.53
4.94e-09 5.36e-05
59.55
5.55
2.68e-13 1.78e-04
16.74
13.91
2.17e-08 1.09e-07
16.72
4.55
2.19e-08 6.94e-04
16.66
8.09
2.26e-08 1.02e-05
16.46
13.91
2.51e-08 1.09e-07
56.71
10.64
4.15e-13 1.07e-06
7.62
6.23
1.62e-05 7.69e-05
33.80
13.80
4.29e-11 1.16e-07
6.67
8.13
4.60e-05 9.76e-06
11.37
8.06
6.09e-07 1.04e-05
34.30
13.91
3.76e-11 1.09e-07
38.73
13.07
1.27e-11 1.85e-07
8.87
10.11
4.79e-06 1.63e-06
6.45
17.52
5.91e-05 1.46e-08
9.24
24.21
3.44e-06 8.39e-10

CBSC
13.26
1.64e-07
17.11
1.79e-08
7.76
1.42e-05
7.76
1.42e-05
7.76
1.42e-05
7.76
1.42e-05
6.03
9.78e-05
5.62
1.63e-04
8.04
1.06e-05
8.91
4.64e-06
4.59
6.55e-04
7.76
1.42e-05
7.82
1.32e-05
5.54
1.80e-04
2.83
9.79e-03
4.43
8.22e-04

causes the accuracy to reduce i.e., the accuracies for
‚ÄúNoisy uniform‚Äù and ‚ÄúNoisy Normal‚Äù datasets are significantly lower than ‚ÄúBase‚Äù and ‚ÄúNormal‚Äù datasets,
respectively. However, as compared to other algorithms
RelDenClu still has better accuracy.
6.2 Comparisons on real-life datasets
This section presents the performance of the proposed
algorithm and other algorithms used for comparison,
on three datasets obtained from the UCI ML repository
[14]. The results for datasets named Magic, Ionosphere
and Breast Cancer are reported in Tables 4, 5 and 6.
In this section we also present an application to Supervised learning, where bicluster membership values
have been used as new features to improve the performance of Naive Bayes classifier for UCI ML datasets
named Credit Card, Image Segmentation and Bioconcentration. The results are reported in Table 9.

16

Namita Jain et al.

Fig. 5: Accuracy obtained using various algorithms for
Normal and Noisy Normal datasets

0.8

CBSC

Proposed
CLIQUE

Subclu
P3C

Unibic
CBSC

Subclu
P3C

Base

CLIQUE
Proclus

0.9

Subclu
P3C

CBSC

Proclus
ITL

Proposed

Unibic

1
CLIQUE

CLIQUE
Proclus

1.05

1

0.95

0.9

0.85

0.8

0.75

0.7

0.65

0.6

0.55

1.1

Proposed

Fig. 4: Accuracy obtained using various algorithms for
Base and transformed datasets

Scaled

Proclus
ITL
Subclu
P3C

0.7

Unibic
CBSC
Proposed

ITL

Subclu
P3C

ITL

Unibic
CBSC
Proposed

0.5

Linear Trasnform

CLIQUE
Proclus

0.4

ITL
Subclu
P3C

Normal

Noisy normal

Unibic
CBSC
Proposed

Square

CLIQUE
Proclus
ITL
Subclu
P3C
Unibic
CBSC
Proposed

Exponential

CLIQUE
Proclus

Fig. 6: Accuracy obtained using various algorithms for
dataset containing overlapping biclusters

ITL
P3C
Unibic
CBSC
Proposed

1
CLIQUE
Proclus

CLIQUE

0.8

Proclus
ITL
Subclu
P3C
Unibic

CBSC

Proposed

ITL

CLIQUE
Proclus
ITL
Subclu
P3C

0.9

CBSC

Subclu
P3C

CBSC

Subclu
P3C
Unibic

CLIQUE
Proclus

ITL

CBSC
Proposed

0.7

CLIQUE
Proclus
ITL
Subclu

Unibic

0.6

P3C
Unibic
CBSC

0.5

CLIQUE
Proclus
ITL
Subclu
P3C

Overlap 2

Proposed

CLIQUE

1

CBSC
Proposed

Fig. 7: Accuracy of various biclustering algorithms (calculated using Equation 8)

CBSC
Subclu

CLIQUE
Proclus
P3C

Unibic

Unibic

0.7

Subclu
P3C

0.8

Proclus

0.9

ITL

0.6
ITL

The parameter values used for real-life datasets are
given in Table 7. Since ReuseAllSeeds is set to TRUE,
the value of ReuseSeedSim is not important. For Magic
dataset, we use the large dataset method to find related dense sets while for Ionosphere and Breast Cancer datasets results are reported using the small dataset
procedure. For Breast Cancer dataset with 683 observations, we have also performed experiments with the
large dataset method and obtained accuracy 0.9414 and
G-scores of 2 classes 0.95 and 0.99. Thus, for datasets
of this size, either method gives good results.
To see whether bicluster detected by the proposed
algorithm corresponds to the meaningful structure in
data, we check if one of the biclusters detected cor-

Overlap 1

CBSC
Proposed

6.2.1 Comparison as an Unsupervised learning method

0.4

P3C

Proposed

Unibic

Unibic
CBSC

CLIQUE

Permutations

Unibic

Proposed

Proclus
ITL
Subclu

Point Proportion Cluster Proportion Noisy uniform

Proposed

Subclu

Proposed

Translated

Proclus

Unibic

0.6

CLIQUE
ITL

0.5
Magic

Cancer

Ionosphere

Relative density-based biclustering

17

Table 4: Results for Magic dataset
Method
Proposed

CBSC[20]

UniBic [35]

P3C [27]

Subclu [21]

ITL [12]

Proclus [1]

CLIQUE [3]

Index Name
Accuracy
Precision
Recall
G-score
Accuracy
Precision
Recall
G-score
Accuracy
Precision
Recall
G-score
Accuracy
Precision
Recall
G-score
Accuracy
Precision
Recall
G-score
Accuracy
Precision
Recall
G-score
Accuracy
Precision
Recall
G-score
Accuracy
Precision
Recall
G-score

Table 5: Results for Breast Cancer dataset
Method

Index Value
0.7374
0.87
0.59
0.70
0.81
0.78
0.69
0.7362
0.85
0.59
0.71
0.78
0.78
0.68
0.6483
0.65
*
1
0
0.81
*
0.7052
0.69
0.92
0.99
0.17
0.82
0.40
0.6104
0.66
0.39
0.84
0.18
0.74
0.26
0.6154
0.47
0.72
0.56
0.65
0.51
0.68
0.6261
0.66
0.41
0.89
0.14
0.76
0.24
0.6845
0.70
0.60
0.88
0.30
0.79
0.43

Proposed

CBSC [20]

UniBic [35]

P3C [27]

Subclu [21]

ITL [12]

Proclus [1]

CLIQUE [3]

responds to one of the known classes in data or not.
Each of the datasets used contains two classes. For each
observation, the membership value corresponding to a
bicluster is 1 if it is included in the bicluster and 0 otherwise. Since in each dataset observations belong to two
classes, the class label can also be named as 0 and 1.
For each bicluster, the number of matches between bicluster membership and the class label is calculated as
below:

Accuracy =

max(XN OR(Biclu mem(O),Class(O)),XOR(Biclu mem(O),Class(O))
length(O)

(8)

where Biclu mem denotes the bicluster membership, O denotes the set of observations in entire dataset
and Class denotes the class label. XN OR and XOR
are the logical operators and length() denotes the size
of O. For the match with the best accuracy, precision,
recall [15] and G-score are also reported (G-score is the
geometric mean of precision and recall) for both the
classes. It is seen that the proposed method yields the
best accuracy for all the three datasets used for investigation. Though other methods attain better precision

Index Name
Accuracy
Precision
Recall
G-score
Accuracy
Precision
Recall
G-score
Accuracy
Precision
Recall
G-score
Accuracy
Precision
Recall
G-score
Accuracy
Precision
Recall
G-score
Accuracy
Precision
Recall
G-score
Accuracy
Precision
Recall
G-score
Accuracy
Precision
Recall
G-score

Index Value
0.9561
0.97
0.93
0.96
0.95
0.97
0.94
0.9444
0.92
1.00
1.00
0.84
0.96
0.92
0.6501
0.65
*
1
0
0.81
*
0.7965
0.76
0.98
0.99
0.43
0.87
0.65
0.8052
0.78
0.90
0.97
0.50
0.87
0.67
0.7013
0.62
0.72
0.37
0.88
0.48
0.80
0.8023
0.78
0.93
0.98
0.47
0.87
0.66
0.9209
0.94
0.88
0.93
0.90
0.94
0.89

or recall in some cases, the proposed method attains
better G-score in all the cases. For better visualization,
accuracies obtained by different methods are presented
in Figure 7 as bar graphs.

6.2.2 Usefulness of proposed algorithm to aid
supervised learning
As another application, we have used RelDenClu and
other biclustering methods to generate new features
that are further used to improve classification performance on three UCI-ML datasets (described in Section
5.4.2). We can see that these datasets belong to diverse fields and pose practically important problems.
To demonstrate the effectiveness of the proposed algorithm for real-life datasets, the results are compared
with those obtained using seven other state-of-the-art
techniques as used in earlier investigations. We try to
improve the classification accuracy for the datasets using biclustering algorithms. For each bicluster generated by an algorithm, the corresponding membership

18

Namita Jain et al.

P3C [27]

Subclu [21]

ITL [12]

Proclus [1]

CLIQUE [3]

0.6

Proposed

Proposed

Original
CLIQUE

ITL
Subclu
P3C
Unibic
CBSC

0.7

Proclus

Proposed

0.8

Original
CLIQUE
Proclus
ITL
Subclu
P3C
Unibic
CBSC

UniBic [35]

0.9

Proclus

CBSC [20]

1

Subclu
P3C
Unibic
CBSC

Proposed

Index Value
0.9174
0.92
0.91
0.95
0.86
0.94
0.88
0.8519
0.89
0.78
0.87
0.82
0.88
0.80
0.6410
0.64
*
1
0
0.80
*
0.6000
0.66
0.76
0.99
0.08
0.79
0.25
0.6923
0.68
0.87
0.98
0.17
0.82
0.38
0.5560
0.34
0.32
0.29
0.50
0.32
0.65
0.7721
0.74
1.00
1.00
0.30
0.85
0.55
0.7493
0.72
1.00
0.1.00 0.30
0.85
0.55

Original

Index Name
Accuracy
Precision
Recall
G-score
Accuracy
Precision
Recall
G-score
Accuracy
Precision
Recall
G-score
Accuracy
Precision
Recall
G-score
Accuracy
Precision
Recall
G-score
Accuracy
Precision
Recall
G-score
Accuracy
Precision
Recall
G-score
Accuracy
Precision
Recall
G-score

ITL

Method

Fig. 8: Improvement in classification accuracy using
various biclustering methods

CLIQUE

Table 6: Results for Ionosphere dataset

0.5
Credit card

Image Segmentation

Bioconcentration

value is taken as a new feature. Naive Bayes classifier
is used here.
We find that the features generated by RelDenclu
provide a greater improvement in accuracy as compared
to other algorithms for the three datasets. The large
dataset method has been used for these datasets. The
results are reported in Table 9 and the parameters used
have been reported in Table 8. The accuracies obtained
using Naive Bayes classifier for original and enhanced
datasets are depicted in Figure 8.

6.3 Execution time and performance on large datasets
Table 7: Details of parameters used for Unsupervised
learning on different real-life datasets
```
``` Dataset
```
Details
`
Size of dataset
Normalizing function
Sim2Seed
ReuseAllSeeds
ObsInMinBase
MinSeedSize = 100
ClusSim

Magic

Cancer

Ionosphere

19020 √ó 10
norm1 ()
0.6

683 √ó 9
norm1 ()
0.6

351 √ó 34
norm2 ()
0.6

TRUE

TRUE

TRUE

5
500
1

3
100
1

15
100
1

Table 8: Details of parameters used for Classification
on
``different real-life datasets
``` Dataset
```
Credit card
Details
``
Size of dataset
30000 √ó 24
Normalizing function
norm1 ()
Sim2Seed
0.6
ReuseAllSeeds
ReuseSeedSim
ObsInMinBase
MinSeedSize = 100
ClusSim

Image Segmentation

Bioconcentration

2310 √ó 19
norm1 ()
0.6

779 √ó 14
norm1 ()
0.9

FALSE

FALSE

TRUE

0.9
7
1000
1

0.8
7
100
0.5

NA
7
100
0.6

We have compared the execution time required for the
proposed algorithm with that needed to execute the
other algorithms discussed in Section 5.1 and reported
the results in Table 10. The datasets used here are the
same as those used in Table 2. As seen from Table 10,
the execution time for the proposed method is high
compared to several density-based methods like Subclu,
Proclus, CLIQUE. However, its performance is significantly better as seen in Table 2. The computer used for
all experiments in this article has Intel Core i7 CPU
and 8 GB memory.
In Table 11 we are reporting execution times and accuracy for a large dataset of size 20000 √ó 100. This data
has been generated in a similar way as ‚ÄúBase‚Äù dataset
mentioned in Section 5.2. Here we generate bicluster
of size 10000 √ó 30 using functions h1 (x) = I(x) = x,
where I is the identity function and hi (x) = ai ‚àó x for
i = 2, 3, ¬∑ ¬∑ ¬∑ , 30. The term ai for i = 2, 3, ¬∑ ¬∑ ¬∑ , 30 are
different random values lying in interval (0, 1). In this
table, execution time of P3C is not reported as R package for this method could not execute the algorithm
for the given dataset, on given system configuration.

Relative density-based biclustering

19

Table 9: Classification accuracy for datasets enhanced using various biclustering methods
Dataset
Credit card
Image Segmentation
Bioconcentration

Accuracy
Mean
Deviation
Mean
Deviation
Mean
Deviation

Original
0.696
0.028
0.795
0.015
0.601
0.048

CLIQUE
0.551
0.006
0.818
0.030
0.577
0.036

Proclus
0.722
0.016
0.735
0.013
0.611
0.063

Methods used
ITL
Subclu
0.659
0.689
0.024
0.015
0.796
0.794
0.023
0.024
0.605
0.606
0.040
0.057

Table 10: Execution time (in seconds) for datasets
Dataset
CLIQUE Proclus
ITL
Subclu
Non-Linear 1
0.0105
0.0595
9.0962
0.0642
Non-Linear 2
0.0065
0.0545
9.2478
0.0515
Base
0.105
0.052
3.171
0.056
Scaled
0.015
0.055
3.302
0.043
Translated
0.015
0.049
3.498
0.05
Linear transform
0.018
0.055
3.276
0.055
Square
0.04
0.057
3.544
0.048
Exp
0.02
0.051
3.485
0.054
Point proportion
0.033
0.12
9.994
0.12
Cluster proportion
0.021
0.088
6.519
0.082
Noisy uniform
0.014
0.056
2.94
0.049
Permutations
0.013
0.063
3.272
0.07
Normal
0.071
0.054
1.187
0.071
Normal noisy
0.044
0.052
1.259
0.061
Overlap cluster
1.041
0.061
2.089
0.06

P3C
0.702
0.030
0.761
0.022
0.578
0.045

UniBic
0.679
0.011
0.740
0.030
0.580
0.059

CBSC
0.670
0.021
0.693
0.023
0.606
0.061

corresponding to Table 2
P3C
UniBic CBSC
0.3587
0.583
37.464
0.6502
0.612
31.82
0.513
0.583
42.416
0.955
0.612
51.4
0.952
0.649
43.964
0.433
0.673
41.313
3.237
0.56
37.105
0.224
0.573
43.985
1.083
2.596
91.973
0.848
1.436
83.796
0.458
0.621
42.304
0.588
0.583
41.654
18.84
0.531
66.659
9.75
0.577
63.821
9.36
0.608
113.15

Proposed
0.766
0.004
0.860
0.016
0.818
0.046

Proposed
2.7663
1.0136
5.204
5.25
5.3
5.532
2.733
2.511
36.437
29.625
13.312
5.468
15.896
18.27
47.19

Table 11: Accuracy and execution time in seconds for dataset of size 20000√ó100 having a bicluster of size 10000√ó30

Method
Accuracy
Execution time

CLIQUE
0.868
3.4

Proclus
0.862
22.814

ITL
0.612
10106

It is noticed that, the proposed algorithm has a lower
execution time as compared to CBSC.
The time complexity of ITL has been reported by
Dhillon et al [12]. The time complexity of other methods used for comparison has been reported by Jain and
Murthy [20]. We are reporting the time complexity of
the proposed method, where N and M , respectively
are the number of observations and features in a given
dataset and mtotal = m1 +m2 +, ¬∑ ¬∑ ¬∑+mk where mi gives
the number of features in the ith bicluster with a total of
k biclusters present. Note that mtotal corresponds to the
total number of columns in the complete set of biclusters i.e., before removing similar biclusters using cosine
measure, and without leaving out seed biclusters which
are similar. The proposed method has time complexity
N 2 M +N M 3 +N m2total and N M +N M 3 +N m2total for
small and large datasets, respectively. Theoretically, the
time complexity of CBSC and the proposed method are
similar. However, the execution time for the proposed

Subclu
0.861
19.17

UniBic
0.735
678.972

CBSC
0.903
12066.662

Proposed
0.9796
1396.400

method is found to be much smaller as compared to
CBSC for the high dimensional dataset. This happens
because the proposed method does not calculate Minimal Spanning tree for each pair of dimensions, which
are needed in CBSC.

7 Applicability of proposed method for
COVID-19
Toward the end of 2019, a disease named COVID-19
emerged as a highly contagious disease and has resulted
in a pandemic of unprecedented scale. However, it is
seen that some countries are affected by this disease to
a greater extent than others. Since this is a recent phenomenon, very less is known about the factors affecting
the spread of the disease. In this section, we attempt to
study the relation between World development indicators and the number of COVID-19 cases for countries
or states of the world.

20

Namita Jain et al.

Table 12: Features from Human Development Index included in bicluster corresponding to 90 percentile of countries
by occurrence of COVID-19
S.No.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17

Feature description
Air transport, passengers carried
Cause of death, by communicable diseases and maternal,
prenatal and nutrition conditions (% of total)
Cause of death, by non-communicable diseases (% of total)
Current health expenditure per capita, PPP (current international $)
GDP per capita, PPP (current international $)
Incidence of tuberculosis (per 100,000 people)
Life expectancy at birth, total (years)
Mortality rate, adult, female (per 1,000 female adults)
Mortality rate, adult, male (per 1,000 male adults)
People using at least basic sanitation services (% of population)
Population ages 15-64 (% of total)
Population ages 65 and above (% of total)
Population density (people per sq. km of land area)
Population, total
Survival to age 65, female (% of cohort)
Survival to age 65, male (% of cohort)
Tuberculosis treatment success rate (% of new cases)

The dataset used in this study is obtained after
joining two datasets. The first data called the WDI
dataset henceforth, is taken from the site of The World
Bank [33]. For some countries, the data at the state
level is included depending on the availability in WDI
dataset. Twenty-seven attributes that intuitively seem
relevant to the spread of COVID-19 and do not contain
too many missing values were selected, so that further
analysis can be done. The list of World Development
indicators in this dataset is provided in the appendix.
The second dataset contains the number of COVID-19
cases for the period 22 January 2020 to 21 May 2020
and the same has been provided by the Center for Systems Science and Engineering (CSSE) at Johns Hopkins
University [13].

Feature Indicator
IS.AIR.PSGR

œÅ

œÅs

-0.053

-0.540

SH.DTH.COMM.ZS

-0.055

0.123

SH.DTH.NCOM.ZS

0.060

-0.008

SH.XPD.CHEX.PP.CD

0.094

0.161

NY.GDP.PCAP.PP.CD
SH.TBS.INCD
SP.DYN.LE00.IN
SP.DYN.AMRT.FE
SP.DYN.AMRT.MA

0.143
-0.061
0.089
-0.068
-0.062

0.166
-0.257
-0.010
-0.004
0.088

SH.STA.BASS.ZS

0.077

0.310

SP.POP.1564.TO.ZS
SP.POP.65UP.TO.ZS
EN.POP.DNST
SP.POP.TOTL
SP.DYN.TO65.FE.ZS
SP.DYN.TO65.MA.ZS
SH.TBS.CURE.ZS

0.028
0.049
0.111
-0.046
0.075
0.071
0.116

-0.035
-0.013
-0.058
-0.843
0.004
-0.080
-0.363

7.1 Use of RelDenClu for finding relevant features in
COVID-19 dataset
To do this we use the number of confirmed COVID-19
cases from 22 Jan to 21 May 2020 at an interval of 7
days i.e., we take the cumulative number of confirmed
cases and deaths till each Friday. This along with WDI
features is used as input for RelDenClu. We define the
infection rate as the number of confirmed COVID-19
cases divided by the population of a region. We find
countries lying above 90 percentile in terms of infection
rate. The bicluster having a maximum match with this
set is identified among all the biclusters obtained using RelDenClu. For a given country we say that there
is a match between a bicluster B and 90 percentile set
if the country belongs to both or does not belong to
either. Since the biclusters are obtained based on the
similarity between features for chosen observations, it
can be inferred that the relationship between these features distinguishes the given set of observations from
other observations. Thus these features are likely to affect the spread of the disease.

Through this experimentation our aim is to find out
the relevant features which may be responsible for the
spread of COVID-19. Pearson correlation and Spearman correlation for most features are low so these methods cannot be used to identify the relevant features.
For identifying the features which have an impact on
COVID-19 we need a technique that works for nonlinear relations as well as will be able to ignore outliers.
7.2 Relevant features in COVID-19 dataset obtained
We analyzed the given data using the proposed method,
using RelDenClu
RelDenClu, as it fulfils both these conditions. The folThe proposed method selected 17 WDI features that
lowing section gives the approach followed to perform
are capable of distinguishing regions with high infecthis analysis. The parameters used by RelDenClu are
tion rate, among the 27 WDI features. Table 12 lists
given as Sim2Seed = 0.98, ReuseAllSeeds = FALSE,
ReuseSeedSim = 0.7, ObsInMinBase = 10, MinSeedSize = the 17 features (Feature Code and Feature Description)
obtained by RelDenClu and also provides the values
15, ClusSim = 0.75. The normalization used is norm1 ()

Relative density-based biclustering

21

Fig. 9: Fraction of population suffering from COVID-19 using relevant features found through proposed algorithm.
The countries which lie in top twenty percent are marked with blue circle.

1

√ó 10-6

1

0.5
2
4
IS.AIR.PSGR

6

0

√ó 108

√ó 10-6

1

0.5

0.5

0

0
0

2000
4000
6000
SH.XPD.CHEX.PP.CD

8000

√ó 10-6

0.5
90

√ó 10-6

0

50
SH.STA.BASS.ZS

100

√ó 10-6

0.5

0.5

0

0
0

1

√ó 10-6

0.5
1
1.5
2
EN.POP.DNST
√ó 104

√ó 10-6

0.5
0
20

0

800

1

200
400
SP.DYN.AMRT.MA

600

10
20
SP.POP.65UP.TO.ZS

30

60
80
SP.DYN.TO65.FE.ZS

100

√ó 10-6

0
100
200
300
SP.DYN.AMRT.FE

400

√ó 10-6

0

1

√ó 10-6

0.5
0
60
80
SP.POP.1564.TO.ZS

100

√ó 10-6

0

1

√ó 10-6

0.5
0

1

200
400
600
SH.TBS.INCD

√ó 10-6

0.5

0
40

1

100

0
5
10
15
NY.GDP.PCAP.PP.CD √ó 104

0.5

0

40
60
80
SH.DTH.NCOM.ZS

0.5

0

1

0
20

1

0
60
70
80
SP.DYN.LE00.IN

0.5

1

80

0.5

0
50

1

20
40
60
SH.DTH.COMM.ZS

√ó 10-6

0

1

√ó 10-6

0.5

0
0

1

1

0.5

0

1

√ó 10-6

5
10
SP.POP.TOTL

15
√ó 108

0
40

√ó 10-6

0.5
0
40
60
80
SP.DYN.TO65.MA.ZS

100

0

50
SH.TBS.CURE.ZS

of the Correlation coefficient (œÅ) and Spearman correlation (œÅs ) between the chosen feature and the infection rate of COVID-19 for various states and countries.
In the following discussion we refer to features using
their serial number (S.No.) in Table 12. From the scatter plots between COVID-19 infection rate and selected
features (seen in Figure 9) we find that most of them
are strong predictors for the infection rate. However, in
many cases, the values of Pearson and Spearman correlation coefficients are low indicating that these techniques are not sufficient for identifying these predictors.
This suggests that the proposed algorithm could be explored to identify important factors impacting COVID19 and, this, in turn, will aid in preparing a strategy, in

100

a more scientific manner, to combat the pandemic situation across countries. Indeed some of these features
have already been studied and are found to be important, as discussed in the following paragraphs.
Before we discuss existing information on effect of
different factors on infection rate of COVID-19, it should
be mentioned that the association between infection
rate and a feature is also affected by the severity of
the symptoms, as patients showing severe symptoms
are more likely to be tested. The disease is known to be
asymptomatic for many people, who are likely to miss
the diagnosis.
Among the features selected by RelDenClu, the percentage of deaths in region due to communicable and

22

non-communicable diseases (S. No. 2 and 3 of Table
12), falls in line with the report given by Centers of
Disease Control and Prevention [6]. We also find that
age-related features (S. No. 11, 12, 15, and 16 of Table
12) are also selected. Features showing mortality rate
for overall population and for each gender also affect
the age distribution of people in a region (S. No. 7, 8, 9
of Table 12 ) are selected by RelDenClu. The selected
features showing total population and population density (S. No. 13 and 14 of Table 12) are obviously related
to the number of confirmed cases and infection rates.
The feature indicating the incidence of tuberculosis (S.
No. 6 of Table 12 ), is interesting. It is also seen that
countries with higher tuberculosis incidences have relativity lower COVID-19 infection rates. A study [10] has
already noted that administering BCG vaccine is likely
to reduce severity of COVID-19 symptoms. Another
factor that may contribute to this association is that
tuberculosis is more rampant in warm humid climates
while COVID-19 is expected to spread faster in cold dry
climate [28]. On the other hand, we find COVID-19 has
a higher infection rate where Tuberculosis detection is
high. This may be because such regions are conducting a higher number of COVID-19 tests, leading to a
higher number of confirmed cases. We also find that
success rate in curing tuberculosis (S. No. 17 of Table
12) in related to infection rate.
The feature showing per capita medical expenditure
and per capita GDP (S. no. 4 and 5 in Table 12) may be
related to higher COVID-19 rate due to higher number
of tests. The feature related to the mobility of people
(S.No. 1 of Table 12 ) indirectly affect the spread of
infection. RelDenClu also selects the feature related to
sanitation services in a region (S. No. 10 of Table 12).
Surprisingly, COVID-19 rates are higher in countries
with better sanitation services. We did not find any specific study on this association. However, if we recall that
COVID-19 infection rates are also higher for countries
with a lower percentage of death due to communicable
diseases, it suggests that many infections prevalent in
countries with lower access to sanitation services might
have a role in increasing immunity against COVID-19.
We can consider this aspect too for further analysis
and thereafter administering the country/ region. The
proposed algorithms will allow us to explore some previously unknown associations. Once features are identified from larger datasets, a more detailed analysis could
be done for individual features.
The features obtained using RelDenClu confirm some
of the existing associations and increase our confidence
in some of the associations which are under investigation. It also points to a few features for which association is not known till date.

Namita Jain et al.

8 Conclusion
In this article, we proposed an algorithm that finds
biclusters based on non-linear relations between features. By analysing local variations in density, the algorithm is seen to perform well on non-linear datasets.
Experiments on simulated datasets have shown that
the proposed algorithm is consistent under linear transforms. It is also seen to provide consistent performance
under many non-linear transformations, yielding improved results as compared to seven other methods
on noisy datasets. The significance of these results has
been shown using t ‚àí test.
The proposed algorithm is seen to be effective in
discovering existing classes for three real-life datasets of
UCI ML repository (Magic, Cancer, Ionosphere). Additionally, RelDenClu provides higher accuracy when considered as a precursor for supervised learning, on three
datasets from UCI ML repository (Credit card, Image
Segmentation, Bioconcentration). It has also been applied to a dataset containing information about number
of COVID-19 cases in different regions and respective
development indicators, to obtain factors (demographic
and others) impacting number of confirmed infections
in a region. This information is likely to be useful to
policy makers as well as medical researchers. The proposed algorithm facilitates us to understand the relationship between subsets of a dataset which is otherwise obscured by unrelated subsets of observations or
overlooked due to non-linearity.
Acknowledgements We would also like to show our grat-

itude to Dr. Ashish Ghosh, Professor, Machine Intelligence
Unit, Indian Statistical Institute, Kolkata for giving us his
valuable advice during the course of this research. We would
also like to thank Mr. Rahul Roy, Jadavpur University, Kolkata
for his suggestions while writing the article.

References
1. Aggarwal CC, Wolf JL, Yu PS, Procopiuc C, Park
JS (1999) Fast algorithms for projected clustering.
SIGMOD Rec 28(2):61‚Äì72
2. Agrawal R, Srikant R (1994) Fast algorithms for
mining association rules in large databases. In: Proceedings of the 20th International Conference on
Very Large Data Bases, Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, VLDB ‚Äô94,
pp 487‚Äì499
3. Agrawal R, Gehrke J, Gunopulos D, Raghavan P
(1998) Automatic subspace clustering of high dimensional data for data mining applications. SIGMOD Rec 27(2):94‚Äì105

Relative density-based biclustering

4. Bergmann S, Ihmels J, Barkai N (2003) Iterative
signature algorithm for the analysis of large-scale
gene expression. Physical Review E, Statistical,
Nonlinear, and Soft matter physics p 031902
5. Carmona-Saez P, Pascual-Marqui RD, Tirado F,
Carazo JM, Pascual-Montano A (2006) Biclustering of gene expression data by non-smooth nonnegative matrix factorization. BMC Bioinformatics
7(1):78
6. Centers for Disease Control and Prevention (2020)
People who are at higher risk for severe illness. URL
cdc.gov/coronavirus/2019-ncov/need-extraprecautions/people-at-higher-risk.html?
7. Cheng Y, Church GM (2000) Biclustering of expression data. In: Proceedings of the Eighth International Conference on Intelligent Systems for
Molecular Biology, AAAI Press, pp 93‚Äì103
8. Cheung L, Yip KY, Cheung DW, Kao B,
Ng MK (2005) On mining micro-array data
by order-preserving submatrix. In: 21st International Conference on Data Engineering Workshops
(ICDEW‚Äô05), pp 1153‚Äì1153
9. Costeira JP, Kanade T (1998) A multibody factorization method for independently moving objects. International Journal of Computer Vision
29(3):159‚Äì179
10. Curtis N, Sparrow A, Ghebreyesus TA,
Netea MG (2020) Considering bcg vaccination to reduce the impact of covid-19.
The Lancet 395(10236):1545‚Äì1546, DOI
10.1016/S0140-6736(20)31025-4, URL https:
//doi.org/10.1016/S0140-6736(20)31025-4
11. Deheuvels P (1984) Strong limit theorems for maximal spacings from a general univariate distribution.
The Annals of Probability 12(4):1181‚Äì1193
12. Dhillon IS, Mallela S, Modha DS (2003)
Information-theoretic co-clustering. In: Proceedings of the Ninth ACM SIGKDD International
Conference on Knowledge Discovery and Data
Mining, ACM, New York, NY, USA, KDD ‚Äô03, pp
89‚Äì98
13. Dong E, Du H, Gardner L (2020) An interactive
web-based dashboard to track covid-19 in real time.
The Lancet Infectious Diseases 20(5):533‚Äì534,
DOI 10.1016/S1473-3099(20)30120-1, URL https:
//doi.org/10.1016/S1473-3099(20)30120-1
14. Dua D, Graff C (2017) UCI machine learning repository
15. Fawcett T (2006) An introduction to ROC analysis.
Pattern Recognition Letters 27(8):861‚Äì874
16. Getz G, Levine E, Domany E (2000) Coupled two-way clustering analysis of gene microarray data. Proceedings of the National

23

17.

18.
19.

20.

21.

22.

23.

24.
25.

26.

27.

28.

29.

Academy of Sciences 97(22):12,079‚Äì12,084, https:
//www.pnas.org/content/97/22/12079.full.pdf
Hartigan JA (1972) Direct clustering of a data matrix. Journal of the American Statistical Association 67(337):123‚Äì129
Hassani M, Hansen M (2015) Subspace: Interface
to OpenSubspace. R package version 1.0.4
Hochreiter S, Bodenhofer U, Heusel M, Mayr
A, Mitterecker A, Kasim A, Khamiakova T,
Van Sanden S, Lin D, Talloen W, Bijnens
L, GoÃàhlmann HWH, Shkedy Z, Clevert DA
(2010) Fabia. Bioinformatics 26(12):1520‚Äì1527,
DOI 10.1093/bioinformatics/btq227, URL http:
//dx.doi.org/10.1093/bioinformatics/btq227
Jain N, Murthy CA (2019) Connectedness-based
subspace clustering. Knowledge and Information
Systems 58(1):9‚Äì34
Kailing K, Kriegel HP, KroÃàger P (2004) Densityconnected subspace clustering for high-dimensional
data. In: Proc. SIAM International Conference on
Data Mining (SDM‚Äô04), vol 4, pp 246‚Äì256
Kriegel HP, Kroger P, Renz M, Wurst S (2005)
A generic framework for efficient subspace clustering of high-dimensional data. In: Proceedings of
the Fifth IEEE International Conference on Data
Mining, IEEE Computer Society, Washington, DC,
USA, ICDM ‚Äô05, pp 250‚Äì257
Kriegel HP, KroÃàger P, Zimek A (2009) Clustering high-dimensional data: A survey on subspace
clustering, pattern-based clustering, and correlation clustering. ACM Transactions on Knowledge
Discovery from Data 3(1):1:1‚Äì1:58
Luxburg U (2007) A tutorial on spectral clustering.
Statistics and Computing 17(4):395‚Äì416
Madeira SC, Oliveira AL (2004) Biclustering algorithms for biological data analysis: a survey.
IEEE/ACM Transactions on Computational Biology and Bioinformatics 1(1):24‚Äì45
Mitra S, Banka H (2006) Multi-objective evolutionary biclustering of gene expression data. Pattern
Recognition 39(12):2464‚Äì2477
Moise G, Sander J, Ester M (2008) Robust projected clustering. Knowledge and Information Systems 14(3):273‚Äì298
New Scientist (2020) Will the spread of covid19 be affected by changing seasons? URL
https://www.newscientist.com/article/
2239380-will-the-spread-of-covid-19be-affected-by-changing-seasons/
Parsons L, Haque E, Liu H (2004) Subspace clustering for high dimensional data: A review. ACM
SIGKDD Explorations Newsletter - Special issue
on learning from imbalanced datasets 6(1):90‚Äì105

24

30. Parzen E (1962) On estimation of a probability density function and mode. The Annals of Mathematical Statistics 33(3):1065‚Äì1076
31. PrelicÃÅ A, Bleuler S, Zimmermann P, Wille A,
BuÃàhlmann P, Gruissem W, Hennig L, Thiele L, Zitzler E (2006) A systematic comparison and evaluation of biclustering methods for gene expression
data. Bioinformatics 22(9):1122‚Äì1129
32. Tanay A, Sharan R, Shamir R (2002) Discovering
statistically significant biclusters in gene expression
data. Bioinformatics 18:S136‚ÄìS144
33. The World Bank (2015) World development indicators. URL https://datacatalog.worldbank.org/
dataset/world-development-indicators
34. Tung AKH, Xu X, Ooi BC (2005) CURLER: Finding and Visualizing Nonlinear Correlation Clusters,
ACM, New York, NY, USA, pp 467‚Äì478. SIGMOD
‚Äô05
35. Wang Z, Li G, Robinson RW, Huang X (2016)
UniBic: Sequential row-based biclustering algorithm for analysis of gene expression data. Scientific
Reports 6

Namita Jain et al.

Relative density-based biclustering
Appendix
Features used from World Development Index 2015 [33]
Air transport, passengers carried
Cause of death, by communicable diseases and maternal, prenatal and nutrition conditions (% of total)
Cause of death, by non-communicable diseases (% of total)
Current health expenditure per capita, PPP (current international $)
Death rate, crude (per 1,000 people)
GDP per capita, PPP (current international $)
Incidence of tuberculosis (per 100,000 people)
International migrant stock, total
International tourism, number of arrivals
Labor force participation rate, total (% of total population ages 15+) (modeled ILO estimate)
Life expectancy at birth, total (years)
Mortality from CVD, cancer, diabetes or CRD between exact ages 30 and 70 (%)
Mortality rate, adult, female (per 1,000 female adults)
Mortality rate, adult, male (per 1,000 male adults)
Out-of-pocket expenditure (% of current health expenditure)
People using at least basic sanitation services (% of population)
PM2.5 air pollution, population exposed to levels exceeding WHO guideline value (% of total)
Population ages 15-64 (% of total)
Population ages 65 and above (% of total)
Population density (people per sq. km of land area)
Population, total
Survival to age 65, female (% of cohort)
Survival to age 65, male (% of cohort)
Trade (% of GDP)
Tuberculosis case detection rate (%, all forms)
Tuberculosis treatment success rate (% of new cases)
Urban population (% of total)

25

26

Namita Jain et al.

Symbols used in this article
i, j, k, p, q, oi, f i
Used as indices for rows, columns, biclusters, cells of grids etc.
D
Data matrix
O
Set of observations
F
Set of features
A
Submatrix of D
A‚àó,j
jth column of matrix A
Ai,‚àó
ith row of matrix A
Ai,j
The element in ith row and jth column of matrix A
N
Number of observations in D
M
Number of features in D
n
Number of observations in a bicluster
m
Number of features in a bicluster
X, Y
Names used to refer to two axes in two-dimensional space
vx , v y
Coordinates for centre of a cell in two-dimensional space given by X and Y
xlen , ylen
Size of a cell along X and Y axes
s
Maximal separation along any particular axis
sx , s y
Maximal separation along X and Y axes
c, C1 , C2
Constants
AN
Area of a cell when data of size N is partitioned in two dimensional space
nX , nY
Number of partitions along X and Y axes
PX (i)
Probability of observation lying in ith bin along X axis
PY (j )
Probability of observation lying in jth bin along Y axis
PXY (i)
Probability of observation lying in ith bin along X axis and jth bin along Y
axis
EXY [fXY ]
Expectation of a function fX,Y with reference to joint distribution of X and
Y
ei,j
x
f (x)
f1 , f 2 , f 3 , ¬∑ ¬∑ ¬∑
S, S 0
B
O1 , O2
F1 , F2

#()
r, 
h1 (x), h2 (x), ¬∑ ¬∑ ¬∑
I (x)
r1 , r2
a1 , a2 , ¬∑ ¬∑ ¬∑ , ai
mB
mE
m1 , m2 , ¬∑ ¬∑ ¬∑ , mk
mtotal

The element of D in ith row and jth column, subscript notation has been
used instead of matrix notation for better readability
Used to denote argument of function
Function of argument x
Used to denote unique features in the dataset (not necessarily consecutive)
Seed biclusters found in Step 4.3.1
Used to denote the seed bicluster while it grows iteratively in Step 4.3.2
Observation sets for first and second bicluster
Feature sets for first and second bicluster
Cardinality of set
Two parameters used by Subclu which represent the radius of disc used for
density estimation and threshold for number of observations in each disc
Different functions of variable x
Identity function
Two random variables used for linear transformation of data
Random variables used for generating simulated data
Membership matrix for bicluster under consideration
Estimated membership matrix for bicluster under consideration
Number of features in biclusters indexed by 1, 2, ¬∑ ¬∑ ¬∑ , k
Sum of number of features in each bicluster obtained for a particular dataset

