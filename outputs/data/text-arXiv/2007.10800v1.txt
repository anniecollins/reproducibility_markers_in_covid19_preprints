Probabilistic Neighbourhood Component Analysis:
Sample Efficient Uncertainty Estimation in Deep Learning
Ankur Mallick1 , Chaitanya Dwivedi1 , Bhavya Kailkhura2 , Gauri Joshi1 , and T. Yong-Jin
Han2

arXiv:2007.10800v1 [cs.LG] 18 Jul 2020

1

2

Carnegie Mellon University
Lawrence Livermore National Laboratory
July 22, 2020
Abstract

While Deep Neural Networks (DNNs) achieve state-of-the-art accuracy in various applications, they
often fall short in accurately estimating their predictive uncertainty and, in turn, fail to recognize when
these predictions may be wrong. Several uncertainty-aware models, such as Bayesian Neural Network
(BNNs) and Deep Ensembles have been proposed in the literature for quantifying predictive uncertainty.
However, research in this area has been largely confined to the big data regime. In this work, we show
that the uncertainty estimation capability of state-of-the-art BNNs and Deep Ensemble models degrades
significantly when the amount of training data is small. To address the issue of accurate uncertainty
estimation in the small-data regime, we propose a probabilistic generalization of the popular sampleefficient non-parametric kNN approach. Our approach enables deep kNN classifier to accurately quantify
underlying uncertainties in its prediction. We demonstrate the usefulness of the proposed approach by
achieving superior uncertainty quantification as compared to state-of-the-art on a real-world application of
COVID-19 diagnosis from chest X-Rays. Our code is available at https://github.com/ankurmallick/sampleefficient-uq.

1

Introduction

Deep Neural Networks (DNNs) have achieved remarkable success in a wide range of applications where a
large amount of labeled training data is available [10, 9]. However, in many emerging applications of machine
learning such as diagnosis and treatment of novel coronavirus disease (COVID-19) [6] a large labeled training
datasets may not be available. Furthermore, test data in these applications may deviate from the training
data distribution, e.g., due to sample selection bias, nonstationarity, and even can be from Out-of-Distribution
in some extreme cases [3]. Note that several of these applications are high-regret in nature implying that
incorrect decisions or predictions have significant costs. Therefore, such applications require not only achieving
high accuracy but also accurate quantification of predictive uncertainties. Accurate predictive uncertainty in
these applications can help practitioners to assess the true performance and risks and to decide whether the
model predictions should (or should not) be trusted.
Unfortunately, DNNs often make overconfident predictions in the presence of distributional shifts and
Out-of-Distribution data. As an example, Fig. 1 shows the predictions of different deep learning models trained
to detect the presence of COVID-19 from chest X-ray images. All models achieve similar accuracy (âˆ¼ 80%)
on in-distribution validation data. However, their quality of uncertainties is widely varied as explained next.
While all models are forced to output some prediction, on every input image, we would want a model to not
be very confident on input data that is very different from the data used to train it. However, we observe that
state-of-the-art deep learning models make highly overconfident predictions on Out-of-Distribution data [17].
1

True Label

COVID-19 (Training Data)

COVID-19 (Unseen Data)

Shoulder (Unrelated Data)

Model
DNN
BNN
Ours

Prediction Confidence
COVID-19
99.5%
COVID-19
95.6%
COVID-19
99.9%

Prediction Confidence
Non-COVID
83.7%
COVID-19
58.2%
COVID-19
60.0%

Prediction Confidence
COVID-19
99.4%
COVID-19
91.1%
COVID-19
50.1%

Figure 1: The predictions of deep learning models that are trained to detect the presence of COVID-19 in chest X-Ray
images from [6]. (a) All models correctly classify in-distribution data from [6], (b) Uncertainty-aware models (BNN,
and our model PNCA) perform better than DNN when test data is from a different source [4], (c) As opposed to
proposed PNCA, both BNN and DNN make overconfident misclassification on Out-of-Distribution data [17] (e.g.,
classifying shoulder X-Ray as COVID-19).

Interestingly, we found that even popular uncertainty-aware models, (e.g., BNNs, deep ensembles) that are
designed to address this precise issue, perform poorly in small data regime. This is an extremely problematic
issue especially owing to the flurry of papers that have been attempting to use DNNs for detecting COVID-19
using chest X-Ray images [15, 5, 21] as real-world test data is almost always different as compared to the
training data.
While there have been separate efforts on improving the sample efficiency [14] and accurate uncertainty
estimation [7] of deep learning, to the best of our knowledge there has not been any effort on studying
these seemingly different issues in a unified manner. Therefore, this paper takes some initial steps towards
(a) studying the effect of training data on the quality of uncertainty and (b) developing sample efficient
uncertainty-aware predictive models. Specifically, to overcome the challenge of providing accurate uncertainties
without compromising the accuracy in the small-data regime, we propose a probabilistic generalization of
the popular non-parametric kNN approach (referred to as probabilistic neighborhood component analysis
(PNCA)). By mapping data into distributions in a latent space before performing classification, we enable
a deep kNN classifier to accurately quantify underlying uncertainties in its prediction. Following [11, 18],
for a meaningful and effective performance evaluation, we compare the quality of predictive uncertainty of
different models under conditions of distributional shift and Out-of-Distribution. We empirically show that
the proposed PNCA approach achieves significantly better uncertainty estimation performance as compared
to state-of-the-art approaches in small data regime.

2

Probabilistic Neighbourhood Component Analysis

In this section, we describe our model to achieve sample-efficient and uncertainty-aware classification. The
details of the algorithm and proof of Proposition 1 are presented in Appendix B.

2

2.1

Neighbourhood Components Analysis (NCA)

Our approach is a generalization of NCA proposed in [8] wherein the authors learn a distance metric for
kNN classification of points xi , . . . , xn âˆˆ RD with corresponding class labels y1 , . . . , yn . A data point x is
projected into a latent space Z âŠ† Rd to give an embedding z = gw (x). Here g can be a linear transformation
like a d Ã— D matrix or a non-linear transformation like a neural network with a dâˆ’dimensional output, and
w are the parameters of the transformation. The probability of a point xi selecting another point xj as its
neighbor is given by applying a softmax activation to the distance between points in the latent space
exp(âˆ’||zi âˆ’ zj ||2 )
,
0 2
i0 6=i exp(âˆ’||zi âˆ’ zi || )

qij = P

qii = 0.

The probability of xi selecting a point in the same class as itself is given by qi =
model parameters are obtained by minimizing the loss
X
L(w) = âˆ’
log(qi ),

(1)
P

j:yj =yi

qij and the optimal

(2)

i

which is the negative log-likelihood of the data under our the model. The authors of [8] experiment with a
variety of transformations gw (.) and classification tasks and show that NCA achieves competitive accuracy.

2.2

Our Model

The lack of data may cause the NCA model to overfit when learning the weights by optimizing the loss in (2).
We expect that the uncertainty due to the scarcity of training data can be better captured by probability
distributions in the latent space Z than by individual data samples. Therefore, we propose a probabilistic
generalization of the model, PNCA, which learns a distribution over the model parameters w and, thus, deals
with both the lack of training data and the task of accurate uncertainty estimation.
Latent Space Mapping using Probabilistic Neural Networks. Each data point x passes through
a probabilistic neural network with parameters W âˆ¼ p(W) to give a random variable Z = gW (x) âˆˆ Z. Due
to the stochasticity of W, each data point xi corresponds to a different distribution p(Z|xi ) in the latent
space.
NCA over Latent Distributions. Observe that the individual terms in the softmax activation
correspond to a kernel between latent embeddings z, e.g., the squared exponential kernel k(z, z0 ) = exp(âˆ’||z âˆ’
z0 ||2 ). Since in our approach the embedding corresponding to a data point xi is the probability distribution
p(Z|xi ), we propose to use the following kernel between distributions
Kij = Ezâˆ¼p(Z|xi ),

z0 âˆ¼p(Z|xj ) [k(z, z

0

)],

(3)

where Kij corresponds to the inner product between distributions p(Z|xi ) and p(Z|xj ) in the Reproducing
Kernel Hilbert Space (RKHS) Hk defined by the kernel k [16] and, thus, captures similarity between
distributions in the same way as k(z, z0 ) captures the similarity between individual embeddings in NCA.

2.3

Training Algorithm

The forward pass described above, is used to compute a kernel between data points xi and xj which can then
be used to compute the probability of xi selecting xj in an analogous fashion to NCA as
X
qij = Kij /
Kii0 , qii = 0.
(4)
i0 6=i

Since the latent embedding Z for a data point x is given by Z = gW (x), W âˆ¼ p(W), we can rewrite (3) as
Kij = Ew,w0 âˆ¼p(W) [k(gw (xi ), gw0 (xj ))] = Kij [p].
3

(5)

8000

15000

6000

0.2

2000

0.2

0.4

0.6

0.8

10000

BNN
NCA
DNN
PNCA
Ensemble

4000

0.0

Number of examples with p(y|x) â‰¥ Ï„

0.4

Number of examples with p(y|x) â‰¥ Ï„

Accuracy on examples with p(y|x) â‰¥ Ï„

0.6

10000

BNN
NCA
DNN
PNCA
Ensemble

0

0.2

5000

0.4

Ï„

0.6

0.8

Ï„

(a) Accuracy (Rotated-MNIST)

(b) Count (Rotated-MNIST)

0

BNN
NCA
DNN
PNCA
Ensemble
0.2

0.4

0.6

0.8

Ï„

(c) Confidence on OoD (Not-MNIST)

Figure 2: Results on MNIST: (a) PNCA has the largest fraction of correctly classified examples in the highconfidence region for Rotated MNIST. (b) and (c) PNCA generally has much lower confidence, i.e., better Uncertainty
Quantification (UQ) than rest of the models on Out-of-Distribution Data â€“ both Rotated-MNIST and not-MNIST.
See Fig. 4 for a comparison by varying number of training samples.

Thus, we can view Kij as a functional of p(W). The negative log likelihood in (2) is then also a functional of
p. The optimal distribution over the model parameters can be obtained by solving
pâˆ— (W) = arg min L[p].

(6)

p(W)âˆˆP

The choice of P is critical to the success of this approach. Following [13], we choose P to be P = {p(u)|u =
w + s(w), w âˆ¼ p0 (w), s âˆˆ HÎº } where HÎº is a RKHS given by a kernel Îº between model parameters w
(note that this is different from the RKHS Hk into which distributions in the latent space Z are embedded
and which is given by the kernel k). This choice of P includes all smooth transformations from the initial
distribution p0 , and the optimization problem (6) now reduces to computing the optimal shift sâˆ— (w). Next,
we provide an expression for the functional gradient of the negative log-likelihood under our model with
respect to the shift s.
Proposition 1. If we draw m realizations of model parameters w1 , . . . , wm âˆ¼ p(w), p âˆˆ P, then
âˆ‡s L |s=0 '
where LÌ‚ is given by substituting KÌ‚ij =

1
m2

m
X

P

Îº(wl , .)âˆ‡wl LÌ‚(w1 , . . . , wm )

(7)

l=1
l,l0

k(gwl (xi ), gwl0 (xj )) in (2).

To estimate the optimal shift, sâˆ— (or optimal distribution pâˆ— ) we draw an initial set of parameters
wi âˆ¼ p0 (w) and iteratively apply the functional gradient descent transformation u = w âˆ’ âˆ‡s L |s=0 as
described in Algorithm 1 in Appendix B

3

Experiments

We consider two small data classification tasks: (1) handwritten digit recognition, and (2) COVID-19 detection
from chest X-Ray images. For both tasks, we compare proposed PNCA to 4 baselines, a Deep Neural Network
(DNN), a Bayesian Neural Network (BNN) trained using the approach of [13], Deep Ensembles [11], and
NCA [8]. For PNCA and NCA, we use a neural network to map the data
P x to embeddings z. For NCA
and PNCA, the predicted class label for a test point xi is yË†i = arg maxc j:yj =c qij . For the other models,
the predicted class label is the one with the highest softmax probability (average softmax probability for
BNNs and Ensembles). Following [18], we use maxc q(yÌ‚ = c|x) as a measure of the confidence of the model
4

150

0.4
0.3

0.1
0.5

0.6

2000
1500

100

BNN
NCA
DNN
PNCA
Ensemble

0.2

0.0

2500

Number of examples with p(y|x) â‰¥ Ï„

Number of examples with p(y|x) â‰¥ Ï„

Accuracy of examples with p(y|x) â‰¥ Ï„

0.5

BNN
NCA
DNN
PNCA
Ensemble

50

0.7

0.8

0.9

0

0.5

0.6

500

0.7

Ï„

(a) Accuracy (COVID-V2)

BNN
NCA
DNN
PNCA
Ensemble

1000

0.8
Ï„

(b) Count (COVID-V2)

0.9

0

0.5

0.6

0.7

0.8

0.9

Ï„

(c) Confidence on OoD (Not-COVID)

Figure 3: While all models have similar performance on slightly shifted data (i.e., COVID-V2 [4]), PNCA exhibits
significantly better uncertainty quantification as the distributional shift increases (i.e., Out-of-Distribution data [17])
as seen by the much fewer high confidence predictions in (c).

in predicting class yÌ‚ for input x and show the accuracy and number of examples vs. confidence for Out-of
Distribution data to quantify the quality of uncertainties. Please refer to Appendix A for further details on
the experiments and additional results.
MNIST Classification All neural networks in this experiment have the same architecture (2 hidden
layers, 200 nodes per layer). Models are trained on a random subset of 100 labeled examples from the
MNIST dataset [12] and results are averaged over 10 trials. Figures 2a and 2b show the performance
comparison on unseen rotated MNIST dataset (MNIST test images rotated by 60â—¦ ). It can be seen that
PNCA outperforms all other models in terms of (a) accuracy vs. confidence (high confidence examples should
have high accuracy) and (b) number of examples with high confidence (only a few examples should have high
confidence). Moreover, Fig. 2c shows the performance comparison on Out-of-Distribution, i.e., not-MNIST
dataset [2] that contains letters instead of handwritten digits. We see that PNCA has significantly fewer
examples with high confidence as compared to rest of the approaches on the not-MNIST dataset illustrating
the superior capability in quantifying uncertainty.
COVID-19 detection There has been an increasing interest in using deep learning to detect COVID-19
from Chest X-Ray (CXR) images [19, 21]. Successful prediction from CXR data can effectively complement
the standard RT-PCR test [22]. However, the lack of large amount of training data and distributional shift
between train and test data are two major challenges in this task [15].
We consider two sources of COVID-19 data â€“ [6], which has been used by most existing works to train
their models for COVID-19 classification and [4], which we use as our unseen test data as it comes from a
different source than the images in [6]. We follow the transfer learning approach of [15] wherein a ResNet-50
model pre-trained on Imagenet is used as a feature extractor and the last layer of the model is re-trained
on [6] with aforementioned approaches â€“ DNN, BNN, Ensemble, NCA, PNCA on the training dataset. We
consider a binary classification problem, i.e., each model outputs a probability q of the presence/absence of
COVID-19 in a given CXR image.
We use the version of [6] available on Kaggle1 as our training dataset, which contains 275 COVID-19
X-Ray images and 76 non-COVID X-Ray images. On the other hand, [4] is used as our test data, which
contains 58 COVID-19 X-Ray images and 127 non-COVID X-Ray images. There is a distributional shift
present between train and test data resulting in relatively low test accuracy for all models in Fig. 3a. We
also look at the number of examples classified with a high confidence for both the test data and completely
Out-of-Distribution data (shoulder and hand X-Rays from [17]). As can be seen, on [4], which potentially has
a different distribution, BNN has slightly lower number of examples classified with high confidence than the
other models. Next, in Fig. 3c, we can see that as the distribution shift increases, PNCA makes significantly
fewer high confidence predictions than all other models corroborating its superior uncertainty quantification.
1 https://www.kaggle.com/bachrr/covid-chest-xray

5

In summary, these experiments demonstrate that PNCA achieves much better uncertainty quantification
than the baselines without losing accuracy in small-data regime.

4

Conclusion and Broader Impact

This work serves as a caution to practitioners interested in applying deep learning for disease detection
especially during the current pandemic since we find that the issues related to overconfident and inaccurate
predictions of DNNs become even more severe in small-data regime. While our approach appears to be less
susceptible to making overconfident misclassifications and have good uncertainty estimation performance, we
acknowledge that there is still room for improvement especially with respect to the accuracy of the model.
With this in mind, we will explore approaches to improve the generalization capability of PNCA in future
work. Further, sample efficient uncertainty calibration approaches such as [24] and more reliable evaluation
approaches for small data regime can be explored.

Acknowledgement
This work was performed under the auspices of the U.S. Department of Energy by Lawrence Livermore
National Laboratory under Contract DE-AC52-07NA27344 (LLNL-CONF-811603).

References
[1] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat, G. Irving, M. Isard,
et al. Tensorflow: A system for large-scale machine learning. In 12th {USENIX} Symposium on Operating
Systems Design and Implementation ({OSDI} 16), pages 265â€“283, 2016.
[2] Y. Bulatov. not-mnist dataset. http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html,
2011.
[3] S. Bulusu, B. Kailkhura, B. Li, P. K. Varshney, and D. Song. Anomalous instance detection in deep
learning: A survey. arXiv preprint arXiv:2003.06979, 2020.
Chung.
Actualmed
covid-19
[4] A.
Actualmed-COVID-chestxray-dataset, 2020.

data.

https://github.com/agchung/

[5] J. P. Cohen, L. Dao, P. Morrison, K. Roth, Y. Bengio, B. Shen, A. Abbasi, M. Hoshmand-Kochi,
M. Ghassemi, H. Li, et al. Predicting covid-19 pneumonia severity on chest x-ray with deep learning.
arXiv preprint arXiv:2005.11856, 2020.
[6] J. P. Cohen, P. Morrison, and L. Dao. Covid-19 image data collection. arXiv preprint arXiv:2003.11597,
2020.
[7] Y. Gal. Uncertainty in deep learning. University of Cambridge, 2016.
[8] J. Goldberger, G. E. Hinton, S. T. Roweis, and R. R. Salakhutdinov. Neighbourhood components
analysis. In Advances in neural information processing systems, pages 513â€“520, 2005.
[9] A. Graves, A.-r. Mohamed, and G. Hinton. Speech recognition with deep recurrent neural networks. In
2013 IEEE international conference on acoustics, speech and signal processing, pages 6645â€“6649. IEEE,
2013.
[10] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural
networks. In Advances in neural information processing systems, pages 1097â€“1105, 2012.

6

[11] B. Lakshminarayanan, A. Pritzel, and C. Blundell. Simple and scalable predictive uncertainty estimation
using deep ensembles. In Advances in Neural Information Processing Systems, pages 6402â€“6413, 2017.
[12] Y. LeCun, C. Cortes, and C. Burges. Mnist handwritten digit database. AT&T Labs [Online]. Available:
http://yann. lecun. com/exdb/mnist, 2:18, 2010.
[13] Q. Liu and D. Wang. Stein variational gradient descent: A general purpose bayesian inference algorithm.
In Advances In Neural Information Processing Systems, pages 2378â€“2386, 2016.
[14] A. Mallick, C. Dwivedi, B. Kailkhura, G. Joshi, and T. Han. Deep probabilistic kernels for sample-efficient
learning. arXiv preprint arXiv:1910.05858, 2019.
[15] S. Minaee, R. Kafieh, M. Sonka, S. Yazdani, and G. J. Soufi. Deep-covid: Predicting covid-19 from chest
x-ray images using deep transfer learning. arXiv preprint arXiv:2004.09363, 2020.
[16] K. Muandet, K. Fukumizu, B. Sriperumbudur, B. SchÃ¶lkopf, et al. Kernel mean embedding of distributions:
A review and beyond. Foundations and Trends R in Machine Learning, 10(1-2):1â€“141, 2017.
[17] P. Rajpurkar, J. Irvin, A. Bagul, D. Ding, T. Duan, H. Mehta, B. Yang, K. Zhu, D. Laird, R. L. Ball,
et al. Mura: Large dataset for abnormality detection in musculoskeletal radiographs. arXiv preprint
arXiv:1712.06957, 2017.
[18] J. Snoek, Y. Ovadia, E. Fertig, B. Lakshminarayanan, S. Nowozin, D. Sculley, J. Dillon, J. Ren, and
Z. Nado. Can you trust your modelâ€™s uncertainty? evaluating predictive uncertainty under dataset shift.
In Advances in Neural Information Processing Systems, pages 13969â€“13980, 2019.
[19] E. Tartaglione, C. A. Barbano, C. Berzovini, M. Calandri, and M. Grangetto. Unveiling covid-19 from
chest x-ray with deep learning: a hurdles race with small data. arXiv preprint arXiv:2004.05405, 2020.
[20] D. Timothy. Incorporating nesterov momentum into adam. Natural Hazards, 3(2):437â€“453, 2016.
[21] L. Wang and A. Wong. Covid-net: A tailored deep convolutional neural network design for detection of
covid-19 cases from chest radiography images. arXiv, 2020.
[22] W. Wang, Y. Xu, R. Gao, R. Lu, K. Han, G. Wu, and W. Tan. Detection of sars-cov-2 in different types
of clinical specimens. Jama, 323(18):1843â€“1844, 2020.
[23] F. X. X. Yu, A. T. Suresh, K. M. Choromanski, D. N. Holtmann-Rice, and S. Kumar. Orthogonal
random features. In Advances in Neural Information Processing Systems, pages 1975â€“1983, 2016.
[24] J. Zhang, B. Kailkhura, and T. Han. Mix-n-match: Ensemble and compositional methods for uncertainty
calibration in deep learning. arXiv preprint arXiv:2003.07329, 2020.

A

Additional Details on Experiments

All models are implemented in TensorFlow [1] on a Titan X GPU with 3072 CUDA cores. We use the Adam
Optimizer with Nesterov Momentum [20] with a learning rate of 0.001 to train the models for 100 epochs.
For DNN, BNN, and Ensemble we use minibatches of size 20 with 1 epoch corresponding to a pass over the
entire dataset, while for NCA and DPNCA, the entire dataset is used to calculate gradients.
Following [13] we use the RBF kernel as the kernel Îº between model parameters w P
in PNCA, with
bandwidth chosen according to the median heuristic described in their work since it causes j Îº(w, wj ) ' 1
for all w, leading Îº to behave like a probability distribution. We also use Orthogonal Random Features [23] to
approximate the kernel between probability distributions in the latent space in (3) for faster computation. We
use 10d features where d is the dimensionality of the latent space and a ReLU activation on the approximate
kernel to set any spurious negative values to zero (since the original squared exponential kernel can never be
negative).
7

Algorithm 1: PNCA
(0)

1
2
3
4
5

Input : Training points X and targets y along with a set of initial model parameters {wi }m
i=1
Output : A set of model parameters {wi }m
i=1 âˆ¼ pÌ‚(w) where pÌ‚(w) is obtained by performing functional
gradient descent
(0)
Initialize model parameters {wi }m
i=1 âˆ¼ p0 (w) for some known p0 (w)
for iteration t do
(t+1)
(t)
(t)
wi
= wi âˆ’ t Ï†(wi ),
Pm
where Ï†(w) = l=1 Îº(w, wl )âˆ‡wl LÌ‚(w1 , . . . , wm )
end

Table 1 contains the accuracy of different models across experiments (MNIST test data [12], Rotated
MNIST test data, COVID-19 validation data [6] and COVID-19 test data [4]). For MNIST, accuracy values
are averaged over 10 trials (where each trial corresponds to a different set of 100 training examples). For
COVID-19 accuracy values, the training data is split into 5 equal folds and in each trial we use 4 folds to
train the model and the 5th fold to calculate in-distribution (validation) accuracy. Since each training data
point is a part of the validation data (5th fold) only once, therefore we do not have any standard deviation
values for the validation accuracy. The accuracy on COVID-19 test data is averaged across all folds.
Method
BNN
NCA
DNN
PNCA
Ensemble

MNIST Test
0.74 Â± 0.02
0.69 Â± 0.01
0.75 Â± 0.02
0.67 Â± 0.03
0.76 Â± 0.02

Rotated MNIST
0.20 Â± 0.04
0.18 Â± 0.04
0.17 Â± 0.02
0.18 Â± 0.03
0.17 Â± 0.03

COVID-19 Validation
0.79
0.77
0.80
0.82
0.79

COVID-19 Test
0.40 Â± 0.04
0.44 Â± 0.05
0.37 Â± 0.02
0.37 Â± 0.03
0.38 Â± 0.03

Table 1: Accuracy for all Models across experiments

B

Proof of Proposition 1

Observe that for any smooth one-to-one transform u = T (w), w âˆ¼ p(W), the kernel between the latent
distributions p(Z|xi ), p(Z|xj ) corresponding to data points xi and xj under the transformed distribution
p[T ] (u) can be written as
Kij = Eu,u0 âˆ¼p[T ] (u) [k(gu (xi ), gu0 (xj ))]
= Ew,w0 âˆ¼p(W) [k(gT (w) (xi ), gT (w0 ) (xj ))].

(8)
(9)

Since the above holds for infinitesimal shifts u = w + s(w), a tractable choice of p0 (For eg. Gaussian),
enables efficient approximation of Kij by sample averages with samples ui , ui = wi + s(wi ), wi âˆ¼ p0 (w).
Moreover Kij in (9) is a functional of the transformation T (for fixed p(W)) i.e. a functional of the shift
s in our case. Therefore, the problem of finding pâˆ— (W) in (6) reduces to the problem of finding the optimal
shift (given p0 (W)) i.e.
sâˆ— (W) = arg min L[s].

(10)

s(W)

Since s âˆˆ HÎº , which is the RKHS for the kernel Îº, we can solve (10) via functional gradient descent.
Defining kij (w, w0 ) = k(gw (xi ), gw0 (xj )) we have
Kij = Ew,w0 âˆ¼p(W) [kij (w + s(w), w0 + s(w0 ))]
8

(11)

Assuming that the distributions p(W) and shifts s(W) are functions in a RKHS H given by the kernel Îº
(Îº, k, and K are all different), we have (from the definition of functional gradient âˆ‡s Kij [s]),
Kij [s + r] = Kij [s] +  < âˆ‡s Kij [s], r >H +O(2 )

(12)

Thus we need to compute the difference Kij [s + r] âˆ’ Kij [s] which, from (11) is given by
Kij [s + r] âˆ’ Kij [s] = Ep [kij (w + s(w) + r(w), w0 + s(w0 ) + r(w0 ))]
âˆ’Ep [kij (w + s(w), w0 + s(w0 ))]

(13)

We use Ep to denote the expectation when w, w0 âˆ¼ p(W). The above equation can be rewritten as
Kij [s + r] âˆ’ Kij [s] = V1 + V2 where
V1 = Ep [kij (w + s(w) + r(w), w0 + s(w0 ) + r(w0 ))] âˆ’ Ep [kij (w + s(w), w0 + s(w0 ) + r(w0 ))]
0

0

0

2

= Ep [âˆ‡w kij (w + s(w), w + s(w ) + r(w ))r(w))] + O( )
0

0

(15)

0

= Ep [(âˆ‡w kij (w + s(w), w + s(w ) + r(w ))

âˆ’ âˆ‡w kij (w + s(w), w0 + s(w0 )) + âˆ‡w kij (w + s(w), w0 + s(w0 )))r(w))] + O(2 )
= Ep [âˆ‡w kij (w + s(w), w0 + s(w0 )))r(w))] + O(2 )
0

(14)

0

2

=  < Ep [âˆ‡w kij (w + s(w), w + s(w )))Îº(w, .))], r >H +O( )

(16)
(17)
(18)

where the last line follows from the RKHS property. Similarly,
V2 = Ep [kij (w + s(w), w0 + s(w0 ) + r(w0 ))] âˆ’ Ep [kij (w + s(w), w0 + s(w0 ))]
0

0

0

2

=  < Ep [âˆ‡w0 kij (w + s(w), w + s(w )))Îº(w , .))], r >H +O( )

(19)
(20)

Since we transform the weights w after every iteration, therefore we only ever need to compute the
gradient at s(w) = 0. Thus, finally, we have the expression
âˆ‡s Kij [s] |s=0 = Ep [âˆ‡w kij (w, w0 )Îº(w, .) + âˆ‡w0 kij (w, w0 )Îº(w0 , .)]

(21)

If we draw m samples of model parameters w1 , . . . , wm âˆ¼ p(w), the empirical estimate of âˆ‡s Kij [s] |s=0
given by replacing expectations with sample averages is given by
m
1 X
âˆ‡s Kij [s] |s=0 ' 2
[âˆ‡wl kij (wl , wl0 ))Îº(wl , .)) + âˆ‡wl0 kij (wl , wl0 ))Îº(wl0 , .))]
m 0

(22)

l,l =1

Without loss of generality consider all the terms in the above expression that contain the gradient with
respect to w1 and let us call that part of the summation T1 . Therefore
T1 =

m
m
1 X
1 X
0 ))Îº(w1 , .)) +
âˆ‡
k
(w
,
w
âˆ‡w1 kij (wl , w1 ))Îº(w1 , .))
w
ij
1
l
1
m2 0
m2
l =1

(23)

l=1

Recall the expression for the empirical estimate of the entries of the kernel matrix Kij
KÌ‚ij '

m
1 X
kij (wl , wl0 )
m2 0

(24)

l,l =1

Differentiating both sides with respect to w1 ,
âˆ‡w1 KÌ‚ij '

m
m
1 X
1 X
0) +
âˆ‡
k
(w
,
w
âˆ‡w1 kij (wl , w1 )
w
ij
1
l
1
m2 0
m2
l =1

l=1

9

(25)

Note that the term âˆ‡w1 kij (w1 , w1 ) occurs in both summmations. This is because âˆ‡w1 kij (u, v) = âˆ‡u kij (u, v)âˆ‡w1 u+
âˆ‡v kij (u, v)âˆ‡w1 v = âˆ‡w1 kij (w1 , w1 ) + âˆ‡w1 kij (w1 , w1 ) when u = v = w1 (u, v, w1 are all variable) .
Substituting (25) in (23)
T1 = Îº(w1 , .)âˆ‡w1 KÌ‚ij

(26)

We can apply the same argument to simplify the terms in (22) that contain gradients with respect to other
weights w2 , . . . , wm in the same fashion. Therefore,
âˆ‡s Kij [s] |s=0 '
=

m
1 X
[âˆ‡wl kij (wl , wl0 ))Îº(wl , .)) + âˆ‡wl0 kij (wl , wl0 ))Îº(wl0 , .))]
m2 0

(27)

l,l =1

m
X

Îº(wl , .)âˆ‡wl KÌ‚ij

(28)

l=1

P
âˆ‚L
From the chain rule for functional gradient descent we have âˆ‡s L =
i,j âˆ‚Kij âˆ‡s Kij [s] and the corP
P
m
responding empirical estimate âˆ‡s L |s=0 ' i,j âˆ‚âˆ‚KÌ‚LÌ‚ âˆ‡s ( l=1 Îº(wl , .)âˆ‡wl KÌ‚ij ). Switching the order of the
ij
summations gives
âˆ‡s L |s=0 '

m
X

Îº(wl , .)âˆ‡wl LÌ‚(w1 , . . . , wm )

l=1

10

(29)

8000

0.2

2000

0.2

15000

6000
4000

0.0

Number of examples with p(y|x) â‰¥ Ï„

0.4

Number of examples with p(y|x) â‰¥ Ï„

Accuracy on examples with p(y|x) â‰¥ Ï„

0.6

10000

BNN
NCA
DNN
PNCA
Ensemble

0.4

0.6

0.8

0

10000

BNN
NCA
DNN
PNCA
Ensemble
0.2

5000

0.4

Ï„

10000

BNN
NCA
DNN
PNCA
Ensemble

8000

4000
2000

0.1
0.2

0.4

0.6

0.8

0

10000

BNN
NCA
DNN
PNCA
Ensemble
0.2

5000

0.4

Ï„

6000

0.2

2000

0.1
0.0

0.2

0.4

0

0.2

0.4

0.6

0.8

0

0.8

(f) Confidence on OoD (Not-MNIST)

10000

BNN
NCA
DNN
PNCA
Ensemble
0.2

5000

0.4

Ï„

(g) Accuracy (Rotated-MNIST)

0.6

15000

8000

4000

0.8

Ï„

Number of examples with p(y|x) â‰¥ Ï„

0.3

0.8

(e) Count (Rotated-MNIST)

Number of examples with p(y|x) â‰¥ Ï„

Accuracy on examples with p(y|x) â‰¥ Ï„

0.4

0.6

10000

BNN
NCA
DNN
PNCA
Ensemble

0.6

BNN
NCA
DNN
PNCA
Ensemble

Ï„

(d) Accuracy (Rotated-MNIST)

0.5

0.4

15000

6000

0.2

0.0

0.2

(c) Confidence on OoD (Not-MNIST)

Number of examples with p(y|x) â‰¥ Ï„

0.3

0

Ï„

(b) Count (Rotated-MNIST)

Number of examples with p(y|x) â‰¥ Ï„

Accuracy on examples with p(y|x) â‰¥ Ï„

0.4

0.8

Ï„

(a) Accuracy (Rotated-MNIST)

0.5

0.6

BNN
NCA
DNN
PNCA
Ensemble

0.6

0.8

Ï„

(h) Count (Rotated-MNIST)

0

BNN
NCA
DNN
PNCA
Ensemble
0.2

0.4

0.6

0.8

Ï„

(i) Confidence on OoD (Not-MNIST)

Figure 4: Results in (a)-(c) are for n = 100 samples, in (d)-(f) are for n = 200 samples, and (g)-(i) are for n = 400
samples. These results show that while the performance of other models (including BNN) worsens (lower accuracy,
higher confidence on OOD data) as the number of samples decreases, that of our approach PNCA is largely unaffected
thus validating its efficacy in small data settings.

11

