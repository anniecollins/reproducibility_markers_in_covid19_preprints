Complement Lexical Retrieval with Semantic Residual Embeddings
Luyu Gao1 Zhuyun Dai1 Tongfei Chen2 Zhen Fan1
Benjamin Van Durme2 Jamie Callan1
1 Carnegie Mellon University
2 Johns Hopkins University

Abstract

lexical features

arXiv:2004.13969v2 [cs.IR] 5 Oct 2020

Query

This paper presents D UAL RM, a Dual Retrieval Model that seeks to complement lexical
retrieval with neural embedding retrieval. D U AL RM uses a residual-based embedding learning method, forcing the embedding to encode
language structures and semantics that lexical retrieval fails to capture. Empirical evaluation demonstrates the advantages of D U AL RM over various lexical retrieval models
and a BERT-based embedding retrieval model,
substantially narrowing the gap between fullcollection retrieval and costly reranking systems. We also provide results showing how
D UAL RM can be used in a zero-shot setting
to build a strong retrieval system for COVID19 papers in the CORD-19 corpus. 1

1

Introduction

State-of-the-art search engines adopt a pipelined
retrieval system: an efficient first-stage retriever
that uses a query to fetch a set of documents
from the entire document collection, and subsequently one or more reranking algorithms that refine the ranking (Nogueira and Cho, 2019). Since
the retrieval stage is performed with respect to all
documents, the retriever needs to run efficiently.
With recent deep neural models like BERT-based
rerankers pushing reranking accuracy to new levels, first-stage retrieval is gradually becoming the
bottleneck in modern search engines.
Typically, the first-stage ranker is a bag-ofwords retrieval model that computes the relevance
score with heuristics defined over the lexical overlap between queries and documents. Lexical
retrieval models such as Okapi BM25 (Robertson and Walker, 1994) had remained state-of-theart for decades and are still widely used today.
Though successful, lexical retrieval struggles to
understand queries and documents beyond surface
1

Code and data will be released upon acceptance.

umass croft
search
embedding vectors

Lex.
index
Emb.
index

âˆª

âœ”
UMass professor W.
Bruce Croft â€¦
information retrieval â€¦
âœ˜ UMass professor
James Allan â€¦
dialogue â€¦

Figure 1: An illustration of D UAL RM. The lexical
retrieval model fails to match â€œsearch/information retrievalâ€. The neural embedding retrieval can soft-match
â€œsearch/information retrievalâ€, but may also generalize â€œCroft/Allenâ€ (common English surnames) which
is not desired. D UAL RM combines the advantages of
both using a dual retrieval model.

lexical forms, failing when the query and the document mention the same concept using different
words (vocabulary mismatch), or sharing highlevel properties of the text, e.g., topics and language styles.
Recent neural methods enable models to compare text by distributional semantics. State-ofthe-art BERT-based rankers (Nogueira and Cho,
2019), may be cost-prohibitive for first-stage retrieval as it is required to compute the interactions
between every pair of tokens in the input. Prior
work in open-domain question answering (Lee
et al., 2019) has addressed this using embeddingbased retrieval: collapsing the entire query or the
document into a single vector, and simply comparing embedding similarities for retrieval. While
this is more efficient, single embedding vectors
have fixed capacity, and may lose specific wordlevel information which is critical to capture the
query (Salton and McGill, 1984; Guo et al., 2016).
That is, a fully dense representation may hamper
the ability to recognize exact lexical matches: such
representations provide generalization.
This paper proposes a novel Dual Retrieval
Model (D UAL RM) that seeks to combine the best
of both worlds, using distributed semantic from latent embedding representations to complement ex-

plicit lexical representations. D UAL RM incorporates a Siamese framework that uses BERT (Devlin et al., 2019) to encode the query and the document, as well as a BM25 lexical retrieval model.
Importantly, unlike existing techniques that train
embeddings directly for ranking (Zamani et al.,
2018; Chang et al., 2020), D UAL RM explicitly
trains the embedding representation on the residual of lexical retrieval, seeking to let the model
learn to fix the errors made by the lexical model,
rather than re-learning something akin to BM25.
During retrieval, D UAL RM runs two retrieval
models in parallel: (1) lexical retrieval from the
inverted index using the surface form of the query
/ document, and (2) embedding retrieval that uses
the query embedding to find the nearest neighbors
(or items that produce maximum inner products)
in the set of document embeddings. As the embeddings are learned with the lexical retrievalâ€™s
residual, the two retrieval models provide additive
gains, as is shown in our experiments.
Our experiments on a large-scale retrieval
dataset using two distinct query sets show substantial and consistent advantages of D UAL RM over
widely-used bag-of-words retrieval models, recent
deep lexical retrieval models, and a strong BERTbased embedding-only retrieval model. Furthermore, D UAL RMâ€™s initial retrieval provides additive gains to downstream rerankers, improving the
end-to-end accuracy and efficiency.
We also test our modelâ€™s ability to retrieve documents concerning the COVID-19 pandemic against the newly-released CORD-19 corpus (Wang et al., 2020) under a zero-shot setup.
Experimental results demonstrate the effectiveness and generalizability of D UAL RM for lowresource ad hoc retrieval, demonstrating realworld applicability of our approach.

2

Related Work

Traditionally, first-stage retrieval has relied on
term weighting models such as BM25 (Robertson and Walker, 1994), or query likelihood (Lafferty and Zhai, 2001), and have explored extensions such as ğ‘›-grams (Metzler and Croft,
2005), controlled vocabularies (Rajashekar and
Croft, 1995), and query expansion (Lavrenko and
Croft, 2001). Bag-of-words representations can
also be improved with recent machine learning
techniques, e.g., by employing machine-learned
query expansion on bag-of-sparse-features (Yao

et al., 2013; Chen and Van Durme, 2017), adjusting termsâ€™ weights (Dai and Callan, 2020b)
with BERT (Devlin et al., 2019), or adding terms
to the document with sequence-to-sequence models (Nogueira et al., 2019). However, these approaches still use the lexical retrieval framework
and may fail to match at a higher semantic level.
Neural models excel at semantic matching with
the use of distributed representations. Neural models for IR can be classified into two
groups (Guo et al., 2016): representation- and
interaction-based models. Representation-based
models learn vector encodings of queries and documents and use a simple scoring function (e.g.,
cosine or inner product) to measure their relevance. Interaction-based approaches, on the other
hand, models the interactions between pairs of
words in queries and documents. Interactionbased approaches were shown to be more effective (Guo et al., 2016; Qiao et al., 2019), but
are cost-prohibitive for first-stage retrieval as the
document-query interactions must be computed
online. For first-stage retrieval, recent research focuses on representation-based models.
Representation-based neural retrieval models
can be traced back to efforts such as LSI (Deerwester et al., 1990), Siamese networks (Bromley
et al., 1993), and MatchPlus (Caid et al., 1995).
Recent research investigated using modern deep
learning techniques to build vector representations: Lee et al. (2019) and Guu et al. (2020)
used BERT-based retrieval to find passages for
QA. Chang et al. (2020) proposes a set of pretraining tasks for sentence retrieval. The majority of embedding-based retrieval use dense vector representations, where it is reduced to a ğ¾nearest neighbor (KNN), or maximum inner product search (Shrivastava and Li, 2014, MIPS) problem. An alternative is to use sparse embeddings
with inverted indices (Salakhutdinov and Hinton,
2009; Zamani et al., 2018).
Although representation-based retrieval using
embeddings has achieved great success on several
NLP tasks, their effectiveness for standard ad-hoc
search is mixed (Guo et al., 2016; Zamani et al.,
2018). All of the representation-based neural retrieval models inherit the same limitation â€“ they
use a fixed number of dimensions, which incurs
the specificity vs. exhaustiveness trade-off found
in all controlled vocabularies (Salton and McGill,
1984). There exist a few studies that consider

combining semantic matching with lexical matching (Guo et al., 2016; Mitra et al., 2017), but they
all use sophisticated models and are exclusive to
the reranking stage. To the best of our knowledge, ours is the first work that investigates jointly
training latent embeddings and lexical retrieval for
first-stage ad hoc retrieval.

3

Proposed Method

D UAL RM consists of a lexical retrieval model and
an embedding retrieval model. Between these two
models, oneâ€™s weakness is the otherâ€™s strength:
lexical retrieval performs exact token matching but
cannot handle vocabulary mismatch; meanwhile,
the embedding retrieval supports semantic matching but loses granular (lexical) information. We
hypothesize that an effective ranking system can
be built by combining the two types of models;
hence we propose a residual-based learning framework that teaches the neural embeddings to be
complementary to the lexical retrieval.
3.1

Lexical Retrieval Model

Lexical retrievers use token overlap information
to score query document pairs. This work uses
BM25 (Robertson and Walker, 1994), a widelyused bag-of-words retrieval model, but it can
also take other lexical retrieval models like vector
space models, query likelihood (Lafferty and Zhai,
2001), or recently proposed machine-learned ones
(Dai and Callan, 2020b; Nogueira et al., 2019).
Given a query ğ‘ and document ğ‘‘, BM25 generates a score with a tf-idf -like scoring function
based on the overlapping words between the pair:2
ğ‘ lex (ğ‘, ğ‘‘) = BM25(ğ‘, ğ‘‘)
âˆ‘ï¸
tf ğ‘¡ ,ğ‘‘
n
o . (1)
=
idf ğ‘¡ Â·
|ğ‘‘ |
ğ‘¡ âˆˆğ‘âˆ©ğ‘‘
tf ğ‘¡ ,ğ‘‘ + ğ‘˜ 1 (1 âˆ’ ğ‘) + ğ‘ avgdl
3.2

Embedding Retrieval Model

The embedding retrieval model encodes an input
query or document sequence into a dense vector.
A dot product between them is used as the similarity measure. The embedding retrieval model
can take various architectures that encode natural
language sequences, as long as the model outputs
a single fixed-length vector for any input.
2

ğ‘¡ is a term, avgdl is the average document length, and ğ‘˜ 1
and ğ‘ are parameters selected from the result of experimentation in the IR community.

This work employs a Transformer (Vaswani
et al., 2017) encoder: we fine-tune BERT (Devlin
et al., 2019) to encode both queries and documents
into vectors in Rğ‘‘ . We employ a Siamese structure, i.e., query and document BERT model parameters are shared to reduce training time memory footprint and storage size. To help the model
differentiate between query and document, we
prepend special token [QRY] to queries and [DOC]
to documents in replacement of the [CLS] token.
For a given query or document, its vector representation of a query vğ‘ or document vğ‘‘ is the meanpool3 of the tuned BERT contextual encoding of
the tokens, including the special [QRY] / [DOC] token. The final neural model score is therefore
ğ‘ emb (ğ‘, ğ‘‘) = vTğ‘ vğ‘‘ .
3.3

(2)

Residual-based Learning

BM25 is static by design: there are no learnable
parameters. Therefore, we propose to optimize the
embedding retrieval model to complement, instead
of to reproduce, the lexical matching model.
We employ a triplet hinge loss (Weston and
Watkins, 1999) for the neural embedding model
over a triplet: a query ğ‘, a relevant document ğ‘‘ + ,
and an irrelevant document ğ‘‘ âˆ’ serving as a negative example (ğ‘š is the margin):
L = [ğ‘š âˆ’ ğ‘ emb (ğ‘, ğ‘‘ + ) + ğ‘ emb (ğ‘, ğ‘‘ âˆ’ )] +

(3)

where [ğ‘¥] + = max{0, ğ‘¥}. In order to train embeddings that complement lexical retrieval, we propose two techniques: sampling negative examples
ğ‘‘ âˆ’ from lexical retrievalâ€™s errors, and adjusting
margin ğ‘š based on lexical retrievalâ€™s residuals.
Error-based Negative Sampling We sample
negative examples (ğ‘‘ âˆ’ in Eq. 3) from those documents mistakenly retrieved by the lexical retrieval
model. For each positive document, we uniformly
sample non-relevant examples from the top ğ‘ documents returned by lexical retrieval with a probability of ğ‘. With such negative samples, the embedding model needs to differentiate relevant documents from confusing ones that are lexically similar to the query but semantically irrelevant.
Residual-based Margin Intuitively, different
query-document pairs require different levels of
3

Our pilot study revealed mean-pooling works slightly
better than using the special [QRY] / [DOC] tokens, as is corroborated with the findings of Reimers and Gurevych (2019).

semantic information for matching. Our negative
sampling strategy does not tell the model the degree of errors made by the lexical retrieval. To
address this challenge, we propose a new residual
margin for the loss function. In particular, we replace the margin ğ‘š in L (Eq. 3) with ğ‘š ğ‘Ÿ , a residual
margin defined on the lexical retrieval:
ğ‘š ğ‘Ÿ = ğœ‰ âˆ’ ğœ†train (slex (ğ‘, ğ‘‘ + ) âˆ’ slex (ğ‘, ğ‘‘ âˆ’ )),

TREC2019
DL Queries

6,980
1
binary

43
95
4-level

Table 1: Characteristics of the two MS MARCO evaluation query sets.

(4)

where ğœ‰ is a constant non-negative value,
slex (ğ‘, ğ‘‘ + )âˆ’slex (ğ‘, ğ‘‘ âˆ’ ) is the residual of the lexical
retrieval, and ğœ†train is a scaling factor that adjusts
the residual. This is akin to the training of structured SVMs (Tsochantaridis et al., 2005), where
the margin is a function in the label space.
When the lexical retrieval model ranks the documents correctly, the residual margin ğ‘š ğ‘Ÿ (Eq. 4)
is small or even becomes negative, and so the neural embedding model receives small or zero gradient updates. When there is a vocabulary mismatch or topic difference, the lexical model may
fail, causing the residual margin to be high and
thereby driving the embedding model to accommodate. The model learns to augment lexical
matching scores rather than reproducing them, so
it can focus on encoding the semantic patterns that
are not captured by text surface forms.
3.4

# of queries
# of rel. doc. per query
Relevance label type

MS MARCO
Dev Queries

Retrieval with D UAL RM

Given a trained embedding model, we may encode
the full set of documents in the corpus and generate an embedding index for maximum inner product search. The lexical retrieval model employs a
typical inverted index.
D UAL RM retrieves from the lexical and embedding index respectively, taking the union of the resulting candidates, and sorts using a final retrieval
score: a weighted average of lexical matching and
neural embedding scores:
ğ‘  D UAL RM (ğ‘, ğ‘‘) = ğœ†test ğ‘ lex (ğ‘, ğ‘‘) + ğ‘ emb (ğ‘, ğ‘‘) (5)
At training and testing time, we use separate
ğœ†train and ğœ†test values due to the train-test gapâ€”
training uses a small set of negative samples, but
testing uses all documents in the collection.
In D UAL RM, lexical retrieval runs fast, taking advantage of inverted indexing, and the embedding retrieval can also scale to millions (potentially billions) of candidates on a modern
GPU through use of approximate nearest-neighbor
(ANN) libraries such as FAISS (Johnson et al.,

2017). As a result, D UAL RM is able to serve the
first-stage, full-collection retrieval.

4

Supervised Retrieval Experiments

We study D UAL RMâ€™s retrieval effectiveness on a
large-scale, supervised retrieval task, and its impact on the downstream reranking pipeline.
Dataset and Metrics We use the MS MARCO
passage ranking dataset (Nguyen et al., 2016),
a widely-used ad-hoc retrieval benchmark with
8.8M passages. The training set contains 0.5M
pairs of queries and relevant passages, where each
query on average has 1 relevant passage4 .
We used two evaluation query sets with different characteristics, as shown in Table 1:
â€¢ MS MARCO Dev Queries is the MS MARCO
datasetâ€™s official dev set, which has been widely
used in prior research (Nogueira and Cho, 2019;
Dai and Callan, 2020b). Most of the queries
have only 1 document judged relevant; the labels are binary. MRR@10 is used to evaluate the
performance on this query set following Nguyen
et al. (2016). We also report recall of the top
1,000 retrieved documents (R@1k), an important metric for first-stage retrieval.
â€¢ TREC2019 DL Queries is the official evaluation query set used in the TREC 2019 Deep
Learning Track shared task (Craswell et al.,
2019). It contains 43 queries that are manually
judged by NIST assessors with 4-level relevance
labels, allowing us to understand the modelsâ€™
behavior on queries with multiple, graded relevance judgments. NDCG@10, MAP@1k and
R@1k are used to evaluate the accuracy on this
query set, following the shared task.
Baselines We use the following first-stage retrieval baselines as well as a state-of-the-art
reranking pipeline:
4 Dataset is available at https://microsoft.
github.io/msmarco/.

â€¢ BM25: The BM25 lexical retrieval model
(Robertson and Walker, 1994) is a widely-used
off-the-shelf lexical-based retrieval baseline.
â€¢ DeepCT: DeepCT (Dai and Callan, 2020b) is
a state-of-the-art neural retrieval model. It
uses BERT to estimate term importance based
on context; in turn these context-aware term
weights are used by BM25 to replace tf (Eq. 1).
â€¢ BM25+RM3: RM3 (Lavrenko and Croft, 2001)
is a popular query expansion technique. It adds
related terms to the original query to compensate for the vocabulary gap between queries and
documents. BM25+RM3 has been proven to be
a strong IR baseline (Lin, 2018).
â€¢ DeepCT+RM3: Dai and Callan (2020a) shows
that using DeepCT term weights with RM3 can
further improve upon BM25+RM3.
â€¢ BM25+BERT reranker: this is a state-of-theart pipelined retrieval system. It uses BM25 for
first-stage retrieval, and reranks the top 1,000
documents using a BERT reranker (Nogueira
and Cho, 2019). Both the BERT- BASE and
the BERT- LARGE reranker provided by Nogueira
and Cho (2019) are explored. Note that BERT
rerankers use cross attention between query and
document tokens, rendering it inefficient, limiting its use only in the reranking stage.
Proposed methods We experiment with the following models:
â€¢ Neural only: This is BERT-based embedding retrieval model without any explicit lexical matching signals, as described in Â§3.2. Note
that although BERT embedding retrieval models
have been tested on several sentence-level tasks
(Reimers and Gurevych, 2019; Chang et al.,
2020) and QA tasks (Lee et al., 2019), their
effectiveness for ad hoc passage/document retrieval remains to be studied.
â€¢ D UAL RM: Our proposed retrieval model.
â€¢ D UAL RM +BERT reranker: a pipelined retrieval system that uses D UAL RM for firststage retreieval, and then followed by a BERT
reranker ( BERT- BASE or BERT- LARGE reranker
from (Nogueira and Cho, 2019)).
Setup Lexical retrieval baselines, including BM25,
BM25+RM3,
DeepCT and
DeepCT+RM3, build upon Anserini (Yang
et al., 2017). We set ğ‘˜ 1 and ğ‘ in BM25 and

DeepCT using values recommended by Dai and
Callan (2020b), which show stronger performance
than the default values. The hyper-parameters in
RM3 are searched through a simple parameter
sweep using 2-fold cross-validation in terms of
MRR@10 and NDCG@10; the hyper-parameters
include the number of feedback documents and
the number of feedback terms (both searched over
{5, 10, Â· Â· Â· , 50}), and the feedback coefficient
(searched over {0.1, 0.2, Â· Â· Â· , 0.9}).
Our neural models were built on top of the
HuggingFace (Wolf et al., 2019) implementation
of BERT. We initialized our models with BERTBASE - UNCASED , as our hardware did not allow
fine-tuning BERT- LARGE models. We used the
negative samples provided by the MS MARCO
dataset, which are uniform-sampled from the top
1,000 documents retrieved by BM25. We set ğœ‰ = 1
in Eq. 4. We fixed ğœ†train = 0.1 in the experiments.
For ğœ†test , we searched over {0, 1, 0.2, Â· Â· Â· , 0.9} on
500 training queries, finding 0.5 to be the most
robust. Models are trained using the Adam optimizer (Kingma and Ba, 2015) with learning rate
2 Ã— 10âˆ’5 , and batch size 28. We trained one epoch
with 4M triplets on one RTX 2080 Ti GPU; using
more training data did not improve performance.
Statistical significance was tested using the permutation test with ğ‘ < 0.05.
4.1

Retrieval Accuracy of D UAL RM

We examine whether D UAL RM improves firststage retrieval performance over baseline retrieval
models (see Table 2).
D UAL RM vs. lexical retrieval D UAL RM surpasses the performance of unsupervised first-stage
retrieval methods BM25 and BM25+RM3, and supervised BERT-based term weighting approaches
DeepCT and DeepCT+RM3. D UAL RM surpasses
non-trained lexical retrieval baselines by large
margins in both recall-oriented metrics (R@1k
and MAP@1k) as well as precision-oriented ones
(MRR@10 and NDCG@10). Its superior performance against DeepCT models shows that supervised term-weighting lexical retrieval is still limited by the strict term matching scheme, demonstrating D UAL RMâ€™s advantages from using embeddings for semantic-level soft matching.
D UAL RM vs. Neural-only embedding retrieval
The effectiveness of neural-only retrieval for standard ad-hoc retrieval remained to be studied. We

MS MARCO Dev

Model

MRR@10

BM25
2 BM25+RM3
3 DeepCT
4 DeepCT+RM3
1

5

Neural only

D UAL RM
âˆ’ Error-based Sampling â‡’ Random Sampling
âˆ’ Residual Margin â‡’ Constant Margin

6

R@1k

TREC2019 DL
NDCG@10

MAP@1k

R@1k

0.166
0.243124
0.23212

0.864
0.861
0.91312
0.91412

0.506
0.5551
0.5511
0.601123

0.3775
0.452135
0.4221
0.481123

0.7385
0.78913
0.7561
0.79413

0.3081âˆ’4

0.928123

0.594123

0.307

0.584

0.3381âˆ’5

0.9691âˆ’5

0.6991âˆ’5

0.5111âˆ’5

0.241â†“

0.926â†“

0.553â†“

0.409â†“

0.314â†“

0.955â†“

0.664â†“

0.455â†“

0.8121âˆ’5
0.779â†“
0.794

0.1912

Table 2: First-stage retrieval effectiveness of D UAL RM on the MS MARCO passage ranking dataset, evaluated
using two query evaluation sets, with ablation studies. Superscripts 1â€“6 indicate statistically significant improvements over methods indexed on the left. â†“ indicates a number being statistically significantly lower than D UAL RM.

Ablation Studies We run ablation studies to
study the impacts of the proposed residual-based
embedding learning approach, by (1) replacing the
error-based negative samples with random negative samples, and (2) replacing the residual margin in the loss function with a constant margin.
Using random negative samples leads to a substantial drop in D UAL RMâ€™s retrieval accuracy (see Table 2), showing that it is important to train the embeddings on the mistakenly-retrieved documents
from lexical retrieval in order to make the two retrieval models additive. Using constant margins
instead of residual margins also lowers the performance of the original D UAL RM model. By enforcing a residual margin explicitly, the embedding model is forced to learn to compensate for the
lexical retrieval, leading to improved performance.
4.2

Reranking on D UAL RM Output

State-of-the-art retrieval pipelines first fetch an
initial set of documents from the collection, fol-

0.36

0.9

0.34

MRR@10

0.8

Recall

find that embedding-only retrieval may not be sufficient as is shown in Table 2. Embedding-only
retrieval is effective on the MS MARCO Dev
queries; but on the TREC2019 DL set, it performs
worse than the untrained BM25 baseline in terms
of MAP@1k and recall. As shown in Table 1,
the main difference between the two query sets is
that MS MARCO Dev queries have only one relevant document per query, while each TREC2019
DL query has multiple documents with graded levels of relevance. The results indicate that a pureneural retriever is able to find those strongly relevant documents, but is likely to miss other less
relevant candidates with diverse patterns.

0.7
0.6
0.5
0.4

0

System
BM25
DualRM w/o reranking
250 500 750 1000

K

(a) Retrieval Recall

0.32
0.30
0.28
0

System
BM25+BERT reranker
DualRM+BERT reranker
DualRM w/o reranking
250 500 750 1000

K

(b) Reranking Accuracy

Figure 2: Impact of D UAL RM on downstream
rerankers with MS MARCO Dev queries. The system
uses the BERT- BASE reranker to rerank top ğ¾ documents retrieved by BM25 or D UAL RM.

lowed by a neural reranker to improve the ranking.
A most widely-used first-stage retriever is BM25.
We investigate the impact of replacing BM25 with
D UAL RM on downstream rerankers.
Figure 2a compares the recall of the top ğ¾ documents retrieval by BM25 and D UAL RM. D U AL RM had higher recall values at all depths,
showing that it is able to provide more relevant passages to the reranker. Figure 2b shows
the performance of a BERT reranker (Nogueira
and Cho, 2019) applied to the top ğ¾ documents
retrieved from either BM25 or D UAL RM. The
accuracy of D UAL RM without reranking is already close to BM25+BERT- BASE reranker. BERT
rerankers have very high computational costs;
D UAL RM substantially narrows the gap between
full-collection retrieval and costly reranking systems. When adding a BERT reranker, D UAL RM
pipelines outperforms the BM25 pipelines; the results are further reported in detail in Table 4. Importantly, the required re-ranking depth decreased

Query

Document retrieved by D UAL RM

BM25

D UAL RM
â†’ Rerank

who is robert gray

Grey started ... dropping his Robert Gotobed alias and using
his birthname Robert Grey.

not
retrieved

rank 496
â†’ rank 7

what is theraderm
used for

A thermogram is a device which measures heat through use of
picture ....

not
retrieved

rank 970
â†’ rank 8

what is the daily life
of thai people

Activities of daily living include are the tasks that are required
to get going in the morning ... 1 walking. 2 bathing. 3 dressing.

not
retrieved

rank 515
â†’ rank 7

Table 3: Examples of documents retrieved by D UAL RM that confuses the BERT reranker. D UAL RM â†’ Rerank
indicates the documentâ€™s initial ranking in D UAL RM, and its ranking after reranked by BERT- LARGE reranker.
Words that are wrongly matched or missed are marked in red, indicating errors of the reranker.

BERT

Dev

TREC

Reranker

MRR@10

NDCG@10

ğ¾

-

0.191
0.3381

0.506
0.6991

-

BM25
4 D UAL RM

BASE

0.3451
0.360123

0.7071
0.71912

1k
20

BM25
6 D UAL RM

LARGE

0.370123
0.3801âˆ’5

0.737123
0.7521âˆ’5

1k
100

Retriever
1
2

BM25
D UAL RM

3

5

BASE

LARGE

Table 4: Comparing D UAL RM and the state-of-the-art
BM25+BERT Reranker pipeline on the MS MARCO
passage ranking dataset with two evaluation sets (Dev:
MS MARCO Dev queries; TREC: TREC2019 DL
queries). Superscripts 1â€“6 indicate statistically significant improvements over the corresponding methods.

from ğ¾=1,000 to ğ¾=20 for BERT- BASE reranker
and ğ¾=100 for BERT- LARGE reranker, reducing
the computational cost by 10Ã—â€“50Ã—. In other
words, D UAL RM generates strong initial rankings that help state-of-the-art rerankers to achieve
higher accuracy with lower computational costs.
Finally, we investigate why reranking more documents retrieved by D UAL RM does not make
much difference. Even as the recall increases
with more reranked documents, the best ranking accuracy is achieved at a small ğ¾ (Figure 2).
We hypothesized that D UAL RM retrieves certain new documents that may confuse the BERT
reranker. This is confirmed through a case study,
as shown in Table 3. It lists examples of irrelevant documents that were retrieved by D UAL RM
and then mistakenly ranked at the top by the
BERT reranker. We found that the BERT reranker
made errors on precise term matching, e.g, matching â€œGrayâ€ to â€œGreyâ€, â€œtheradermâ€™ to â€œthermogramâ€, and missing concepts (â€œthai peopleâ€). Previously, these limitations of BERT rerankers are
not obvious because the first-stage uses lexical

retrieval models like BM25, and those confusing documents are not retrieved in the first place.
D UAL RMâ€™s embedding retrieval module brings
those confusing documents into the reranking set;
the reranker mistakenly predicts them as relevant,
ranks them at the very top, and therefore hurts the
final performance.
To summarize, this collection of results shows
that a single round of first-stage retrieval from D U AL RM can perform similarly to a pipelined system with a sophisticated reranker, and that the
gain from D UAL RM is additive to downstream
rerankers, improving end-to-end accuracy, efficiency, ans scalablity. More importantly, our results indicate that with a stronger first-stage retriever, current state-of-the-art BERT rerankers
are no longer sufficient. The BERT reranker, although using sophisticated token-level attentions,
fails to match tokens precisely, leading to an inferior performance on exact term matching, subword
matching, and concept coverage. This provides
new challenges for future research to improve neural rerankers.

5

Zero-shot adaptation to TREC-COVID

We analyze the performance of D UAL RM on the
CORD-19 dataset (Wang et al., 2020) (TRECCOVID ad-hoc search challenge5 ) to test its performance under a zero-shot, cross-domain ad hoc
retrieval scenario with zero training data. CORD19 contains scientific articles related to the recent
COVID-19 pandemic. An effective retrieval system can help researchers, clinicians, and policymakers to understand the virus and combat the
pandemic. However, state-of-the-art neural models may face challenges of lacking training data.
5 https://ir.nist.gov/covidSubmit/
index.html

We follow the settings of TREC-COVIDâ€™s
Round 2. The document set is the 5/1/2020 version of CORD-19, roughly containing 60k articles
related to COVID-19. We use the Round 2 relevance judgments for evaluation, the query set of
which contains 35 queries. Similar to TREC2019
DL queries, these queries are judged by NIST
assessors with multiple relevant documents and
graded labels. No training set is available.

Table 5: Zero-shot results on TREC-COVID Round 2.
Again, superscripts 1â€“4 indicate statistically significant
improvements over the corresponding methods.

Zero-Shot Adaptation We follow MacAvaney
et al. (2020) and initialize D UAL RM with SciBERT (Beltagy et al., 2019) instead of BERT. Additionally, we fine-tune the masked language model
of SciBERT on the CORD-19 documents for 10
epochs, to pick up COVID-19 specific vocabularies. D UAL RM is trained with 160k examples from
the MS MARCO passage ranking dataset. Lexical retrieval during inference adopted the Anserini
BM25 fusion results using multiple document
fields, which show stronger performance than the
original BM25 on TREC-COVID6 . For the neural embedding part, only the title and abstract are
used due to their brevity; the two fields are concatenated and encoded into a single vector. The
trained D UAL RM is used on TREC-COVID without any in-domain gradient update.

learned by D UAL RM are transferable, and can
complement the lexical retrieval under an out-ofdomain scenario. On the other hand, zero-shot
neural-only retriever is significantly worse than
BM25, indicating that embeddings alone are less
transferable to other domains. Notably, zero-shot
D UAL RM outperforms GUIR-S2-run1, the best
automatic run that uses a computation-expensive
BERT reranker and pseudo-relevance feedback
procedures. In contrast, D UAL RM only performed a single round of retrieval without reranking. These results demonstrate the generalizability
of D UAL RM under low-resource, cross-domain
settings, showing promise for rapidly building effective and efficient retrieval systems for new domains with limited training data.

Baselines We include the following baselines
for analysis: BM25 (fusion), which is the same
BM25 with multi-field fusion method as in the lexical part of D UAL RM; ZS Neural only, a pure
neural embedding retriever using the same zeroshot adaptation settings as D UAL RM; and the
best automatic run7 submitted to TREC-COVID
Round 2. The run is named GUIR-S2-run1 which
used BM25 initial retrieval, SciBERT reranking,
pseudo-relevance feedback, and a fusion of these
rankings to further improve the accuracy.8
Results Results are shown in Table 5. ZeroShot D UAL RM substantially outperforms BM25
(fusion), demonstrating that the semantic patterns
6 Document contain 3 fields: title, abstract, and body. The
fusion method computes BM25 on these fields respectively
and fuses under equal weights. Detailed information can
be found at https://ir.nist.gov/covidSubmit/
archive/round2/r2.fusion2.pdf
7 Non-automatic runs modified queries or models using relevance feedback from the previous run or with manual modification, hence are not included for comparison.
Information can be found at https://ir.nist.gov/
covidSubmit/archive/archive-round2.html.
8 Detailed information for GUIR-S2-run1 can be found at
https://ir.nist.gov/covidSubmit/archive/
round2/GUIR_S2_run1.pdf

Model

P@5

NDCG@10

R@1k

BM25 (fusion)
2 ZS Neural only

0.6802

0.5552

0.468

0.371

0.6472
0.553

3

ZS D UAL RM

0.76012

0.636124

0.716124

4

GUIR-S2-run1

0.74912

0.62512

0.66012

1

6

Conclusion

Classic lexical retrieval models struggle to understand the underlying meanings of queries and documents. Neural embedding based retrieval models
can soft match queries and documents, but they
lose specific word-level matching information.
We present D UAL RM, a Dual Retrieval Model
that complements lexical retrieval with distributed
embedding retrieval. Importantly, instead of a linear interpolation of two models, the embedding retrieval in D UAL RM is exactly trained to fix the errors of lexical retrieval, providing additive gains.
Experiments show that D UAL RM achieves the
new state-of-the-art first-stage retrieval effectiveness on two distinct evaluation sets, outperforming classic bag-of-words and recent deep lexical
retrieval models, and a BERT-based pure neural
retrieval model. Additionally, D UAL RM outperformed the most recent TREC-COVID experiment
under zero-shot setup, demonstrating its effectiveness for ad hoc retrieval even applied to a crossdomain setting. These strong results point towards
building efficient and scalable neural retrieval systems in the future.

References
Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. Scibert: A pretrained language model for scientific text.
In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and
the 9th International Joint Conference on Natural
Language Processing, pages 3613â€“3618.
Jane Bromley, Isabelle Guyon, Yann LeCun, Eduard
SaÌˆckinger, and Roopak Shah. 1993. Signature verification using a siamese time delay neural network.
In Advances in Neural Information Processing Systems 6, pages 737â€“744.
William R. Caid, Susan T. Dumais, and Stephen I. Gallant. 1995. Learned vector-space models for document retrieval. Inf. Process. Manag., 31(3):419â€“
429.
Wei-Cheng Chang, Felix X. Yu, Yin-Wen Chang, Yiming Yang, and Sanjiv Kumar. 2020. Pre-training
tasks for embedding-based large-scale retrieval. In
8th International Conference on Learning Representations.
Tongfei Chen and Benjamin Van Durme. 2017. Discriminative information retrieval for question answering sentence selection. In Proceedings of the
15th Conference of the European Chapter of the Association for Computational Linguistics, pages 719â€“
725.
Nick Craswell, Bhaskar Mitra, Emine Yilmaz, and
Daniel Campos. 2019. Overview of the TREC 2019
deep learning track. In TREC (to appear).
Zhuyun Dai and Jamie Callan. 2020a. Context-aware
document term weighting for ad-hoc search. In
WWW â€™20: The Web Conference 2020, pages 1897â€“
1907.
Zhuyun Dai and Jamie Callan. 2020b. Context-aware
term weighting for first-stage passage retrieval. In
The 42nd International ACM SIGIR Conference on
Research and Development in Information Retrieval
(to appear).
Scott C. Deerwester, Susan T. Dumais, Thomas K. Landauer, George W. Furnas, and Richard A. Harshman.
1990. Indexing by latent semantic analysis. J. Am.
Soc. Inf. Sci., 41(6):391â€“407.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: pre-training of
deep bidirectional transformers for language understanding. pages 4171â€“4186.
Jiafeng Guo, Yixing Fan, Qingyao Ai, and W. Bruce
Croft. 2016. A deep relevance matching model for
ad-hoc retrieval. In Proceedings of the 25th ACM International Conference on Information and Knowledge Management, pages 55â€“64.
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. 2020.
REALM:

retrieval-augmented language model pre-training.
CoRR, abs/2002.08909.
Jeff Johnson, Matthijs Douze, and HerveÌ JeÌgou. 2017.
Billion-scale similarity search with GPUs. CoRR,
abs/1702.08734.
Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In 3rd International Conference on Learning Representations.
John D. Lafferty and ChengXiang Zhai. 2001. Document language models, query models, and risk minimization for information retrieval. In SIGIR 2001:
Proceedings of the 24th Annual International ACM
SIGIR Conference on Research and Development in
Information Retrieval, pages 111â€“119.
Victor Lavrenko and W. Bruce Croft. 2001. Relevancebased language models. In SIGIR 2001: Proceedings of the 24th Annual International ACM SIGIR
Conference on Research and Development in Information Retrieval, pages 120â€“127.
Kenton Lee, Ming-Wei Chang, and Kristina Toutanova.
2019. Latent retrieval for weakly supervised open
domain question answering. In Proceedings of the
57th Conference of the Association for Computational Linguistics, pages 6086â€“6096.
Jimmy Lin. 2018. The neural hype and comparisons
against weak baselines. volume 52, pages 40â€“51.
Sean MacAvaney, Arman Cohan, and Nazli Goharian.
2020. SLEDGE: A simple yet effective baseline
for coronavirus scientific knowledge search. CoRR,
abs/2005.02365.
Donald Metzler and W. Bruce Croft. 2005. A markov
random field model for term dependencies. In SIGIR
2005: Proceedings of the 28th Annual International
ACM SIGIR Conference on Research and Development in Information Retrieval, pages 472â€“479.
Bhaskar Mitra, Fernando Diaz, and Nick Craswell.
2017. Learning to match using local and distributed
representations of text for web search. In Proceedings of the 26th International Conference on World
Wide Web, pages 1291â€“1299.
Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao,
Saurabh Tiwary, Rangan Majumder, and Li Deng.
2016. MS MARCO: A human generated machine
reading comprehension dataset. In Proceedings
of the Workshop on Cognitive Computation: Integrating neural and symbolic approaches 2016 colocated with the 30th Annual Conference on Neural
Information Processing Systems.
Rodrigo Nogueira and Kyunghyun Cho. 2019. Passage
re-ranking with bert. arXiv:1901.04085.
Rodrigo Nogueira, Wei Yang, Jimmy Lin, and
Kyunghyun Cho. 2019. Document expansion by
query prediction. CoRR, abs/1904.08375.

Yifan Qiao, Chenyan Xiong, Zheng-Hao Liu, and
Zhiyuan Liu. 2019. Understanding the behaviors of
BERT in ranking. CoRR, abs/1904.07531.
T. B. Rajashekar and W. Bruce Croft. 1995. Combining
automatic and manual index representations in probabilistic retrieval. J. Am. Soc. Inf. Sci., 46(4):272â€“
283.
Nils Reimers and Iryna Gurevych. 2019. Sentencebert: Sentence embeddings using siamese bertnetworks. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 3980â€“3990.
Stephen E. Robertson and Steve Walker. 1994. Some
simple effective approximations to the 2-poisson
model for probabilistic weighted retrieval. In Proceedings of the 17th Annual International ACMSIGIR Conference on Research and Development in
Information Retrieval, pages 232â€“241.
Ruslan Salakhutdinov and Geoffrey E. Hinton. 2009.
Semantic hashing.
Int. J. Approx. Reason.,
50(7):969â€“978.
Gerard Salton and Michael McGill. 1984. Introduction to Modern Information Retrieval. McGraw-Hill
Book Company.
Anshumali Shrivastava and Ping Li. 2014. Asymmetric LSH (ALSH) for sublinear time maximum inner
product search (MIPS). In Advances in Neural Information Processing Systems 27, pages 2321â€“2329.
Ioannis Tsochantaridis, Thorsten Joachims, Thomas
Hofmann, and Yasemin Altun. 2005. Large margin methods for structured and interdependent output variables. J. Mach. Learn. Res., 6:1453â€“1484.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural
Information Processing Systems 2017, pages 5998â€“
6008.
Lucy Lu Wang, Kyle Lo, Yoganand Chandrasekhar,
Russell Reas, Jiangjiang Yang, Darrin Eide, Kathryn
Funk, Rodney Kinney, Ziyang Liu, William Merrill, Paul Mooney, Dewey Murdick, Devvret Rishi,
Jerry Sheehan, Zhihong Shen, Brandon Stilson,
Alex D. Wade, Kuansan Wang, Chris Wilhelm, Boya
Xie, Douglas Raymond, Daniel S. Weld, Oren Etzioni, and Sebastian Kohlmeier. 2020. CORD19: the Covid-19 open research dataset. CoRR,
abs/2004.10706.
Jason Weston and Chris Watkins. 1999. Support vector machines for multi-class pattern recognition. In
ESANN 1999, 7th European Symposium on Artificial
Neural Networks, pages 219â€“224.

Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, ReÌmi Louf, Morgan Funtowicz, and Jamie Brew. 2019. Huggingfaceâ€™s transformers: State-of-the-art natural language processing. CoRR, abs/1910.03771.
Peilin Yang, Hui Fang, and Jimmy Lin. 2017. Anserini:
Enabling the use of lucene for information retrieval
research. In Proceedings of the 40th International
ACM SIGIR Conference on Research and Development in Information Retrieval, pages 1253â€“1256.
Xuchen Yao, Benjamin Van Durme, and Peter Clark.
2013. Automatic coupling of answer extraction and
information retrieval. In Proceedings of the 51st Annual Meeting of the Association for Computational
Linguistics, pages 159â€“165.
Hamed Zamani, Mostafa Dehghani, W. Bruce Croft,
Erik G. Learned-Miller, and Jaap Kamps. 2018.
From neural re-ranking to neural ranking: Learning a sparse representation for inverted indexing. In
Proceedings of the 27th ACM International Conference on Information and Knowledge Management,
pages 497â€“506.

