arXiv:2006.06373v1 [stat.ME] 11 Jun 2020

The Limits to Learning an SIR Process: Granular
Forecasting for Covid-19

Jackie Baek
MIT
baek@mit.edu
Tianyi Peng
MIT
tianyi@mit.edu

Vivek F. Farias
MIT
vivekf@mit.edu

Andreea Georgescu
MIT
andreeag@mit.edu

Deeksha Sinha
MIT
deeksha.sinha7@gmail.com

Retsef Levi
MIT
retsef@mit.edu

Joshua Wilde
MIT
jtwilde@mit.edu

Andrew Zheng
MIT
atz@mit.edu

Abstract
A multitude of forecasting efforts have arisen to support management of the
ongoing COVID-19 epidemic. These efforts typically rely on a variant of the SIR
process [1] and have illustrated that building effective forecasts for an epidemic in
its early stages is challenging. This is perhaps surprising since these models rely
on a small number of parameters and typically provide an excellent retrospective
fit to the evolution of a disease. So motivated, we provide an analysis of the
limits to estimating an SIR process. We show that no unbiased estimator can
hope to learn this process until observing enough of the epidemic so that one
is approximately two-thirds of the way to reaching the peak for new infections.
Our analysis provides insight into a regularization strategy that permits effective
learning across simultaneously and asynchronously evolving epidemics. This
strategy has been used to produce accurate, granular predictions for the COVID-19
epidemic that has found large-scale practical application in a large US state.

1

Introduction

The so-called Susceptible-Infected-Recovered (SIR) model, proposed nearly a century ago [2],
remains a cornerstone for the forecasting of epidemics. In numerous retrospective studies the model
has been found to fit the trajectory of epidemics well, while simultaneously providing a meaningful
level of interpretability. As such, the plurality of models used for forecasting efforts related to the
COVID-19 epidemic are either SIR models or close cousins thereof. Surprisingly, the experience
with these forecasts has illustrated that predicting the cumulative number of cases (or peak number of
cases) in an epidemic, early in its course, is a challenging task.
Ultimately, this paper is motivated by producing high quality forecasts for the progression of epidemics such as COVID-19. However, we begin with a fundamental question that, despite the SIR
modelâ€™s prevalence, has apparently not been asked: What are the limits of learning in epidemic
models such as the SIR model? Surprisingly, we find that it is fundamentally difficult to predict
the cumulative number of infections (or the â€˜peakâ€™ of an infection) until quite late in an epidemic.
In particular, we show that no estimator can hope to learn these quantities until observing enough
of the epidemic so that one is approximately two-thirds of the way to reaching the peak for new
infections. As far as we can tell, this result is the first of its kind for epidemic modeling. We find
that this hardness is driven by uncertainty in the initial prevalence of the infection in the population,
which is not observable with incomplete testing and/ or asymptomatic patients. On the other hand, we
show that certain other important parameters in the SIR model â€“ including the so-called reproduction
number and infectious period â€“ are actually easy to learn.
Preprint. Under review.

Our analysis on the limits of learning above suggests a specific regularization approach that dramatically impacts forecast performance when learning across regions. In greater detail, we consider
a differentiable model, where the true infection curve is latent and follows SIR dynamics. The
observations are the results of limited testing, which censors the true infection curve, and deaths. Our
learning rate analysis suggests that by suitably â€˜matchingâ€™ regions early in the epidemic to regions
that are further along, we can build useful estimators of initial disease prevalence in the former
regions. This matching is effectively enabled through a regularization strategy in our approach where
certain region specific random effect terms are regularized to zero.
We demonstrate that our approach allows us to learn granular epidemic models (essentially at the
level of individual counties) that are substantially more accurate than a highly referenced benchmark
[3]. As such, our model has found practical application in a large US state. Among other uses, the
forecasts inform decisions related to providing surge capacities in hospitals across the state; such
decisions necessitate a granular forecast.
Related Literature: Extant epidemiological models are typically compartmental models, of which
the SIR model [2] is perhaps the best known. The plurality of COVID-19 modeling efforts are
founded on SIR-type models, eg. [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]. Some of these efforts consider
generalizations to the SIR model that add additional states or â€˜compartmentsâ€™. For instance the
(retrospectively fit) model studied in [6] considers an eight state model (as opposed to three for the
vanilla SIR). Not surprisingly, learning gets harder with as the number of states increases [15].
Whereas the identifiability of the SIR model (in a deterministic sense) is well understood [16], this is
not the case for the natural stochastic variant of this model. Specifically, calibrating a vanilla SIR
model to data requires learning the so-called infectious period and basic reproduction rate. Both
these parameters are relatively easy to calibrate with limited data; this is supported both by the
present paper, but also commonly observed empirically; see for instance [15]. In addition to these
parameters, however, one needs to measure both the initial number of infected individuals and the
size of the susceptible population. Estimating the number of infected individuals poses a challenge
in the presence of limited testing and asymptomatic carriers. Indeed, epidemiological models for
COVID-19 typically assume that measured infections are some fraction of true infections; eg. [4, 6].
This challenge is closely related to that of measuring the true fraction of cases that lead to fatalities
(or the so-called Infection Fatality Rate) [17].
Our main theorem shows that having to learn the true initial prevalence of the infection presents
a fundamental difficulty to learning SIR models with limited data. Our theoretical results are
complemented by empirical findings in the literature, see [18, 19]. Our result is the first to characterize
the limits to learning an SIR process [1] and relies on a non-trivial application of the Cramer-Rao
bound.
Finally, on the empirical front, we compare the quality of our predictions to publicly available
predictions for the IHME model [3]: this highly publicized benchmark model has a large number
of forecast vintages concurrently available which makes possible an analysis of relative prediction
accuracy as a function of available data.

2

The Deterministic SIR Model and a Stochastic SIR Process

We begin by describing the deterministic SIR model. Let s(t), i(t) and r(t) be the size of the
susceptible, infected and recovered populations respectively, as observed at time t. The SIR model is
defined by the following system of ODEs, specified by the tuple of parameters (Î±, Î², Î³):
ds
s
di
s
dr
= âˆ’Î²
i,
=Î²
i âˆ’ Î³i,
= Î³i.
(1)
dt
Î±N
dt
Î±N
dt
The rate of recovery is specified by Î³ > 0; 1/Î³ is frequently referred to as the infectious period.
Î² > 0 quantifies the rate of transmission; Î²/Î³ , R0 is also referred to as the basic reproduction
number. N is the total population (assumed known). The typical exposition of this model sets Î± = 1
corresponding to the setting where the infection is entirely observed. On the other hand, the quantity
Î± âˆˆ (0, 1] corresponds to the fraction of true infections we actually observe. The role of Î± is made
clear by the following Proposition:
Proposition 2.1. Let {(s0 (t), i0 (t), r0 (t)) : t â‰¥ 0} be a solution to (1) for parameters Î± =
1, Î² = Î² 0 , Î³ = Î³ 0 and initial conditions i(0) = i0 (0), s(0) = s0 (0). Then, for any Î±0 > 0,
2

{(Î±0 s0 (t), Î±0 i0 (t), Î±0 r0 (t)) : t â‰¥ 0} is a solution to (1) for parameters Î± = Î±0 , Î² = Î² 0 , Î³ = Î³ 0 and
i(0) = Î±0 i0 (0), s(0) = Î±0 s0 (0).
The SIR model described above is identifiable if i(t) is observable, and N is known. Specifically:
Proposition 2.2. Fix N , and let i(t) be observed over some open set in R+ . Then the parameters
(Î±, Î², Î³) are identifiable.
A self-contained proof follows immediately from the identity theorem; a more involved argument
based on identification results for non-linear systems can be found in [16].
Stochastic SIR Process: Of course, any real world model must incorporate noise, and we describe
next a natural continuous-time Markov chain variant of the deterministic model described above,
proposed by [1]. Specifically, the stochastic SIR process, {(S(t), I(t), R(t)) : t â‰¥ 0}, is a multivariate counting process, with RCLL paths, determined by the parameters (Î±, Î², Î³). The jumps in this
process occur at rate Î»(t) (to be defined) and correspond either to a new observed infection (where
I(t) increments by one, and S(t) decrements by one) or to a new observed recovery (where I(t)
decrements by one, and R(t) increments by one). Let C(t) = I(t) + R(t) denote the cumulative
number of infections observed up to time t. Denote by tk the time of the kth jump, and let Tk be the
time between the k âˆ’ 1st and kth jumps. Finally, let Ik , I(tk ), and similarly define Rk , Sk and Ck .
The SIR process is then completely specified by:


Î²Skâˆ’1
Ck âˆ’ Ckâˆ’1 âˆ¼ Bern
(2)
Î²Skâˆ’1 + Î±N Î³



Î²Skâˆ’1
+ Î³ Ikâˆ’1 .
Tk âˆ¼ Exp
(3)
Î±N
It is well known that solutions to the deterministic SIR model (1) provide a good approximation to
sample paths of the SIR process (described by (2), (3)) in the so-called fluid regime; see [20].
The next section analyzes the rate at which one may hope to learn the unknown parameters (Î±, Î², Î³)
as a function of k; our key result will illustrate that in large systems, Î± is substantially harder to learn
than Î² or Î³. In fact, an approximation suggests that the time taken to learn this parameter accurately
will be approximately two-thirds of the time required to hit the peak for infections. In Section 4
we will consider a differentiable model inspired by the SIR process and show that a regularization
strategy motivated by our learning analysis yields material performance gains.

3

Limits to Learning

This section characterizes the rate at which one may hope to learn the parameters of a stochastic SIR
process, simply from observing the process.
Observations: Define the stopping time Ï„ = inf{k : Ik = 0}; clearly Ï„ is bounded. For clarity,
when k > Ï„ , we define Ck = Ckâˆ’1 , Ik = 0, and Tk = âˆž. We define the k-th information set
Ok = (I0 , R0 , T1 , C1 , . . . , Tk , Ck ) for all k â‰¥ 1. Note that Ik and Rk are deterministic given Ck , I0 ,
and R0 .
We next characterize a sequence of systems of increasing size. Specifically, consider a sequence
of SIR processes, indexed by n, such that Î±n Nn â†’ âˆž while Î²n = Î², Î³n = Î³. Moreover, let
mn = o(Î±n Nn ) and Î² > Î³. Finally, assume that In,0 , Rn,0 â‰¤ mn . Our main theorem characterizes
the Fisher information of Omn relative to Î±n as n grows large.
Theorem 3.1. Assume Î² > Î³ are known and In,0 â‰¥ D, where D is a constant that depends only on
Î² and Î³. Then, the Fisher information of Omn relative to Î±n is


m3n
JOmn (Î±n ) = Î˜
.
(4)
Î±n4 Nn2
Components of the proof of this result are provided in Section 3.1. Now since Î±n âˆˆ (0, 1] and
since all points in this interval are regular in the terminology of [21], we have by the constrained
Cramer-Rao bound (Lemma 4 in [21]):
3

Corollary 3.2 (Constrained Cramer-Rao Bound). Assume Î² > Î³ are known and In,0 â‰¥ D, where D
depends only on Î² and Î³. Let Î±Ì‚n be any unbiased estimator of Î±n based on the observations Omn .
Then,
 4 2
Î±n Nn
var(Î±Ì‚n ) = â„¦
.
(5)
m3n
Interpretation of Corollary 3.2: Now since we know (from the fluid model (1)) that cumulative
and peak infections scale with Î±n Nn , it is reasonable to require that var(Î±Ì‚n ) scale like o(Î±n2 ). For
this, Corollary 3.2 shows that we require mn = Ï‰((Î±n Nn )2/3 ) observations. How long might
this take relative to the point at which, say, the epidemic itself is past its peak? One way of
characterizing such a benchmark is comparing the time it takes to get to (Î±n Nn )2/3 observations
(call this time T1,n ) with the time taken for the instantaneous reproductive number for the epidemic
Rt , Î²Î³ Sn (t)/(Î±n Nn ) to fall below 1 (the expected increment in the infection process is negative

at times when Rt < 1); call this time T2,n . More precisely, T1,n = inf t : Cn (t) â‰¥ (Î±n Nn )2/3
and T2,n = inf {t : Î²Sn (t)/(Î±n Nn ) < Î³}. Unfortunately, characterizing either of these (random)
times exactly in the stochastic SIR process appears to be a difficult taskand so we consider analyzing
d
these times in the deterministic model, where we denote T1,n
= inf t : cn (t) â‰¥ (Î±n Nn )2/3 for
d
the process defined by (1) and similarly T2,n = inf {t : Î²sn (t)/(Î±n Nn ) < Î³}. We have:
Proposition 3.3. If cn (0) = O(log(Î±n Nn )),
d
T1,n
2
â‰¥ .
d
nâ†’âˆž T
3
2,n

lim

In summary, this suggests that the sampling requirements made precise by Corollary 3.2 can only be
met at such time where we are close to reaching the peak infection rate.
We next turn our attention to learning Î² and Î³:
Theorem
q3.4. Assume Î² âˆˆ [0, Î²max ], Î³ âˆˆ [0, Î³max ]. Let Cn,0 , mn , Î±n , Nn satisfy mn (mn +Cn,0 ) â‰¤
Î± N âˆ’m âˆ’Cn,0

mn
Î²
n n
n
â‰¤ 14 and Î²+Î³
Î±n Nn , 5 log
mn
Î±n Nn
and Î³Ì‚mn , both functions of Omn , such that

Î²
> 12 ( Î²+Î³
+ 12 ). Then, there exist estimators Î²Ì‚mn

log mn
2
+ B1 Î²max
eâˆ’B2 In,0 ,
mn
log mn
2
2
+ B1 Î³max
eâˆ’B2 In,0 ,
E[(Î³Ì‚mn âˆ’ Î³)2 ] â‰¤ M2 Î²max
mn
where B1 , B2 > 0 depend only on Î² and Î³ and M1 , M2 > 0 are absolute constants.
2
E[(Î²Ì‚mn âˆ’ Î²)2 ] â‰¤ M1 Î²max

(6)
(7)

In stark contrast with Corollary 3.2, Theorem 3.4 shows that the variance in estimating Î² and Î³ grows
small as mn and In,0 grow, but is independent of Î±n and Nn . Consequently, to achieve any desired
level of accuracy, we simply need the number of events mn and the initial number of infections In,0
to exceed a constant that is independent of the size of the system Nn .
3.1

Proof of Theorem 3.1

Î²
Define p , 12 ( Î²+Î³
+ 12 ) > 12 , and let Pn = Î±n Nn . We fix n large enough so that mn + C0 â‰¤
n âˆ’mn âˆ’C0 )
and Î²(PÎ²(P
> p (this is possible since
n âˆ’mn âˆ’C0 )+Pn Î³
subscript n henceforth.

Î²
Î²+Î³

Pn
2

> p and mn = o(Pn )). We remove the

Define the indicator variable Ek = 1{Ï„ > k} on the event which the SIR process has not terminated
after k jumps. The following lemma states that both Ek and Ik can be determined from Ck , I0 , and
R0 , which will allow us to decouple variables in Om in the analysis of the Fisher information. The
result follows from the definitions of Ï„ , Ek , and Ck ; the details can be found in the Appendix.
0
Lemma 3.5. Define rk , I0 +k+2R
for all k â‰¥ 0. For all k, Ek = 1{Ck > rk }. Moreover, when
2
Ek = 1, Ik = 2Ck âˆ’ k âˆ’ I0 âˆ’ 2R0 > 0.

The next lemma writes an exact expression for JOm (Î±).
4

Lemma 3.6. The Fisher information of the observations Om with respect to the parameter Î± is
#
"
m
2
X
N 2 Ckâˆ’1
Ekâˆ’1 = 1 .
JOm (Î±) =
(8)
Pr(Ekâˆ’1 = 1)E
P 2 (P âˆ’ Ckâˆ’1 )((P âˆ’ Ckâˆ’1 ) + Î²Î³ P )
k=1

Proof. We first define conditional Fisher information and state some known properties.
Definition 3.7. Suppose X, Y are random variables defined on the same probability space whose
âˆ‚
distributions depend on a parameter Î¸. Let gX|Y (x, y, Î¸) = âˆ‚Î¸
log fX|Y ;Î¸ (x|y)2 be the square of the
score of the conditional distribution of X given Y = y with parameter
Î¸ evaluated
at x. Then, the


conditional Fisher information is defined as JX|Y (Î¸) = EX,Y gX|Y (X, Y, Î¸) .
Pn
Property 3.8. JX1 ,...,Xn (Î¸) = JX1 (Î¸) + i=2 JXi |X1 ,...,Xiâˆ’1 (Î¸).
Property 3.9. If X is independent of Z conditioned on Y , JX|Y,Z (Î¸) = JX|Y (Î¸).
Property 3.10. If X is deterministic given Y = y, gX|Y (X, y, Î¸) = 0.
dÎ¸ 2
) .
Property 3.11. If Î¸(Î·) is a continuously differentiable function of Î·, JX (Î·) = JX (Î¸(Î·))( dÎ·

Since I0 and R0 are known and not random, the Fisher information of Om is equal to the Fisher
information of (T1 , C1 , T2 , C2 , . . . , Tm , Cm ). Then, Property 3.8 implies
JOm (Î±) = JT1 (Î±) + JC1 |T1 (Î±) + JT2 |T1 ,C1 (Î±) + JC2 |T1 ,C1 ,T2 (Î±) + Â· Â· Â· + JCM |T1 ,C1 ,...,Tm (Î±).
Note that for any k, Ck and Tk only depend on Ckâˆ’1 . Indeed, since Ckâˆ’1 determines Ekâˆ’1 , if
Ekâˆ’1 = 0 (the stopping time has passed), then Ck = Ckâˆ’1 and Tk = âˆž. When Ekâˆ’1 = 1, the
distributions of Ck and Tk are given in (2)-(3). Since Î², Î³, I0 , R0 are known, Skâˆ’1 = P âˆ’ Ckâˆ’1 ,
and Ikâˆ’1 can be determined from Ckâˆ’1 (Lemma 3.5), the distributions of Ck and Tk are determined
by Ckâˆ’1 . Therefore, we use Property 3.9 to simplify the above equation to
JOm (Î±) =

m
X

(JCk |Ckâˆ’1 (Î±) + JTk |Ckâˆ’1 (Î±)),

k=1

where we used JT1 (Î±) = JT1 |C0 (Î±), JC1 (Î±) = JC1 |C0 (Î±). Moreover, when Ekâˆ’1 = 0, Ck and
Tk are deterministic conditioned on Ckâˆ’1 , which implies the score in this case is 0 (Property 3.10).
Therefore, we can condition on Ekâˆ’1 = 1 to write
JOm (Î±) =

m
X

E[gCk |Ckâˆ’1 (Ck , Ckâˆ’1 , Î±) + gTk |Ckâˆ’1 (Tk , Ckâˆ’1 , Î±)|Ekâˆ’1 = 1] Pr(Ekâˆ’1 = 1).

k=1

The last step is to evaluate gCk |Ckâˆ’1 (Ck , Ckâˆ’1 , Î±) and gTk |Ckâˆ’1 (Tk , Ckâˆ’1 , Î±). When Ekâˆ’1 = 1, the
distributions of Ck and Tk conditioned on Ckâˆ’1 have a simple form provided in (2)-(3). Property 3.11
allows for straight-forward calculations, resulting in (8). See Appendix for details of this last step.
Proof of Theorem 3.1. We show both upper and lower bounds for JOm (Î±) starting from Equation
(8). For the upper bound, we have that Ck â‰¤ k + I0 + R0 by definition. Since I0 , R0 â‰¤ m by
assumption, Ck â‰¤ 3m. Moreover, by assumption, Ck â‰¤ m + C0 â‰¤ P2 . Plugging these into (8)
results in
mâˆ’1
X
N 2 (3m)2
m3
JOm (Î±) â‰¤
Pr(Ekâˆ’1 = 1) 2
â‰¤ H1 4 2 ,
Î³
1
1
Î± N
P (P âˆ’ 2 P )((P âˆ’ 2 P ) + Î² P )
k=0

for a constant H1 . For the lower bound, we first prove a lower bound for Pr(Em = 1).
Lemma 3.12. There exists a constant D that only depends on Î² and Î³ such that if
p and I0 â‰¥ D, then Pr(Em = 1) â‰¥ 12 .

Î²(P âˆ’mâˆ’C0 )
Î²(P âˆ’mâˆ’C0 )+P Î³

>

This result relies on an interesting stochastic dominance argument and can be found in the Appendix.
3
1
Then, similarly to the upper bound, JOm (Î±) â‰¥ H2 Î±m
4 N 2 follows from using Pr(Em = 1) â‰¥ 2 and
the fact that Ck â‰¥ k+I02+2R0 when Ek = 1 (Lemma 3.5). Combining the upper and lower bounds
finish the proof.
5

4

Forecasting in the real world

This section describes a practical SIR model that can incorporate a number of real-world features and
datasets. We then consider an approximation to the MLE estimate for this model. Finally we propose
an approach to overcome the difficulty in learning Î±. We present comprehensive experimental results
at a â€˜regionalâ€™ granularity (essentially close to county level) that illustrate strong relative merits.
Discrete-time SIR Recall the stochastic SIR process, {(S(t), I(t), R(t)) : t â‰¥ 0}, a multi-variate
counting process determined by parameters (Î±, Î², Î³). We now allow Î² to be time-varying, yielding
a counting process with jumps Ck âˆ’ Ckâˆ’1 âˆ¼ Bern {Î²Skâˆ’1 /(Î²k Skâˆ’1 + Î±N Î³)} and rate Î»(t) =
(Î²(t)S(t)/Î±N + Î³) I(t). We obtain discrete-time SIR processes, {(Si [t], Ii [t], Ri [t]) : t âˆˆ N} for
regions i âˆˆ I by considering the Euler-approximation to this counting process (e.g. [22]). Specifically,
let âˆ†I[t] = I[t]âˆ’I[tâˆ’1], and define âˆ†S[t] and âˆ†R[t] analogously. The discrete-time approximation
to the SIR process is then given by:
S
âˆ†Si [t + 1] = âˆ’Î²i [t](Si [t]/Î±i Ni )Ii [t] + Î½i,t
I
âˆ†Ii [t + 1] = Î²i [t](Si [t]/Î±i Ni )Ii [t] âˆ’ Î³Ii [t] + Î½i,t
R
âˆ†Ri [t + 1] = Î³Ii [t] + Î½i,t
S
I
R
where {Î½i,t
}, {Î½i,t
}, {Î½i,t
} are appropriately defined martingale difference sequences.

Model Parameters and Covariates For each region i, the observability parameter Î±i and reproduction factor {Î²i [t] : t âˆˆ N} must be learned from data, but we assume the total population Ni
and recovery rate Î³ are known (a typical assumption; for COVID-19, Î³ âˆ¼ 1/4). Demographic and
mobility factors influence the reproduction rate of the disease. To model these effects, we estimate
Î²i [t] as a mixed effects model incorporating these covariates: Î²i [t] = exp(Xi [t]> Î¸) + i , where
Xi [t] = (Zi , Mi [t]) âˆˆ Rd1 +d2 is a set of observed covariates for region i partitioned into static
Zi âˆˆ Rd1 and time-varying Mi (t) âˆˆ Rd2 . We estimate the parameters i , representing stationary
per-region random effects, and Î¸ âˆˆ Rd1 +d2 , a vector of fixed effects shared across regions.
4.1

Learning and a Differentiable Model

In the real world, the SIR model is a latent process â€“ we never directly observe any of the state
variables Si [t], Ii [t], Ri [t]. Instead, we observe Ci [t] = Ii [t] + Ri [t] = Î±i Ni âˆ’ Si [t]. Note that other
processes (deaths, hospitalizations, etc.) that depend on the latent state may also be observable, and
can easily be incorporated
into this learning scheme. The MLE problem for parameters (Î¸, Î±, ) is
P
simply max(Î¸,Î±,) i,t log P (Ci [t]|Î¸, Î±, ).
This is a difficult non-linear filtering problem (and an interesting direction for research). We therefore
consider an approximation: Denote by {(si [t], ii [t], ri [t]) : t âˆˆ N} the deterministic process obtained
by ignoring the martingale difference terms in the definition of the discrete time SIR process. We
consider the approximation
Ci [t] = Î±i Ni âˆ’ Si [t] âˆ¼ (Î±i Ni âˆ’ si [t])Ï‰i [t]
where Ï‰i [t] is log-normally distributed with mean 1 and variance exp(Ïƒ 2 ) âˆ’ 1. Under this approximation, the MLE problem is now transformed to
X
2
min
(log Ci [t] âˆ’ log (Î±i Ni âˆ’ si [t]))
(9)
(Î¸,Î±,)

i,t

which constitutes a differentiable model. We solve (9), (or a weighted version) using Adam [23].1
Working around the limits to learning Theorem 3.4 asserts that Î² is easy to learn via MLE,
suggesting that the parameters Î¸, {i } underlying our model of Î² can be estimated as well. However,
Corollary 3.2 illustrates that we cannot estimate {Î±i } in reasonable time or accuracy from infections
alone. This necessitates augmenting the estimation problem (9) with auxiliary information. We
1

Adam was run for 20k iterations, with learning rate tuned over a coarse grid. A weighted version of the loss
function in (9) with weights for (i, t)th observation set to Ci [t] worked well.

6

propose to take advantage of heterogeneity across regions: infection curves start at different times
in each region, and we typically have access to some set P [t] âŠ† I of regions that have already
experienced enough infections to reliably estimate Î±i for i âˆˆ P [t] via MLE. At a high level, our
strategy will be to identify the set P [t], estimate Î±i for i âˆˆ P [t], then extrapolate these estimates to
obtain Î±i for i âˆˆ
/ P [t]. We define the set
P [t] = {i âˆˆ I : Ci [t] âˆ’ Ci [t âˆ’ 1] â‰¤ âˆ’Î³1 max (Ci [Ï„ ] âˆ’ Ci [Ï„ âˆ’ 1])}
Ï„ â‰¤t

(10)

where Î³1 is a hyperparameter. In the fluid model this identifies the first time t where d2 s(t)/dt2 < 0.
We expect Î±i to vary across regions due to different demographics (that impact asymptomatic rates)
and testing policies. Given a set of parameters {Î±Ìƒi : i âˆˆ P [t]} estimated via MLE, a natural approach
to estimate Î±i for regions i âˆˆ
/ P [t] is by matching to regions in P [t] with similar covariates. It is also
worth noting that since in large systems in the fluid regime, we may show that Ci (t)/Ni â†’ Î±i as
t â†’ âˆž, we also enforce Î±i â‰¥ Ci [t]/Ni .
Overall Learning Algorithm (â€˜Two-Stageâ€™) So motivated, given data up to time T , we now define
our overall learning algorithm â€˜Two-Stageâ€™, as follows:

exp(Ï†> Zi + Î´i ),
if i âˆˆ P [T ]

Î±i =
max exp(Ï†> Zi ), Î³2 Ci [T ]/Ni , otherwise
where Î³2 is a hyper-parameter and Î´i are region specific random effect terms. We then estimate the
model parameters (Î¸, Ï†, Î´, ) by minimizing the following specialization of (9):
X
2
min
(log Ci [t] âˆ’ log (Î±i (Ï†, Î´i ) Ni âˆ’ si [t]))
(11)
(Î¸,Ï†,Î´,)

i,t

The hyper-parameters Î³1 , Î³2 and the learning rates are tuned via cross-validation.
4.2

Experimental results

We use our estimates to forecast cumulative infections in the US at the level of sub-state â€˜regionsâ€™,
corresponding broadly to public health service areas. The median state has seven regions. The static
covariates Zi for each region i comprise of 60 demographic features from multiple data providers,
ranging from socio-economic to health indicators. The time varying covariates Mi (t) correspond to
mobility data inferred from aggregated cell phone data. All datasets are widely available and are in
active use in other models. The Appendix provides a detailed description of these covariates.
This section focuses on two goals: First, we perform an ablation study focused on the impact of
assumed structure on the parameter Î±i . This allows us to understand whether and why optimizing
(11) provides improved performance. Second, and perhaps more importantly, our overall goal was
simply to produce accurate forecasts; here we compare the performance of our approach to published
forecast performance for the most broadly cited forecaster for COVID-19, [3].
The impact of learning Î±: We compare four approaches to learning Î±: Default: Ignoring the
modeling of Î± altogether by setting Î±i = 1 âˆ€i âˆˆ I (the â€˜defaultâ€™ SIR choice); MLE: Learn Î±i for
each region via the MLE approximation (9); Two-Stage: The two-stage approach specified by (11);
and Idealized: Using a value of Î± computed in-sample (i.e. by looking into the future) via (9).
Figure 1 shows weighted mean absolute percentage error (WMAPE) over regions, with weights
proportional to infections on May 21, 2020, for two metrics relevant to decision making: cumulative
infections by May 21, 2020 and maximum daily infections, for regions that have peaked by May 21,
2020. Model vintages vary along the x-axis so that moving from left to right models are trained on
data approaching the target date of May 21.
Default fails, with a WMAPE over 70% even on May 21 (an in-sample fit).2 At the other extreme,
Idealized exhibits consistently low error even for early model vintages. This bears out the prediction
of Theorem 3.4: given Î±, Î² is easy to learn even early in the infection with few samples. MLE
performs poorly until close to the target date of May 21 at which point sufficient data is available to
learn Î±. This empirically illustrates the difficult of learning Î±, as described in Corollary 3.2.
2

We do not show this model in Figure 1 as the WMAPEs are substantially larger than the scale shown.

7

Cum. Cases

Peak Cases
250%

40.0%

200%

WMAPE

30.0%
150%
20.0%
100%
10.0%

50%

Apr 27

May 04

May 11

0%

May 18

Apr 27

May 04

May 11

May 18

Model Vintage
Ideal alpha

MLE

Two-Stage

Figure 1: Prediction errors by model vintage, for regions that have peaked by May 21, 2020. Colors
denote different approaches to learning Î±i .

Not Yet Peaked

Peaked

1e-01

1e-02

+0
0
1e

-0
1
1e

-0
2
1e

-0
3
1e

+0
1e 0
-0
4

1e

-0
1
1e

-0
2
1e

1e

1e

-0
3

1e-03

-0
4

Î± for May 21, 2020 vintage

1e+00

Î± for April 21, 2020 vintage
MLE

Two-Stage

Figure 2: Î± estimated on May 21 vs. April 21. Size indicates cumulative cases on May 21. MLE
underestimates Î± in non-peaked regions (i.e. with limited data). Two-Stage does not.

Turning to our proposed approach, we see that Two-Stage significantly outperforms MLE far away
from the test date. Close to the test date the two approaches are comparable. For maximum daily
infections â€“ perhaps the most critical metric for capacity planning â€“ MLE drastically underperforms
Two-Stage far from the test date. Our approach to learning from peaked regions significantly mitigates
the difficulty of learning Î±. As further evidence, Figure 2 shows how well Î± from the April 21 vintage
predicts Î± in the final May 21 vintage, for regions that have peaked by May 21. As Proposition 3.3
predicts, the April 21 Î± are close to the May 21 Î± values for peaked regions, in both MLE and
Two-Stage models. For non-peaked regions, the April 21 MLE Î± estimates vary much more than the
Two-Stage estimates, and tend to underestimate the true Î± value.

Overall model performance Finally, we compare our analyzed models to the widely used IHME
model [3]. Figure 3 compares state-level3 WMAPE for MLE, Two-Stage and IHME models, for
vintages stretching back 28 days. The IHME model is, in effect, an SI model with carefully tuned
parameters. We report published IHME forecasts; 10 vintages of that model were reported between
April 21 and May 21. Two Stage dominates IHME across all model vintages.

3

Due to IHME only providing state-level predictions. Additionally IHME only offers deaths predictions for
these vintages; we show WMAPE on deaths for IHME and WMAPE on infections for MLE and Two-Stage.

8

40.0%

WMAPE

30.0%

20.0%

10.0%

Apr 20

Apr 27

May 04

May 11

May 18

Model Vintage
IHME

MLE

Two-Stage

Figure 3: WMAPE for predicting state-level cumulative cases on May 21, 2020, comparing MLE
and the Two-Stage approach against IHME.

Broader Impact
In contrast with the plurality of available forecast models (which make predictions at the level
of a state), the models we construct here are at a much finer level of granularity (essentially, the
county) and provide accurate predictions early in an epidemic. As a result our models can guide
the deployment of resources in states with heterogeneity in resource availability and prevalence.
The model was deployed in such an operational fashion in a large US state, where it was used to
proactively place hospital resources (mobile surge capacity) in areas where we anticipated large peaks
in infections. Many of these predictions were proven accurate and timely in hindsight. This would
not be possible with a state-level forecast (let alone a forecast with limited predictive power). While
not every state was able to make such data driven decisions in resource deployment, we believe that
in the event of a second outbreak, the approach we develop here can serve this need broadly.

Acknowledgements
The authors express their gratitude to Danial Mirza, Suzana Iacob, El Ghali Zerhouni, Neil Sanjay
Pendse, Shen Chen, Celia Escribe and Jonathan Amar for their excellent support in data collection
and organization.

References
[1] MS Bartlett. Some evolutionary stochastic processes. Journal of the Royal Statistical Society.
Series B (Methodological), 11(2):211â€“229, 1949.
[2] William Ogilvy Kermack and Anderson G McKendrick. A contribution to the mathematical
theory of epidemics. Proceedings of the royal society of london. Series A, Containing papers of
a mathematical and physical character, 115(772):700â€“721, 1927.
[3] IHME, 2020. http://www.healthdata.org/sites/default/files/files/Projects/
COVID/RA_COVID-forecasting-USA-EEA_042120.pdf.
[4] Giuseppe C Calafiore, Carlo Novara, and Corrado Possieri. A modified sir model for the
covid-19 contagion in italy. arXiv preprint arXiv:2003.14391, 2020.
[5] Giuseppe Gaeta. A simple sir model with a large set of asymptomatic infectives. arXiv preprint
arXiv:2003.08720, 2020.
[6] Giulia Giordano, Franco Blanchini, Raffaele Bruno, Patrizio Colaneri, Alessandro Di Filippo,
Angela Di Matteo, and Marta Colaneri. Modelling the covid-19 epidemic and implementation
of population-wide interventions in italy. Nature Medicine, pages 1â€“6, 2020.
[7] FA Binti Hamzah, C Lau, H Nazri, DV Ligot, G Lee, CL Tan, et al. Coronatracker: world-wide
covid-19 outbreak data analysis and prediction. Bull World Health Organ. E-pub, 19, 2020.
[8] Adam J Kucharski, Timothy W Russell, Charlie Diamond, Yang Liu, John Edmunds, Sebastian
Funk, Rosalind M Eggo, Fiona Sun, Mark Jit, James D Munday, et al. Early dynamics of
9

transmission and control of covid-19: a mathematical modelling study. The lancet infectious
diseases, 2020.
[9] Joseph T Wu, Kathy Leung, and Gabriel M Leung. Nowcasting and forecasting the potential
domestic and international spread of the 2019-ncov outbreak originating in wuhan, china: a
modelling study. The Lancet, 395(10225):689â€“697, 2020.
[10] Cleo Anastassopoulou, Lucia Russo, Athanasios Tsakris, and Constantinos Siettos. Data-based
analysis, modelling and forecasting of the covid-19 outbreak. PloS one, 15(3):e0230405, 2020.
[11] Kathakali Biswas, Abdul Khaleque, and Parongama Sen. Covid-19 spread: Reproduction of
data and prediction using a sir model on euclidean network. arXiv preprint arXiv:2003.07063,
2020.
[12] Maria Chikina and Wesley Pegden. Modeling strict age-targeted mitigation strategies for
covid-19. arXiv preprint arXiv:2004.04144, 2020.
[13] ClÃ©ment Massonnaud, Jonathan Roux, and Pascal CrÃ©pey. Covid-19: Forecasting short term
hospital needs in france. medRxiv, 2020.
[14] Rahul Goel and Rajesh Sharma. Mobility based sir model for pandemicsâ€“with case study of
covid-19. arXiv preprint arXiv:2004.13015, 2020.
[15] Kimberlyn Roosa and Gerardo Chowell. Assessing parameter identifiability in compartmental
dynamic models using a computational approach: application to infectious disease transmission
models. Theoretical Biology and Medical Modelling, 16(1):1, 2019.
[16] Neil D Evans, Lisa J White, Michael J Chapman, Keith R Godfrey, and Michael J Chappell.
The structural identifiability of the susceptible infected recovered model with seasonal forcing.
Mathematical biosciences, 194(2):175â€“197, 2005.
[17] Anirban Basu. Estimating the infection fatality rate among symptomatic covid-19 cases in the
united states: Study estimates the covid-19 infection fatality rate at the us county level. Health
Affairs, pages 10â€“1377, 2020.
[18] Gerardo Chowell. Fitting dynamic models to epidemic outbreaks with quantified uncertainty: a
primer for parameter uncertainty, identifiability, and forecasts. Infectious Disease Modelling,
2(3):379â€“398, 2017.
[19] Alex Capaldi, Samuel Behrend, Benjamin Berman, Jason Smith, Justin Wright, and Alun L
Lloyd. Parameter estimation and uncertainty quantication for an epidemic model. Mathematical
biosciences and engineering, page 553, 2012.
[20] RWR Darling, James R Norris, et al. Differential equation approximations for markov chains.
Probability surveys, 5:37â€“79, 2008.
[21] John D Gorman and Alfred O Hero. Lower bounds for parametric estimation with constraints.
IEEE Transactions on Information Theory, 36(6):1285â€“1301, 1990.
[22] Jean Jacod, Thomas G Kurtz, Sylvie MÃ©lÃ©ard, and Philip Protter. The approximate euler method
for lÃ©vy driven stochastic differential equations. In Annales de lâ€™IHP ProbabilitÃ©s et statistiques,
volume 41, pages 523â€“558, 2005.
[23] Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
[24] Svante Janson. Tail bounds for sums of geometric and exponential variables. Statistics &
Probability Letters, 135:1â€“6, 2018.
[25] Joel C Miller. Mathematical models of sir disease spread with combined non-sexual and sexual
transmission routes. Infectious Disease Modelling, 2(1):35â€“55, 2017.
[26] Joel C Miller. A note on the derivation of epidemic final sizes. Bulletin of mathematical biology,
74(9):2125â€“2141, 2012.
[27] Ensheng Dong, Hongru Du, and Lauren Gardner. An interactive web-based dashboard to track
covid-19 in real time. The Lancet infectious diseases, 20(5):533â€“534, 2020.
[28] Safegraph Social Distancing Metrics, 2020.
social-distancing-metrics.

https://docs.safegraph.com/docs/

[29] University of Michigan Health and Retirement Study, 2020. https://hrs.isr.umich.edu/
data-products.
10

[30] Claritas, 2020. https://www.claritas.com/.
[31] CDC Interactive Atlas of Heart Disease and Stroke, 2020. https://www.cdc.gov/dhdsp/
maps/atlas/index.htm.
[32] CDC Social Vulnerability Index, 2020. https://svi.cdc.gov/.

11

Appendices
Appendices A and B contain the proofs of Theorems 3.1 and 3.4 respectively. Appendix C contains
the proofs of Propositions 2.1, 2.2, and 3.3, each in their own subsections. Appendix D contains a
result justfying the use of (10) as a sufficient condition in reliably estimating Î±. Appendix E contains
a description of the covariates used in the practical SIR model.

A

Proof of Theorem 3.1

We finish the sections of the proof that were not included in the main paper. This includes the proof
of Lemma 3.5, Lemma 3.12, and calcuations for Lemma 3.6.


âˆ’Ckâˆ’1 )
Î²(Î±N âˆ’Ckâˆ’1 )
We define Î»(Î±, k âˆ’ 1, Ckâˆ’1 ) = Î²(Î±NÎ±N
+ Î³ Ikâˆ’1 and Î·(Î±, Ckâˆ’1 ) = Î²(Î±N
âˆ’Ckâˆ’1 )+Î±N Î³ .
Thus, for k â‰¤ Ï„ , Î»(N, k âˆ’ 1, Ckâˆ’1 ) is the mean of the k-th inter-arrival time and Î·(Î±, Ckâˆ’1 ) is the
probability that the arrival in the k-th instance is a new infection rather than a recovery.
A.1

Proof of Lemma 3.5

Proof. Suppose k < Ï„ i.e Ek = 1. Then, k is equal to total number of jumps that have occurred so
far (the number of movements from S to I and from I to R). The number of individuals that have
moved from S to I is Ck âˆ’ I0 âˆ’ R0 , and the number of movements from I to R is Ck âˆ’ Ik âˆ’ R0 .
Therefore, k = 2Ck âˆ’ I0 âˆ’ Ik âˆ’ 2R0 . Since Ik > 0, Ck > rk .
Suppose k â‰¥ Ï„ i.e Ek = 0. Then, k is greater than or equal to the total number of jumps, which is
still equal to 2Ck âˆ’ I0 âˆ’ Ik âˆ’ 2R0 . Hence Ck â‰¤ rk in this case.

A.2

Proof of Lemma 3.12
iid

Proof. Let Xk âˆ¼ Bern(p) for k = 1, 2, . . . . Let {Ak : k â‰¥ 0} be a stochastic process defined by:
ï£±
ï£²C 0
Ak = C0 + X1 + Â· Â· Â· + Xk
ï£³
Akâˆ’1

if k = 0
if Ai > ri âˆ€i < k
otherwise.

(12)

Let Ï„A = min{k : Ak â‰¤ rk } be the â€œstopping timeâ€ of this process.

Claim A.1. Pr(Ï„ â‰¤ m) â‰¤ Pr(Ï„A â‰¤ m).

The proof of this claim involves showing the process {Ak } is stochastically less than {Ck }; the proof
can be found in Section A.2.1. We now upper bound Pr(Ï„A â‰¤ m). Ï„A â‰¤ m if and only if Ak â‰¤ rk
for some k â‰¤ m. Before this happens, Ak = C0 + X1 + Â· Â· Â· + Xk . Therefore, if Ï„A â‰¤ m, it must be
that C0 + X1 + Â· Â· Â· + Xk â‰¤ k+I02+2R0 for some k â‰¤ m.
m
X



k + I0 + 2R0
Pr(Ï„A â‰¤ m) â‰¤
Pr C0 + X1 + Â· Â· Â· + Xk â‰¤
2
k=1



m
X
2pk âˆ’ k + I0
=
Pr X1 + Â· Â· Â· + Xk < pk 1 âˆ’
.
2pk
k=1

12

(13)
(14)

Pk
Since E[X1 + Â· Â· Â· + Xk ] = pk, using the Chernoff bound (multiplicative form: Pr( i=1 Xi â‰¤
(1 âˆ’ Î´)Âµ) â‰¤ exp(âˆ’Î´ 2 Âµ/2)) gives


2 !
m
X
pk
1
I0
Pr(Ï„A â‰¤ m) â‰¤
exp âˆ’
1âˆ’
+
(15)
2
2p
2pk
k=1
!

2


m
X
1
I0
1
I02
pk
(16)
=
1âˆ’
âˆ’
1âˆ’
âˆ’
exp âˆ’
2
2p
2
2p
8pk
k=1

2

!
m
X
pk
1
I0
1
â‰¤
exp âˆ’
1âˆ’
âˆ’
1âˆ’
(17)
2
2p
2
2p
k=1
 
 X

2 !
m
1
1
1
pk
â‰¤ exp âˆ’
âˆ’
I0
1âˆ’
exp âˆ’
(18)
2 4p
2
2p
k=1

â‰¤ B1 exp(âˆ’B2 I0 ),


2 
Pâˆž
pk
1
for constants B1 = k=1 exp âˆ’ 2 1 âˆ’ 2p
, B2 =

(19)
1
2

âˆ’

1
4p

> 0. (B1 is a constant since

it is a geometric series with a ratio smaller than 1, since p > 1/2.) Let D be the solution to
B1 exp(âˆ’B2 D) = 12 . Then, if I0 â‰¥ D, Pr(Em ) = 1 âˆ’ Pr(Ï„ â‰¤ m) â‰¥ 1 âˆ’ Pr(Ï„A â‰¤ m) â‰¥ 21 .

A.2.1

Proof of Claim B.5.

Definition A.2. For scalar random variables X, Y , we say that X is stochastically less than Y
(written X â‰¤st Y ) if for all t âˆˆ R,
Pr(X > t) â‰¤ Pr(Y > t).

(20)

For random vectors X, Y âˆˆ Rn we say that X â‰¤st Y if for all increasing functions Ï† : Rn â†’ R,
Ï†(X1 , . . . , Xn ) â‰¤st Ï†(Y1 , . . . , Yn ).

(21)

We make use of the following known result for establishing stochastic order for stochastic processes.
Theorem A.3 (Veinott 1965). Suppose X1 , . . . , Xn , Y1 , . . . , Yn are random variables such that
X1 â‰¤st Y1 and for any x â‰¤ y,
(Xk |X1 = x1 , . . . , Xkâˆ’1 = xkâˆ’1 ) â‰¤st (Yk |Y1 = y1 , . . . , Ykâˆ’1 = ykâˆ’1 )

(22)

for every 2 â‰¤ k â‰¤ n. Then, (X1 , . . . , Xn ) â‰¤st (Y1 , . . . , Yn ).
âˆ’mâˆ’C0 )
Proof of Claim B.5. Because of the condition Î²(PÎ²(P
âˆ’mâˆ’C0 )+P Î³ > p, for k â‰¤ m and k â‰¤ Ï„ ,
Ck âˆ’ Ckâˆ’1 âˆ¼ Bern(q) for q > p. First, we show (A0 , A1 , . . . , Am ) â‰¤st (C0 , C1 , . . . , Cm ) using Theorem A.3. C0 â‰¤st A0 since C0 = A0 = I0 . We condition on Akâˆ’1 = x and Ckâˆ’1 = y for
x â‰¤ y, and we must show Ak â‰¤st Ck . (We do not need to condition on all past variables since the
both processes are Markov.) If x â‰¤ rkâˆ’1 , then Ak = Akâˆ’1 = x â‰¤ y = Ckâˆ’1 â‰¤ Ck . Otherwise,
the process Ak has not stopped, and neither has Ck since y â‰¥ x. Then, Ak âˆ¼ x + Bern(p) and
Ck âˆ¼ y + Bern(q) for some q â‰¥ p. Clearly, Ak â‰¤st Ck in this case. We apply Theorem A.3, which
implies Am â‰¤st Cm .

Define the function u : Rm+1 â†’ {0, 1}, u(x0 , x1 , . . . , xm ) = 1{âˆªm
k=1 {xk â‰¤ rk }}. Then,
u(A0 , A1 , . . . , Am ) = 1 if and only if Ï„A â‰¤ m, and u(C0 , C1 , . . . , Cm ) = 1 if and only if
Ï„ â‰¤ m. u is a decreasing function. Therefore, u(A0 , A1 , . . . , Am ) â‰¥st u(C0 , C1 , . . . , Cm ). Then,
Pr(Ï„ â‰¤ m) = Pr(u(C0 , C1 , . . . , Cm ) â‰¥ 1) â‰¤ Pr(u(A0 , A1 , . . . , Am ) â‰¥ 1) = Pr(Ï„A â‰¤ m) as
desired.
13

A.3

Calculations for Lemma 3.6

Derivation of ECk [gCk |Ckâˆ’1 (Ck , Ckâˆ’1 , Î±)|Ekâˆ’1 = 1]. When Ekâˆ’1 = 1, we have Ck âˆ¼ Ckâˆ’1 +
Bern(Î·(Î±, Ckâˆ’1 )). Therefore, ECk [gCk |Ckâˆ’1 (Ck , Ckâˆ’1 , Î±)|Ekâˆ’1 = 1] = JCk âˆ¼Bern(Î·(Î±,Ckâˆ’1 )) (Î±).
We reparameterize to write the Fisher information as:

2
âˆ‚
(23)
ECk [gCk |Ckâˆ’1 (Ck , Ckâˆ’1 , Î±)|Ekâˆ’1 = 1] = JCk âˆ¼Bern(Î·) (Î·)
Î·(Î±, Ckâˆ’1 )
âˆ‚Î±

2
1
âˆ‚
=
(24)
Î·(Î±, Ckâˆ’1 ) .
Î·(1 âˆ’ Î·) âˆ‚Î±
Use Î·(Î±, Ckâˆ’1 ) =

Î²(Î±N âˆ’Ckâˆ’1 )
Î²(Î±N âˆ’Ckâˆ’1 )+Î±N Î³

to derive

âˆ‚
Î²N (Î²(Î±N âˆ’ Ckâˆ’1 ) + Î³Î±N ) âˆ’ Î²N (Î±N âˆ’ Ckâˆ’1 )(Î² + Î³)
Î·(Î±, Ckâˆ’1 ) =
âˆ‚Î±
(Î²N (Î±N âˆ’ Ckâˆ’1 ) + Î³Î±N )2
Î²N Î³Ckâˆ’1
=
.
(Î²(Î±N âˆ’ Ckâˆ’1 ) + Î³Î±N )2
Also,

1
Î·(1âˆ’Î·)

=

(25)
(26)

(Î²(Î±N âˆ’Ckâˆ’1 )+Î±N Î³)2
(Î±N âˆ’Ckâˆ’1 )Î²Î±N Î³ .

Substituting,
ECk [gCk |Ckâˆ’1 (Ck , Ckâˆ’1 , Î±)|Ekâˆ’1

(Î²(Î±N âˆ’ Ckâˆ’1 ) + Î±N Î³)2
= 1] =
Î²(Î±N âˆ’ Ckâˆ’1 )Î±N Î³



Î²N Î³Ckâˆ’1
(Î²(Î±N âˆ’ Ckâˆ’1 ) + Î³Î±N )2
(27)

2
Î²N Î³Ckâˆ’1
Î±(Î±N âˆ’ Ckâˆ’1 )(Î²(Î±N âˆ’ Ckâˆ’1 ) + Î±N Î³)2
2
Î²N 2 Î³Ckâˆ’1
.
=
P (P âˆ’ Ckâˆ’1 )(Î²(P âˆ’ Ckâˆ’1 ) + P Î³)2

=

(28)
(29)

Derivation of ETk [gTk |Ckâˆ’1 (Tk , Ckâˆ’1 , Î±)|Ekâˆ’1 = 1]. Similarly, conditioned on Ekâˆ’1 = 1, Tk âˆ¼
Exp(Î»(Î±, kâˆ’1, Ckâˆ’1 )). Therefore, ETk [gTk |Ckâˆ’1 (Tk , Ckâˆ’1 , Î±)] = JTk âˆ¼Exp(Î»(Î±,kâˆ’1,Ckâˆ’1 )) (Î±). We
reparameterize to write
2

âˆ‚
ETk [gTk |Ckâˆ’1 (Tk , Ckâˆ’1 , Î±)] = JTk âˆ¼Exp(Î») (Î»)
Î»(Î±, k âˆ’ 1, Ckâˆ’1 )
(30)
âˆ‚Î±

2
1
âˆ‚
= 2
Î»(Î±, k âˆ’ 1, Ckâˆ’1 ) .
(31)
Î»
âˆ‚Î±
âˆ’Ckâˆ’1 )
+ Î³)(2Ckâˆ’1 âˆ’ (k âˆ’ 1) âˆ’ I0 âˆ’ 2R0 ) to derive
Use Î»(Î±, k âˆ’ 1, Ckâˆ’1 ) = ( Î²(Î±NÎ±N

âˆ‚
N Î²Ckâˆ’1 (2Ckâˆ’1 âˆ’ (k âˆ’ 1) âˆ’ I0 âˆ’ 2R0 )
Î»(Î±, k âˆ’ 1, Ckâˆ’1 ) =
âˆ‚Î±
Î±2 N 2
Î±N
1
=
.
Î»(Î±, k âˆ’ 1, Ckâˆ’1 )
(Î²(Î±N âˆ’ Ckâˆ’1 ) + Î³Î±N )(2Ckâˆ’1 âˆ’ (k âˆ’ 1) âˆ’ I0 âˆ’ 2R0 )

(32)
(33)

Substituting,


Î²N Ckâˆ’1
ETk [gTk |Ckâˆ’1 (Tk , Ckâˆ’1 , Î±)] =
Î±N (Î²(Î±N âˆ’ Ckâˆ’1 ) + Î³Î±N )

2
Î²N Ckâˆ’1
=
P (Î²(P âˆ’ Ckâˆ’1 ) + Î³P )
Derivation
of
JOm (Î±).
Using
ECk [gCk |Ckâˆ’1 (Ck , Ckâˆ’1 , Î±)|Ekâˆ’1 = 1] and

the

14

expressions

2

derived

(34)
(35)
above

for

2

ETk [gTk |Ckâˆ’1 (Tk , Ckâˆ’1 , Î±)], we get
ECk [gCk |Ckâˆ’1 (Ck , Ckâˆ’1 , Î±)|Ekâˆ’1 = 1] + ETk [gTk |Ckâˆ’1 (Tk , Ckâˆ’1 , Î±)]

2
2
Î²N 2 Î³Ckâˆ’1
Î²N Ckâˆ’1
+
=
P (P âˆ’ Ckâˆ’1 )(Î²(P âˆ’ Ckâˆ’1 ) + P Î³)2
P (Î²(P âˆ’ Ckâˆ’1 ) + Î³P )
2
N 2 Ckâˆ’1
= 2
P (P âˆ’ Ckâˆ’1 )((P âˆ’ Ckâˆ’1 ) + Î²Î³ P )
Thus,
JOm (Î±) =
=

m
X
k=1
m
X
k=1

B

E[gCk |Ckâˆ’1 (Ck , Ckâˆ’1 , Î±) + gTk |Ckâˆ’1 (Tk , Ckâˆ’1 , Î±)|Ekâˆ’1 = 1] Pr(Ekâˆ’1 = 1)
#
2
N 2 Ckâˆ’1
Ekâˆ’1 = 1 Pr(Ekâˆ’1 = 1).
E
P 2 (P âˆ’ Ckâˆ’1 )((P âˆ’ Ckâˆ’1 ) + Î²Î³ P )
"

Proof of Theorem 3.4

Fix an instance in which the assumptions of the theorem statement hold. We remove the subscript n,
Î²
Î²
âˆ’C0
and let P = Î±N . Let p , 12 ( Î²+Î³
be an estimator for Î²+Î³
, BÌ‚ = SÌƒmm be
+ 12 ) > 12 . Let AÌ‚ = Cmm
P
min(m,Ï„ )
1
an estimator for Î²+Î³
for SÌƒm = k=1
Ikâˆ’1 Tk . Let Î²Ì‚ = AÌ‚/BÌ‚ and Î³Ì‚ = 1/BÌ‚ âˆ’ Î²Ì‚.
This first lemma follows from (19) of the proof of Lemma 3.12.
Î² P âˆ’mâˆ’C0
Lemma B.1. If Î²+Î³
> p, Pr(Ï„ < m) â‰¤ B1 eâˆ’B2 I0 , where B1 , B2 > 0 are constant that
P
depend only on Î² and Î³.

The next two lemmas give a high probability confidence bound for estimators AÌ‚ and BÌ‚.
Î² P âˆ’mâˆ’C0
Lemma B.2. For any m, I0 where Î²+Î³
> 12 , for any Î´ > 0,
P




Cm âˆ’ C0
Î²
P âˆ’ m âˆ’ C0
Î²
Pr
âˆˆ
/
(1 âˆ’ Î´)
,
(1 + Î´) , Ï„ â‰¥ m â‰¤ 2 exp(âˆ’mÎ´ 2 /(4 + 2Î´)).
m
Î²+Î³
P
Î²+Î³
(36)
Pmin(m,Ï„ )
Lemma B.3. Let SÌƒm = k=1
Ikâˆ’1 Tk . Then
!
P âˆ’mâˆ’C0
(1 âˆ’ Î´) (1 + Î´)
P
SÌƒm
âˆˆ
/[
,
(37)
], Ï„ â‰¥ m â‰¤ 2eâˆ’m P (Î´âˆ’ln(1+Î´)) .
Pr
m
Î² + Î³ Î² + Î³ P âˆ’ m âˆ’ C0

The next proposition combines the two estimators from the above lemmas and into estimators Î²Ì‚ and
Î³Ì‚.
Î² P âˆ’mâˆ’C0
> p. Let
Î²+Î³
P
âˆ’m(Î´âˆ’ln(1+Î´))
âˆ’mÎ´ 2 /(4+2Î´)

Proposition B.4. Assume Î² > Î³ > 0. Let I0 â‰¤ m < P such that

0
z = P âˆ’mâˆ’C
. Then, for any 0 < Î´ < 1, with probability 1 âˆ’ 4e
âˆ’ 4e
P
âˆ’B2 I0
2B1 e
,


(1 âˆ’ Î´)z 2 1 + Î´
Î²Ì‚ âˆˆ Î²
,Î²
1+Î´
1âˆ’Î´


z
(1 âˆ’ Î´)z âˆ’ (1 + Î´)2
1
1 + Î´ âˆ’ (1 âˆ’ Î´)2 z 2
Î³Ì‚ âˆˆ Î³
+Î²
,Î³
+Î²
,
1+Î´
(1 + Î´)(1 âˆ’ Î´)
1âˆ’Î´
(1 âˆ’ Î´)(1 + Î´)

âˆ’

(38)
(39)

where B1 , B2 > 0 are constants that depend on Î² and Î³.
We first show Theorem 3.4 using these results. We then prove Lemma B.2, Lemma B.3, and
Proposition B.4 in Appendix B.2.
15

B.1

Proof of Theorem 3.4
q
5 log m
Proof. Let Î´ =
m . First, we claim that the probability in Proposition B.4 is greater than
2

8
1âˆ’ m
âˆ’ 2B1 eâˆ’B2 I0 . Note that ln(1 + Î´) â‰¤ Î´ âˆ’ Î´2 + Î´ 3 , implying Î´ âˆ’ ln(1 + Î´) â‰¥ Î´ 2 ( 21 âˆ’ Î´). Since
1
Î´ â‰¤ 4,

4eâˆ’m(Î´âˆ’ln(1+Î´) â‰¤ 4eâˆ’m
Using Î´ â‰¤

1
4

Î´2
4

â‰¤

4
.
m

(40)

=

4
.
m

(41)

again,
4eâˆ’mÎ´

2

â‰¤ 4eâˆ’m

/(4+2Î´)

Î´2
5

Hence, the bound in B.4 holds with probability greater than 1 âˆ’
Since we assume m(m + C0 ) â‰¤ P and z = 1 âˆ’

8
m

âˆ’ 2B1 eâˆ’B2 I0 .

m+C0
,
P

1âˆ’z â‰¤

1
.
m

(42)

Let R be the event that the confidence bounds (38)-(39) hold. Note that
1âˆ’Î´
1
1+Î´ â‰¥ 1 âˆ’ 3Î´ for Î´ < 4 .
2
E[(Î²Ì‚ âˆ’ Î²)2 |R] â‰¤ Î² 2 1 + 3Î´ âˆ’ (1 âˆ’ 3Î´)z 2

1+Î´
1âˆ’Î´

â‰¤ 1 + 3Î´ and

(43)

2

2

â‰¤ Î² ((1 âˆ’ z) + 3Î´(1 + z))
!2
r
1
5 log m
2
â‰¤Î²
+6
m
m

(44)
(45)

log m
m
for an absolute constant M3 > 0. The second last step uses (42) and 1 + z â‰¤ 2.
â‰¤ Î² 2 M3

(46)

Similarly,
 


2
1
z
1 + Î´ âˆ’ (1 âˆ’ Î´)2 z 2
(1 âˆ’ Î´)z âˆ’ (1 + Î´)2
E[(Î³Ì‚ âˆ’ Î³) |R] â‰¤ Î³
âˆ’
+Î²
âˆ’
.
1âˆ’Î´
1+Î´
(1 âˆ’ Î´)(1 + Î´)
(1 + Î´)(1 âˆ’ Î´)
(47)
2

Using the fact that (1 âˆ’ Î´)(1 + Î´) â‰¥ 12 ,
1
z
âˆ’
â‰¤ 2((1 âˆ’ z) + Î´(1 + z)) â‰¤ 2
1âˆ’Î´
1+Î´

1
+2
m

r

5 log m
m

!
.

(48)

(1 âˆ’ Î´)z âˆ’ (1 + Î´)2
(1 + Î´) âˆ’ (1 âˆ’ Î´)z + (1 + Î´)2 âˆ’ (1 âˆ’ Î´)2 z 2
1 + Î´ âˆ’ (1 âˆ’ Î´)2 z 2
âˆ’
=
(1 âˆ’ Î´)(1 + Î´)
(1 + Î´)(1 âˆ’ Î´)
1 âˆ’ Î´2
(49)
1âˆ’Î´ 2
1+Î´
âˆ’
z
1âˆ’Î´
1+Î´
â‰¤ 2(1 âˆ’ z) + 8Î´ + (1 + 3Î´) âˆ’ (1 âˆ’ 3Î´)z 2
â‰¤ 2(1 âˆ’ z) + 4Î´(1 + z) +

2

2

â‰¤ 2(1 âˆ’ z) + 8Î´ + (1 âˆ’ z ) + 6Î´(1 + z )
2

â‰¤ (1 âˆ’ z)(3 + z) + Î´(8 + 6(1 + z ))
r
4
5 log m
â‰¤
+ 20
.
m
m
16

(50)
(51)
(52)
(53)
(54)

Substituting back into (47) results in
2

E[(Î³Ì‚ âˆ’ Î³) |R] â‰¤

Î³

2
+4
m

r

5 log m
m

!
+Î²

4
+ 20
m

r

5 log m
m

!!2

log m
,
m
for an absolute constant M2 , since Î² > Î³.
â‰¤ M4 Î² 2

(55)
(56)

Therefore,
2
E[(Î²Ì‚ âˆ’ Î²)2 ] â‰¤ E[(Î²Ì‚ âˆ’ Î²)2 |R] + Î²max
Pr(RÌ„)


8
2 log m
2
âˆ’B2 I0
+ Î²max
+ 2B1 e
â‰¤ M3 Î²
m
m
log m
2
2
â‰¤ M1 Î²max
+ 2B1 Î²max
eâˆ’B2 I0 ,
m
for an absolute constant M1 > 0. Similarly,


log m
8
2
2
E[(Î³Ì‚ âˆ’ Î³)2 ] â‰¤ M4 Î²max
+ Î³max
+ 2B1 eâˆ’B2 I0
m
m
log m
2
2
â‰¤ M2 Î²max
+ 2B1 Î³max
eâˆ’B2 I0 .
m

B.2
B.2.1

(57)
(58)
(59)

(60)
(61)

Proofs of Intermediate Results
Proof of Lemma B.2.

Proof. Fix m, let z :=

P âˆ’mâˆ’C0
,p
P

=

Î²
Î²+Î³ z.

Then p >

1
2.

Define three stochastic processes

{Ak : k â‰¥ 0}, {Bk : k â‰¥ 0}, {CÌƒk : k â‰¥ 0}:

C0
if k = 0
Ak =
Akâˆ’1 + Bern(p) otherwise.

C0
if k = 0
Bk =
Bkâˆ’1 + Bern(p/z) otherwise.
(
C0
n
o if k = 0
CÌƒk =
Î²(P âˆ’CÌƒkâˆ’1 )
CÌƒkâˆ’1 + Bern Î²(P âˆ’CÌƒ )+P Î³
otherwise.

(62)
(63)
(64)

kâˆ’1

Note that CÌƒk is a modified version of Ck where CÌƒk still evolves after the stopping time.
Claim B.5. Am is stochastically less than CÌƒm (Am â‰¤st CÌƒm ); CÌƒm is stochastically less than Bm
(CÌƒm â‰¤st Bm ); that is, for any ` âˆˆ R,
Pr(Bm â‰¤ `) â‰¤ Pr(CÌƒm â‰¤ `) â‰¤ Pr(Am â‰¤ `).

(65)

This claim follows from Theorem A.3, using a similar argument to Claim B.5.
Let Ak = C0 + X1 + X2 + . . . Xk where Xi âˆ¼ Bern(p) are independent. We provide the left tail
d

bound for Cm . Note that when Ï„ â‰¥ m, Cm = CÌƒm . Hence,
Pr(Cm â‰¤ mp(1 âˆ’ Î´) + C0 , Ï„ â‰¥ m) = Pr(CÌƒm â‰¤ mp(1 âˆ’ Î´) + C0 , Ï„ â‰¥ m)
â‰¤ Pr(CÌƒm â‰¤ mp(1 âˆ’ Î´) + C0 )
â‰¤ Pr(Am â‰¤ mp(1 âˆ’ Î´) + C0 ).
Using the Chernoff bound gives,
Pr(Am â‰¤ mp(1 âˆ’ Î´) + C0 ) = Pr(C0 + X1 + Â· Â· Â· + Xm â‰¤ pm(1 âˆ’ Î´) + C0 )
= Pr (X1 + Â· Â· Â· + Xm â‰¤ mp (1 âˆ’ Î´))
 mp 
â‰¤ exp âˆ’
Î´2 .
2
17

(66)
(67)
(68)
(69)
(70)
(71)

Therefore,

Pr


p
Cm âˆ’ C0
â‰¤ (1 âˆ’ Î´)z, Ï„ â‰¥ m = Pr (Cm â‰¤ mp(1 âˆ’ Î´) + C0 , Ï„ â‰¥ m)
m
z
 mp 
â‰¤ exp âˆ’
Î´ 2 â‰¤ exp(âˆ’mÎ´ 2 /4).
2

(72)
(73)

Let Bk = C0 + Y1 + . . . + Yk where Yi âˆ¼ Bern(p/z) are independent. Similarly, for the upper tail
bound, we have


Cm âˆ’ C0
p
Pr
â‰¥ (1 + Î´), Ï„ â‰¥ m = Pr(Cm â‰¥ mp/z(1 + Î´) + C0 , Ï„ â‰¥ m)
(74)
m
z
â‰¤ Pr(Bm â‰¥ mp/z(1 + Î´) + C0 )
(75)
â‰¤ Pr(C0 + Y1 + Â· Â· Â· + Ym â‰¥ mp/z(1 + Î´) + C0 ) (76)
mp/z 2
â‰¤ exp(âˆ’
Î´ ) â‰¤ exp(âˆ’mÎ´ 2 /(4 + 2Î´))
(77)
2+Î´
Z

2

due to the multiplicative Chernoff bound Pr(Z â‰¥ E[Z](1 + Î´)) â‰¤ eâˆ’ 2+Î´ Î´ where Z is the sum of
i.i.d Bernoulli random variables.
Î²
Combine upper and lower tail bounds and note that p/z = Î²+Î³
. Then, we can conclude, for any
Î´ > 0,


Cm âˆ’ C0
Î²
Î²
Pr
âˆˆ
/[
(1 âˆ’ Î´)z,
(1 + Î´)], Ï„ â‰¥ m â‰¤ 2 exp(âˆ’mÎ´ 2 /(4 + 2Î´)). (78)
m
Î²+Î³
Î²+Î³

B.2.2

Proof of Lemma B.3.

Proof. Conditioned on (I0 , C0 , I1 , C1 , . . . , Imâˆ’1 , Cmâˆ’1 ) with Ï„ â‰¥ m, we have


P âˆ’ Ckâˆ’1
Ikâˆ’1 Tk âˆ¼ Exp Î²
+Î³
P
are independent exponential random variables.
TheoremP
5.1 in [24] gives us a tail bound for the sum of independent exponential random variables:
n
let X = i=1 Xi with Xi âˆ¼ Exp(ai ) independent, then for Î´ > 0,
1 âˆ’aâˆ— Âµ(Î´âˆ’ln(1+Î´))
Pr(X â‰¥ (1 + Î´)Âµ) â‰¤
e
â‰¤ eâˆ’aâˆ— Âµ(Î´âˆ’ln(1+Î´))
(79)
1+Î´
Pr(X â‰¤ (1 âˆ’ Î´)Âµ) â‰¤ eâˆ’aâˆ— Âµ(Î´âˆ’ln(1+Î´))
(80)
where Âµ = E[X], aâˆ— = min1â‰¤iâ‰¤n ai .
Let SÌƒm|C,
~ I~ be SÌƒm conditioned on (I0 , C0 , I1 , C1 , . . . , Imâˆ’1 , Cmâˆ’1 ) with Ï„ â‰¥ m. Let Âµ =
Pm
1
E[SÌƒm|C,
~ I~] =
k=1 Î²(P âˆ’Ckâˆ’1 )/P +Î³ , aâˆ— = min1â‰¤kâ‰¤m Î²(P âˆ’ Ckâˆ’1 )/P + Î³. It is easy to verify the following facts
m
X
aâˆ—
P âˆ’ m âˆ’ C0
Âµaâˆ— â‰¥
â‰¥m
(81)
(Î² + Î³)
P
k=1

1
Âµ
1
P
â‰¤
â‰¤
.
Î²+Î³
m
Î² + Î³ P âˆ’ m âˆ’ C0
Combining these with Eqs. (79) and (80), we have

!
SÌƒm|C,
~ I~
(1 âˆ’ Î´) (1 + Î´)
P
Pr
âˆˆ
/
,
â‰¤ Pr
m
Î² + Î³ Î² + Î³ P âˆ’ m âˆ’ C0

SÌƒm|C,
~ I~
m

(82)



Âµ(1 âˆ’ Î´) Âµ(1 + Î´)
âˆˆ
/
,
m
m

!
(83)

P âˆ’mâˆ’C0
âˆ’m
P

â‰¤ 2e
18

(Î´âˆ’ln(1+Î´))

.

(84)

Therefore,
! Z
SÌƒm
âˆˆ
/ I, Ï„ â‰¥ m =
Pr
m
~ I|Ï„
~ â‰¥m
C,

Pr

P âˆ’mâˆ’C0
P

(Î´âˆ’ln(1+Î´))

Pr(Ï„ â‰¥ m)

(86)

P âˆ’mâˆ’C0
âˆ’m
P

(Î´âˆ’ln(1+Î´))

.

(87)

â‰¤ 2eâˆ’m
â‰¤ 2e

B.2.3

!
SÌƒm
~ I,
~ Ï„ â‰¥ m f (C,
~ I|Ï„
~ â‰¥ m) Pr(Ï„ â‰¥ m)
âˆˆ
/ I | C,
m
(85)

Proof of Proposition B.4.

Proof. Let Î²Ì‚ =
Î´)1/z]. Then,

Cm âˆ’C0
,z
SÌƒm

=

P âˆ’C0 âˆ’m
.
P

Suppose x âˆˆ

Î²
Î²+Î³ [(1

âˆ’ Î´)z, 1 + Î´], y âˆˆ

1
Î²+Î³ [1

âˆ’ Î´, (1 +



x
(1 âˆ’ Î´)z 2 1 + Î´
âˆˆ Î²
,Î²
y
1+Î´
1âˆ’Î´

(88)
2

z
1
1+Î´
âˆ’ Î²Ì‚. Suppose a âˆˆ (Î² + Î³)[ 1+Î´
, 1âˆ’Î´
], b âˆˆ Î²[ (1âˆ’Î´)z
1+Î´ , 1âˆ’Î´ ]. Then


(1 âˆ’ Î´)z âˆ’ (1 + Î´)2
1
1 + Î´ âˆ’ (1 âˆ’ Î´)2 z 2
z
+Î²
,Î³
+Î²
aâˆ’bâˆˆ Î³
.
1+Î´
(1 + Î´)(1 âˆ’ Î´)
1âˆ’Î´
(1 âˆ’ Î´)(1 + Î´)

Similarly, let Î³Ì‚ =

m
SÌƒm

(89)

Then, for any sets U1 , U2 ,
Pr(Î²Ì‚ âˆˆ U1 , Î³Ì‚ âˆˆ U2 ) â‰¥ 1 âˆ’ Pr(Î²Ì‚ âˆˆ
/ U1 ) âˆ’ Pr(Î³Ì‚ âˆˆ
/ U2 )

(90)

â‰¥ 1 âˆ’ Pr(Î²Ì‚ âˆˆ
/ U1 , Ï„ > m) âˆ’ Pr(Î²Ì‚ âˆˆ
/ U2 , Ï„ > m) âˆ’ 2 Pr(Ï„ < m)
â‰¥ 1 âˆ’ 4eâˆ’m(Î´âˆ’ln(1+Î´)) âˆ’ 4eâˆ’mÎ´

2

/(4+2Î´)

âˆ’ 2B1 eâˆ’B2 I0 ,

(91)
(92)

where the last step uses Lemma B.1, Lemma B.2 and Lemma B.3, using the intervals (88) and (89)
for U1 and U2 respectively.

C
C.1

Proofs of Propositions
Proof of Proposition 2.1

Proof. As in [25, 26], the solution {(s0 (t), i0 (t), r0 (t)) : t â‰¥ 0} with Î± = 1 can be written:
0

s0 (t) = s0 (0)eâˆ’Î¾ (t)
0

0

(93)
0

i (t) = N âˆ’ s (t) âˆ’ r (t)
Î³0N
r0 (t) = r(0) + 0 Î¾ 0 (t)
Î²
0 Z t
Î²
Î¾ 0 (t) =
i0 (tâˆ— ) dtâˆ—
N 0

(94)
(95)
(96)

Making the appropriate substitutions yields the following equivalent system:


Î²0
Î³0N
i0 (t) = N âˆ’ s0 (0) exp âˆ’ Î¾(t) âˆ’ r(0) âˆ’ 0 Î¾ 0 (t)
N
Î²
0 Z t
Î²
Î¾ 0 (t) =
i0 (tâˆ— ) dtâˆ— .
N 0
19

(97)
(98)

Therefore, it remains to show that for Î±0 > 0, {(s(t), i(t), r(t)) : t
{(Î±0 s0 (t), Î±0 i0 (t), Î±0 r0 (t)) : t â‰¥ 0} is a solution for (97) and (98). Starting with (97),

â‰¥

Î³0N
i0 (t) = N âˆ’ s0 (0) exp (âˆ’Î¾ 0 (t)) âˆ’ r0 (0) âˆ’ 0 Î¾ 0 (t)
Î²


Î³0N
Î±0 i0 (t) = Î±0 N âˆ’ s0 (0) exp (âˆ’Î¾ 0 (t)) âˆ’ r0 (0) âˆ’ 0 Î¾ 0 (t)
Î²
Î³ 0 Î±0 N
Î¾(t)
= Î±0 N âˆ’ Î±s0 (0) exp (âˆ’Î¾(t)) âˆ’ Î±0 r(0) âˆ’
Î²0

0}

,

(99)
(100)
(101)

0 Rt
where Î¾(t) = Î¾ 0 (t) = NÎ²Î±0 0 Î±0 i0 (tâˆ— ) dtâˆ— . Noting that Î¾ 0 (t) = Î¾(t) and substituting i(t) = Î±0 i0 (t)
yields the equations below, clearly showing that {(s(t), i(t), r(t)) : t â‰¥ 0} satisfy (97) and (98):

i(t) = Î±0 N âˆ’ s(0) exp (âˆ’Î¾(t)) âˆ’ r(0) âˆ’
Z t
Î²0
Î¾(t) =
i (tâˆ— ) dtâˆ— .
N Î±0 0

C.2

Î³ 0 Î±0 N
Î¾(t)
Î²0

(102)
(103)

Proof of Proposition 2.2

Proof. Consider initial conditions (s(0), i(0), 0), as in [25, 26], the analytical solution is given by
s(t) = s(0)eâˆ’Î¾(t) ,
i(t) = Î±N âˆ’ s(t) âˆ’ r(t),
Î³ Î±N
Î¾(t),
r(t) =
Î²
Z t
Î²
Î¾(t) =
i(t0 )dt0 .
Î±N 0
Consider two SIR models with parameters (Î±, Î², Î³) and (Î±0 , Î² 0 , Î³ 0 ), and initial conditions (s0 , i0 , 0)
and (s00 , i00 , 0) respectively. We claim that infection trajectories i(t) and i0 (t) being identical on an
open set [0, T ) implies the parameters and initial conditions are identical as well.
Assume i(t) = i0 (t) for all t âˆˆ [0, T ); then, given the exact solution above it follows that
h Z T
i
0
Î²
âˆ’ Î±N
x
0
0 âˆ’ Î±Î²0 N x
0
Î±N âˆ’ s0 e
âˆ’ Î³x = Î± N âˆ’ s0 e
âˆ’ Î³ x,
for all x âˆˆ 0,
i(t)dt

(104)

0

As functions of x, both the RHS and LHS in the equality above are holomorphic, and hence, using the
0
Î²
identity theorem, we conclude that Î±N = Î±0 N , s0 = s00 , âˆ’ Î±N
= âˆ’ Î±Î²0 N , Î³ = Î³ 0 , which completes
the proof.
C.3

Proof of Proposition 3.3

d
d
Proof. The crux of the problem is summarised in two smaller results, bounding T1,n
and T2,n
respectively. To ease our exposition, we will let Pn = Î±n Nn and drop the index n in these helper
results.

Proposition C.1. There exists a constant Î½1 that only depends on Î³, Î² such that
!
2/3 

1
2
Î½
P
Î½
c(0)
1
T1d â‰¥
log
+ log 1
1 âˆ’ 2/3
.
Î²âˆ’Î³ 3
c(0)
c(0)3/2
P
20

Proposition C.2. Let Ï1 = 1 âˆ’
constant C = O(1), such that

1
log log P

T2d â‰¤

, there exists a constant Î½2 that only depends on Î³, Î² and a

1
Î½2 P
C
log
+
.
Î²Ï1 âˆ’ Î³
i(0)
1 âˆ’ Ï1

The argument follows directly by taking the limit of the bounds we provide in Propositions C.1-C.2.
Specifically, using that the constants Î½1,n , Î½2,n do not change with n, we arrive at
d
T2,n
â‰¤ lim
nâ†’âˆž
nâ†’âˆž T d
1,n

1
Î²Ï1 âˆ’Î³

lim

= lim

nâ†’âˆž

1
Î²âˆ’Î³



2
3

log

Î²âˆ’Î³
Â·
Î²Ï1 âˆ’ Î³

log

Î½1 Pn
cn (0)3/2

Î½2 Pn
in (0)

+

Cn
(1âˆ’Ï1 )
2/3

+ log

Î½1
cn (0)



1âˆ’

cn (0)
2/3
Pn



log Pn + log Î½2 âˆ’ log in (0)
2
3

log Pn +

4
3


log Î½1 âˆ’ 2 log cn (0) + log 1 âˆ’

cn (0)
2/3
Pn



(Î² âˆ’ Î³)Cn log log Pn

+ lim

nâ†’âˆž 2
3

log Pn +

4
3


log Î½1 âˆ’ 2 log cn (0) + log 1 âˆ’

cn (0)
2/3
Pn



Since cn (0) = O(log(Pn )) by assumption (and in (0) â‰¤ cn (0)), and Cn = O(1) by Proposition C.2,
the limits of the two summands above are 3/2 and 0 respectively, which concludes the proof.
C.3.1

Proof of Proposition C.1.

Proof of Proposition C.1. Define iÌƒ(t) such that iÌƒ(0) = i(0) and

diÌƒ
dt

= (Î² âˆ’ Î³)iÌƒ, implying

iÌƒ(t) = i(0) exp{(Î² âˆ’ Î³)t}.
Since

diÌƒ
dt

â‰¥

di
dt

(105)

for all t, iÌƒ(t) â‰¥ i(t) for all t. Then, for all t,
ds
s
= âˆ’Î² i â‰¥ âˆ’Î²i â‰¥ âˆ’Î² iÌƒ.
dt
P

(106)

Hence we can write
Z

t

âˆ’Î² iÌƒ(t0 )dt0
Z t
= s(0) âˆ’ Î²i(0)
exp{(Î² âˆ’ Î³)t0 }dt0

s(t) â‰¥ s(0) +

(107)

0

(108)

0

= s(0) âˆ’

Î²i(0)
(exp{(Î² âˆ’ Î³)t} âˆ’ 1)
Î²âˆ’Î³

(109)
(110)

Since s(0) âˆ’ s(T1d ) = P 2/3 âˆ’ c(0), setting t = T1d and solving for T1d in the inequality above results
in


1
Î² âˆ’ Î³ 2/3
T1d â‰¥
log
(P
âˆ’ c(0))
(111)
Î²âˆ’Î³
Î²i(0)


1
Î² âˆ’ Î³ 2/3
log
(P
âˆ’ c(0))
(112)
â‰¥
Î²âˆ’Î³
Î²c(0)


1
Î² âˆ’ Î³ 2/3
Î² âˆ’Î³
c(0) 
=
log
(P ) + log
1 âˆ’ 2/3
(113)
Î²âˆ’Î³
Î²c(0)
Î²c(0)
P
!
2/3
1
2
Î½1 P
Î½1 
c(0) 
=
log
+ log
1 âˆ’ 2/3
(114)
Î²âˆ’Î³ 3
c(0)
c(0)3/2
P
for Î½1 =



Î²âˆ’Î³
Î²

3/2

as desired.
21

C.3.2

Proof of Proposition C.2.

For Ï âˆˆ [0, Î²Î³ ], let tÏ be the time t when s(t)
P = Ï. Ï will represent the fraction of the total population
Î³
that is susceptible. Since Ï â‰¤ Î² , i is increasing for the time period of interest.
Let Î² > Î³, P be fixed. Let Ï1 = 1 âˆ’
Ï1 > Ï2 , hence tÏ1 < tÏ2 . T2d = tÏ2 .

1
log log P

and Ï2 =

Lemma C.3. For any Ï âˆˆ [0, Î²Î³ ], i(tÏ ) â‰¥ P (1 âˆ’ Ï) Î²Ïâˆ’Î³
Î²Ï âˆ’

Î³
Î².

We assume P is large enough that

c(0)
2 .

Proof of Lemma C.3. Fix Ï. At time tÏ , the total number of people infected is c(tÏ ) = i(tÏ )+r(tÏ ) =
Î²

s(t)

âˆ’Î³

P (1 âˆ’ Ï), by definition. At any time t â‰¤ tÏ , the rate of increase in i is Ps(t) â‰¥ Î²Ïâˆ’Î³
Î²Ï of the rate
Î² P



Î²Ïâˆ’Î³
Î²Ïâˆ’Î³
of increase in c. Therefore, i(tÏ ) âˆ’ i(0) â‰¥ Î²Ï
c(tÏ ) âˆ’ c(0) and i(tÏ ) â‰¥ Î²Ï P (1 âˆ’ Ï) âˆ’
Î²Ïâˆ’Î³
Î²Ï c(0)

c(0)
2

+ i(0). Using the fact that i(0) â‰¥

and rearranging terms gives the desired result.

Lemma C.4. For t âˆˆ [tÏ1 , tÏ2 ], where Ï2 > Ï1 for Ï1 , Ï2 âˆˆ [0, Î²Î³ ], tÏ2 âˆ’ tÏ1 â‰¤

P (Ï1 âˆ’Ï2 )
Î²Ï2 i(tÏ1 ) .

Proof of Lemma C.4. The difference in s between tÏ1 and tÏ2 is s(tÏ1 ) âˆ’ s(tÏ2 ) = P (Ï1 âˆ’ Ï2 ). As
s(t )âˆ’s(t )
a consequence of the mean value theorem, ÏtÏ2 âˆ’tÏ Ï1 â‰¤ maxtâˆˆ[tÏ1 ,tÏ2 ] { ds
dt }. Using these two
2
1
expressions,




P (Ï1 âˆ’ Ï2 )
ds
s(t)
â‰¥ min âˆ’
i(t) : t âˆˆ [tÏ1 , tÏ2 ] â‰¥ Î²Ï2 i(tÏ1 )
(115)
= min Î²
tÏ2 âˆ’ tÏ1
dt
P
The desired expression follows from rearranging terms.
Lemma C.5. For any Ï â‰¤ min{ Î²Î³ , 1/2}, tÏ â‰¤

1
Î²Ïâˆ’Î³

log

Î½2
i(0) P ,

for Î½2 =

2(Î²âˆ’Î³)
.
Î²

The proof of this lemma follows the exact same procedure as the proof of Proposition C.1.
Proof of Lemma C.5. We proceed in the same way as the proof of Proposition C.1 except in this case
we will lower bound s(0) âˆ’ s(t). We achieve this by letting iÌƒ be defined to grow slower than i, so it
diÌƒ
is used as a lower bound. Define iÌƒ(t) such that iÌƒ(0) = i(0) and dt
= (Î²Ï âˆ’ Î³)iÌƒ, implying
iÌƒ(t) = i(0) exp{(Î²Ï âˆ’ Î³)t}.
diÌƒ
Since dt
â‰¤
t < tÏ2 ,

di
dt

when , iÌƒ(t) â‰¤ i(t) for all t < tÏ2 . In addition, when t < tÏ2 ,
ds
s
= âˆ’Î² i â‰¤ âˆ’Î²ÏiÌƒ.
dt
P

(116)
s
P

â‰¥

Î³
Î²

â‰¥ Ï. Then, for

(117)

Hence we can write
Z

t

âˆ’Î²ÏiÌƒ(t0 )dt0
Z t
= s(0) âˆ’ Î²Ïi(0)
exp{(Î²Ï âˆ’ Î³)t0 }dt0

s(t) â‰¤ s(0) +

(118)

0

(119)

0

= s(0) âˆ’

Î²Ïi(0)
(exp{(Î²Ï âˆ’ Î³)t} âˆ’ 1)
Î²Ï âˆ’ Î³

(120)
(121)

Since s(tÏ ) = ÏP ,
ÏP â‰¤ s(0) âˆ’

Î²Ïi(0)
(exp{(Î²Ï âˆ’ Î³)tÏ } âˆ’ 1).
Î²Ï âˆ’ Î³
22

Solving for tÏ results in
log
tÏ â‰¤
where Î½2 =

2(Î²âˆ’Î³)
,
Î²



Î²Ïâˆ’Î³
Î²Ïi(0) (s(0)

âˆ’ ÏP ) + 1


â‰¤

Î²Ï âˆ’ Î³

 Î½

1
2
log
P
Î²Ï âˆ’ Î³
i(0)

(122)

using the fact that Ï â‰¤ 1/2.

Proof of Proposition C.2. Using the results from Lemmas C.3-C.5,
tÏ2 = tÏ1 + (tÏ2 âˆ’ tÏ1 )
 Î½
 P (Ï âˆ’ Ï )
1
2
1
2
â‰¤
log
P +
Î²Ï1 âˆ’ Î³
i(0)
Î²Ï2 i(tÏ1 )
 Î½

P (Ï1 âˆ’ Ï2 )
1
2
log
P + Ï2
â‰¤
Î²Ï2
Î²Ï1 âˆ’ Î³
i(0)
Ï1 P (1 âˆ’ Ï1 )(Î²Ï1 âˆ’ Î³) âˆ’ 2 c(0)
 Î½

1
C
2
=
log
P +
,
Î²Ï1 âˆ’ Î³
i(0)
1 âˆ’ Ï1
Ï1 âˆ’Ï2
where C = Ï2
. Note that, as required in the statement, C = O(1). Indeed,
Î²Ï2
c(0)
Ï1

(Î²Ï1 âˆ’Î³)âˆ’

C=

2

P (1âˆ’Ï1 )

(Ï1 âˆ’ Ï2 )
Ï2
Ï1 (Î²Ï1

âˆ’ Î³) âˆ’

c(0)
Î²Ï2
2 P (1âˆ’Ï1 )

=

1
log log P
log P
âˆ’ Î²Ï2 2 c(0) log
P

1 âˆ’ Ï2 âˆ’
Î²Ï2 âˆ’

Î³Ï2
1
1âˆ’ log log
P

,

(123)

and so, as P grows large, C tends to (1 âˆ’ Ï2 )/Ï2 (Î² âˆ’ Î³) (recall that c(0) = O(log log P )).

D

Sufficient Condition for P [t]

In the practical model, P [t] represents the regions that have enough observations to reliably estimate
d
Î± at time t. In this section, we justify why the definition (10) is a sufficient condition. Let T3,n
=
2

inf{t : ddt2s > 0} be the time at which the rate of new infections is highest in the deterministic SIR
model.
d
d
Proposition D.1. As n â†’ âˆž, T1,n
â‰¤ T3,n
.
Proof.


d2 s
âˆ’Î²
di
ds
=
i
+
s
dt2
Î±n Nn dt
dt


 
âˆ’Î²
âˆ’Î²s 2
Î²s
=
i +
âˆ’ Î³ is
Î±n Nn Î±n Nn
Î±n Nn
= âˆ’Î²is (âˆ’Î²i + Î²s âˆ’ Î³Î±n Nn )


Î³
= Î² 2 is i âˆ’ s + Î±n Nn
Î²
From (127), we see that

d2 s
dt2

> 0 if and only if
Î³
s < Î±n Nn + i.
Î²

(124)
(125)
(126)
(127)

(128)

d
At t â‰¤ T1,n
, c(t) â‰¤ (Î±n Nn )2/3 which implies i(t) â‰¤ (Î±n Nn )2/3 and s(t) â‰¥ Î±n Nn âˆ’ (Î±n Nn )2/3 .
We assume n is large enough so that 2(Î±n Nn )âˆ’1/3 < 1 âˆ’ Î²Î³ . Then, (128) cannot hold:
Î³
2(Î±n Nn )âˆ’1/3 < 1 âˆ’
(129)
Î²
Î³
+ (Î±n Nn )âˆ’1/3 < 1 âˆ’ (Î±n Nn )âˆ’1/3
(130)
Î²
Î³
Î³
Î±n Nn + i(t) â‰¤ Î±n Nn + (Î±n Nn )2/3 < Î±n Nn âˆ’ (Î±n Nn )2/3 â‰¤ s(t).
(131)
Î²
Î²

23

d
d
Therefore, T1,n
â‰¤ T3,n
.

E

Covariate Details for Practical SIR Model

For observed COVID-19 cases, we use publicly available case data from the ongoing COVID-19
epidemic provided by [27]. Xi [t] consists of static demographic covariates and time-varying mobility
features that affect the disease transmission rate.
The dynamic covariates proxy mobility by estimating the daily fraction of people staying at home
relative to a region-specific benchmark of activity in early March before social distancing measures
were put in place. We also include a regional binary indicator of the days when the fraction of
people staying home exceeds the benchmark by 0.2 or more. These data are provided by Safegraph,
a data company that aggregates anonymized location data from numerous applications in order to
provide insights about physical places. To enhance privacy, SafeGraph excludes census block group
information if fewer than five devices visited an establishment in a month from a given census block
group. Documentation can be found at [28].
The static covariates capture standard demographic features of a region that influence variation in
infection rates. These features fall into several categories:
â€¢ Fraction of individuals that live in close proximity or provide personal care to relatives in
other generations. These covariates are reported by age group by state from survey responses
conducted by [29].
â€¢ Family size from U.S. Census data, aggregated and cleaned by [30].
â€¢ Fraction of the population living in group quarters, including colleges, group homes, military
quarters, and nursing homes (U.S. Census via [30]).
â€¢ Population-weighted urban status (US Census via [30])
â€¢ Prevalence of comorbidities, such as cardiovascular disease and hypertension ([31])
â€¢ Measures of social vulnerability and poverty (U.S. Census via [30]; [32])
â€¢ Age, race and occupation distributions (U.S. Census via [30])

24

