Sensitivity Analysis of Error-Contaminated
Time Series Data under Autoregressive
Models with Application of COVID-19 Data

arXiv:2008.05649v1 [stat.ME] 13 Aug 2020

Qihuang Zhang and Grace Y. Yi1

Abstract
Autoregressive (AR) models are useful tools in time series analysis. Inferences under such
models are distorted in the presence of measurement error, which is very common in practice. In this article, we establish analytical results for quantifying the biases of the parameter
estimation in AR models if the measurement error effects are neglected. We propose two
measurement error models to describe different processes of data contamination. An estimating equation approach is proposed for the estimation of the model parameters with
measurement error effects accounted for. We further discuss forecasting using the proposed
method. Our work is inspired by COVID-19 data, which are error-contaminated due to
multiple reasons including the asymptomatic cases and varying incubation periods. We
implement our proposed method by conducting sensitivity analyses and forecasting of the
mortality rate of COVID-19 over time for the four most populated provinces in Canada.
The results suggest that incorporating or not incorporating measurement error effects yields
rather different results for parameter estimation and forecasting.

Keywords: Autoregressive Model, COVID-19, Forecasting, Measurement Error, Sensitivity
Analysis, Time Series.
Short title: Time Series with Measurement Error COVID-19
1
Corresponding Author: Department of Statistical and Actuarial Sciences, Department of Computer
Science, University of Western Ontario, London, Ontario, Canada, N6A 5B7. Department of Statistics and
Actuarial Science, University of Waterloo. Email: gyi5@uwo.ca

1

Introduction

Time series data are common in the fields of epidemiology, economics, and engineering.
Various models and methods have been developed for analyzing such data. The validity of
these methods, however, hinges on the condition that time series data are precisely collected.
This condition is restrictive in applications. Measurement error is often inevitable. In the
study of air pollution, for example, it is difficult or even impossible to precisely obtain the
true measurement of the daily air population level.
Some work on time series subject to measurement error is available in the literature.
Tanaka (2002) proposed a Lagrange multiplier test to assess the presence of measurement
error in time series data. Staudenmayer and Buonaccorsi (2005) explored the classical measurement error model for the autoregressive process. Tripodis and Buonaccorsi (2009) studied
measurement error in forecasting using the Kalman filter. Dedecker et al. (2014) considered
a non-linear AR(1) model with measurement error. Despite available discussions of measurement error in time series, several limitations restrict the application scope of the existing
work. Most available methods consider only autoregressive models without the drift and
assume the simplest additive measurement error model. Furthermore, most work involves a
complex formulation to adjust for the measurement error effects, which is not straightforward to implement for practitioners. In addition, to our knowledge, there is no available
work addresses measurement error effects on prediction under autoregressive models.
In this article, we systematically explore analysis of error-prone time series data under
autoregressive models. We propose two types of models to delineate measurement error
processes: additive regression models and multiplicative models. These modeling schemes
offer us great flexibility in facilitating different applications. We investigate the impact of the
naive analysis which ignores the feature of measurement error in the inferential procedures,
and we obtain analytical results for characterizing the biases incurred in the naive analysis.
We develop an estimating equation approach to adjust for the measurement error effects on
time series analysis. We establish asymptotic results for the proposed estimators, and develop
the theoretical results for the forecasting of times series in the presence of measurement
error. Finally, we describe a block bootstrap algorithm for computing standard errors of the
1

proposed estimators.
Our work is partially motivated by the data of COVID-19, a wide-spread disease that has
become a global health challenge and has caused over ten million infections and half million
deaths as of August, 2020. Because of the special features of the disease, the COVID-19 data
introduce a number of new challenges: 1) due to the asymptomatic infected cases and the
patients with light symptoms who do not go to hospitals, the number of reported cases with
COVID-19 is typically smaller than the true number of infected cases; 2) due to the limited
test resources, many infected cases are not able to be identified instantly; and 3) the varying
incubation periods lead to the delay of the identification of the infections. Consequently, the
discrepancy between the reported case number and the true case number can be substantial,
and ignoring these features and applying the traditional time series analysis method would
no longer produce valid results.
In this paper, we apply the developed methods to analyze the COVID-19 data. We are
interested in studying how the mortality rate in a region may change over time and describing
the trajectory of the death rate. While the mortality rate of a disease is defined as the death
number divided by the case number, the determination of the mortality rate of COVID-19
is challenging. In contrast to the standard definition, Baud et al. (2020) estimated mortality
rates by dividing the number of deaths on a given day by the number of patients with
confirmed COVID-19 infections 14 days earlier, driven by the consideration of the maximum
incubation time to be 14 days. Due to the unique features of COVID-19, there does not seem
to be a precise way to define the mortality rate of COVID-19. In this paper, we conduct
sensitivity analyses to assess the severity of the pandemic by using different definitions of the
mortality rate and considering different ways of modeling measurement error in the data.
The remainder of the article is organized as follows. The notation and the setup for
autoregressive time series models and the proposed measurement error models are introduced
in Section 2. In Section 3, we present the theoretical results for characterizing the impact of
measurement error on the analysis of time series data. In Section 4, we develop an estimating
equation approach to adjust for the biases due to measurement error. In Section 5, we
implement the proposed method to analyze the COVID-19 data in four Canadian provinces.
The article is concluded with a discussion presented in Section 6.
2

2

Model Setup and Framework

2.1

Time Series Model

Consider a T Ã— 1 vector of time series, X (T ) = (X1 , X2 , . . . , XT )T . We are interested in
modeling the dependence of Xt on it previous observations X (tâˆ’1) and we consider it to be
postulated by an autoregressive model with lag p
Xt = Ï†0 +

p
X

Ï†j Xtâˆ’j + t ,

(1)

j=1

where p is an integer smaller than T , (t) = (1 , . . . , t )T is independent of X (t) = (X1 , . . . , Xt )T
with each t having zero mean and variance Ïƒ2 , Ï†0 is a constant drift, and Ï† = (Ï†1 , . . . , Ï†p )T
is the regression coefficient.
The additive form in (1) and the zero mean assumption of t show that Ï†0 and Ï† are
constrained by
etâˆ’1 )}T Ï†,
Ï†0 = E(Xt ) âˆ’ {E(X

(2)

etâˆ’1 = (Xtâˆ’1 , . . . , Xtâˆ’p )T . To make the process of Xt stationary, Ï†1 , . . . , Ï†p are further
where X
constrained such that all the roots of the equation in z
z p âˆ’ Ï†1 z pâˆ’1 âˆ’ Â· Â· Â· âˆ’ Ï†p = 0
have absolute values smaller than 1 (Brockwell and Davis 2002, Sec.3.1.). For example,
a stationary AR(1) process requires that |Ï†1 | < 1, and a stationary AR(2) process needs
that (Ï†1 + Ï†2 ) < 1, (Ï†2 âˆ’ Ï†1 ) < 1 and |Ï†2 | < 1. Here we are interested in the estimation
of parameters, Ï† and Ï†0 . Let Âµ denote the mean E(Xt ) of the time series, which equals
Ï†0
1âˆ’Ï†1 âˆ’...âˆ’Ï†p

if Xt is (weakly) stationary. When p = 1, the stationarity of a time series implies

Var(Xt ) =

Ïƒ2
1âˆ’Ï†21

2.2

for t = 1, . . . , T .

Estimation of Model Parameters

The estimation of the parameters in the AR(p) time series model (1) can be carried out by
the least squares method. To see this, we first focus on estimation of Ï† = (Ï†1 , . . . , Ï†p )T . Let
P
P
S(Ï†) = Tt=p+1 {Xt âˆ’ (Ï†0 + pj=1 Ï†j Xtâˆ’j )}2 be the sum of the squared difference between
3

Xt and its linearly combined history with lag p. Then applying the constraint (2) gives
h
i2
P
etâˆ’1 âˆ’ E(X
etâˆ’1 )}T Ï† .
S(Ï†) = Tt=p+1 {Xt âˆ’ E(Xt )} âˆ’ {X
To minimize S(Ï†) with respect to Ï†, we solve
(LS)

Ï†b

=

âˆ‚S(Ï†)
âˆ‚Ï†

T
n
on
oT
X
e
e
e
e
Xtâˆ’1 âˆ’ E(Xtâˆ’1 ) Xtâˆ’1 âˆ’ E(Xtâˆ’1 )

= 0 for Ï† and obtain the solution

!âˆ’1

t=p+1

T
n
o
X
e
e
Xtâˆ’1 âˆ’ E(Xtâˆ’1 ) {Xt âˆ’ E(Xt )} ,
t=p+1

(3)
where for t = 1, . . . , T , E(Xt ) can be estimated by

1
T

PT

t=1

Xt , which is denoted as Âµ
b.

Next, by the constraint (2), replacing E(Xt ) by Âµ
b gives an estimator of Ï†0 :
Ï†b(LS)
bâˆ’Âµ
b
0 = Âµ

p
X

Ï†bj .

(4)

j=1

Re-expressing (1) as t = Xt âˆ’ (Ï†0 +

Pp

j=1

Ï†j Xtâˆ’j ) and by the definition of S(Ï†), we may

estimate Var(t ) = Ïƒ2 by
Ïƒ
b2(LS) =
=

1
b
S(Ï†)
T âˆ’p
T
X
1

{Xt âˆ’ E(Xt )}2 âˆ’

T âˆ’ p t=p+1

T
X
2
etâˆ’1 âˆ’ E(X
etâˆ’1 )}T Ï†b
{Xt âˆ’ E(Xt )}{X
T âˆ’ p t=p+1

T
X
1
etâˆ’1 âˆ’ E(X
etâˆ’1 )}{X
etâˆ’1 âˆ’ E(X
etâˆ’1 )}T Ï†b
Ï†bT {X
+
T âˆ’ p t=p+1

(5)

with E(Xt ) estimated by Âµ
b.
Estimators (3)â€“(5) can be derived in an alternative way. First, by the stationarity of the
Xt , for k = 0, . . . , p and p â‰¤ t, Cov(Xt , Xtâˆ’k ) is time-independent and let Î³k denote it; it is
clear that Î³0 represents Var(Xt ) for any t. Let Î“ be the autocovariance matrix
ï£«
ï£¶
Î³0 Â· Â· Â· Î³pâˆ’1
ï£¬
ï£·
.. ï£·
ï£¬ .
..
Î“ = ï£¬ ..
.
. ï£·.
ï£­
ï£¸
Î³pâˆ’1 Â· Â· Â· Î³0
bk =
Let Î³
b = (Î³b1 , Â· Â· Â· , Î³bp )T with Î³

1
T âˆ’k

PT

t=k+1 (Xt

âˆ’Âµ
b)(Xtâˆ’k âˆ’ Âµ
b) being an estimator of Î³k for

b be the estimator of Î“ with Î³k replaced by Î³bk for k = 0, . . . , p âˆ’ 1.
k = 0, . . . , p, and let Î“
Next, we examine the summation terms in (3) and (5) by using the fact that as T â†’ âˆ,
PT
PT
1
1
2 p
T p
e
e
â†’ Î³0 , T âˆ’p
â†’ Î³, and
t=p+1 {Xt âˆ’ E(Xt )} âˆ’
t=p+1 {Xt âˆ’ E(Xt )}{Xtâˆ’1 âˆ’ E(Xtâˆ’1 )} âˆ’
T âˆ’p
4

1
T âˆ’p

PT

t=p+1 {Xtâˆ’1 âˆ’E(Xtâˆ’1 )}{Xtâˆ’1 âˆ’E(Xtâˆ’1 )}

e

e

e

e

T

p

âˆ’â†’ Î“. Then, (3)â€“(5) motivate an alternative

method of finding estimators for Ï†, Ï†0 , and Ïƒ2 , by solving the estimating equations:
bâˆ’1 Î³
Ï†=Î“
b;
Ï†0 =

1âˆ’

p
X

!
Ï†i

Âµ
b;

(6)

i=1

b
Ïƒ2 = Î³b0 âˆ’ 2Ï†T Î³
b + Ï†T Î“Ï†,
b Ï†b0 and Ïƒ
for Ï†, Ï†0 , and Ïƒ2 . Let Ï†,
b2 denote the resultant estimators of Ï†, Ï†0 , and Ïƒ2 ,
respectively. These estimators are asymptotically equivalent to the least squares estimators
p
p
p
b2(LS) âˆ’â†’ 0, as
â†’ 0 and Ïƒ
b2 âˆ’ Ïƒ
Ï†b(LS) , Ï†b(LS)
b2(LS) in a sense that Ï†b âˆ’ Ï†b(LS) âˆ’â†’ 0, Ï†b0 âˆ’ Ï†b(LS)
0 , and Ïƒ
0 âˆ’

T â†’ âˆ, and hence, they are consistent (Box et al. 2015, Ch.7, A.7.4).
Estimating equations (6) offer a unified estimation framework in its connections with
not only the least squares estimation but also the maximum likelihood method under the
assumption of Gaussian error as well as the Yule-Walker method. Similar to the least squares
method, finding estimators using one of those approaches is asymptotically equivalent to
solving (6) for Ï†, Ï†0 and Ïƒ2 (Box et al. 2015, Ch.7, A.7.4).

3

Measurement Error and Impact

3.1

Measurement Error Models

Suppose that for t = 1, . . . , T , the observation of Xt is subject to measurement error and
the precise measurement of Xt may not be observed, but its surrogate measurement Xtâˆ— is
available. We consider two measurement error models.
The first measurement error model takes an additive form
Xtâˆ— = Î±0 + Î±1 Xt + et

(7)

for t = 1, . . . , T , where the error term et is independent of Xt with mean 0 and timeindependent variance Ïƒe2 and is assumed to be independent for t = 1, . . . , T , and Î± = (Î±0 , Î±1 )T
is the parameter vector. Here, Î±0 represents the systematic error and Î±1 represents the
5

constant inflation (or shrinkage) due to the measurement error. For instance, if Î±0 = 0, then
setting Î±1 < 1 (or Î±1 > 1) features the scenario where Xtâˆ— tends to be smaller (or larger)
than Xt if the noise term is ignored. This model generalizes the classical additive model
considered by Staudenmayer and Buonaccorsi (2005) who considered the case with Î±0 = 0
and Î±1 = 1.
By the stationarity of the Xt , we note that model (7) yields E(Xtâˆ— ) = Î±0 + Î±1 Âµ and
Var(Xtâˆ— ) = Î±12 Î³0 + Ïƒe2 ;

(8)

the variability of the Xtâˆ— can be greater or smaller than that of the Xt , depending on the
value of Î±1 .
The second measurement error model assumes a multiplicative form:
Xtâˆ— = Î²0 ut Xt ,

(9)

for t = 1, . . . , T , where Î²0 is a positive scaling parameter, and the ut are the error terms
which are independent of each other as well as of the Xt , and have mean one and timeindependent variance Ïƒu2 . Depending on the distribution of the error term ut , (9) can feature
different types of discrepancy between Xt and Xtâˆ— .
The stationarity of the Xt together with model (9) implies E(Xtâˆ— ) = Î²0 Âµ, and

Var(Xtâˆ— ) = Î²02 (Ïƒu2 + 1)Î³0 + Ïƒu2 Âµ2 ,

(10)

where we use the independence of Xt and ut .
Since E(Xtâˆ— ) is time-independent for both (7) and (9), in the following discussion, we
let Âµâˆ— denote E(Xtâˆ— ) for t = 1, . . . , T . The modeling of the measurement error process by
(7) or (9) introduces extra parameters {Î±0 , Î±1 , Ïƒe2 } or {Î²0 , Ïƒu2 }, where the variance of the
error term is bounded by the variability of Xtâˆ— together with others. Clearly, (8) shows that
Ïƒe2 < Var(Xtâˆ— ) and (10) implies that Ïƒu2 <

3.2

Var(Xtâˆ— )
.
Î²02 Âµ2

Naive Estimation and Bias for AR(1) Model

Estimating equations (6) are useful when measruements of Xt are available. However, due
to the measurement error, Xt is not observed so (6) cannot be directly used for estimation of
6

the parameters for model (1). As the surrogate Xtâˆ— for Xt is available, one may attempt to
employ the naive analysis to model (1) with Xt replaced by Xtâˆ— . Here we study the impact
of measurement error on the naive analysis disregarding the difference between Xt and Xtâˆ— .
We start with the AR(1) model, i.e., model (1) with p = 1.
If we naively replace Xt in (1) by Xtâˆ— , then the time series model (1) becomes
âˆ—
+ âˆ—t ,
Xtâˆ— = Ï†âˆ—0 + Ï†âˆ—1 Xtâˆ’1

(11)

where (Ï†âˆ—0 , Ï†âˆ—1 )T and âˆ—t show possible differences from the corresponding quantity in the
model (1). To estimate Ï†âˆ—0 and Ï†âˆ—1 , we may employ the ordinary least squares (OLS) method.
P
âˆ—
)2 with respective to Ï†âˆ—0 and
Specifically, we minimize S(Ï†âˆ—0 , Ï†âˆ—1 ) = Tt=2 (Xtâˆ— âˆ’ Ï†âˆ—0 âˆ’ Ï†âˆ—1 Xtâˆ’1
Ï†âˆ—1 , yielding the OLS estimators of Ï†âˆ—1 and Ï†âˆ—0 :
PT
Ï†bâˆ—1

âˆ—
âˆ—
âˆ—
t=2 (Xtâˆ’1 âˆ’ XÌ„(âˆ’1) )(Xt âˆ’
PT
âˆ—
âˆ—
2
t=2 (Xtâˆ’1 âˆ’ XÌ„(âˆ’1) )

XÌ„ âˆ— )

,

Ï†bâˆ—0 = XÌ„tâˆ— âˆ’ Ï†bâˆ—1 XÌ„ âˆ— ,

and
âˆ—
=
where XÌ„(âˆ’1)

=

1
T âˆ’1

PT

t=2

Theorem 1 Let Ï‰1 =

âˆ—
and XÌ„ âˆ— =
Xtâˆ’1

Î±21 Ïƒ2
,
Î±21 Ïƒ2 +Ïƒe2 (1âˆ’Ï†21 )

1
T âˆ’1

(12)

PT

t=2

Xtâˆ— .


Ï†âˆ—1 = Ï†1 Ï‰1 , and Ï†âˆ—0 = Î±0 +

Î± 1 Ï†0
1âˆ’Ï†1



(1 âˆ’ Ï†1 Ï‰1 ). Assume

the stationarity of the times series. If the measurement error process satisfies (7), then
p
p
(1) Ï†bâˆ—1 âˆ’âˆ’â†’ Ï†âˆ—1 and Ï†bâˆ—0 âˆ’âˆ’â†’ Ï†âˆ—0 as T â†’ âˆ,

(2) âˆ—t = Î±0 (1 âˆ’ Ï†âˆ—1 ) + Î±1 Ï†0 âˆ’ Ï†âˆ—0 + Î±1 (Ï†1 âˆ’ Ï†âˆ—1 )Xtâˆ’1 + (1 âˆ’ Ï†âˆ—1 )et + Î±1 t for t = 1, . . . , T ,
 2 
Ïƒ
+ (1 âˆ’ Ï‰1 Ï†1 )2 Ïƒe2 + Î±12 Ïƒ2 .
and hence Var(âˆ—t ) = Ï†21 Î±12 (1 âˆ’ Ï‰1 )2 1âˆ’Ï†
2
1

The proof of the theorem is included in Supplementary Appendix A.2. This theorem
essentially implies that the naive estimator under the additive form in (7) is inconsistent
because Ï†âˆ—1 6= Ï†1 and Ï†âˆ—0 6= Ï†0 . The naive estimator Ï†bâˆ—1 attenuates and the attenuation factor
Ï‰1 depends on the parameters Î±1 and Ïƒe2 of the measurement error model (7) as well as Ï†1
and Ïƒ2 in the time series model (1). The coefficient Î±1 in the measurement error model
(7) affects the estimation of the both naive estimators Ï†bâˆ—1 and Ï†bâˆ—0 , while the intercept Î±0
influences the estimation of Ï†âˆ—0 only, but not Ï†âˆ—1 or Var(âˆ— ).
7

Theorem 2 Let Ï‰2 = {1 + Ïƒu2 +

2 Ï†2
(1+Ï†1 )Ïƒu
0 âˆ’1
} ,
(1âˆ’Ï†1 )Ïƒ2

Ï†âˆ—1 = Ï†1 Ï‰2 , and Ï†âˆ—0 =

Î² 0 Ï†0
1âˆ’Ï†1

(1 âˆ’ Ï‰2 Ï†1 ). If the

times series is stationary and the measurement error process satisfies (9), then
p
p
(1) Ï†bâˆ—1 âˆ’âˆ’â†’ Ï†âˆ—1 and Ï†bâˆ—0 âˆ’âˆ’â†’ Ï†âˆ—0 as T â†’ âˆ,

(2) âˆ—t = Î²0 Ï†0 ut âˆ’ Ï†âˆ—0 + Î²0 Xtâˆ’1 (Ï†1 ut âˆ’ Ï‰2 Ï†1 utâˆ’1 ) + Î²0 ut t for t = 1, . . . , T ,
and hence Var(âˆ—t ) = Î²02 {Ïƒu2 Ï†20 + (1 + Ïƒu2 )Ïƒ2 } + Î²02 Ï†21

(1+Ï‰22 ) Ïƒ2
.
Ï‰2 (1âˆ’Ï†21 )

The proof of the theorem is included in Supplementary Appendix A.3. This theorem
says the attenuation effect resulting from the measurement error on estimation of Ï†1 . The
constant scaling parameter Î²0 in the measurement error model (9) does not influence the
estimation of Ï†1 but affects the estimation of Ï†0 and Ïƒ2 . The attenuation factor Ï‰2 is
determined by the magnitude Ïƒu2 of measurement error as well as the values of Ï†0 , Ï†1 , and
Ïƒ2 of the time series model (1).

3.3

Naive Estimation and Bias for AR(p) Model with p â‰¥ 2

We now extend the discussion in Section 3.2 to the AR(p) model with p â‰¥ 2. Replacing Xt
with Xtâˆ— in (1) gives the working model
Xtâˆ—

=

Ï†âˆ—0

+

p
X

âˆ—
Ï†âˆ—j Xtâˆ’j
+ âˆ—t ,

(13)

j=1

where Ï†âˆ— = (Ï†âˆ—1 , . . . , Ï†âˆ—p )T and âˆ—t may differ from the corresponding symbol in (1). If mimicking the procedure of using (6) with Xt replaced by Xtâˆ— to estimate Ï†âˆ— , Ï†âˆ—0 and Ïƒ2âˆ— in (13),
then we let Ï†bâˆ— = (Ï†bâˆ—1 , . . . , Ï†bâˆ—p )T , Ï†bâˆ—0 and Ïƒ
bâˆ—2 denote the resultant estimators. Similar to Î³
bk and
P
P
T âˆ’k
1
âˆ—
âˆ—
Âµ
b, we define Âµ
bâˆ— = T1 Tt=1 Xtâˆ— and Î³
bkâˆ— = T âˆ’k
bâˆ— )(Xt+k
âˆ’Âµ
bâˆ— ) for k = 1, . . . , p. Let
t=1 (Xt âˆ’ Âµ
P
Î³
bâˆ— = (b
Î³1âˆ— , . . . , Î³
bpâˆ— )T and Î³
b0âˆ— = T1 Tt=1 (Xtâˆ— âˆ’ Âµ
bâˆ— )(Xtâˆ— âˆ’ Âµ
bâˆ— ).
We now discuss the asymptotic results of the naive estimators under different measurement error models.
Theorem 3 Let 1p be the pÃ—1 unit and let Ip be the pÃ—p identity matrix. Define Î³ âˆ— = Î±12 Î³,
Î³0âˆ— = Î±12 Î³0 + Ïƒe2 , Ï†âˆ— = Î±12 (Î±12 Î“ + Ïƒe2 Ip )âˆ’1 Î³, Ï†âˆ—0 = (1 âˆ’ Ï†âˆ— Â· 1p ) (Î±0 + Î±1 Âµ) and Ïƒ2âˆ— = Î±12 Î³0 +
Ïƒe2 âˆ’ Î±14 Î³ T (Î±12 Î“ + Ïƒe2 Ip )

âˆ’1

Î³. Under regularity conditions, if the time series is stationary and

the measurement error process satisfies (7), then
8

p

p

(1) Î³
bâˆ— âˆ’âˆ’â†’ Î³ âˆ— and Î³
b0âˆ— âˆ’âˆ’â†’ Î³0âˆ—

as T â†’ âˆ.

p
p
p
b2âˆ— âˆ’âˆ’â†’ Ïƒ2âˆ—
(2) Ï†bâˆ— âˆ’âˆ’â†’ Ï†âˆ— , Ï†bâˆ—0 âˆ’âˆ’â†’ Ï†âˆ—0 , and Ïƒ

as T â†’ âˆ.

(3) Let Q1 denote the (p+1)Ã—(p+1) asymptotic covariance matrix of

âˆš  âˆ— âˆ—T T
T (b
Î³0 , Î³
b ) âˆ’ (Î³0âˆ— , Î³ âˆ—T )T

as T â†’ âˆ. Then the elements of Q1 are given by
âˆ—
q100
= Î±14 q00 + 4Î±12 Î³0 Ïƒe2 + E(e4t ) âˆ’ Ïƒe4 ;
âˆ—
q10p
= Î±14 q0p + 4Î±12 Î³p Ïƒe2 ;
âˆ—
q1pr
= Î±14 qpr + 2Î±12 Ïƒe2 (Î³|pâˆ’r| + Î³p+r ) for r 6= 0, r 6= p;
âˆ—
q1pp
= Î±14 qpp + 2Î±12 Ïƒe2 (Î³0 + Î³2p ) + Ïƒe4 ;

for p â‰¥ 1, where qjk is the (j, k) element of the asymptotic covariance matrix of
(b
Î³0 , Î³
bT )T , given by (Brockwell et al. 1991, Sec. 7.3)
âˆ
X

qjk = (Î· âˆ’ 3)Î³j Î³k +

(Î³i Î³iâˆ’j+k + Î³i+k Î³iâˆ’j )

(14)

i=âˆ’âˆ

for (j, k) = (0, 0), (0, p), (p, p) and (p, r) with r 6= 0 and r 6= p, with Î· = E(4t )/Ïƒ4 .
The proof of Theorem 3 is presented in Supplementary Appendix A.4. Similar to the
results in Theorem 1, the intercept Î±0 only influence Ï†0 and does not influence Ï†.
Theorem 4 Let Î³ âˆ— = Î²02 Î³, Î³0âˆ— = Î²02 {(Ïƒu2 + 1)Î³0 + Ïƒu2 Âµ2 }, Ï†âˆ— = {Î“ + Ïƒu2 (Î³0 + Âµ2 )Ip }

âˆ’1

Ï†âˆ—0 = Î²0 (1 âˆ’ Ï†âˆ—T Â· 1p ) Âµ, and Ïƒ2âˆ— = Î²02 (Ïƒu2 + 1)Î³0 + Î²02 Ïƒu2 Âµ2 âˆ’ Î²02 Î³ T {Î“ + Ïƒu2 (Î³0 + Âµ2 )Ip }

âˆ’1

Î³,
Î³.

Under regularity conditions, if the time series are stationary and the measurement error
process satisfy (9), then
p

p

(1) Î³
bâˆ— âˆ’âˆ’â†’ Î³ âˆ— and Î³
b0âˆ— âˆ’âˆ’â†’ Î³0âˆ—

as T â†’ âˆ.

p
p
p
(2) Ï†bâˆ— âˆ’âˆ’â†’ Ï†âˆ— , Ï†bâˆ—0 âˆ’âˆ’â†’ Ï†âˆ—0 , and Ïƒ
b2âˆ— âˆ’âˆ’â†’ Ïƒ2âˆ—

as T â†’ âˆ.

(3) Let Q2 denote the (p+1)Ã—(p+1) asymptotic covariance matrix of

9

âˆš  âˆ— âˆ—T T
T (b
Î³0 , Î³
b ) âˆ’ (Î³0âˆ— , Î³ âˆ—T )T

as T â†’ âˆ. Then the elements of Q2 are given by
âˆ—
q200
= Î²04 (Ïƒu2 + 1)2 q00 + Î²04 {E(u4t ) âˆ’ (Ïƒu2 + 1)2 }E(Xt âˆ’ Âµ)4

+ 4ÂµÎ²04 Ïƒu2 (Ïƒu2 + 1)v0 + 4ÂµÎ²04 {E(u4t ) âˆ’ E(u3t ) âˆ’ Ïƒu2 (Ïƒu2 + 1)}E(Xt âˆ’ Âµ)3

+ 2Âµ2 Î²04 E(u4t ) âˆ’ 2E(u3t ) + 1 âˆ’ Ïƒu4 Î³0
"
#
âˆ
X



2 4
4
4
3
2
4
+ 4Âµ Î²0 Ïƒu
Î³h + E(ut ) âˆ’ 2E(ut ) + Ïƒu + 1 âˆ’ Ïƒu Î³0 + Âµ4 Î²04 E{(ut âˆ’ 1)4 } âˆ’ Ïƒu4 ;
h=âˆ’âˆ




âˆ—
q20p
= Î²04 qp (Ïƒu2 + 1) + Î²04 E(u3t ) âˆ’ (Ïƒu2 + 1) E{(Xt âˆ’ Âµ)3 (Xt+p âˆ’ Âµ)} + E{(Xt âˆ’ Âµ)3 (Xtâˆ’p âˆ’ Âµ)}


+ 2ÂµÎ²04 Ïƒu2 v0p + ÂµÎ²04 E{3u3t âˆ’ 3u2t âˆ’ 2Ïƒu2 } E{(Xt âˆ’ Âµ)2 (Xtâˆ’p âˆ’ Âµ)} + E{(Xt âˆ’ Âµ)2 (Xt+p âˆ’ Âµ)}

âˆ—
q2pr

+ 6Âµ2 Î²04 E(ut âˆ’ 1)3 Î³p + 4Âµ2 Î²04 Ïƒu2 Î³p ;

= Î²04 qpr + Î²04 Ïƒu2 E{(Xt âˆ’ Âµ)2 (Xt+p âˆ’ Âµ)(Xt+r âˆ’ Âµ)} + E{(Xt âˆ’ Âµ)(Xt+p âˆ’ Âµ)2 (Xt+p+r âˆ’ Âµ)}

+E{(Xtâˆ’r âˆ’ Âµ)(Xt âˆ’ Âµ)2 (Xt+p âˆ’ Âµ)} + E{(Xt âˆ’ Âµ)(Xt+pâˆ’r âˆ’ Âµ)(Xt+p âˆ’ Âµ)2 }
+ ÂµÎ²04 Ïƒu2 [E{(Xt âˆ’ Âµ)(Xt+p âˆ’ Âµ)(Xt+r âˆ’ Âµ)} + E{(Xt âˆ’ Âµ)(Xt+p âˆ’ Âµ)(Xt+p+r âˆ’ Âµ)}
+E{(Xtâˆ’r âˆ’ Âµ)(Xt âˆ’ Âµ)(Xt+p âˆ’ Âµ)} + E{(Xt âˆ’ Âµ)(Xt+pâˆ’r âˆ’ Âµ)(Xt+p âˆ’ Âµ)}]
+ 2Âµ2 Î²04 Ïƒu2 (Î³|pâˆ’r| + Î³p+r ) for r 6= p, r 6= 0;

âˆ—
q2pp
= Î²04 qpp + Î²04 (Ïƒu4 + 2Ïƒu2 )Var{(Xt âˆ’ Âµ)(Xt+p âˆ’ Âµ)} + 2Î²04 E{(Xt âˆ’ Âµ)(Xt+p âˆ’ Âµ)2 (Xt+2p âˆ’ Âµ)}

+ ÂµÎ²04 Ïƒu2 E{(Xt âˆ’ Âµ)(Xt+p âˆ’ Âµ)2 } + 2E{(Xt âˆ’ Âµ)(Xt+p âˆ’ Âµ)(Xt+2p âˆ’ Âµ)}

+ E{(Xt âˆ’ Âµ)2 (Xt+p âˆ’ Âµ)} + 2Âµ2 Î²04 Ïƒu4 Î³p + 2Âµ2 Î²04 Ïƒu2 (Î³0 + Î³2p ) + Âµ4 Î²04 Ïƒu4 ;

where the qjk are given by (14), for (j, k) = (0, 0), (0, p), (p, p) and (p, r) with r 6= 0
P P
and r 6= p, and vp = limT â†’âˆ T1 Tt=1 Ts=1 E{(Xt âˆ’ Âµ)(Xt+p âˆ’ Âµ)(Xs âˆ’ Âµ)}.
The proof of the theorem is presented in Supplementary Appendix A.5. The multiplicative measurement error ut contributes to the biasedness of the parameter estimation for Ï†,
while the scaling parameter Î²0 has no effects on the naive estimator Ï†bâˆ— .

4

Methodology of Correcting Measurement Error Effects

4.1

Estimation of Model Parameters

In the presence of measurement error, measurements of the Xt are not always available but
surrogate measurements Xtâˆ— are available. It may be tempting to conduct a naive analysis
10

by implementing (6) with the Xt replaced by the Xtâˆ— , or equivalently with Âµ
b and Î³
bk replaced
by Âµ
bâˆ— and the Î³
bkâˆ— , respectively, to find estimators of Ï†, Ï†0 and Ïƒ2 . However, by Theorems 3â€“
4, such a procedure typically yields biased estimators. In this section, we develop new
estimators accounting for the measurement error effects described by either the additive
model (7) or the multiplicative model (9).
Our idea is still to employ (6) to find consistent estimators of Ï†, Ï†0 and Ïƒ2 , but instead
of replacing Âµ
b and the Î³
bk with Âµ
bâˆ— and the Î³
bkâˆ— as in the naive analysis, we replace Âµ
b and
the Î³
bk in (6) with new functions of the Xtâˆ— , denoted as Âµ
e and the Î³
ek , which adjust for the
measurement error effects. Specifically, if we can find Âµ
e and the Î³
ek such that they resemble
Âµ
b and the Î³
bk in the sense that as T â†’ âˆ,
Âµ
e and Âµ
b have the same limit in probability,
and

Î³
ek and Î³
bk have the same limit in probability for k = 0, . . . , p,

(15)

then substituting Âµ
b and the Î³
bk with Âµ
e and the Î³
ek in (6) yields consistent estimators of Ï†, Ï†0
and Ïƒ2 .
e denote Î“ with the Î³k replaced by
With the availability of the Î³
ek satisfying (15), let Î“
the Î³
ek . Then provided regularity conditions, consistent estimators of Ï†, Ï†0 and Ïƒ2 can be
obtained by solving the estimating equations for Ï†, Ï†0 , and Ïƒ2 :
eâˆ’1 Î³
Ï†=Î“
e,
Ï†0 =

1âˆ’

p
X

!
Ï†i

Âµ
e,

(16)

i=1

e
Ïƒ2 = Î³
e0 âˆ’ 2Ï†T Î³
e + Ï†T Î“Ï†.
It is immediate to obtain the following result.
Theorem 5 Assume regularity conditions hold and the time series are stationary. If Âµ
e and
e Ï†e0 , and
the Î³
ek are functions of the Xtâˆ— with t = 1, . . . , T and they satisfy (15), and let Ï†,
Ïƒe 2 denote the estimators for Ï†, Ï†0 and Ïƒ2 , respectively, obtained by solving (16). Then, as
T â†’âˆ
p
p
p
(1) Ï†e âˆ’â†’ Ï†, Ï†e0 âˆ’â†’ Ï†0 , and Ïƒe 2 âˆ’â†’ Ïƒ2 ;

11

(2)

âˆš

d

n(Ï†e âˆ’ Ï†) âˆ’âˆ’â†’ N (0, GQGT ),

bâˆ—T )T .
where G is the matrix of derivatives of Ï†e with respect to the components of (b
Î³0âˆ— , Î³
Here Q = Q1 , the matrix in Theorem 3, if measurement error follows the model (7);
and Q = Q2 , the matrix in Theorem 4, if measurement error follows the model (9).
Now we discuss explicitly how to determine Âµ
e and the Î³
ek under the measurement error
model (7) or (9). With (7), take Âµ
e=
With (9), take Âµ
e=

Âµ
bâˆ—
Î²0

,Î³
e0 =

Î³0âˆ—
2 )Î² 2
(1+Ïƒu
0

Âµ
bâˆ—
Î±1

âˆ’ Î±0 , Î³
e0 =

âˆ’

2 Âµ2
Ïƒu
2 +1
Ïƒu

, and

1
(b
Î³0âˆ— âˆ’ Ïƒe2 ),
Î±21
Î³
bâˆ—
Î³
ek = Î²k2 for
0

and Î³
ek =

Î³
bkâˆ—
Î±21

for k = 1, . . . , p.

k = 1, . . . , p. By the results

in Theorem 3(1) and Theorem 4(1), it can be easily verified that these Âµ
e and the Î³
ek satisfy
(15).
We conclude this section with a procedure of estimating the asymptotic covariance mae While Theorem 5 presents the sandwich form of the asymptotic
trix for the estimator Ï†.
e its evaluation involves lengthy calculations. We may alternatively
covariance matrix of Ï†,
employ the block bootstrap algorithm (Lahiri 1999) to obtain variance estimates for Ï†e using
the following steps. Firstly, we set a positive integer, say N , as the number for the bootstrap sampling; N can be set as a large number such as 1000. Next, we repeat through the
following five steps:
Step 1: At iteration n âˆˆ {1, . . . , N }, we initialize a null time series X (n,0) of dimension 0
and specify a block length, say b, which is an integer between 0 and T . Initialize
m=1.
(mâˆ’1)

Step 2: Sample an index, say i, from {0, . . . , T âˆ’ b}, and then define Xadd

=

{Xi+1 , . . . , Xi+b }.
(mâˆ’1)

Step 3: Update the previous time series X (n,mâˆ’1) by appending Xadd

to it, and let

X (n,m) denote the new time series.
Step 4: If the dimension X (n,m) is smaller than T then return to Steps 2 and 3; otherwise drop the elements in the time series with the index greater than T to
ensure the dimension of X (n,m) is identical to T and then go to Step 5.

12

Step 5: Obtain an estimate Ï†e(n) of parameter Ï† by applying the times series X (n,m)
to (16). If n < N , then set n to be n + 1 and go back to Step 1 to repeat;
otherwise stop.
Â¯
Let Ï†e(n) =

1
N

PN

n=1

Â¯
Ï†e(n) be the sample mean. The bootstrap variance of Ï†e is then given

by,
N
1 X e(n) Â¯e(n) 2
e
Varboot (Ï†) =
(Ï† âˆ’ Ï† ) .
N n=1

4.2

Forecasting and Prediction Error

Forecasting is an important application of the autoregressive models. Specifically, in forecasting based on the observed time series X(T ) = {x1 , . . . , xT }, we are interested in the predictions
of {XT +1 , . . . , XT +H } for a positive integer H, which is done one by one starting from the
nearest time point T + 1 to the farthest time point T + H. To this end, let h = 1, . . . , H,
the h-step forecasting of XT +h is based on its history of lag-p, {XT +hâˆ’1 , . . . , XT +hâˆ’p }, by
bT +h , where for
using the conditional expectation E(XT +h |xT +hâˆ’1 , . . . , xT +hâˆ’p ), denoted X
j = T + h âˆ’ 1, . . . , T + h âˆ’ p, xj is the observe value of Xj if j â‰¤ T ; and xj is the prebj , if j > T . This prediction minimizes the squared prediction error
dicted value of Xj , X
bT +h âˆ’ XT +h )2 (e.g., Box et al. 2015, p.131).
E(X
If no measurement error is involved, due to the zero mean of the random error term t in
the AR(p) model (1), for h = 1, . . . , H, the conditional expectation can be calculated by
bT +h = Ï†0 + Ï†1 xT +hâˆ’1 + . . . + Ï†p xT +hâˆ’p .
X

(17)

When measurement error appears, the observe values xj for j = T, . . . , T âˆ’ p + 1 in (17)
are no longer available but their surrogates Xjâˆ— are available. We now provide a sensible
estimate of Xj by using the measurement error model for characterizing the relationship of
Xj and Xjâˆ— . If measurement error follows (7), we â€œestimateâ€ Xj by
bj = 1 (Xjâˆ— âˆ’ Î±0 )
X
Î±1

for j = t, . . . , t âˆ’ p + 1;

(18)

bj is â€œestimatedâ€ by
if the measurement error follows (9), then X
bj =
X

Xjâˆ—
Î²0

for j = t, . . . , t âˆ’ p + 1.
13

(19)

bj ) = Xj for j = t, . . . , t âˆ’ p + 1.
These â€œestimatesâ€ are unbiased in the sense that E(X
Consequently, for h = 1, . . . , H, XT +h is predicted as
bT +h = Ï†0 + Ï†1 X
bT +hâˆ’1 + Â· Â· Â· + Ï†p X
bT +hâˆ’p .
X

(20)

In contrast to the observed values {xT , . . . , xT âˆ’p+1 }, also referred to as the initial values
of the forecasting of XT +1 , . . . , XT +H , the estimates determined by (18) or (19) introduce
additional prediction error which should be characterized. Without the loss of generality, we
consider p = 1 to illustrate the recursive calculation of the prediction error; the prediction
error with higher orders of the autoregressive process can be derived recursively in a similar
way but with more complex expressions.
If the measurement error follows (7), the mean squared prediction error of the 1-step
prediction is given by
2
b
P(1)
e = E(XT +1 âˆ’ XT +1 )

bT ) âˆ’ (Ï†0 + Ï†1 XT + T +1 )}2
= E{(Ï†0 + Ï†1 X

2
 
eT
âˆ’ Ï†1 XT âˆ’ T +1
= E Ï†1 Xt +
Î±1
Ï†2 Ïƒ 2
= 1 2 e + Ïƒ2 ,
Î±1
where the last step is due to the independence between et and t+1 , as well as E(e2t ) = Ïƒe2
and E(2t ) = Ïƒ2 .
Then, the h-step prediction error is given by
2
b
P(h)
e = E(XT +h âˆ’ XT +h )
n 

o2
bT +hâˆ’1 âˆ’ XT +hâˆ’1 âˆ’ T +1
= E Ï†1 X

= Ï†21 P(hâˆ’1)
+ Ïƒ2
e
hâˆ’1

Ï†2h Ïƒ 2 X 2i 2
Ï†1 Ïƒ ,
= 1 2e +
Î±1
i=0

(21)
(hâˆ’1)

where the last step comes from the recursive evaluation of Pe

.

Similarly, if the measurement error follows (9), the mean squared prediction error is given

14

by
2
b
P(1)
e = E(XT +1 âˆ’ XT +1 )


Ïƒ2
2
2
= Ï†1
+ Âµ Ïƒu2 + Ïƒ2 ,
1 âˆ’ Ï†21

where we use the independence of t+1 , ut and Xt , E(ut ) = 1, and Var(Xt ) =

Ïƒ2
1âˆ’Ï†21

due to

the stationary AR(1) process. Hence,
2
b
P(h)
e = E(XT +h âˆ’ XT +h )
n 

o2
bT +hâˆ’1 âˆ’ XT +hâˆ’1 âˆ’ T +1
= E Ï†1 X

+ Ïƒ2
= Ï†21 P(hâˆ’1)
e
=

Ï†2hâˆ’2
P(1)
1
e

+

hâˆ’2
X

2
Ï†2i
1 Ïƒ

i=0

=

Ï†2h
1




hâˆ’1
X
Ïƒ2
2
2
2
+ Âµ Ïƒu +
Ï†2i
1 Ïƒ .
1 âˆ’ Ï†21
i=0
(h)

The evaluation of the mean squared prediction error Pe

(22)

is carried out by replacing the

parameters with their estimators. We comment that the common second term in (21) and
P
2i 2
(22), hâˆ’1
i=0 Ï†1 Ïƒ , is the mean squared prediction error for the AR(1) model for error-free
settings (e.g. Box et al. 2015, p.152), which equals

1âˆ’Ï†2h
1
Ïƒ2.
1âˆ’Ï†21 

For an Î± with 0 < Î± < 1, then h-step (1 âˆ’ Î±)-prediction interval is constructed as
h
i
bT +h âˆ’ q Î± P(h) , X
bT +h + q Î± P(h) ,
X
e
e
2
2
bT +h âˆ’ XT +h . In practice, under normal
where q Î±2 the Î±-level quantile of the distribution of X
assumption of t and et , one can take q Î±2 to be the Î±-level quantile of the standard normal
distribution (Brockwell and Davis 2002, p.108).

5
5.1

Analysis of COVID-19 Death Rates
Study Objective

Using Canadian provincial COVID-19 data containing the daily confirmed cases and deaths
from April 3, 2020 to May 4, 2020, we compare the times series of death rates for British
15

Columbia, Ontario, Quebec, and Alberta, the four provinces in Canada which experience severe situations. The daily confirmed cases and fatalities are taken from â€œ1Point3Acres.comâ€
(https://coronavirus.1point3acres.com/).
In epidemiology, the mortality rate, defined as the proportion of cumulative deaths of
the disease in the total number of people diagnosed with the disease (Kanchan et al. 2015),
is often used to measure the severeness of an infectious disease. For COVID-19, determining
the mortality rate is not trivial due to the difficulty in precisely determining the number of
infected cases. Due to the limited test capacity, individuals with light symptoms are not
being tested. Asymptomatic infections and the incubation period make it difficult to acquire
an accurate number of infections. To circumvent this, we explore different definitions of
death rates. Definition 1 is from Baud et al. (2020) who estimated mortality rates by
dividing the number of deaths on a given day by the number of patients with confirmed
COVID-19 infection 14 days before, with the consideration of the maximum incubation time
to be 14 days. On the other hand, the median time from symptom onset to intensive care
unit admission is about 10 days ([3] in Baud et al. 2020), so we consider Definition 2 which
is the number of deaths of COVID-19 on day t divided by the number of confirmed cases at
day (t âˆ’ 10). In comparison, we also consider Definition 3 by calculating the death rate on
day t as the ratio of the number of deaths on day t to the number of confirmed cases on day
t.
While the first two ways may help more reasonably estimate mortality rates than the third
definition, these calculated rates still differ from the true mortality rates because of underreported cases which are primarily due to limited test capacity and undetected asymptomatic
infections. To reflect the discrepancy between the reported and the true mortality rates for
each province, for each definition of the mortality rate, we let X1,t , X2,t , X3,t , and X4,t ,
represent the true mortality rate on day t for British Columbia, Ontario, Quebec and Alberta,
âˆ—
âˆ—
âˆ—
âˆ—
respectively; and let X1,t
, X2,t
, X3,t
and X4,t
denote the reported mortality rate on day t in

British Columbia, Ontario, Quebec and Alberta, respectively. The objective is to use the
reported mortality rates {Xitâˆ— : t = 1, . . . , 31} to infer the true mortality rates Xi,t which
are modeled by (1) separately for i = 1, . . . , 4. In addition, we want to forecast the true
mortality rate of COVID-19 for a future time period. Due to the undetected asymptomatic
16

âˆ—
cases and untested cases for light symptoms, the reported mortality rates Xi,t
are typically
âˆ—
overestimated (i.e., Xi,t
â‰¥ Xi,t ) for i = 1, . . . , 4. As there is no exact information to guide

us how to characterize the relationship between Xitâˆ— and Xit , here we conduct sensitivity
âˆ—
studies by considering measurement error model (7) or (9). We use the observed data Xi,t
âˆ—
: t = 1, ..., Ti } with T1 = T2 = 31, to estimate
from April 3, 2020 to May 4, 2020, i.e., {Xi,t

the model parameters in (1) with measurement error effects accounted for, and then forecast
the mortality rate of COVID-19, from May 5, 2020 to May 9, 2020, in British Columbia,
Ontario, Quebec and Alberta, Canada.

5.2

Models Building

Figure 1 displays the trajectory of the mortality rates of COVID-19 in the four provinces
that are obtained from the three definitions. To assess the stationarity of the Xitâˆ— , we conduct
âˆ—
the augmented DickeyFuller (ADF) tests (Cheung and Lai 1995) to times series {Xi,t
:t=
âˆ—
âˆ—
: t = 1, . . . , T } for i = 1, . . . , 4 in
âˆ’ Xi,t
1, . . . , T }, or its differencing transformation {Xi,(t+1)

each definition. Supplementary Table 4 presents the test statistics and p-value of the ADF
test for each time series, where â€œTSVâ€ represents a test statistics value.
[ Place Figure 1 About Here ]
To determine the lag value p for the autoregression model (1) used for the time series
{Xi,t : t = 1, ..., Ti } with T1 = T2 = 31 for i = 1, . . . , 4, we fit the naive model (13) with âˆ—t
assumed to follow a normal distribution N (0, Ïƒâˆ—2 ), and use the AIC criterion by minimizing
âˆ’2

T
X

logf (xâˆ—t |xâˆ—tâˆ’1 , . . . , xâˆ—tâˆ’p ) + 2p,

(23)

t=p
âˆ—
âˆ—
where f (xâˆ—t |xâˆ—tâˆ’1 , . . . , xâˆ—tâˆ’p ) is the conditional probability of Xtâˆ— given Xtâˆ’1
, . . . , Xtâˆ’p
. The

results are summarized in Supplementary Table 5, where no-differencing or 1-differencing is
applied, the entries with â€œ-â€ indicate that the corresponding model is not applicable due to
the ADF test results.
We take those lag values for an AR(p) model to feature the true mortality rate Xi,t for each
definition and i = 1, . . . , 4. To be specific, for the British Columbia data, with Definition 1
17

we consider two models: AR(1) model for the time series with 1-order differencing and
AR(2) model for the time series with no-differencing; with Definitions 2 and 3, we consider
AR(2) and AR(1) models, respectively, for the time series with 1-order differencing. For the
Ontario data, we consider AR(1) and AR(4) for the time series with 1-order differencing in
Definitions 1 and 3, respectively, and AR(2) for Definition 2 with no transformation. For
the Quebec data, we consider AR(1) and AR(2) models for the times series with 1-order
differencing in Definitions 1 and 2, respectively. For Alberta data, we consider an AR(1)
model for the times series with 1-order differencing for both Definitions 1 and 2.

5.3

Sensitivity Analyses

As there are no additional data available for estimating the parameters for the model (7)
or (9), we conduct sensitivity analyses using the findings in the literature. Different studies
showed different estimates of the asymptomatic infection rates, changing from 17.9% to
78.3% (Kimball 2020; Day 2020). To accommodate the heterogeneity of different studies,
He et al. (2020) carried out a meta-analysis and obtained an estimate of the asymptomatic
infection rate to be 46%. If under-reported confirmed cases are only caused from undetected
asymptomatic cases, then Xt = (1 âˆ’ Ï„A )Xtâˆ— , or equivalently,
Xtâˆ— =

1
Xt ,
1 âˆ’ Ï„A

(24)

where Ï„A represents the rate of asymptomatic infections.
Now we use (24) as a starting point to conduct sensitivity analyses. In the multiplicative
model (9), we take Î²0 ut =

1
.
1âˆ’Ï„A

With E(ut ) = 1, we set Î²0 =

1
1âˆ’Ï„A

by setting Ï„A = 46%,

the value from the meta-analysis of He et al. (2020). To see different degrees of error, we
2
2
consider Ïƒu2 to take a small value, say Ïƒu1
, and a large value, say, Ïƒu2
, which is alternatively

reflected by the change of the coefficient of variation, CV =

Ïƒu
,
E(ut )

of the error term ut from

Ïƒu1 Ã— 100% to Ïƒu2 Ã— 100%.
When using the additive model (7) to characterize the measurement error process, motivated by (24), we set Î±0 = 0 and Î±1 =

1
,
1âˆ’46%

2
and let Ïƒe2 take a small value, say Ïƒe1
, and

2
a large value, say Ïƒe2
, to feature an increasing degree of measurement error. Due to the

18

constraints for the parameters discussed for (8) and (10), we set the values for Ïƒu1 , Ïƒu2 , Ïƒe1 ,
and Ïƒe2 case by case for each definition and for each province, which are recorded in Table 6.
The model fitting results are reported in Tables 1â€“2 and Supplementary Table 7 for the
three definitions of mortality rates, where the point estimates (EST), the associated standard
errors (SE), and the p-values for the model parameters are included. Table 1 shows that with
Definition 1, the estimates of Ï†0 in the absolute value from the proposed method are smaller
than those of naive method, while the estimates of Ï†1 produced from the proposed and naive
methods exhibit an opposite direction. As expected, the standard errors for the proposed
method are generally larger than those of the naive method. However, both methods find no
evidence to support that Ï†0 and Ï†1 are different from zero for the data of British Columbia
and Ontario, suggesting that the mortality rates of these two provinces remain statistically
unchanged. At the significance level 0.1, the naive method and the proposed method show
different evidence for the data of Quebec and Alberta. The naive method suggests a likely
downward trend with p-value 0.071 and 0.061 for testing of Ï†0 for Quebec and Alberta,
respectively. The proposed method, on the other hand, show that Ï†0 is insignificant for
these two provinces.
Table 2 displays the results for Definition 2. For the British Columbia data, the estimates
of the three parameters Ï†1 , Ï†2 and Ï†3 produced from the proposed method are smaller than
those yielded from the naive method, whereas the standard errors output from the proposed
method are larger than those from the naive method. However, at the significance level 0.05,
both methods find no evidence to show the significance of Ï†0 , Ï†1 and Ï†2 , suggesting that
the mortality rate of British Columbia remain unchanged with time. Similar findings are
revealed for the Alberta data except that the parameter estimates output from the proposed
method are larger than those produced from the naive method. For the Ontario and Quebec
data, the revealings from the two methods are quite different. For Ontario, both methods
show that Ï†0 is insignificant and Ï†1 is significant. The evidence of Ï†2 , however, depends on
the nature of measurement error. On the contrary, the findings for Quebec do not tend to
show a definite direction, and they vary with the model form or degree of the measurement
error process.
Table 7 shows the results for Definition 3. For the British Columbia data, the estimates
19

produced by the proposed method are smaller than those yielded from the naive method. The
standard errors output from the proposed methods inflate as the degree of measurement error
increases. The naive and proposed methods reveal different evidence for the significance of Ï†0
and Ï†1 , and the degree of measurement error affects the findings too. For the Ontario data,
both methods uncover the same type of evidence for all the parameters at the significance
level 0.05, except for the case with the large error under the multiplicative model.
[ Place Tables 1â€“2 About Here ]

5.4

Forecasting

With the fitted model for each time series in Section 5.3, we forecast the true mortality rate
for the subsequent five days (May 5 â€“ May 9) using the method described in Section 4.2.
Specifically, since the true mortality rates are not observable, we â€œestimateâ€ them using (18)
and (19), respectively, for the measurement error models (7) and (9), and then we forecast
the values of Xi,32 , Xi,33 , Xi,34 , Xi,35 , and Xi,36 using (20).
(h)

To quantify the forecasting performance, we calculate Pe

for h = 1, . . . , H for each

specified model of the mortality rates Xi,t , and we report the results, together with the total
PH
(h)
in Tables 8â€“10, where H is set as 5. For h = 1, . . . , H, we report the observed
h=1 Pe
bT +h )2 , and the expected prediction error defined in (21) and (22).
prediction error (XT +h âˆ’ X
Forecasting results based on the three definitions of mortality rates are reported in Figures 4â€“3 for the four provinces, where the prediction results after May 4 are marked in blue
and red for the measurement error models (7) and (9), respectively, together with prediction areas marked in shaded parts, as well as the prediction results obtained from the naive
method by using (20) with naive estimates of Ï† (marked in dark yellow). In comparison,
we display the reported mortality rate (in black) from Apr 3, 2020 to May 9, 2020 as well
as the adjusted mortality rates obtained from (24) (in green); in addition, we report the
fitted values using (17) in blue points. To compare the forecasting results in the presence
of different degrees of measurement error. We report the results derived from a mild degree of measurement error in top subfigures and place those obtained from a large degree of
measurement error in bottom subfigures.
20

[ Place Figure 2 About Here ]
The results for British Columbia are presented in Figure 2 and Web Figures 4â€“6. With
Definition 1, the methods with measurement error effects accommodated suggest that the
mortality rate in the past and its forecasting values are around 4%, whereas the results
obtained from the method without accounting for measurement error effects indicate that
the mortality rates over time are higher than 6%. With Definition 2, the methods with
or without accounting for measurement error effects reveal that the mortality rates over
time are, respectively, below 3.5% and above 5%. With Definition 3, the methods with or
without accounting for measurement error effects indicate that the mortality rates over time
are, around 3% and above 4%, respectively.
[ Place Figure 3 About Here ]
The results for Ontario are presented in Figure 3 and Supplementary Figures 7â€“8. With
Definition 1, the methods with measurement error effects accommodated suggest that the
mortality rate over time is around 7% over time, while the reported mortality rate over time
is about 12.5%. With Definition 2, the methods with and without incorporating the feature
of measurement error indicate the mortality rate in the past and its forecasting values are,
respectively, below 6% and around 10%. With Definition 3, the mortality rate increases over
time substantially. The methods with measurement error effects accommodated suggest that
the mortality rate increases from 2% to above 4% whereas the reported mortality rate shows
that rate increases from below 4% to above 8%.
The results for Quebec are presented in Supplementary Figures 9â€“10. With Definition 1
the methods with measurement error effects accommodated show that the mortality rate is
around 6.5% over time, whereas the method without considering measurement error indicates
the mortality rate is over 10%. With Definition 2, the methods with or without addressing
the measurement error effects show that the mortality rates over time are, respectively, below
6% and above 7.5%.
The results for Alberta are presented in Supplementary Figures 11â€“12. With Definition 1
the methods with and without measurement error accommodated suggest that the mortality
21

rates are, respectively, around 2% and 4% over time. With Definition 2, the methods with
or without addressing the measurement error effects show that the historical mortality rate
and its predictions are, respectively, below 2% and above 2%.

5.5

Model Assessment

The specification of lag p for model (1) of the true mortality rates {Xi,t : t = 1, . . . , T } is
âˆ—
: t = 1, . . . , T }, but
based on (23) which is derived from the reported mortality rates {Xi,t

not from {Xi,t : t = 1, . . . , T } itself. This discrepancy introduces the possibility of model
misspecification when featuring the series Xi,t using (1). To investigate this, we conduct a
sensitivity analysis by considering the AR(p) with a different value of p for the Xi,t from
Definition 1. As Table 5 indicates the feasibility of using AR(1) for all four provinces, here
we further employ the AR(2) model to do forecasting for the period from May 5 to May 9.
In Table 3, we report the observed and expected prediction errors of the forecasting
using AR(2) models in comparison with AR(1) models. Comparing different lag orders of
the autoregressive models, we find that in terms of the observed prediction error, the selected
AR(1) models have better performance than the AR(2) models for the data of Ontario and
Alberta, and the results for British Columbia and Quebec are fairly similar. It is noticed
that both the observed prediction error and the expected prediction error associated with
the proposed method tend to become small when the degree of measurement error increases
for British Columbia, Ontario, and Quebec.
[ Place Table 3 About Here ]

6

Discussion

In this article, we investigate the impact of measurement error on time series analysis under
autoregressive models and establish analytic results under the additive and multiplicative
measurement error models. We propose an estimating equation method to correct for the
biases induced from the naive analysis which disregards the differences between the true
measurements and their surrogate measurements. We rigorously establish the theoretical re22

sults for the proposed method. As a genuine application, we apply to the proposed method
to analyze the mortality rates of COVID-19 data in four provinces, British Columbia, Ontario, Quebec, and Alberta, which have the most severe virus outbreaks in Canada. The real
data analysis clearly demonstrates that incorporating measurement error in the analysis can
uncover various different results.
Our method has the flexibility or robustness in that distribution assumptions are required
to describe the measurement error process as well as the time series autoregressive process.
While our research is motivated by the faulty nature of COVID-19 data, the proposed method
can be applied to handle other problems related to error-contaminated time series. Our
development here is directed to using autoregressive models to delineate time series data.
The same principles can be applied to other model forms such as moving average models or
autoregressive moving average models which may be used to handle error-prone time series
data, where technical details can be more notationally involved.
When checking the stationarity of time series, we apply the ADF test to the observed
time series Xtâˆ— , which is mainly driven by the unavailability of the true values of Xt , as well as
the fact that the weakly stationarity of observed time series implies the weakly stationarity
of the true time series if measurement error is featured with (7) or (9). It is interesting to
rigorously develop a formal test similar to the ADF test to handle time series subject to
measurement error.

Acknowledgements
This research is partially supported by the Natural Sciences and Engineering Research Council of Canada (NSERC) as well as the Rapid Response Program COVID-19 of the Canadian
Statistical Sciences Institute (CANSSI). Yi is Canada Research Chair in Data Science (Tier
1). Her research was undertaken, in part, thanks to funding from the Canada Research
Chairs Program.

23

References
Baud, D., Qi, X., Nielsen-Saines, K., Musso, D., Pomar, L., and Favre, G. (2020). Real
estimates of mortality following COVID-19 infection. The Lancet Infectious Diseases.
Box, G. E., Jenkins, G. M., Reinsel, G. C., and Ljung, G. M. (2015). Time Series Analysis:
Forecasting and Control. New Jersey, NJ: John Wiley & Sons.
Brockwell, P. J. and Davis, R. A. (2002). Introduction to Time Series and Forecasting. New
York, NY: Springer-Verlag.
Brockwell, P. J., Davis, R. A., and Fienberg, S. E. (1991). Time Series: Theory and Methods.
New York, NY: Springer Science & Business Media.
Cheung, Y.-W. and Lai, K. S. (1995). Lag order and critical values of the augmented dickeyfuller test. Journal of Business & Economic Statistics, 13(3):277â€“280.
Day, M. (2020). COVID-19: four fifths of cases are asymptomatic, China figures indicate.
The BMJ, 369.
Dedecker, J., Samson, A., and Taupin, M.-L. (2014). Estimation in autoregressive model
with measurement error. ESAIM: Probability and Statistics, 18:277â€“307.
He, W., Yi, G. Y., and Zhu, Y. (2020). Estimation of the basic reproduction number, average
incubation time, asymptomatic infection rate, and case fatality rate for COVID-19: Metaanalysis and sensitivity analysis. Journal of Medical Virology.
Kanchan, T., Kumar, N., and Unnikrishnan, B. (2015). Mortality: Statistics. In PayneJames, J. and Byard, R. W., editors, Encyclopedia of Forensic and Legal Medicine: Second
Edition, pages 572â€“577. Oxford, OX:Elsevier.
Kimball, A. (2020). Asymptomatic and presymptomatic SARS-CoV-2 infections in residents of a long-term care skilled nursing facilityKing County, Washington, March 2020.
Morbidity and Mortality Weekly Report, 69:377â€“381.

24

Lahiri, S. N. (1999). Theoretical comparisons of block bootstrap methods. The Annals of
Statistics, 27(1):386â€“404.
Staudenmayer, J. and Buonaccorsi, J. P. (2005). Measurement error in linear autoregressive
models. Journal of the American Statistical Association, 100(471):841â€“852.
Tanaka, K. (2002). A unified approach to the measurement error problem in time series
models. Econometric Theory, 18(2):278â€“296.
Tripodis, Y. and Buonaccorsi, J. P. (2009). Prediction and forecasting in linear models with
measurement error. Journal of statistical planning and inference, 139(12):4039â€“4050.

25

26
Ï†1
Ï†0
Ï†1

Large
2 )
(Ïƒu2

The Proposed Method

with Multiplicative Error

Ï†1

2 )
(Ïƒe2

Ï†0

Ï†0

Large

with Additive Error

2 )
(Ïƒu1

Ï†1

Small

Ï†0

2 )
(Ïƒe1

Ï†1

0.192

-0.025

0.151

-0.027

0.146

-0.027

0.146

-0.027

0.138

-0.050

Ï†0

Small

-

Naive

EST

Parameter

The Proposed Method

Error Degree

Method

0.300

0.024

0.236

0.024

0.468

0.025

0.532

0.025

0.214

0.043

SE

0.535

0.308

0.535

0.286

0.760

0.298

0.788

0.313

0.533

0.272

p-value

British Columbia

used to fit the data of British Columbia, Ontario, Quebec and Alberta

0.476

-0.078

0.275

-0.107

0.345

-0.097

0.237

-0.113

0.215

-0.215

EST

3.955

1.690

0.238

0.152

0.939

0.263

0.280

0.134

0.157

0.243

SE

Ontario

0.905

0.964

0.260

0.488

0.717

0.715

0.406

0.406

0.183

0.384

p-value

0.031

-0.180

0.016

-0.183

0.027

-0.181

0.014

-0.183

0.012

-0.340

EST

1.327

0.127

0.166

0.099

0.323

0.100

1.566

0.111

0.124

0.180

SE

Quebec

0.981

0.170

0.923

0.078

0.934

0.083

0.993

0.112

0.923

0.071

p-value

0.087

-0.016

0.060

-0.017

0.183

-0.014

0.056

-0.017

0.052

-0.031

EST

0.360

0.015

0.180

0.009

1.596

0.073

0.185

0.009

0.144

0.016

SE

Alberta

0.812

0.299

0.740

0.080

0.909

0.845

0.764

0.088

0.721

0.061

p-value

Table 1: Definition 1: The parameter estimation under different measurement error models: the AR(1) model with â€œorder-1 differencingâ€ is

27

0.034

Ï†0

-0.393

Ï†2

0.039
-0.584

Ï†1

Ï†0
2 )
Large (Ïƒu2

with Multiplicative Error

-0.273

-0.439

0.034

Ï†0
Ï†1

-0.320

Ï†2

Small

-0.497

Ï†1

0.036

-0.268

Ï†2

The Proposed Method

2 )
(Ïƒu1

Large

Ï†0

with Additive Error
2 )
(Ïƒe2

Ï†2

-0.432

-0.254

Ï†1

-0.415

Ï†2

0.062

Ï†0
Ï†1

EST

Parameter

The Proposed Method

Small

-

Naive

2 )
(Ïƒe1

Error Degree

Method

0.322

0.339

0.032

0.204

0.205

0.020

0.354

0.265

0.024

0.205

0.201

0.020

0.185

0.186

0.034

SE

0.245

0.111

0.236

0.205

0.053

0.115

0.384

0.085

0.164

0.215

0.053

0.114

0.195

0.046

0.097

p-value

British Columbia

differencingâ€ is used to fit the data of British Columbia and Quebec.

-0.451

1.255

1.112

-0.389

1.188

1.139

-0.390

1.189

1.138

-0.375

1.173

1.146

-0.370

1.167

2.126

EST

0.510

0.503

0.747

0.162

0.231

0.748

0.172

0.239

0.747

0.141

0.216

0.759

0.140

0.209

1.388

SE

Ontario

0.384

0.020

0.149

0.024

<0.001

0.141

0.032

<0.001

0.140

0.014

<0.001

0.144

0.014

<0.001

0.138

p-value

-0.353

-0.143

0.127

0.128

-0.394

-0.164

0.132

0.162

-0.327

-0.130

0.124

0.174

-0.309

-0.122

0.225

EST

0.111

0.194

0.036

0.042

0.199

0.229

0.041

0.044

0.096

0.165

0.032

0.042

0.092

0.136

0.058

SE

Quebec

0.004

0.467

0.002

0.006

0.059

0.480

0.004

0.001

0.002

0.435

0.001

0.000

0.003

0.380

0.001

p-value

-

-0.205

-0.008

-

-0.144

-0.007

-

-0.158

-0.007

-

-0.131

-0.007

-

-0.124

-0.013

EST

-

0.317

0.012

-

0.205

0.012

-

0.247

0.012

-

0.185

0.012

-

0.172

0.022

SE

Alberta

-

0.524

0.546

-

0.487

0.564

-

0.529

0.554

-

0.486

0.567

-

0.477

0.561

p-value

to fit the data of Ontario, the AR(1) model with â€œorder-1 differencingâ€ is used to fit the data of Alberta, and the AR(2) model with â€œorder-1

Table 2: Definition 2: The parameter estimation under different measurement error models: the AR(2) model with â€œno differencingâ€ is used

28

a

AR(2)

AR(2)

AR(2)

0.005
0.006

Mild

Moderate

0.006

0.005

Mild

Moderate

0.003

0.005

Moderate

-

0.004

0.006

Mild

AR(1)a

0.004

Moderate

0.002

-

Mild

0.038

Moderate
Alberta

0.051

Mild

0.032

0.052

Mild

Moderate

0.129

0.060

Moderate

-

0.061

0.060

Mild

AR(1)a

0.061

Moderate

0.163

-

Mild

0.085

Moderate
Quebec

0.034

Mild

0.045

0.029
AR(2)

Mild

Moderate

0.073

0.004

Moderate

-

0.000

0.000

Mild

AR(1)a

0.001

Moderate

0.020

-

Mild

0.010

Moderate
Ontario

0.010

Mild

0.010

0.010

Mild

Moderate

0.016

0.010

-

Moderate

The selected model

Multiplicative

Additive

Naive

Multiplicative

Additive

Naive

Multiplicative

Additive

Naive

Multiplicative

Additive

Naive

Multiplicative

Additive

Naive

Multiplicative

Additive

Naive

Multiplicative

Additive

Naive

Multiplicative

0.010
0.010

AR(1)a

Mild

Moderate

0.010

Additive

0.015

-

Mild

Naive

Day 1

British Columbia

Model

2
2
Ïƒe
(or Ïƒu
)

Method

0.019

0.016

0.018

0.016

0.010

0.013

0.012

0.017

0.012

0.007

0.141

0.190

0.109

0.195

0.524

0.215

0.216

0.215

0.216

0.607

0.001

0.012

0.008

0.014

0.107

0.010

0.002

0.000

0.004

0.087

0.005

0.005

0.005

0.005

0.014

0.005

0.005

0.005

0.005

0.015

Day 2

0.059

0.052

0.056

0.051

0.033

0.045

0.044

0.052

0.044

0.027

0.333

0.438

0.247

0.446

1.226

0.477

0.479

0.478

0.479

1.357

0.071

0.027

0.031

0.026

0.240

0.014

0.003

0.000

0.007

0.196

0.010

0.010

0.010

0.010

0.031

0.011

0.011

0.011

0.011

0.032

Day 3

0.109

0.099

0.104

0.097

0.064

0.089

0.087

0.098

0.087

0.055

0.560

0.723

0.396

0.734

2.115

0.776

0.778

0.776

0.778

2.289

0.024

0.076

0.063

0.083

0.550

0.000

0.044

0.023

0.056

0.521

0.010

0.010

0.010

0.010

0.042

0.011

0.011

0.011

0.011

0.043

Day 4

0.141

0.129

0.136

0.127

0.081

0.118

0.115

0.129

0.115

0.070

0.774

0.988

0.519

1.002

3.085

1.050

1.053

1.051

1.053

3.294

0.310

0.222

0.221

0.227

1.111

0.035

0.152

0.110

0.175

1.059

0.000

0.000

0.000

0.000

0.019

0.000

0.000

0.000

0.000

0.020

Day 5

Observed Prediction Error

0.334

0.301

0.320

0.296

0.191

0.270

0.263

0.302

0.262

0.160

1.847

2.390

1.303

2.429

7.079

2.578

2.586

2.580

2.587

7.709

0.491

0.370

0.368

0.379

2.081

0.063

0.201

0.134

0.243

1.884

0.034

0.035

0.035

0.035

0.122

0.037

0.037

0.037

0.037

0.126

h=1 OPE(h)

PH

0.022

0.030

0.081

0.112

0.122

0.022

0.031

0.035

0.115

0.125

0.332

0.345

0.413

1.375

1.746

0.205

0.399

0.811

1.561

1.811

0.454

0.571

1.470

2.256

2.517

0.270

0.558

1.453

2.264

2.527

0.034

0.043

0.151

0.151

0.161

0.034

0.044

0.154

0.154

0.164

Day 1

0.022

0.031

0.081

0.112

0.122

0.022

0.031

0.035

0.115

0.125

0.332

0.345

0.413

1.375

1.746

0.205

0.399

0.811

1.561

1.811

0.469

0.606

1.658

2.398

2.648

0.331

0.599

1.626

2.391

2.643

0.035

0.044

0.155

0.155

0.165

0.035

0.044

0.157

0.157

0.167

Day 2

0.022

0.031

0.085

0.115

0.125

0.022

0.031

0.035

0.115

0.125

0.234

0.356

0.407

1.447

1.809

0.205

0.399

0.811

1.561

1.811

0.415

0.603

1.646

2.398

2.648

0.345

0.603

1.646

2.399

2.649

0.035

0.044

0.157

0.157

0.167

0.035

0.044

0.157

0.157

0.167

Day 3

0.022

0.031

0.085

0.115

0.125

0.022

0.031

0.035

0.115

0.125

0.234

0.356

0.407

1.447

1.809

0.205

0.399

0.811

1.561

1.811

0.390

0.603

1.649

2.399

2.649

0.348

0.603

1.649

2.399

2.649

0.035

0.044

0.157

0.157

0.167

0.035

0.044

0.157

0.157

0.167

Day 4

0.022

0.031

0.085

0.115

0.125

0.022

0.031

0.035

0.115

0.125

0.187

0.357

0.402

1.451

1.811

0.205

0.399

0.811

1.561

1.811

0.375

0.603

1.649

2.399

2.649

0.348

0.603

1.649

2.399

2.649

0.035

0.044

0.157

0.157

0.167

0.035

0.044

0.157

0.157

0.167

Day 5

Expected Prediction Error
EPE(h)

0.109

0.155

0.419

0.570

0.621

0.109

0.157

0.177

0.577

0.627

1.319

1.760

2.043

7.096

8.921

1.025

1.995

4.057

7.807

9.057

2.103

2.986

8.072

11.851

13.111

1.642

2.965

8.023

11.853

13.117

0.173

0.220

0.778

0.777

0.828

0.174

0.222

0.784

0.783

0.834

h=1

PH

Table 3: The observed prediction error and expected prediction error for different lag order of autoregressive models

British Columbia

Ontario

Quebec

Alberta

Fatality Rate (%)

30

20

Definition of
Death Rate
Definition 1
Definition 2
Definition 3
10

0
Apr 06

Apr 13

Apr 20

Apr 27

May 04

Apr 06

Apr 13

Apr 20

Apr 27

May 04

Apr 06

Apr 13

Apr 20

Apr 27

May 04

Apr 06

Apr 13

Apr 20

Apr 27

May 04

Date

Figure 1: The time series plots of the death rate with different definitions

29

30

2

3

4

5

2

3

4

Apr 13

Apr 20

Apr 27

May 04

Day

Apr 13

Apr 20

Apr 27

Multiplicative

May 04

Moderate

Multiplicative

Additive

95% Prediction Interval

Naive

Multiplicative

Additive

Measurement Error Type

Reported Fatality

Fitted Fatality

Adjusted Fatality

Reference Time Series

mortality rates (in black) and the adjusted true mortality rate accounting for the asymptomatic cases (in green).

5 - May 9) based on the additive (in blue) or multiplicative (in red) versus the naive model (in dark yellow); the reported

Figure 2: British Columbia by Definition 3 (AR(1), order-1 differencing): A 5-day forecasting of the true mortality rate (May

Fatality Rate (%)

5

Additive

Mild

31

2

4

6

8

2

4

6

Apr 13

Apr 20

Apr 27

May 04

Day

Apr 13

Apr 20

Apr 27

Multiplicative

May 04

Moderate

Multiplicative

Additive

95% Prediction Interval

Naive

Multiplicative

Additive

Measurement Error Type

Reported Fatality

Fitted Fatality

Adjusted Fatality

Reference Time Series

(in black) and the adjusted true mortality rate accounting for the asymptomatic cases (in green).

based on the additive (in blue) or multiplicative (in red) versus the naive model (in dark yellow); the reported mortality rates

Figure 3: Ontario by Definition 3 (AR(4), order-1 differencing): A 5-day forecasting of the true mortality rate (May 5 - May 9)

Fatality Rate (%)

8

Additive

Mild

Supplementary Materials for â€œSensitivity
Analysis of Error-Contaminated Time Series
Data under Autoregressive Models with
Application of COVID-19 Dataâ€
A

Appendix

A.1

Regularity Conditions

(R1) The time series {Xt : t = 1 . . . , T } is stationary.
(R2) The observed error-prone time series {Xtâˆ— : t = 1 . . . , T } is stationary.
(R3) For any t âˆˆ {1, . . . , T },
(R4) For any p,

1
T

PT PT
t=1

1
T

s=1

PT

s=1

Î³|sâˆ’t| â†’ 0 as T â†’ âˆ.

E{(Xt âˆ’ Âµ)(Xt+p âˆ’ Âµ)(Xs âˆ’ Âµ)} < âˆ.

While the two process {Xt : t = 1, . . . , T } and {Xtâˆ— : t = 1, . . . , T } are constrained
by the measurement error model (7) or (9), they can both be assumed to be stationary
without inducing conflicting requirements on the associated processes. Obviously, the weak
stationarity of {Xt : t = 1, . . . , T } implies the weak stationarity of {Xtâˆ— : t = 1, . . . , T } if
they are linked by (7) or (9). Condition (R3) says that as the time series goes long enough,
the average of the covariances between any paired variables is is negligible. Condition (R4)
requires the summation of the third moment of Xt is O(T ), which is needed in Theorem 4
when Ï†0 6= 0; this condition can be satisfied if E(3t ) = 0, for example.

A.2

The proof of Theorem 1

Applying the weak law of large numbers to Ï†bâˆ—1 given by (12), we obtain that the estimator
Ï†bâˆ—1 converges in probability to

âˆ— )
Cov(Xtâˆ— ,Xtâˆ’1
,
âˆ—
Var(Xtâˆ’1 )

which is denoted as Ï†âˆ—1 . Now we further examine

Ï†âˆ—1 by using the AR(1) model (1) and the measurement error model (7):
âˆ—
Cov(Xtâˆ— , Xtâˆ’1
)
âˆ—
Var(Xtâˆ’1 )
Cov(Î±0 + Î±1 Xt + et , Î±0 + Î±1 Xtâˆ’1 + etâˆ’1 )
=
Var(Î±0 + Î±1 Xt + et )
2
Î± Cov(Xt , Xtâˆ’1 )
= 2 1
Î±1 Var(Xt ) + Var(et )
Î±2 Cov(Ï†0 + Ï†1 Xtâˆ’1 + t , Xtâˆ’1 )
= 1
Î±12 Var(Xt ) + Var(et )
Î±12 Var(Xtâˆ’1 )
= Ï†1 Â· 2
,
Î±1 Var(Xt ) + V ar(et )

Ï†âˆ—1 =

where the second step is due to (7), the third step is because of the independence among the
Xt and the et , and the fourth step is because of (1). Since the time series {Xt } is stationary,
it follows that Var(Xt ) = Var(Xtâˆ’1 ) =
Ï†âˆ—1

Ïƒ2
,
1âˆ’Ï†21

and hence

Î±12 Ïƒ2
= Ï†1 Ï‰1 .
= Ï†1 Â· 2 2
Î±1 Ïƒ + Ïƒe2 (1 âˆ’ Ï†21 )

(S.1)

Next, applying the Slutskyâ€™s theorem to (12), we have that as T â†’ âˆ,
p
Ï†bâˆ—0 â†’
âˆ’ E(Xtâˆ— ) âˆ’ Ï†âˆ—1 E(Xtâˆ— ),


Î± 1 Ï†0
Î± 1 Ï†0
where the limit equals Î±0 + 1âˆ’Ï†1 (1 âˆ’ Ï†1 Ï‰1 ) by (S.1) and the fact that E(Xtâˆ— ) = Î±0 + 1âˆ’Ï†
.
1

Finally, plugging the AR(1) model (1) into the measurement error model (11), we obtain
that
Xtâˆ— = Î±0 + Î±1 (Ï†0 + Ï†1 Xtâˆ’1 + t ) + et .

(S.2)

On the other hand, plugging the measurement error model (7) into the working model (11),
we obtain that
Xtâˆ— = Ï†âˆ—0 + Ï†âˆ—1 (Î±0 + Î±1 Xtâˆ’1 + et ) + âˆ—t .
Then equating (S.2) and (S.3) that
âˆ— = Î±0 (1 âˆ’ Ï†âˆ—1 ) + Î±1 Ï†0 âˆ’ Ï†âˆ—0 + Î±1 (Ï†1 âˆ’ Ï†âˆ—1 )Xtâˆ’1 + (1 âˆ’ Ï†âˆ—1 )et + Î±1 t .
Consequently, by the independence assumption for Xtâˆ’1 , et and t , we obtain that
V ar(âˆ—t ) = Ï†21 Î±12 (1 âˆ’ Ï‰1 )2 Var(Xtâˆ’1 ) + (1 âˆ’ Ï‰1 Ï†1 )2 Var(et ) + Î±12 Var(t )


Ïƒ2
2 2
2
= Ï†1 Î±1 (1 âˆ’ Ï‰1 )
+ (1 âˆ’ Ï‰1 Ï†1 )2 Ïƒe2 + Î±12 Ïƒ2 .
1 âˆ’ Ï†21
2

(S.3)

A.3

The proof of Theorem 2

p
As noted in the beginning of A.2, as T â†’ âˆ, Ï†bâˆ—1 âˆ’â†’ Ï†âˆ—1 where
âˆ—
Cov(Xtâˆ— , Xtâˆ’1
)
âˆ—
b
Ï†1 =
.
âˆ—
Var(Xtâˆ’1 )

Now we further examine Ï†âˆ—1 by using the AR(1) model (1) and the measurement error
model (9):
âˆ—
)
Cov(Xtâˆ— , Xtâˆ’1
âˆ—
Var(Xtâˆ’1 )
Cov(Î²0 ut Xt , Î²0 utâˆ’1 Xtâˆ’1 )
=
Var(Î²0 utâˆ’1 Xtâˆ’1 )
2
Î² Cov(ut Xt , utâˆ’1 Xtâˆ’1 )
= 0 2
Î²0 Var(utâˆ’1 Xtâˆ’1 )
Cov{ut (Ï†0 + Ï†1 Xtâˆ’1 + t ), utâˆ’1 Xtâˆ’1 }
=
Var(Xtâˆ’1 utâˆ’1 )
Cov(ut Xtâˆ’1 , utâˆ’1 Xtâˆ’1 )
= Ï†1
Var(utâˆ’1 Xtâˆ’1 )
2
E(ut utâˆ’1 Xtâˆ’1
) âˆ’ E(ut Xtâˆ’1 )E(utâˆ’1 Xtâˆ’1 )
= Ï†1
2
2
E(utâˆ’1 Xtâˆ’1
) âˆ’ E 2 (utâˆ’1 Xtâˆ’1 )
2
) âˆ’ E(ut )E(utâˆ’1 )E 2 (Xtâˆ’1 )
E(ut )E(utâˆ’1 )E(Xtâˆ’1
= Ï†1
2
E(u2tâˆ’1 )E(Xtâˆ’1
) âˆ’ E 2 (utâˆ’1 Xtâˆ’1 )
E(ut )E(utâˆ’1 )Var(Xtâˆ’1 )
= Ï†1
2
{Var(utâˆ’1 ) + E (utâˆ’1 )}{Var(Xtâˆ’1 ) + E 2 (Xtâˆ’1 )} âˆ’ E 2 (utâˆ’1 )E 2 (Xtâˆ’1 )
Var(Xtâˆ’1 )
= Ï†1
{Var(utâˆ’1 ) + 1}{Var(Xtâˆ’1 ) + E 2 (Xtâˆ’1 )} âˆ’ E 2 (Xtâˆ’1 )
Var(Xtâˆ’1 )
= Ï†1
,
Var(utâˆ’1 )Var(Xtâˆ’1 ) + Var(utâˆ’1 )E 2 (Xtâˆ’1 ) + Var(Xtâˆ’1 )

Ï†âˆ—1 =

(S.4)

where the second step is due to measurement error model (9), the seventh step is because
ut , utâˆ’1 and Xtâˆ’1 are mutually independent, and the second last step is due to E(ut ) = 1.
Since the time series {Xt } is stationary, it follows that E(Xt ) = E(Xtâˆ’1 ) =

3

Ï†0
1âˆ’Ï†1

and

Var(Xt ) = Var(Xtâˆ’1 ) =
Ï†âˆ—1 = Ï†1
= Ï†1

Ïƒ2
.
1âˆ’Ï†21

Hence (S.4) becomes

Var(Xtâˆ’1 )
Var(utâˆ’1 )Var(Xtâˆ’1 ) + Var(utâˆ’1 )E 2 (Xtâˆ’1 ) + Var(Xtâˆ’1 )
Ïƒ2
1âˆ’Ï†21
2

Ïƒ
2
Ïƒu2 1âˆ’Ï†
2 + Ïƒu
1

= Ï†1



Ï†0
1âˆ’Ï†1

2

+

Ïƒ2
1âˆ’Ï†21

Ïƒ2
= Ï†1 Ï‰2 .
1
Ïƒ2 Ïƒu2 + Ïƒ2 + Ïƒu2 Ï†20 1+Ï†
1âˆ’Ï†1

(S.5)

Next, applying the Slustkyâ€™s Theorem to (12) gives that as T â†’ âˆ,


Î²
Ï†
p
0
0
âˆ—
(1 âˆ’ Ï†1 Ï‰2 )
Ï†b0 âˆ’âˆ’â†’
1 âˆ’ Ï†1
by (S.5) as well as E(Xtâˆ— ) =

Î²0 Ï†0
.
1âˆ’Ï†1

Finally plugging the AR(1) model (1) into the measurement error model (9), we obtain
that
Xtâˆ— = Î²0 (Ï†0 + Ï†1 Xtâˆ’1 + t )ut .

(S.6)

On the other hand, plugging the measurement error model (9) into the working model (11),
we obtain that
Xtâˆ— = Ï†âˆ—0 + Ï†âˆ—1 (Î²0 Xtâˆ’1 utâˆ’1 ) + âˆ—t .
Then equating (S.6) and (S.7) gives that
âˆ— = Î²0 Ï†0 ut âˆ’ Ï†âˆ—0 + Î²0 Xtâˆ’1 (Ï†1 ut âˆ’ Ï‰2 Ï†1 utâˆ’1 ) + Î²0 ut t .

4

(S.7)

yielding that
V ar(âˆ—t ) = Ï†20 Î²02 Var(ut ) + Î²02 Ï†21 Var(Xtâˆ’1 ut ) + Î²02 Ï‰22 Ï†21 Var(Xtâˆ’1 utâˆ’1 ) + Î²02 V ar(ut t )
2
= Ï†20 Î²02 Ïƒu2 + (Î²02 Ï†21 + Î²02 Ï‰22 Ï†21 ){E(Xtâˆ’1
u2tâˆ’1 ) âˆ’ E 2 (Xt )E 2 (utâˆ’1 )} + Î²02 {E(u2t )E(2t ) âˆ’ E 2 (ut )E 2 (t )}
2
= Ï†20 Î²02 Ïƒu2 + (Î²02 Ï†21 + Î²02 Ï‰22 Ï†21 ){E(Xtâˆ’1
)E(u2tâˆ’1 ) âˆ’ E 2 (Xt )E 2 (utâˆ’1 )} + Î²02 (Ïƒu2 + 1)Ïƒ2

= Î²02 {Ïƒu2 Ï†20 + (1 + Ïƒu2 )Ïƒ2 }


+ Î²02 Ï†21 (1 + Ï‰22 ) {Var(utâˆ’1 ) + E 2 (utâˆ’1 )}{Var(Xtâˆ’1 ) + E 2 (Xtâˆ’1 )} âˆ’ E 2 (Xtâˆ’1 )


= Î²02 {Ïƒu2 Ï†20 + (1 + Ïƒu2 )Ïƒ2 } + Î²02 Ï†21 (1 + Ï‰22 ) {Var(utâˆ’1 ) + 1}{Var(Xtâˆ’1 ) + E 2 (Xtâˆ’1 )} âˆ’ E 2 (Xtâˆ’1 )

= Î²02 {Ïƒu2 Ï†20 + (1 + Ïƒu2 )Ïƒ2 } + Î²02 Ï†21 (1 + Ï‰22 ) Var(utâˆ’1 )Var(Xtâˆ’1 ) + Var(utâˆ’1 )E 2 (Xtâˆ’1 ) + Var(Xtâˆ’1 )
V ar(Xtâˆ’1 )
Ï‰2
2
2
1 + Ï‰2 Ïƒ
,
= Î²02 {Ïƒu2 Ï†20 + (1 + Ïƒu2 )Ïƒ2 } + Î²02 Ï†21
Ï‰2 1 âˆ’ Ï†21
= Î²02 {Ïƒu2 Ï†20 + (1 + Ïƒu2 )Ïƒ2 } + Î²02 Ï†21 (1 + Ï‰22 )

where the second step is because of the independence assumption as well as E(u2tâˆ’1 ) = E(u2t )
and E(utâˆ’1 ) = E(ut ) such that Var(Xtâˆ’1 ut ) = Var(Xtâˆ’1 utâˆ’1 ), and the second last step is
due to Ï‰2 =

A.4

Var(Xtâˆ’1 )
Var(utâˆ’1 )Var(Xtâˆ’1 )+Var(utâˆ’1 )E 2 (Xtâˆ’1 )+Var(Xtâˆ’1 )

in (S.5).

The proof of Theorem 3

Proof of Theorem 3(1):

For k = 1, . . . , p, applying the weak law of large numbers to Î³
bkâˆ— , we obtain that as T â†’ âˆ,
âˆ—
the estimator Î³
bkâˆ— converges in probability to Cov(Xtâˆ— , Xtâˆ’k
), denoted Î³kâˆ— .

Next, we examine Î³k . By the form of measurement error model (7), we have that for
0 < k < t,
âˆ—
Cov(Xtâˆ— , Xtâˆ’k
) = Cov(Î±0 + Î±1 Xt + et , Î±0 + Î±1 Xtâˆ’k + etâˆ’k )

= Î±12 Cov(Xt , Xtâˆ’k ) = Î±12 Î³k ,
and by (8), Var(Xtâˆ— ) = Î±12 Î³0 + Ïƒe2 , which is denoted as Î³0âˆ— .
Thus, Theorem 3(1) follows.

5

Proof of Theorem 3(2):
First, by Theorem 3(1), we write
Î³
bâˆ— = Î±12 Î³ + op (1)

(S.8)

and
bâˆ— = Î±12 Î“ + Ïƒe2 Ip + op (1),
Î“
ï£¶
âˆ—
Î³
b0âˆ— Â· Â· Â· Î³
bpâˆ’1
ï£·
ï£¬
..
.. ï£·
..
bâˆ— = ï£¬
bk in
where Î“
.
ï£¬ .
. ï£·. Then the naive estimator Ï†bâˆ— is obtained by replacing Î³
ï£¸
ï£­
âˆ—
Â·Â·Â· Î³
b0âˆ—
Î³
bpâˆ’1
(6) with Î³
bkâˆ— ,
ï£«

âˆ’1


Ï†bâˆ— = Î±12 Î“ + Ïƒe2 Ip + op (1)

âˆ’1

and hence Ï†âˆ— = Î±12 (Î±12 Î“ + Ïƒe2 Ip )



Î±12 Î³ + op (1) = Î±12 Î±12 Î“ + Ïƒe2 Ip

âˆ’1

Î³ + op (1),

(S.9)

p
Î³ such that Ï†bâˆ— âˆ’â†’ Ï†âˆ— as T â†’ âˆ.

Again, replacing Î³
bk in (6) with Î³
bkâˆ— gives the naive estimator Ï†bâˆ—0
!
!
p
T
T
X
X
X
1
1
âˆ—
Ï†bâˆ—0 =
Ï†bâˆ—k
Xâˆ— âˆ’
Xtâˆ’k
T âˆ’ p t=p t
T
âˆ’
p
t=p
k=1
= E(Xtâˆ— ) âˆ’ E(Xtâˆ— )

p
X

Ï†bâˆ—k + op (1)

k=1

= Î±0 + Î±1 E(Xt ) âˆ’ {Î±0 + Î±1 E(Xt )}

p
X

{Ï†âˆ—k + op (1)} + op (1)

k=1

= (1 âˆ’ Ï†âˆ—T Â· 1p ) (Î±0 + Î±1 Âµ) + op (1),
where Ï†bk and Ï†k are respectively the kth element of Ï†b and Ï†, the third step is because
Ï†bk = Ï†k + op (1) by (S.9) as well as the model form (7), and the last step is due to the
stationarity of the time series {Xt } such that E(Xt ) = Âµ.
bâˆ— Ï†bâˆ— by
bâˆ— + Ï†bâˆ—T Î“
Finally, noting that the native estimator Ïƒ
b2âˆ— is given by Ïƒ
b2âˆ— = Î³
b0âˆ— âˆ’ 2Ï†bâˆ—T Î³
applying a version similar to (6), we obtain that
bâˆ— Ï†bâˆ—
Ïƒ
b2âˆ— = Î³
b0âˆ— âˆ’ 2Ï†bâˆ—T Î³
bâˆ— + Ï†bâˆ—T Î“
= (Î±12 Î³02 + Ïƒe2 ) âˆ’ 2Î±14 Î³ T (Î±12 Î“ + Ïƒe2 Ip )âˆ’1 Î³ + Î±14 Î³ T (Î±12 Î“ + Ïƒe2 Ip )âˆ’1 (Î±12 Î“ + Ïƒe2 Ip )(Î±12 Î“ + Ïƒe2 Ip )âˆ’1 Î³ + op (1)
= Î±12 Î³0 + Ïƒe2 âˆ’ Î±14 Î³ T (Î±12 Î“ + Ïƒe2 Ip )âˆ’1 Î³ + op (1),
6

where the second step is due to (8), (S.8) and (S.9).
Proof of Theorem 3(3):
Step 1: We show certain identities before proving Theorem 3(3).
1. By model (7), we have that
T

Xtâˆ—

1X
âˆ’Âµ
b = Î±0 + Î±1 Xt + et âˆ’
(Î±0 + Î±1 Xt + et )
T t=1
!
!
T
T
1X
1X
= Î± 1 Xt âˆ’
Xt + et âˆ’
et
T t=1
T t=1
âˆ—

= Î±1 (Xt âˆ’ Âµ
b) + (et âˆ’ eÌ„),
where the first step is because Âµ
bâˆ— =

1
T

PT

t=1

Xtâˆ— and in the last step eÌ„ =

(S.10)
1
T

PT

t=1 et .

2. For any t and s, we have that

Cov (Xt âˆ’ Âµ
b)2 , (Xs âˆ’ Âµ
b)(es âˆ’ eÌ„)
=E{(Xt âˆ’ Âµ
b)2 (Xs âˆ’ Âµ
b)(es âˆ’ eÌ„)} âˆ’ {E(Xt âˆ’ Âµ
b)2 }E{(Xs âˆ’ Âµ
b)(es âˆ’ eÌ„)}
=E{(Xt âˆ’ Âµ
b)2 (Xs âˆ’ Âµ
b)}E(es âˆ’ eÌ„) âˆ’ {E(Xt âˆ’ Âµ
b)2 }E(Xs âˆ’ Âµ
b)E(es âˆ’ eÌ„)
=0,

(S.11)

where the second step is due to the independence of et and Xt , and the last step is by
E(es âˆ’ eÌ„) = 0.
3. By the independence of et and es for t 6= s, we have that
Cov {(Xt âˆ’ Âµ
b)(et âˆ’ eÌ„), (Xs âˆ’ Âµ
b)(es âˆ’ eÌ„)}
=E{(Xt âˆ’ Âµ
b)(et âˆ’ eÌ„)(Xs âˆ’ Âµ
b)(es âˆ’ eÌ„)} âˆ’ E{(Xt âˆ’ Âµ
b)(et âˆ’ eÌ„)}E{(Xs âˆ’ Âµ
b)(es âˆ’ eÌ„)}
=E{(Xt âˆ’ Âµ
b)(Xs âˆ’ Âµ
b)}E{(et âˆ’ eÌ„)}E{(es âˆ’ eÌ„)} âˆ’ E{(Xt âˆ’ Âµ
b)}E{(et âˆ’ eÌ„)}E{(Xs âˆ’ Âµ
b)}E{(es âˆ’ eÌ„)}
=0,

(S.12)

where the second step is due to the independence of the et and the Xt , and the last step is
by E(es âˆ’ eÌ„) = 0.

7

4. For any t, we have that
Var {(Xt âˆ’ Âµ
b)(et âˆ’ eÌ„)}
=E{(Xt âˆ’ Âµ
b)2 (et âˆ’ eÌ„)2 } âˆ’ E 2 {(Xt âˆ’ Âµ
b)(et âˆ’ eÌ„)}
=E{(Xt âˆ’ Âµ
b)2 }E{(et âˆ’ eÌ„)2 } âˆ’ E 2 {(Xt âˆ’ Âµ
b)}E 2 {(et âˆ’ eÌ„)}
=E{(Xt âˆ’ Âµ
b)2 }E{(et âˆ’ eÌ„)2 }.

(S.13)

5. For any t, we have
lim E{(Xt âˆ’ Âµ
b)2 }

T â†’âˆ

= lim E{(Xt âˆ’ Âµ)2 + (Âµ âˆ’ Âµ
b)2 + 2(Xt âˆ’ Âµ)(Âµ âˆ’ Âµ
b)}
T â†’âˆ

=Î³0 + lim E{(b
Âµ âˆ’ Âµ)2 } + 2 lim E{(Xt âˆ’ Âµ)(Âµ âˆ’ Âµ
b)}
T â†’âˆ
T â†’âˆ
#
"
T
X
1
(Xs âˆ’ Âµ)}
=Î³0 + lim E{(b
Âµ âˆ’ Âµ)2 } âˆ’ 2 lim E (Xt âˆ’ Âµ){
T â†’âˆ
T â†’âˆ
T s=1
T
1X
=Î³0 + lim V ar(b
Âµ) âˆ’ 2 lim
E{(Xt âˆ’ Âµ)(Xs âˆ’ Âµ)}
T â†’âˆ
T â†’âˆ T
s=1
T
1X
=Î³0 + 0 âˆ’ 2 lim
Î³|sâˆ’t|
T â†’âˆ T
s=1

=Î³0 ,
where the third step is due to Âµ
bâˆ’Âµ =

(S.14)
1
T

P

s=1 (Xs

âˆ’ Âµ), and the fourth step is because

E(b
Âµ âˆ’Âµ) = 0 by stationarity of the time series, the second last step is due to lim V ar(b
Âµ) = 0
T â†’âˆ

(Brockwell et al. 1991, Theorem 7.1.1.), and the last step due to Condition (R3).
6. Similar to (S.14), we have that
lim E{(Xt âˆ’ Âµ
b)(Xtâˆ’p âˆ’ Âµ
b)}

T â†’âˆ

= lim E{(Xt âˆ’ Âµ + Âµ âˆ’ Âµ
b)(Xtâˆ’p âˆ’ Âµ + Âµ âˆ’ Âµ
b)}
T â†’âˆ

= lim [E{(Xt âˆ’ Âµ)(Xtâˆ’p âˆ’ Âµ)} + E{(Âµ âˆ’ Âµ
b)(Xtâˆ’p âˆ’ Âµ)} + E{(Âµ âˆ’ Âµ
b)(Xt âˆ’ Âµ)} + E{(Âµ âˆ’ Âµ
b)(Âµ âˆ’ Âµ
b)}]
T â†’âˆ

T
1X
(Î³|tâˆ’s| + Î³|tâˆ’sâˆ’p| ) + lim Var(b
Âµ)
=Î³p + lim
T â†’âˆ T
T â†’âˆ
s=1

=Î³p ,

(S.15)
8

where the last step is due to Condition (R3) and lim V ar(b
Âµ) = 0 (Brockwell et al. 1991,
T â†’âˆ

Theorem 7.1.1).
7. For any t, we have
E{(et âˆ’ eÌ„)2 }
=E{e2t âˆ’ 2et eÌ„ + eÌ„2 }
)
(
T
T X
T
X
X
1
2
E(et es ) + 2
E(et es )
= E(e2t ) âˆ’
T s=1
T t=1 s=1
(
)
T
X
2
1
=E(e2t ) + âˆ’ E(et et ) + 2
E(e2t )
T
T t=1
=

T âˆ’1 2
T âˆ’1
E(e2t ) =
Ïƒe ,
T
T

(S.16)

so lim E{(et âˆ’ eÌ„)2 } = Ïƒe2 .
T â†’âˆ

8. By the independence of et and Xt , for any s and t, we have that
Cov{(Xt âˆ’ Âµ
b)(et âˆ’ eÌ„), (es âˆ’ eÌ„)2 }
=E{(Xt âˆ’ Âµ
b)(et âˆ’ eÌ„)(es âˆ’ eÌ„)2 } âˆ’ E{(Xt âˆ’ Âµ
b)(et âˆ’ eÌ„)}E{(es âˆ’ eÌ„)2 }
=E(Xt âˆ’ Âµ
b)E{(et âˆ’ eÌ„)(es âˆ’ eÌ„)2 } âˆ’ E(Xt âˆ’ Âµ
b)E(et âˆ’ eÌ„)E(es âˆ’ eÌ„)2
=0,

(S.17)

where the last step is due to E(Xt âˆ’ Âµ
b) = 0 and E(et âˆ’ eÌ„) = 0.
9. For any t 6= s, Cov {(et âˆ’ eÌ„)2 , (es âˆ’ eÌ„)2 } = 0; and for t = s,
Var{(et âˆ’ eÌ„)2 }
=E{(et âˆ’ eÌ„)4 } âˆ’ E 2 {(et âˆ’ eÌ„)2 }
=E(e4t ) âˆ’ 4E(e3t eÌ„) + 6E(e2t eÌ„2 ) âˆ’ 4E(et eÌ„3 ) + E(eÌ„4t ) âˆ’ {E(e2t ) âˆ’ 2E(et eÌ„) + E(eÌ„2 )}2




4
6(T âˆ’ 1)
6
4
1
3(T âˆ’ 1)
4
4
2 2
4
4
4
2 2
=E(et ) âˆ’ E(et ) +
{E(et )} + 2 E(et ) âˆ’ 3 E(et ) +
E(et ) +
{E(et )}
T
T2
T
T
T3
T3

2
2
1
2
2
2
âˆ’ E(et ) âˆ’ E(et ) + E(et ) ,
(S.18)
T
T
so lim Var{(et âˆ’ eÌ„)2 } = E(e4t ) âˆ’ {E(e2t )}2 = E(e4t ) âˆ’ Ïƒe4 .
T â†’âˆ

9

10. Similar to the derivation in (S.18), we can show Cov{(et âˆ’ eÌ„)2 , (es âˆ’ eÌ„)(es+p âˆ’ eÌ„)} = 0
for s 6= t and s 6= t âˆ’ p. For a given t,
Cov{(et âˆ’ eÌ„)2 , (et âˆ’ eÌ„)(et+p âˆ’ eÌ„)}
=E{(et âˆ’ eÌ„)3 (et+p âˆ’ eÌ„)} âˆ’ E{(et âˆ’ eÌ„)2 }E{(et âˆ’ eÌ„)(et+p âˆ’ eÌ„)},

(S.19)

which can be derived analogously to the (S.18) that limT â†’âˆ E{(et âˆ’ eÌ„)3 (et+p âˆ’ eÌ„)} âˆ’ E{(et âˆ’
eÌ„)2 }E{(et âˆ’ eÌ„)(et+p âˆ’ eÌ„)} = E{e3t et+p } âˆ’ E{e2t }E{et et+p } = 0 and similarly lim Cov{(et âˆ’
T â†’âˆ

2

eÌ„) , (etâˆ’p âˆ’ eÌ„)(et âˆ’ eÌ„)} = 0.
11. For any t,
Cov {(Xt âˆ’ Âµ
b)(et+p âˆ’ eÌ„), (Xt+pâˆ’r âˆ’ Âµ
b)(et+p âˆ’ eÌ„)}
 

= E (Xt âˆ’ Âµ
b)(Xt+pâˆ’r âˆ’ Âµ
b)(et+p âˆ’ eÌ„)2 âˆ’ E(Xt âˆ’ Âµ
b)E(Xt+pâˆ’r âˆ’ Âµ
b)E 2 (et+p âˆ’ eÌ„)

= E {(Xt âˆ’ Âµ
b)(Xt+pâˆ’r âˆ’ Âµ
b)} E (et+p âˆ’ eÌ„)2


T âˆ’1
Ïƒe2 ,
(S.20)
= Î³|pâˆ’r|
T
where the second step is because of E(Xt âˆ’ Âµ
b) = 0 and the independence of Xt and et , the
third step is due to (S.16) and (S.15). Hence,
lim Cov {(Xt âˆ’ Âµ
b)(et+p âˆ’ eÌ„), (Xt+pâˆ’r âˆ’ Âµ
b)(et+p âˆ’ eÌ„)} = Î³|pâˆ’r| Ïƒe2 .

T â†’âˆ

Similarly,
lim Cov {(Xt+p âˆ’ Âµ
b)(et âˆ’ eÌ„), (Xtâˆ’r âˆ’ Âµ
b)(et âˆ’ eÌ„)} = Î³|pâˆ’r| Ïƒe2 .

T â†’âˆ

Then, similarly,
Cov {(Xt âˆ’ Âµ
b)(et+p âˆ’ eÌ„), (Xt+p+r âˆ’ Âµ
b)(et+p âˆ’ eÌ„)}
 

= E (Xt âˆ’ Âµ
b)(Xt+p+r âˆ’ Âµ
b)(et+p âˆ’ eÌ„)2 âˆ’ E(Xt âˆ’ Âµ
b)E(Xt+pâˆ’r âˆ’ Âµ
b)E 2 (et+p âˆ’ eÌ„)

= E {(Xt âˆ’ Âµ
b)(Xt+p+r âˆ’ Âµ
b)} E (et âˆ’ eÌ„)2


T âˆ’1
(S.21)
= Î³p+r
Ïƒe2 ,
T
and hence limT â†’âˆ Cov {(Xt âˆ’ Âµ
b)(et+p âˆ’ eÌ„), (Xt+p+r âˆ’ Âµ
b)(et+p âˆ’ eÌ„)} = Î³p+r Ïƒe2 . Similarly,
limT â†’âˆ Cov {(Xt+p âˆ’ Âµ
b)(et âˆ’ eÌ„), (Xt+r âˆ’ Âµ
b)(et âˆ’ eÌ„)} = Î³p+r Ïƒe2 .
10

12. By independence assumption between {et }, if t 6= s or p 6= r, we have that
Cov {(et âˆ’ eÌ„)(et+p âˆ’ eÌ„), (es âˆ’ eÌ„)(es+r âˆ’ eÌ„)} = 0.

(S.22)

In addition, by (S.16), we have that
Var {(et âˆ’ eÌ„)(et+p âˆ’ eÌ„)}

= E (et âˆ’ eÌ„)2 (et+p âˆ’ eÌ„)2


= E (et âˆ’ eÌ„)2 E (et+p âˆ’ eÌ„)2 ,
2

T âˆ’1
Ïƒe4 ,
=
T
so limT â†’âˆ Var {(et âˆ’ eÌ„)(et+p âˆ’ eÌ„)} = Ïƒe4 .

Step 2: Now we prove the results in (3).

11

(S.23)

âˆ—
1â—¦ . We first show the derivation of q100
as follows:
(
)
T
T
X
X
1
1
âˆ—
q100
= lim T Cov
(X âˆ— âˆ’ Âµ
bâˆ— )2 ,
(X âˆ— âˆ’ Âµ
bâˆ— )2
T â†’âˆ
T t=1 t
T s=1 s
"
T
1 X 2
= lim T Cov
Î±1 (Xt âˆ’ Âµ
b)2 + 2Î±1 (Xt âˆ’ Âµ
b)(et âˆ’ eÌ„) + (et âˆ’ eÌ„)2 ,
T â†’âˆ
T t=1
#
T
1X 2
Î± (Xs âˆ’ Âµ
b)2 + 2Î±1 (Xs âˆ’ Âµ
b)(es âˆ’ eÌ„) + (es âˆ’ eÌ„)2
T s=1 1
(
)
T
T
X
X
1
1
= Î±14 lim T Cov
(Xt âˆ’ Âµ
b)2 ,
(Xs âˆ’ Âµ
b)2
T â†’âˆ
T t=1
T s=1
(
)
T
T
1X
1X
+ lim T Cov
2Î±1 (Xt âˆ’ Âµ
b)(et âˆ’ eÌ„),
2Î±1 (Xs âˆ’ Âµ
b)(es âˆ’ eÌ„)
T â†’âˆ
T t=1
T s=1
(
)
T
T
X
1
1X
(et âˆ’ eÌ„)2 ,
(es âˆ’ eÌ„)2
+ lim T Cov
T â†’âˆ
T t=1
T s=1

=

Î±14 q00

T
T
4Î±12 X X
Cov {(Xt âˆ’ Âµ
b)(et âˆ’ eÌ„), (Xs âˆ’ Âµ
b)(es âˆ’ eÌ„)}
+ lim
T â†’âˆ T
t=1 s=1
T
T

1 XX
+ lim
Cov (et âˆ’ eÌ„)2 , (es âˆ’ eÌ„)2
T â†’âˆ T
t=1 s=1

=

Î±14 q00

T
4Î±12 X
Cov {(Xt âˆ’ Âµ
b)(et âˆ’ eÌ„), (Xt âˆ’ Âµ
b)(et âˆ’ eÌ„)}
+ lim
T â†’âˆ T
t=1

T

1X
Cov (et âˆ’ eÌ„)2 , (et âˆ’ eÌ„)2
T â†’âˆ T
t=1


= Î±14 q00 + 4Î±12 E (Xt âˆ’ Âµ
b)2 (et âˆ’ eÌ„)2 + E(e4t ) âˆ’ E(e2t )

+ lim

2

= Î±14 q00 + 4Î±12 Î³0 Ïƒe2 + E(e4t ) âˆ’ Ïƒe4 ,
where the second step is due to (S.10), the third step is because of (S.11), (S.17), and the
o
n P
P
b)2 , T1 Ts=1 (Xs âˆ’ Âµ
b)2 , the fifth step is due
definition q00 = limT â†’âˆ T Cov T1 Tt=1 (Xt âˆ’ Âµ
to (S.12) and (S.18), and the sixth step is because (S.13) and (S.18), and the last step is
because (S.16) and (S.17).

12

âˆ—
2â—¦ . We derive the value of q10p
:
(
)
T âˆ’p
T
X
X
1
1
âˆ—
âˆ—
q10p
= lim T Cov
(Xtâˆ— âˆ’ Âµ
bâˆ— )2 ,
(Xsâˆ— âˆ’ Âµ
bâˆ— )(Xs+p
âˆ’Âµ
bâˆ— )
T â†’âˆ
T t=1
T âˆ’ p s=1
"
T
1 X 2
= lim T Cov
Î±1 (Xt âˆ’ Âµ
b)2 + 2Î±1 (Xt âˆ’ Âµ
b)(et âˆ’ eÌ„) + (et âˆ’ eÌ„)2 ,
T â†’âˆ
T t=1
T âˆ’p

1 X 2
Î± (Xs âˆ’ Âµ
b)(Xs+p âˆ’ Âµ
b) + Î±1 (Xs âˆ’ Âµ
b)(es+p âˆ’ eÌ„)
T âˆ’ p s=1 1
#
+ Î±1 (Xs+p âˆ’ Âµ
b)(es âˆ’ eÌ„) + (es âˆ’ eÌ„)(es+p âˆ’ eÌ„)
)
T âˆ’p
T
X
X
1
1
(Xt âˆ’ Âµ
b)2 ,
(Xs âˆ’ Âµ
b)(Xs+p âˆ’ Âµ
b)
= Î±14 lim T Cov
T â†’âˆ
T t=1
T âˆ’ p s=1
(
)
T âˆ’p
T
1X
1 X
+ lim T Cov
2Î±1 (Xt âˆ’ Âµ
b)(et âˆ’ eÌ„),
Î±1 (Xs âˆ’ Âµ
b)(es+p âˆ’ eÌ„)
T â†’âˆ
T t=1
T âˆ’ p s=1
(
)
T âˆ’p
T
1X
1 X
+ lim T Cov
2Î±1 (Xt âˆ’ Âµ
b)(et âˆ’ eÌ„),
Î±1 (Xs+p âˆ’ Âµ
b)(es âˆ’ eÌ„)
T â†’âˆ
T t=1
T âˆ’ p s=1
)
(
T âˆ’p
T
X
1
1X
(et âˆ’ eÌ„)2 ,
(es âˆ’ eÌ„)(es+p âˆ’ eÌ„)
+ lim T Cov
T â†’âˆ
T t=1
T âˆ’ p s=1
(

T

=

Î±14 q0p

T âˆ’p

2Î±12 X X
+ lim
Cov {(Xt âˆ’ Âµ
b)(et âˆ’ eÌ„), (Xs âˆ’ Âµ
b)(es+p âˆ’ eÂ¯s )}
T â†’âˆ T âˆ’ p
t=1 s=1
T

T âˆ’p

2Î±12 X X
Cov {(Xt âˆ’ Âµ
b)(et âˆ’ eÌ„), (Xs+p âˆ’ Âµ
b)(es âˆ’ eÌ„)}
T â†’âˆ T âˆ’ p
t=1 s=1

+ lim

=

Î±14 q0p

T
2Î±12 X
+ lim
Cov {(Xt âˆ’ Âµ
b)(et âˆ’ eÌ„), (Xtâˆ’p âˆ’ Âµ
b)(et âˆ’ eÌ„)}
T â†’âˆ T âˆ’ p
t=p
(s=tâˆ’p)
T âˆ’p
2Î±12 X
Cov {(Xt âˆ’ Âµ
b)(et âˆ’ eÌ„), (Xt+p âˆ’ Âµ
b)(et âˆ’ eÌ„)}
T â†’âˆ T âˆ’ p
t=1

+ lim

(s=t)


b)(Xt+p âˆ’ Âµ
b)(et âˆ’ eÌ„)2
= Î±14 q0p + 2Î±12 E (Xt âˆ’ Âµ
b)(Xtâˆ’p âˆ’ Âµ
b)(et âˆ’ eÌ„)2 + 2Î±12 E (Xt âˆ’ Âµ


= Î±14 q0p + 4Î±12 Î³p Ïƒe2 ,
where the second step is due to (S.10), the third step is because of (S.11) and (S.17), the
n P
o
PT âˆ’p
1
b)2 , T âˆ’p
(X
âˆ’
Âµ
b
)(X
âˆ’
Âµ
b
)
fourth step is by definition that q0p = limT â†’âˆ T Cov T1 Tt=1 (Xt âˆ’ Âµ
s
s+p
s=1
and (S.19), the fifth step is due to (S.12), and the last step is result from (S.16) and (S.15).
13

âˆ—
3â—¦ . We derive q1pr
for p > 0, r > 0 and p 6= r:

(
âˆ—
q1pr

= lim T Cov
T â†’âˆ

)
T âˆ’p
T âˆ’r
1 X âˆ—
1 X âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
(X âˆ’ Âµ
b )(Xt+p âˆ’ Âµ
b ),
(X âˆ’ Âµ
b )(Xs+r âˆ’ Âµ
b)
T âˆ’ p t=1 t
T âˆ’ r s=1 s
T âˆ’p

"

1 X 2
= lim T Cov
Î±1 (Xt âˆ’ Âµ
b)(Xt+p âˆ’ Âµ
b) + Î±1 (Xt âˆ’ Âµ
b)(et+p âˆ’ eÌ„)
T â†’âˆ
T âˆ’ p t=1
+Î±1 (Xt+p âˆ’ Âµ
b)(et âˆ’ eÌ„) + (et âˆ’ eÌ„)(et+p âˆ’ eÌ„)} ,
T âˆ’r

1 X 2
b)(Xs+r âˆ’ Âµ
b) + Î±1 (Xs âˆ’ Âµ
b)(es+r âˆ’ eÌ„)
Î± (Xs âˆ’ Âµ
T âˆ’ r s=1 1
#
+ Î±1 (Xs+r âˆ’ Âµ
b)(es âˆ’ eÌ„) + (es âˆ’ eÌ„)(es+r âˆ’ eÌ„)
)
T âˆ’p
T âˆ’r
1 X
1 X
lim T Cov
(Xt âˆ’ Âµ
b)(Xt+p âˆ’ Âµ
b),
(Xs âˆ’ Âµ
b)(Xs+r âˆ’ Âµ
b)
T â†’âˆ
T âˆ’ p t=1
T âˆ’ r s=1
(
)
T âˆ’p
T âˆ’r
1 X
1 X
Î±1 (Xt âˆ’ Âµ
b)(et+p âˆ’ eÌ„),
+ lim T Cov
Î±1 (Xs âˆ’ Âµ
b)(es+r âˆ’ eÌ„)
T â†’âˆ
T âˆ’ p t=1
T âˆ’ r s=1
(
)
T âˆ’p
T âˆ’r
1 X
1 X
+ lim T Cov
Î±1 (Xt âˆ’ Âµ
b)(et+p âˆ’ eÌ„),
Î±1 (Xs+r âˆ’ Âµ
b)(es âˆ’ eÌ„)
T â†’âˆ
T âˆ’ p t=1
T âˆ’ r s=1
)
(
T âˆ’p
T âˆ’r
1 X
1 X
Î±1 (Xt+p âˆ’ Âµ
b)(et âˆ’ eÌ„),
Î±1 (Xs âˆ’ Âµ
b)(es+r âˆ’ eÌ„)
+ lim T Cov
T â†’âˆ
T âˆ’ p t=1
T âˆ’ r s=1
(
)
T âˆ’p
T âˆ’r
1 X
1 X
Î±1 (Xt+p âˆ’ Âµ
b)(et âˆ’ eÌ„),
+ lim T Cov
Î±1 (Xs+r âˆ’ Âµ
b)(es âˆ’ eÌ„)
T â†’âˆ
T âˆ’ p t=1
T âˆ’ r s=1
(

=

Î±14

=

Î±14 qpr

+

+

Î±12

Î±12

T
lim
T â†’âˆ (T âˆ’ p)(T âˆ’ r)

T âˆ’p
X

Cov {(Xt âˆ’ Âµ
b)(et+p âˆ’ eÌ„), (Xt+pâˆ’r âˆ’ Âµ
b)(et+p âˆ’ eÌ„)}

t=max(1,râˆ’p+1)
(s=t+pâˆ’r)

T âˆ’pâˆ’r
X
T
lim
Cov {(Xt âˆ’ Âµ
b)(et+p âˆ’ eÌ„), (Xt+p+r âˆ’ Âµ
b)(et+p âˆ’ eÌ„)}
T â†’âˆ (T âˆ’ p)(T âˆ’ r)
t=1
(s=t+p)

+

Î±12

T âˆ’p
X
T
lim
Cov {(Xt+p âˆ’ Âµ
b)(et âˆ’ eÌ„), (Xtâˆ’r âˆ’ Âµ
b)(et âˆ’ eÌ„)}
T â†’âˆ (T âˆ’ p)(T âˆ’ r)
t=r+1
(s=tâˆ’r)

+

Î±12

T
lim
T â†’âˆ (T âˆ’ p)(T âˆ’ r)

T âˆ’max(p,r)

X

Cov {(Xt+p âˆ’ Âµ
b)(et âˆ’ eÌ„), (Xt+r âˆ’ Âµ
b)(et âˆ’ eÌ„)}

t=1
(s=t)

= Î±14 qpr + 2Î±12 Ïƒe2 (Î³|pâˆ’r| + Î³p+r ),

(S.24)
14

where the second step is due to (S.10), the third step is because of (S.11) and a similar
version to (S.17), the fourth step is because (S.22) and by the definition that
o
n
PT âˆ’p
PT âˆ’r
1
1
b)(Xt+p âˆ’ Âµ
b), T âˆ’r s=1 (Xs âˆ’ Âµ
b)(Xs+r âˆ’ Âµ
b) , and the
qpr = limT â†’âˆ T Cov T âˆ’p t=1 (Xt âˆ’ Âµ
last step is from (S.20) and (S.21).

15

âˆ—
4â—¦ . Finally, we present the derivation of q1pp
for p 6= 0,
)
(
T âˆ’p
T âˆ’p
X
X
1
1
âˆ—
âˆ—
âˆ—
(X âˆ— âˆ’ Âµ
bâˆ— )(Xt+p
âˆ’Âµ
bâˆ— ),
(X âˆ— âˆ’ Âµ
bâˆ— )(Xs+p
âˆ’Âµ
bâˆ— )
q1pp
= lim T Cov
T â†’âˆ
T âˆ’ p t=1 t
T âˆ’ p s=1 s
"
T âˆ’p
1 X 2
Î±1 (Xt âˆ’ Âµ
b)(Xt+p âˆ’ Âµ
b) + Î±1 (Xt âˆ’ Âµ
b)(et+p âˆ’ eÌ„)
= lim T Cov
T â†’âˆ
T âˆ’ p t=1

+Î±1 (Xt+p âˆ’ Âµ
b)(et âˆ’ eÌ„) + (et âˆ’ eÌ„)(et+p âˆ’ eÌ„)} ,
T âˆ’p

1 X 2
Î± (Xs âˆ’ Âµ
b)(Xs+p âˆ’ Âµ
b) + Î±1 (Xs âˆ’ Âµ
b)(es+p âˆ’ eÌ„)
T âˆ’ p s=1 1
#
+ Î±1 (Xs+p âˆ’ Âµ
b)(es âˆ’ eÌ„) + (es âˆ’ eÌ„)(es+p âˆ’ eÌ„)
)
T âˆ’p
T âˆ’p
X
X
1
1
(Xt âˆ’ Âµ
b)(Xt+p âˆ’ Âµ
b),
(Xs âˆ’ Âµ
b)(Xs+p âˆ’ Âµ
b)
= Î±14 lim T Cov
T â†’âˆ
T âˆ’ p t=1
T âˆ’ p s=1
(
)
T âˆ’p
T âˆ’p
1 X
1 X
+ lim T Cov
Î±1 (Xt âˆ’ Âµ
b)(et+p âˆ’ eÌ„),
Î±1 (Xs âˆ’ Âµ
b)(es+p âˆ’ eÌ„)
T â†’âˆ
T âˆ’ p t=1
T âˆ’ p s=1
(
)
T âˆ’p
T âˆ’p
1 X
1 X
+ lim T Cov
Î±1 (Xt âˆ’ Âµ
b)(et+p âˆ’ eÌ„),
Î±1 (Xs+p âˆ’ Âµ
b)(es âˆ’ eÌ„)
T â†’âˆ
T âˆ’ p t=1
T âˆ’ p s=1
(
)
T âˆ’p
T âˆ’p
1 X
1 X
+ lim T Cov
Î±1 (Xt+p âˆ’ Âµ
b)(et âˆ’ eÌ„),
Î±1 (Xs âˆ’ Âµ
b)(es+p âˆ’ eÌ„)
T â†’âˆ
T âˆ’ p t=1
T âˆ’ p s=1
(
)
T âˆ’p
T âˆ’p
1 X
1 X
+ lim T Cov
Î±1 (Xt+p âˆ’ Âµ
b)(et âˆ’ eÌ„),
Î±1 (Xs+p âˆ’ Âµ
b)(es âˆ’ eÌ„)
T â†’âˆ
T âˆ’ p t=1
T âˆ’ p s=1
(
)
T âˆ’p
T âˆ’p
1 X
1 X
+ lim T Cov
(et âˆ’ eÌ„)(et+p âˆ’ eÌ„),
(es âˆ’ eÌ„)(es+p âˆ’ eÌ„)
T â†’âˆ
T âˆ’ p t=1
T âˆ’ p s=1
(

=

Î±14 qpp

+

Î±12

T âˆ’p
X
T
lim
Cov {(Xt âˆ’ Âµ
b)(et+p âˆ’ eÌ„), (Xt âˆ’ Âµ
b)(et+p âˆ’ eÌ„)}
T â†’âˆ (T âˆ’ p)2
t=1
s=t
T âˆ’2p

X
T
Cov {(Xt âˆ’ Âµ
b)(et+p âˆ’ eÌ„), (Xt+2p âˆ’ Âµ
b)(et+p âˆ’ eÌ„)}
T â†’âˆ (T âˆ’ p)2
t=1

+ Î±12 lim

s=t+p

+

Î±12

T âˆ’p
X
T
lim
Cov {(Xt+p âˆ’ Âµ
b)(et âˆ’ eÌ„), (Xtâˆ’p âˆ’ Âµ
b)(et âˆ’ eÌ„)}
T â†’âˆ (T âˆ’ p)2
t=1+p
s=tâˆ’p

+

Î±12

T
X
T
lim
Cov {(Xt+p âˆ’ Âµ
b)(et âˆ’ eÌ„), (Xt+p âˆ’ Âµ
b)(et âˆ’ eÌ„)}
T â†’âˆ (T âˆ’ p)2
t=1
s=t

+ Î±12 lim

T â†’âˆ

T
Var {(et âˆ’ eÌ„)(et+p âˆ’ eÌ„)} = Î±14 qpp + 2Î±12 Ïƒe2 (Î³0 + Î³2p ) + Ïƒe4 , (S.25)
2
(T âˆ’ p)
16

where the second step is due to (S.10), the third step is because of (S.11) and a similar
version to (S.17), the fourth step is because (S.22) and by the definition that
o
n
PT âˆ’p
PT âˆ’p
1
1
b)(Xt+p âˆ’ Âµ
b), T âˆ’p s=1 (Xs âˆ’ Âµ
b)(Xs+p âˆ’ Âµ
b) , the last
qpp = limT â†’âˆ T Cov T âˆ’p t=1 (Xt âˆ’ Âµ
step is because of (S.23), and (S.20) and (S.21) with q = p.

A.5

The proof of Theorem 4

Proof of Theorem 4(1):
For k = 1, . . . , p, applying the weak law of large numbers to Î³
bkâˆ— , we obtain that as T â†’ âˆ,
âˆ—
), which is denoted as Î³kâˆ— .
the estimator Î³
bkâˆ— converges in probability to Cov(Xtâˆ— , Xtâˆ’k

Next, we examine Î³k . By the form of measurement error model (9), we have that for
0 < k < t,
âˆ—
Cov(Xtâˆ— , Xtâˆ’k
)

= Cov(Î²0 Xt ut , Î²0 Xtâˆ’k utâˆ’k )
= Î²02 {E(Xt ut Xtâˆ’k utâˆ’k ) âˆ’ E(Xt ut )E(Xtâˆ’k utâˆ’k )}
= Î²02 {E(ut )E(utâˆ’k )Cov(Xt , Xtâˆ’k )}
= Î²02 {Cov(Xt , Xtâˆ’k )} = Î²02 Î³k ,
and by (10), Var(Xtâˆ— ) = Î²02 {(Ïƒu2 + 1)Î³0 + Ïƒu2 Âµ2 }, which is denoted as Î³0âˆ— . Thus, Theorem 4(1)
follows.

Proof of Theorem 4(2):
First, by Theorem 4(1), we write
Î³
bâˆ— = Î²02 Î³ + op (1)
and
ï£«
ï£¶
2
2
2 2
2
2
Î² (Ïƒ + 1)Î³0 + Î²0 Ïƒu Âµ
Î²0 Î³1 Â· Â· Â·
Î²0 Î³pâˆ’1
ï£¬ 0 u
ï£·
.
..
ï£¬
ï£·
..
bâˆ— = ï£¬
..
Î“
.
ï£· + op (1)
.
ï£­
ï£¸
2
2
2
2
2 2
Î²0 Î³pâˆ’1
Î²0 Î³pâˆ’2 Â· Â· Â· Î²0 (Ïƒu + 1)Î³0 + Î²0 Ïƒu Âµ

= Î²02 Î“ + Ïƒu2 (Î³0 + Âµ2 )Ip + op (1).
17

ï£«

Î³
b0âˆ—

Â·Â·Â·
ï£¬
.
ï£¬
..
bâˆ— = ï£¬ ..
where Î“
.
ï£­
âˆ—
Â·Â·Â·
Î³
bpâˆ’1
âˆ—
(6) with Î³
bk ,

ï£¶

âˆ—
Î³
bpâˆ’1
ï£·

.. ï£·
bk in
. ï£·. Then the naive estimator Ï†bâˆ— is obtained by replacing Î³
ï£¸
Î³
b0âˆ—



Ï†bâˆ— = [Î²02 Î“ + Ïƒu2 (Î³0 + Âµ2 )Ip + op (1)]âˆ’1 {Î²02 Î³ + op (1)} = Î“ + Ïƒu2 (Î³0 + Âµ2 )Ip

âˆ’1

Î³ + op (1),
(S.26)

and hence Ï†âˆ— = {Î“ + Ïƒu2 (Î³0 + Âµ2 )Ip }

âˆ’1

p

Î³ such that Ï†bâˆ— âˆ’â†’ Ï†âˆ— as T â†’ âˆ.

Again, by replacing Î³
bk in (6) with Î³
bkâˆ— gives the naive estimator Ï†bâˆ—0
!
!
p
T
T
X
X
X
1
1
âˆ—
Ï†bâˆ—0 =
Ï†bâˆ—k
Xâˆ— âˆ’
Xtâˆ’k
T âˆ’ p t=p t
T
âˆ’
p
t=p
k=1
= E(Xtâˆ— ) âˆ’ E(Xtâˆ— )

p
X

Ï†bâˆ—k + op (1)

k=1

= Î²0 E(Xt ) âˆ’ Î²0 E(Xt )

p
X

{Ï†âˆ—k + op (1)} + op (1)

k=1

= Î²0 (1 âˆ’ Ï†âˆ—T 1p )Âµ + op (1),
where Ï†bk and Ï†k are respectively the kth element of Ï†b and Ï†, the third step is because
Ï†bk = Ï†k + op (1) by (S.26) as well as the model form (9), and the last step is due to the
stationarity of the time series {Xt } such that E(Xt ) = Âµ.
bâˆ— Ï†bâˆ— by
b0âˆ— âˆ’ 2Ï†bâˆ—T Î³
bâˆ—2 = Î³
Finally, noting that the native estimator Ïƒb âˆ—2 is given by Ïƒ
bâˆ— + Ï†bâˆ—T Î“
applying a version similar to (6), we obtain that
bâˆ— Ï†bâˆ—
Ïƒ
bâˆ—2 =b
Î³0âˆ— âˆ’ 2Ï†bâˆ—T Î³
bâˆ— + Ï†bâˆ—T Î“

=Î²02 (Ïƒu2 + 1)Î³0 + Ïƒu2 Âµ2 âˆ’ 2Î²02 Î³ T {Î“ + Ïƒu2 (Î³0 + Âµ2 )I}âˆ’1 Î³
+ Î²02 Î³ T {Î“ + Ïƒu2 (Î³0 + Âµ2 )I}âˆ’1 {Î“ + Ïƒu2 (Î³0 + Âµ2 )I}{Î“ + Ïƒu2 (Î³0 + Âµ2 )I}âˆ’1 Î³ + op (1)

=Î²02 (Ïƒu2 + 1)Î³0 + Ïƒu2 Âµ2 âˆ’ Î²02 Î³ T {Î“ + Ïƒu2 (Î³0 + Âµ2 )I}âˆ’1 Î³ + op (1).
Proof of Theorem 4(3):
Step 1: We show that as T â†’ âˆ,
âˆš
T

!
T âˆ’p
T
X
1X âˆ—
1
âˆ—
âˆ—
(Xt âˆ’ Âµâˆ— )(Xt+p
âˆ’ Âµâˆ— ) âˆ’
(Xtâˆ— âˆ’ Âµ
bâˆ— )(Xt+p
âˆ’Âµ
bâˆ— ) = op (1).
T t=1
T âˆ’ p t=1
18

(S.27)

With some simple algebra,
(
)
T âˆ’p
T
X
âˆš
1
1X âˆ—
âˆ—
âˆ—
T
(X âˆ’ Âµâˆ— )(Xt+p
âˆ’ Âµâˆ— ) âˆ’
(X âˆ— âˆ’ Âµ
bâˆ— )(Xt+p
âˆ’Âµ
bâˆ— )
T t=1 t
T âˆ’ p t=1 t
(
)
T âˆ’p
T
X
âˆš
1X âˆ—
1
âˆ—
âˆ—
= T
(X âˆ’ Âµâˆ— )(Xt+p
âˆ’ Âµâˆ— ) âˆ’
(X âˆ— âˆ’ Âµâˆ— + Âµâˆ— âˆ’ Âµ
bâˆ— )(Xt+p
âˆ’ Âµâˆ— + Âµâˆ— âˆ’ Âµ
bâˆ— )
T t=1 t
T âˆ’ p t=1 t
(
T âˆ’p
T
âˆš
1X âˆ—
1 X âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
(Xt âˆ’ Âµ )(Xt+p âˆ’ Âµ ) âˆ’
(Xt âˆ’ Âµâˆ— )(Xt+p
âˆ’ Âµâˆ— )
= T
T t=1
T âˆ’ p t=1
)
T âˆ’p
T âˆ’p
T âˆ’p
X
X
1 X âˆ—
1
1
âˆ’
(X âˆ’ Âµâˆ— )(Âµâˆ— âˆ’ Âµ
bâˆ— ) âˆ’
(X âˆ— âˆ’ Âµâˆ— )(Âµâˆ— âˆ’ Âµ
bâˆ— ) âˆ’
(Âµâˆ— âˆ’ Âµ
bâˆ— )2
T âˆ’ p t=1 t
T âˆ’ p t=1 t+p
T âˆ’ p t=1


T âˆ’p
T
X
âˆš
T âˆ’p
1 X âˆ—
1
âˆ—
âˆ—
âˆ—
âˆ—
= T
(X âˆ’ Âµ )(Xt+p âˆ’ Âµ ) + âˆš
âˆ’1
(Xtâˆ— âˆ’ Âµâˆ— )(Xt+p
âˆ’ Âµâˆ— )
T
T âˆ’ p t=1 t
T t=T âˆ’p+1
!
T âˆ’p
T âˆ’p
X
X
âˆš
1
1
Xâˆ— +
Xâˆ— âˆ’ Âµ
Âµâˆ— âˆ’ Âµâˆ— )
bâˆ— âˆ’ Âµâˆ—
+ T (b
(S.28)
T âˆ’ p t=1 t
T âˆ’ p t=1 t+p
, I1 + I2 + I3 .
Now we examine each term in (S.28) as T â†’ âˆ separately. First,
T âˆ’p
p
1 X âˆ—
âˆ—
I1 = âˆ’ âˆš
(Xt âˆ’ Âµâˆ— )(Xt+p
âˆ’ Âµâˆ— )
T
âˆ’
p
T
t=1
p
= âˆ’ âˆš {Î³pâˆ— + op (1)} = op (1)
as T â†’ âˆ.
(S.29)
T
P
1
âˆ—
âˆ’
Next, we examine the second term I2 in (S.28). Since T âˆ’ 2 E[ Tt=T âˆ’p+1 (Xtâˆ— âˆ’ Âµâˆ— )(Xt+p
1

1

Âµâˆ— )] â‰¤ T âˆ’ 2 pVar(Xt ) (Brockwell et al. 1991, p.230) and T âˆ’ 2 pVar(Xt ) â†’ 0 as T â†’ âˆ, we
have that
1
I2 = âˆš
T

T
X

âˆ—
(Xtâˆ— âˆ’ Âµâˆ— )(Xt+p
âˆ’ Âµâˆ— ) = op (1).

t=T âˆ’p+1

19

(S.30)

Finally, we examine I3 in (S.28).
T âˆ’p

1 X âˆ—
X âˆ’Âµ
bâˆ—
T âˆ’ p t=1 t+p
=

T âˆ’p
p
T
1 X âˆ—
1X âˆ— 1 X âˆ—
Xt+p âˆ’
Xt âˆ’
X
T âˆ’ p t=1
T t=1
T t=p+1 t

T âˆ’p
p
T âˆ’p
1X âˆ— 1X âˆ—
1 X âˆ—
X âˆ’
X âˆ’
X
=
T âˆ’ p t=1 t+p T t=1 t
T t=1 t+p

=(

T âˆ’p
p
1X âˆ—
1 X âˆ—
1
Xt+p âˆ’
âˆ’ )
X
T âˆ’ p T t=1
T t=1 t

=op (1)
where Âµ
bâˆ— =

1
T

PT

t=1

Xtâˆ— , and

1
T

as

Pp

t=1

T â†’ âˆ,

(S.31)

Xtâˆ— = op (1) because E( T1

Pp

t=1

Xtâˆ— ) =

1
pE(Xt )
T

â†’ 0 as

T â†’ âˆ. In addition, by the weak law of large numbers,
T âˆ’p

1 X âˆ—
p
Xt âˆ’ Âµâˆ— âˆ’â†’ 0
T âˆ’ p t=1

as

T â†’ âˆ.

(S.32)

By condition (R2) and the central limit theorem for strictly stationary p-dependent sequences
(Brockwell et al. 1991, Theorem 6.4.2), we have
âˆš

T (b
Âµâˆ— âˆ’ Âµâˆ— ) = Op (1).

(S.33)

Therefore, applying (S.29), (S.30), (S.31), (S.32) and (S.33) yields (S.27).

Step 2: We show that as T â†’ âˆ, the asymptotic covariance matrix of

âˆš  âˆ— âˆ—T T
T (b
Î³0 , Î³
b ) âˆ’ (Î³0âˆ— , Î³ âˆ—T )T

equals
(
lim Cov

T â†’âˆ

)
T
T
X
1
1 X âˆ—
âˆ—
âˆ—
âˆš
âˆ’ Âµâˆ— ), âˆš
âˆ’ Âµâˆ— ) .
(Xt âˆ’ Âµâˆ— )(Xt+r
(Xsâˆ— âˆ’ Âµâˆ— )(Xs+q
T t=1
T s=1

20

For k â‰¤ p
âˆš

T (b
Î³k âˆ’ Î³k )
(
)
T âˆ’k
âˆš
1 X âˆ—
âˆ—
= T
(X âˆ’ Âµ
bâˆ— )(Xt+k
âˆ’Âµ
bâˆ— ) âˆ’ Î³k
T âˆ’ k t=1 t
(
)
T
âˆš
1X âˆ—
âˆ—
= T
(X âˆ’ Âµâˆ— )(Xt+k
âˆ’ Âµâˆ— ) âˆ’ Î³k
T t=1 t
)
(
T
âˆ’k
T
X
X
âˆš
1
1
âˆ—
âˆ—
(Xtâˆ— âˆ’ Âµ
bâˆ— )(Xt+k
âˆ’Âµ
bâˆ— ) âˆ’
(Xtâˆ— âˆ’ Âµâˆ— )(Xt+k
âˆ’ Âµâˆ— )
+ T
T âˆ’ k t=1
T t=1
)
(
T
1 X âˆ—
âˆ—
âˆ’ Âµâˆ— ) âˆ’ Î³k + op (1),
= âˆš
(Xt âˆ’ Âµâˆ— )(Xt+k
T t=1
where the last step is due to (S.27).
Hence, the (r, q) element of matrix lim Var
T â†’âˆ

(
lim Cov

T â†’âˆ

âˆš 

T (b
Î³0âˆ— , Î³
bâˆ—T )T âˆ’ (Î³0âˆ— , Î³ âˆ—T )T
is given by

)
T
T
X
1
1 X âˆ—
âˆ—
âˆ—
âˆš
(Xt âˆ’ Âµâˆ— )(Xt+r
âˆ’ Âµâˆ— ), âˆš
(Xsâˆ— âˆ’ Âµâˆ— )(Xs+q
âˆ’ Âµâˆ— ) .
T t=1
T s=1

Step 3: We show certain identities to be used for proving Theorem 4(3):
1. By model (9), we have that
Xtâˆ— âˆ’ Âµâˆ— = Î²0 Xt ut âˆ’ Î²0 Âµ
= Î²0 Xt ut âˆ’ Î²0 ut Âµ + Î²0 ut Âµ âˆ’ Î²0 Âµ
= Î²0 {ut (Xt âˆ’ Âµ) + Âµ(ut âˆ’ 1)}
where the first step is because Âµâˆ— = E(Î²0 Xt ut ) = Î²0 E(Xt )E(ut ) = Î²0 Âµ.

21

(S.34)

2. We have that
T
T

1 XX
Cov u2t (Xt âˆ’ Âµ)2 , u2s (Xs âˆ’ Âµ)2
lim
T â†’âˆ T
t=1 s=1
T
T

1 XX
E{u2t u2s (Xt âˆ’ Âµ)2 (Xs âˆ’ Âµ)2 } âˆ’ E(u2t )E(u2s )E{(Xt âˆ’ Âµ)2 }E{(Xs âˆ’ Âµ)2 } ,
T â†’âˆ T
t=1 s=1

= lim

T
T

1 XX
E(u2t u2s )E{(Xt âˆ’ Âµ)2 (Xs âˆ’ Âµ)2 } âˆ’ E(u2t )E(u2s )E{(Xt âˆ’ Âµ)2 }E{(Xs âˆ’ Âµ)2 }
T â†’âˆ T
t=1 s=1

= lim

s6=t

T

1 X
E(u4t )E{(Xt âˆ’ Âµ)4 } âˆ’ E 2 (u2t )E 2 {(Xt âˆ’ Âµ)2 } ,
+ lim
T â†’âˆ T
t=1
s=t

1
T â†’âˆ T

= lim

T X
T
X
t=1 s=1
s6=t

1
T â†’âˆ T

+ lim



T

1X 2 2
E(u2t )E(u2s )Cov{(Xt âˆ’ Âµ)2 , (Xs âˆ’ Âµ)2 } + lim
E (ut )Var{(Xt âˆ’ Âµ)2 }
T â†’âˆ T
t=1
s=t

T
X



E(u4t ) âˆ’ E 2 (u2t ) E{(Xt âˆ’ Âµ)4 }

t=1
s=t

T
T
T

1 XX
1 X
E(u2t )E(u2s )Cov{(Xt âˆ’ Âµ)2 , (Xs âˆ’ Âµ)2 } + lim
E(u4t ) âˆ’ E 2 (u2t ) E{(Xt âˆ’ Âµ)4 }
T â†’âˆ T
T â†’âˆ T
t=1 s=1
t=1

= lim

=(Ïƒu2 + 1)2 q00 + {E(u4t ) âˆ’ (Ïƒu2 + 1)2 }E{(Xt âˆ’ Âµ)4 },

(S.35)

where the second and third step is due to the independence between ut and Xt . In the last
T P
T
P
step, we use the definition q00 = limT â†’âˆ T1
Cov {(Xt âˆ’ Âµ)2 , (Xs âˆ’ Âµ)2 }, E(u2t ) = Ïƒu2 +1,
t=1 s=1

and the fact that E(u4t ) and E{(Xt âˆ’ Âµ)4 } are time-independent which are derived from
Conditions (R1) and (R2) together with independence between ut and Xt .
3. Similar to the derivation in (S.35), now we derive the summation of Cov{Î²02 u2t (Xt âˆ’

22

Âµ)2 , Î²02 us us+p (Xs âˆ’ Âµ)(Xs+p âˆ’ Âµ)} for p > 0,
T
T
1 XX
lim
Cov{Î²02 u2t (Xt âˆ’ Âµ)2 , Î²02 us us+p (Xs âˆ’ Âµ)(Xs+p âˆ’ Âµ)}
T â†’âˆ T
t=1 s=1
T
T
Î²04 X X 
E(u2t us us+p )E{(Xt âˆ’ Âµ)2 (Xs âˆ’ Âµ)(Xs+p âˆ’ Âµ)}
T â†’âˆ T
t=1 s=1

âˆ’E(u2t )E(us )E(us+p )E(Xt âˆ’ Âµ)2 E{(Xs âˆ’ Âµ)(Xs+p âˆ’ Âµ)}

= lim

T
T
Î²04 X X
E(u2t )E(us )E(us+p )Cov{(Xt âˆ’ Âµ)2 , (Xs âˆ’ Âµ)(Xs+p âˆ’ Âµ)}
= lim
T â†’âˆ T
t=1 s=1
T
Î²04 X 
+ lim
E(u3t )E(ut+p ) âˆ’ E(u2t )E(ut )E(ut+p ) E{(Xt âˆ’ Âµ)3 (Xt+p âˆ’ Âµ)}
T â†’âˆ T
t=1

+ lim

T â†’âˆ

s=t
T
Î²04 X

T
T



E(u3t )E(utâˆ’p ) âˆ’ E(u2t )E(ut )E(utâˆ’p ) E{(Xt âˆ’ Âµ)3 (Xtâˆ’p âˆ’ Âµ)}

t=1
s=tâˆ’p
T

Î²4 X X 2
= lim 0
(Ïƒu + 1)Cov{(Xt âˆ’ Âµ)2 , (Xs âˆ’ Âµ)(Xs+p âˆ’ Âµ)}
T â†’âˆ T
t=1 s=1

+ Î²04 E(u3t ) âˆ’ E(u2t ) E{(Xt âˆ’ Âµ)3 (Xt+p âˆ’ Âµ)}

+ Î²04 E(u3t ) âˆ’ E(u2t ) E{(Xt âˆ’ Âµ)3 (Xtâˆ’p âˆ’ Âµ)},



= Î²04 q0p (Ïƒu2 + 1) + Î²04 E(u3t ) âˆ’ (Ïƒu2 + 1) E{(Xt âˆ’ Âµ)3 (Xt+p âˆ’ Âµ)} + E{(Xt âˆ’ Âµ)3 (Xtâˆ’p âˆ’ Âµ)} ,
(S.36)
where the first step is because Xt and ut are independent, and the second last step is due
to E(u2t ) = V ar(ut ) + E(u2t ) = Ïƒu2 + 1 and is derived similar to the second and third step in
T P
T
P
Cov{(Xt âˆ’
(S.35), and the last step is because of the definition that q0p = limT â†’âˆ T1
t=1 s=1

Âµ)2 , (Xs âˆ’ Âµ)(Xs+p âˆ’ Âµ)} and the fact that E{(Xt âˆ’ Âµ)3 (Xt+p âˆ’ Âµ)}, E{(Xt âˆ’ Âµ)3 (Xtâˆ’p âˆ’ Âµ)}
and E(u3t ) are time-independent, derived from Conditions (R1) and (R2) together with the
independence between ut and Xt .
4.

Analogous to the derivation in (S.35) and (S.36), we derive the summation of

23

Cov{ut ut+p (Xt âˆ’ Âµ)(Xt+p âˆ’ Âµ), us us+r (Xs âˆ’ Âµ)(Xs+r âˆ’ Âµ)} for p > 0, r > 0 and p 6= r,
Î²04

T
T
1 XX
lim
Cov{ut ut+p (Xt âˆ’ Âµ)(Xt+p âˆ’ Âµ), us us+r (Xs âˆ’ Âµ)(Xs+r âˆ’ Âµ)}
T â†’âˆ T
t=1 s=1
T
T
1 XX
E(ut ut+p us us+r )Cov{(Xt âˆ’ Âµ)(Xt+p âˆ’ Âµ), (Xs âˆ’ Âµ)(Xs+r âˆ’ Âµ)}
T â†’âˆ T
t=1 s=1

=Î²04 lim

T
1 X
E(u2t )E(ut+p )E(ut+r ) âˆ’ 1 E{(Xt âˆ’ Âµ)2 (Xt+p âˆ’ Âµ)(Xt+r âˆ’ Âµ)}
T â†’âˆ T
t=1

+ Î²04 lim

1
T â†’âˆ T

+ Î²04 lim

s=t
T
X



E(u2t+p )E(ut )E(ut+p+r ) âˆ’ 1 E{(Xt âˆ’ Âµ)(Xt+p âˆ’ Âµ)2 (Xt+p+r âˆ’ Âµ)}

t=1
s=t+p

T
1 X 
E(u2t )E(ut+p )E(utâˆ’r ) âˆ’ 1 E{(Xtâˆ’r âˆ’ Âµ)(Xt âˆ’ Âµ)2 (Xt+p âˆ’ Âµ)}
T â†’âˆ T
t=1

+ Î²04 lim

s=tâˆ’r

1
T â†’âˆ T

+ Î²04 lim

T
X


E(u2t+p )E(ut )E(ut+pâˆ’r ) âˆ’ 1 E{(Xt âˆ’ Âµ)(Xt+pâˆ’r âˆ’ Âµ)Xt+p âˆ’ Âµ)2 }

t=1
s=t+pâˆ’r

=Î²04 qpr + Î²04 Ïƒu2 E{(Xt âˆ’ Âµ)2 (Xt+p âˆ’ Âµ)(Xt+r âˆ’ Âµ)} + Î²04 Ïƒu2 E{(Xt âˆ’ Âµ)(Xt+p âˆ’ Âµ)2 (Xt+p+r âˆ’ Âµ)}
+ Î²04 Ïƒu2 E{(Xtâˆ’r âˆ’ Âµ)(Xt âˆ’ Âµ)2 (Xt+p âˆ’ Âµ)} + Î²04 Ïƒu2 E{(Xt âˆ’ Âµ)(Xt+pâˆ’r âˆ’ Âµ)(Xt+p âˆ’ Âµ)2 },
(S.37)
where the third step is derived analogously to the second step of (S.36), and E(ut ut+p us us+r ) =
T P
T
P
Cov{(Xt âˆ’ Âµ)(Xt+p âˆ’
1, and the last step is due to the definition qpr = limT â†’âˆ T1
t=1 s=1

Âµ), (Xs âˆ’Âµ)(Xs+r âˆ’Âµ)} and the fact that E{(Xt âˆ’Âµ)2 (Xt+p âˆ’Âµ)(Xt+r âˆ’Âµ)}, E{(Xt âˆ’Âµ)(Xt+p âˆ’
Âµ)2 (Xt+p+r âˆ’ Âµ)}, E{(Xtâˆ’r âˆ’ Âµ)(Xt âˆ’ Âµ)2 (Xt+p âˆ’ Âµ)}, and E{(Xt âˆ’ Âµ)(Xt+p âˆ’ Âµ)2 (Xt+2p âˆ’ Âµ)}
are time-independent derived from Conditions (R1) and (R2).
5. Similar to the derivation in (S.35), (S.36), and (S.37), we derive the summation of

24

Cov{ut ut+p (Xt âˆ’ Âµ)(Xt+p âˆ’ Âµ), us us+p (Xs âˆ’ Âµ)(Xs+p âˆ’ Âµ)} for p > 0,
Î²04

T
T
1 XX
lim
Cov{ut ut+p (Xt âˆ’ Âµ)(Xt+p âˆ’ Âµ), us us+p (Xs âˆ’ Âµ)(Xs+p âˆ’ Âµ)}
T â†’âˆ T
t=1 s=1
T
T
1 XX
E(ut )E(ut+p )E(us )E(us+p )Cov{(Xt âˆ’ Âµ)(Xt+p âˆ’ Âµ), (Xs âˆ’ Âµ)(Xs+p âˆ’ Âµ)
T â†’âˆ T
t=1 s=1

= Î²04 lim

T
1 X
E(u2t )E(u2t+p ) âˆ’ 1 Var{(Xt âˆ’ Âµ)(Xt+p âˆ’ Âµ)}
T â†’âˆ T
t=1

+ Î²04 lim

1
T â†’âˆ T

+ Î²04 lim

s=t
T
X



E(u2t+p )E(ut )E(ut+2p ) âˆ’ 1 E{(Xt âˆ’ Âµ)(Xt+p âˆ’ Âµ)2 (Xt+2p âˆ’ Âµ)}

t=1
s=t+p

T
1 X 
E(u2t )E(utâˆ’p )E(ut+p ) âˆ’ 1 E{(Xtâˆ’p âˆ’ Âµ)(Xt âˆ’ Âµ)2 (Xt+p âˆ’ Âµ)}
T â†’âˆ T
t=1

+ Î²04 lim

s=tâˆ’p

= Î²04 qpp + Î²04 (Ïƒu4 + 2Ïƒu2 )Var{(Xt âˆ’ Âµ)(Xt+p âˆ’ Âµ)} + 2Î²04 E{(Xt âˆ’ Âµ)(Xt+p âˆ’ Âµ)2 (Xt+2p âˆ’ Âµ)},
(S.38)
where the last step is by the definition qpp = limT â†’âˆ

1
T

T P
T
P

Cov{(Xt âˆ’ Âµ)(Xt+p âˆ’ Âµ), (Xs âˆ’

t=1 s=1

Âµ)(Xs+p âˆ’ Âµ)} and E{(Xt âˆ’ Âµ)(Xt+p âˆ’ Âµ)2 (Xt+2p âˆ’ Âµ)} = E{(Xtâˆ’p âˆ’ Âµ)(Xt âˆ’ Âµ)2 (Xt+p âˆ’ Âµ)}
due to the stationarity of the time series and the fact that Var{(Xt âˆ’ Âµ)(Xt+p âˆ’ Âµ)} and
E{(Xt âˆ’Âµ)(Xt+p âˆ’Âµ)2 (Xt+2p âˆ’Âµ)} are time-independent, resulting from the Conditions (R1)
and (R2).
6. For any t, s and p, we have that
Cov{(Xt âˆ’ Âµ)(Xtâˆ’p âˆ’ Âµ), (Xs âˆ’ Âµ)}
=E{(Xt âˆ’ Âµ)(Xtâˆ’p âˆ’ Âµ)(Xs âˆ’ Âµ)} âˆ’ E{(Xt âˆ’ Âµ)(Xtâˆ’p âˆ’ Âµ)}E(Xs âˆ’ Âµ)
=E{(Xt âˆ’ Âµ)(Xtâˆ’p âˆ’ Âµ)(Xs âˆ’ Âµ)},
where the last step is because E(Xs âˆ’ Âµ) = 0.

25

(S.39)

7. For any t and s, we have that
Cov{ut (ut âˆ’ 1)(Xt âˆ’ Âµ), us (us âˆ’ 1)(Xs âˆ’ Âµ)}
=E{ut (ut âˆ’ 1)(Xt âˆ’ Âµ)us (us âˆ’ 1)(Xs âˆ’ Âµ)} âˆ’ E{ut (ut âˆ’ 1)(Xt âˆ’ Âµ)}E{us (us âˆ’ 1)(Xs âˆ’ Âµ)}
=E{ut (ut âˆ’ 1)(Xt âˆ’ Âµ)us (us âˆ’ 1)(Xs âˆ’ Âµ)}
=E{ut (ut âˆ’ 1)us (us âˆ’ 1)}E{(Xt âˆ’ Âµ)(Xs âˆ’ Âµ)},

(S.40)

where the second step is because of the independence between ut and Xt and that E(Xt âˆ’Âµ) =
0. Then, E{ut (ut âˆ’1)us (us âˆ’1)} = Ïƒu4 for t 6= s and E{u2t (ut âˆ’1)2 } = E(u4t )âˆ’2E(u3t )+Ïƒu2 +1
for any t.
By (S.40), we have that
T
T
1 XX
Cov {ut (ut âˆ’ 1)(Xt âˆ’ Âµ), us (us âˆ’ 1)(Xs âˆ’ Âµ)}
T â†’âˆ T
t=1 s=1

lim

T
T
1 XX
E{ut (ut âˆ’ 1)us (us âˆ’ 1)}E{(Xt âˆ’ Âµ)(Xs âˆ’ Âµ)}
T â†’âˆ T
t=1 s=1

= lim

T
T
T
1 XX 4
1 X
= lim
Ïƒu E{(Xt âˆ’ Âµ)(Xs âˆ’ Âµ)} + lim
E(u4t ) âˆ’ 2E(u3t ) + Ïƒu2 + 1 âˆ’ Ïƒu4 E{(Xt âˆ’ Âµ)2 }
T â†’âˆ T
T â†’âˆ T
t=1
t=1 s=1
s=t

=Ïƒu4

âˆ
X


Î³h + E(u4t ) âˆ’ 2E(u3t ) + Ïƒu2 + 1 âˆ’ Ïƒu4 Î³0 ,

(S.41)

h=âˆ’âˆ

where the last is because limT â†’âˆ

1
T

PT PT
t=1

s=1

E{(Xt âˆ’Âµ)(Xs âˆ’Âµ)} =

Pâˆ

h=âˆ’âˆ

Î³h (Brockwell

et al. 1991, Theorem 7.1.1).
8. For any t, s and p > 0, we have that
Cov{ut (ut âˆ’ 1)(Xt âˆ’ Âµ), us+p (us âˆ’ 1)(Xs+p âˆ’ Âµ)}
=E{ut (ut âˆ’ 1)(Xt âˆ’ Âµ)us+p (us âˆ’ 1)(Xs+p âˆ’ Âµ)} âˆ’ E{ut (ut âˆ’ 1)(Xt âˆ’ Âµ)}E{us+p (us âˆ’ 1)(Xs+p âˆ’ Âµ)}
=E{ut (ut âˆ’ 1)us+p (us âˆ’ 1)}E{(Xt âˆ’ Âµ)(Xs+p âˆ’ Âµ)}
=E{ut (ut âˆ’ 1)us+p (us âˆ’ 1)}Î³|s+pâˆ’t| ,

(S.42)

where the second step is because of the independence between ut and Xt and that E(Xt âˆ’Âµ) =
0. Then, E{ut (ut âˆ’ 1)us+p (us âˆ’ 1)} = 0 for t 6= s and E{ut (ut âˆ’ 1)2 ut+p } = E{ut (ut âˆ’ 1)2 } =
E{(ut âˆ’ 1)3 } + Ïƒu2 for any s = t.
26

9. By independence of ut and us , for t 6= s, we have that
Cov{u2t (Xt âˆ’ Âµ)2 , (us âˆ’ 1)2 } = 0,

(S.43)

and for any t,
Cov{u2t (Xt âˆ’ Âµ)2 , (ut âˆ’ 1)2 }
=E{u2t (ut âˆ’ 1)2 (Xt âˆ’ Âµ)2 } âˆ’ E{u2t (Xt âˆ’ Âµ)2 }E{(ut âˆ’ 1)2 }


= E{u2t (ut âˆ’ 1)2 } âˆ’ E(u2t )E(ut âˆ’ 1)2 E{(Xt âˆ’ Âµ)2 }

= E(u4t ) âˆ’ 2E(u3t ) + Ïƒu2 + 1 âˆ’ Ïƒu4 âˆ’ Ïƒu2 Î³0

= E(u4t ) âˆ’ 2E(u3t ) + 1 âˆ’ Ïƒu4 Î³0 .

(S.44)

10. By independence of ut and us , for s 6= t, s 6= t + p and any p, we have that
Cov{ut ut+p (Xt âˆ’ Âµ)(Xt+p âˆ’ Âµ), (us âˆ’ 1)2 } = 0.

(S.45)

For any t and p > 0,
Cov{ut ut+p (Xt âˆ’ Âµ)(Xt+p âˆ’ Âµ), (ut âˆ’ 1)2 }
=E{ut ut+p (ut âˆ’ 1)2 (Xt âˆ’ Âµ)(Xt+p âˆ’ Âµ)} âˆ’ E{ut ut+p (Xt âˆ’ Âµ)(Xt+p âˆ’ Âµ)}E{(ut âˆ’ 1)2 }


= E{ut ut+p (ut âˆ’ 1)2 } âˆ’ E(ut ut+p )E{(ut âˆ’ 1)2 } E{(Xt âˆ’ Âµ)(Xt+p âˆ’ Âµ)}

=E (ut âˆ’ 1)3 Î³p ,
(S.46)
and

Cov{ut utâˆ’p (Xt âˆ’ Âµ)(Xtâˆ’p âˆ’ Âµ), (ut âˆ’ 1)2 } = E (ut âˆ’ 1)3 Î³p .
11. For any t and s, and r 6= p and r > 0, we have that
Cov{ut ut+p (Xt âˆ’ Âµ)(Xt+p âˆ’ Âµ), (us âˆ’ 1)(us+r âˆ’ 1)} = 0.

(S.47)

By independence of ut and us , for t 6= s and any p, we have that
Cov{ut ut+p (Xt âˆ’ Âµ)(Xt+p âˆ’ Âµ), (us âˆ’ 1)(us+p âˆ’ 1)} = 0,

27

(S.48)

and for any t and p > 0,
Cov{ut ut+p (Xt âˆ’ Âµ)(Xt+p âˆ’ Âµ), (ut âˆ’ 1)(ut+p âˆ’ 1)}
=E{ut ut+p (ut âˆ’ 1)(tt+p âˆ’ 1)(Xt âˆ’ Âµ)(Xt+p âˆ’ Âµ)}
âˆ’ E{ut ut+p (Xt âˆ’ Âµ)(Xt+p âˆ’ Âµ)}E{(ut âˆ’ 1)(ut+p âˆ’ 1)}
=E{ut (ut âˆ’ 1)}E{ut+p (ut+p âˆ’ 1)}E{(Xt âˆ’ Âµ)(Xt+p âˆ’ Âµ)}
=Ïƒu4 Î³p .

(S.49)

12. For any t, we have that
Cov{ut (ut âˆ’ 1)(Xt âˆ’ Âµ), (us âˆ’ 1)2 }
= E{ut (ut âˆ’ 1)(Xt âˆ’ Âµ)(us âˆ’ 1)2 } âˆ’ E{ut (ut âˆ’ 1)(Xt âˆ’ Âµ)}E{(us âˆ’ 1)2 }


= E{ut (ut âˆ’ 1)(us âˆ’ 1)2 } âˆ’ E{ut (ut âˆ’ 1)}E{(us âˆ’ 1)2 } E(Xt âˆ’ Âµ) = 0,

(S.50)

where the last step is because E(Xt âˆ’ Âµ) = 0.
13. By independence assumption between {ut }, if t 6= s or p 6= r, we have that
Cov {(ut âˆ’ 1)(ut+p âˆ’ 1), (us âˆ’ 1)(us+r âˆ’ 1)} = 0.

(S.51)

In addition, for any t and p we have that
Var {(ut âˆ’ 1)(ut+p âˆ’ 1)}

= E (ut âˆ’ 1)2 (ut+p âˆ’ 1)2


= E (ut âˆ’ 1)2 E (ut+p âˆ’ 1)2
= Ïƒu4 ,

(S.52)

and for any t, we have that
Var(ut âˆ’ 1)2
=E{(ut âˆ’ 1)4 } âˆ’ E 2 {(ut âˆ’ 1)2 }
=E{(ut âˆ’ 1)4 } âˆ’ Ïƒu4 .

28

(S.53)

Step 4: Now we prove the results in (3).
âˆ—
1â—¦ . We first show the derivation of q200
as follows:
(
)
T
T
X
X
1
1
âˆ—
= lim T Cov
q200
(Xtâˆ— âˆ’ Âµâˆ— )2 ,
(Xsâˆ— âˆ’ Âµâˆ— )2
T â†’âˆ
T t=1
T s=1
T
T

Î²04 X X
= lim
Cov u2t (Xt âˆ’ Âµ)2 + 2Âµut (ut âˆ’ 1)(Xt âˆ’ Âµ) + Âµ2 (ut âˆ’ 1)2 ,
T â†’âˆ T
t=1 s=1

u2s (Xs âˆ’ Âµ)2 + 2Âµus (us âˆ’ 1)(Xs âˆ’ Âµ) + Âµ2 (us âˆ’ 1)2
T
T

Î²04 X X
Cov u2t (Xt âˆ’ Âµ)2 , u2s (Xs âˆ’ Âµ)2
T â†’âˆ T
t=1 s=1

= lim

T
T

4ÂµÎ²04 X X
Cov u2t (Xt âˆ’ Âµ)2 , us (us âˆ’ 1)(Xs âˆ’ Âµ)
T â†’âˆ T
t=1 s=1

+ lim

T
T

2Âµ2 Î²04 X X
+ lim
Cov u2t (Xt âˆ’ Âµ)2 , (us âˆ’ 1)2
T â†’âˆ
T t=1 s=1
T
T
4Âµ2 Î²04 X X
Cov {ut (ut âˆ’ 1)(Xt âˆ’ Âµ), us (us âˆ’ 1)(Xs âˆ’ Âµ)}
+ lim
T â†’âˆ
T t=1 s=1
T
T

Âµ4 Î²04 X X
+ lim
Cov (ut âˆ’ 1)2 , (us âˆ’ 1)2
T â†’âˆ T
t=1 s=1

= Î²04 (Ïƒu2 + 1)2 q0 + Î²04 {E(u4t ) âˆ’ (Ïƒu2 + 1)2 }E{(Xt âˆ’ Âµ)4 }
+ 4ÂµÎ²04 Ïƒu2 (Ïƒu2 + 1)v00 + 4ÂµÎ²04 {E(u4t ) âˆ’ E(u3t ) âˆ’ Ïƒu2 (Ïƒu2 + 1)}E{(Xt âˆ’ Âµ)3 }

+ 2Âµ2 Î²04 E(u4t ) âˆ’ 2E(u3t ) + 1 âˆ’ Ïƒu4 Î³0
"
#
âˆ
X

+ 4Âµ2 Î²04 Ïƒu4
Î³h + E(u4t ) âˆ’ 2E(u3t ) + Ïƒu2 + 1 âˆ’ Ïƒu4 Î³0
h=âˆ’âˆ



+ Âµ4 Î²04 E{(ut âˆ’ 1)4 } âˆ’ Ïƒu4 ,
where the second step is due to (S.34), the third step is because of (S.50), the last step is by
(S.35), (S.39), (S.41), (S.43), (S.44), and (S.53).

29

âˆ—
2â—¦ . Then we derive the value of q20p
:

(
âˆ—
q20p
= lim T Cov
T â†’âˆ

= lim

T
T
X
1X âˆ—
âˆ— 2 1
âˆ—
(Xt âˆ’ Âµ ) ,
(Xsâˆ— âˆ’ Âµâˆ— )(Xs+p
âˆ’ Âµâˆ— )
T
T
t=1

T
T
Î²04 X X

T â†’âˆ

T

)

s=1


Cov u2t (Xt âˆ’ Âµ)2 + 2Âµut (ut âˆ’ 1)(Xt âˆ’ Âµ) + Âµ2 (ut âˆ’ 1)2 ,

t=1 s=1

us us+p (Xs âˆ’ Âµ)(Xs+p âˆ’ Âµ) + Âµus (us+p âˆ’ 1)(Xs âˆ’ Âµ) + Âµus+p (us âˆ’ 1)(Xs+p âˆ’ Âµ) + Âµ2 (us âˆ’ 1)(us+p âˆ’ 1)
T
T

Î²04 X X
Cov u2t (Xt âˆ’ Âµ)2 , us us+p (Xs âˆ’ Âµ)(Xs+p âˆ’ Âµ)
T â†’âˆ T

= lim

+

+

+

t=1 s=1
T
T

ÂµÎ²04 X X
lim
Cov u2t (Xt âˆ’ Âµ)2 , us (us+p âˆ’ 1)(Xs âˆ’ Âµ) + us+p (us âˆ’ 1)(Xs+p
T â†’âˆ T
t=1 s=1
T
T
4
2ÂµÎ²0 X X
lim
Cov {us us+p (Xs âˆ’ Âµ)(Xs+p âˆ’ Âµ), ut (ut âˆ’ 1)(Xt âˆ’ Âµ)}
T â†’âˆ T
t=1 s=1
T
T

Âµ2 Î²04 X X 
Cov u2t (Xt âˆ’ Âµ)2 , (us âˆ’ 1)(us+p âˆ’ 1)
lim
T â†’âˆ T
t=1 s=1

âˆ’ Âµ)



+ Cov (ut âˆ’ 1)2 , us us+p (Xs âˆ’ Âµ)(Xs+p âˆ’ Âµ)
T
T
2Âµ2 Î²04 X X
Cov {ut (ut âˆ’ 1)(Xt âˆ’ Âµ), us (us+p âˆ’ 1)(Xs âˆ’ Âµ)}
T â†’âˆ
T

+ lim

t=1 s=1

+ lim

T â†’âˆ

+ lim

T â†’âˆ

2Âµ2 Î²04

T X
T
X

T

Cov {ut (ut âˆ’ 1)(Xt âˆ’ Âµ), us+p (us âˆ’ 1)(Xs+p âˆ’ Âµ)}

t=1 s=1
T
T
Âµ4 Î²04 X X

T


Cov (ut âˆ’ 1)2 , (us âˆ’ 1)(us+p âˆ’ 1)

t=1 s=1




+ 1) + Î²04 E(u3t ) âˆ’ (Ïƒu2 + 1) E{(Xt âˆ’ Âµ)3 (Xt+p âˆ’ Âµ)} + E{(Xt âˆ’ Âµ)3 (Xtâˆ’p âˆ’ Âµ)}


+ ÂµÎ²04 E{u3t âˆ’ u2t } E{(Xt âˆ’ Âµ)2 (Xtâˆ’p âˆ’ Âµ)} + E{(Xt âˆ’ Âµ)2 (Xt+p âˆ’ Âµ)}


+ 2ÂµÎ²04 Ïƒu2 v0p + 2ÂµÎ²04 E{u3t âˆ’ u2t âˆ’ Ïƒu2 } E{(Xt âˆ’ Âµ)2 (Xtâˆ’p âˆ’ Âµ)} + E{(Xt âˆ’ Âµ)2 (Xt+p âˆ’ Âµ)}

+ 2Âµ2 Î²04 E(ut âˆ’ 1)3 Î³p + 4Âµ2 Î²04 E(ut âˆ’ 1)3 + Ïƒu2 Î³p + Âµ4 Î²04 Ïƒu4



= Î²04 qp (Ïƒu2 + 1) + Î²04 E(u3t ) âˆ’ (Ïƒu2 + 1) E{(Xt âˆ’ Âµ)3 (Xt+p âˆ’ Âµ)} + E{(Xt âˆ’ Âµ)3 (Xtâˆ’p âˆ’ Âµ)}


+ 2ÂµÎ²04 Ïƒu2 vp + ÂµÎ²04 E{3u3t âˆ’ 3u2t âˆ’ 2Ïƒu2 } E{(Xt âˆ’ Âµ)2 (Xtâˆ’p âˆ’ Âµ)} + E{(Xt âˆ’ Âµ)2 (Xt+p âˆ’ Âµ)}
=

Î²04 q0p (Ïƒu2

+ 6Âµ2 Î²04 E(ut âˆ’ 1)3 Î³p + 4Âµ2 Î²04 Ïƒu2 Î³p ,

where the second step is by (S.34), the third step is because (S.39) and (S.50), and the
second last step is because (S.36), (S.47), (S.46), (S.42), and (S.51).

30

âˆ—
3â—¦ . Then we derive the value of q2pr
for r 6= p

(
âˆ—
q2pr

= lim T Cov
T â†’âˆ

T
T
1X âˆ—
1X âˆ—
âˆ—
âˆ—
(Xt âˆ’ Âµâˆ— )(Xt+p
âˆ’ Âµâˆ— ),
(X âˆ’ Âµâˆ— )(Xs+r
âˆ’ Âµâˆ— )
T t=1
T s=1 s

)

T
T
Î²04 X X
Cov {ut ut+p (Xt âˆ’ Âµ)(Xt+p âˆ’ Âµ)
T â†’âˆ T
t=1 s=1

= lim

+ Âµut (ut+p âˆ’ 1)(Xt âˆ’ Âµ) + Âµut+p (ut âˆ’ 1)(Xt+p âˆ’ Âµ) + Âµ2 (ut âˆ’ 1)(ut+p âˆ’ 1),
us us+r (Xs âˆ’ Âµ)(Xs+r âˆ’ Âµ) + Âµus (us+r âˆ’ 1)(Xs âˆ’ Âµ) + Âµus+r (us âˆ’ 1)(Xs+r âˆ’ Âµ) + Âµ2 (us âˆ’ 1)(us+r âˆ’ 1)
T
T
Î²04 X X
Cov {ut ut+p (Xt âˆ’ Âµ)(Xt+p âˆ’ Âµ), us us+r (Xs âˆ’ Âµ)(Xs+r âˆ’ Âµ)}
T â†’âˆ T
t=1 s=1

= lim

T
T
ÂµÎ²04 X X
+ lim
Cov {us us+r (Xs âˆ’ Âµ)(Xs+r âˆ’ Âµ), ut (ut+p âˆ’ 1)(Xt âˆ’ Âµ)}
T â†’âˆ T
t=1 s=1
T
T
ÂµÎ²04 X X
Cov {us us+r (Xs âˆ’ Âµ)(Xs+r âˆ’ Âµ), ut+p (ut âˆ’ 1)(Xt+p âˆ’ Âµ)}
T â†’âˆ T
t=1 s=1

+ lim

T
T
ÂµÎ²04 X X
Cov {ut ut+p (Xt âˆ’ Âµ)(Xt+p âˆ’ Âµ), us (us+r âˆ’ 1)(Xs âˆ’ Âµ)}
T â†’âˆ T
t=1 s=1

+ lim

T
T
ÂµÎ²04 X X
Cov {ut ut+p (Xt âˆ’ Âµ)(Xt+p âˆ’ Âµ), us+r (us âˆ’ 1)(Xs+r âˆ’ Âµ)}
T â†’âˆ T
t=1 s=1

+ lim

T
T
2Âµ2 Î²04 X X
Cov {ut ut+p (Xt âˆ’ Âµ)(Xt+p âˆ’ Âµ), (us âˆ’ 1)(us+r âˆ’ 1)}
T â†’âˆ
T t=1 s=1

+ lim

T
T
Âµ2 Î²04 X X
Cov {ut (ut+p âˆ’ 1)(Xt âˆ’ Âµ), us (us+r âˆ’ 1)(Xs âˆ’ Âµ)}
T â†’âˆ T
t=1 s=1

+ lim

T
T
Âµ2 Î²04 X X
Cov {ut (ut+p âˆ’ 1)(Xt âˆ’ Âµ), us+r (us âˆ’ 1)(Xs+r âˆ’ Âµ)}
T â†’âˆ T
t=1 s=1

+ lim

T
T
Âµ2 Î²04 X X
+ lim
Cov {ut+p (ut âˆ’ 1)(Xt+p âˆ’ Âµ), us (us+r âˆ’ 1)(Xs âˆ’ Âµ)}
T â†’âˆ T
t=1 s=1
T
T
Âµ2 Î²04 X X
Cov {ut+p (ut âˆ’ 1)(Xt+p âˆ’ Âµ), us+r (us âˆ’ 1)(Xs+r âˆ’ Âµ)}
T â†’âˆ T
t=1 s=1

+ lim

T

T

Âµ4 Î²04 X X
Cov {(ut âˆ’ 1)(ut+p âˆ’ 1), (us âˆ’ 1)(us+q âˆ’ 1)} ,
T â†’âˆ T
t=1 s=1

= Î²04 qpr + Î²04 Ïƒu2 E{(Xt âˆ’ Âµ)2 (Xt+p âˆ’ Âµ)(Xt+r âˆ’ Âµ)} + E{(Xt âˆ’ Âµ)(Xt+p âˆ’ Âµ)2 (Xt+p+r âˆ’ Âµ)}

+E{(Xtâˆ’r âˆ’ Âµ)(Xt âˆ’ Âµ)2 (Xt+p âˆ’ Âµ)} + E{(Xt âˆ’ Âµ)(Xt+pâˆ’r âˆ’ Âµ)(Xt+p âˆ’ Âµ)2 }
+ lim

+ ÂµÎ²04 Ïƒu2 [E{(Xt âˆ’ Âµ)(Xt+p âˆ’ Âµ)(Xt+r âˆ’ Âµ)} + E{(Xt âˆ’ Âµ)(Xt+p âˆ’ Âµ)(Xt+p+r âˆ’ Âµ)}
+E{(Xtâˆ’r âˆ’ Âµ)(Xt âˆ’ Âµ)(Xt+p âˆ’ Âµ)} + E{(Xt âˆ’ Âµ)(Xt+pâˆ’r âˆ’ Âµ)(Xt+p âˆ’ Âµ)}]
+ 2Âµ2 Î²04 Ïƒu2 (Î³|pâˆ’r| + Î³p+r ),

(S.54)

where the second step is by (S.34), the third step is because (S.39) and (S.50), and the
31

second last step is because (S.36), (S.47), (S.42), and (S.51).
âˆ—
âˆ—
4â—¦ . Finally, similar to the derivation of q2pq
, now we derive the value of q2pp

(
âˆ—
q2pp

= lim T Cov
T â†’âˆ

)
T
T
1X âˆ—
1X âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
(X âˆ’ Âµ )(Xt+p âˆ’ Âµ ),
(X âˆ’ Âµ )(Xs+p âˆ’ Âµ )
T t=1 t
T s=1 s

T
T
Î²04 X X
Cov {ut ut+p (Xt âˆ’ Âµ)(Xt+p âˆ’ Âµ) + Âµut (ut+p âˆ’ 1)(Xt âˆ’ Âµ)
T â†’âˆ T
t=1 s=1

= lim

+ Âµut+p (ut âˆ’ 1)(Xt+p âˆ’ Âµ) + Âµ2 (ut âˆ’ 1)(ut+p âˆ’ 1),
us us+r (Xs âˆ’ Âµ)(Xs+p âˆ’ Âµ) + Âµus (us+p âˆ’ 1)(Xs âˆ’ Âµ) + Âµus+p (us âˆ’ 1)(Xs+p âˆ’ Âµ) + Âµ2 (us âˆ’ 1)(us+p âˆ’ 1)
T
T
Î²04 X X
Cov {ut ut+p (Xt âˆ’ Âµ)(Xt+p âˆ’ Âµ), us us+p (Xs âˆ’ Âµ)(Xs+p âˆ’ Âµ)}
T â†’âˆ T
t=1 s=1

= lim

T
T
2Âµ2 Î²04 X X
Cov {ut ut+p (Xt âˆ’ Âµ)(Xt+p âˆ’ Âµ), (us âˆ’ 1)(us+p âˆ’ 1)}
T â†’âˆ
T t=1 s=1

+ lim

T
T
ÂµÎ²04 X X
Cov {us us+p (Xs âˆ’ Âµ)(Xs+p âˆ’ Âµ), ut (ut+p âˆ’ 1)(Xt âˆ’ Âµ)}
T â†’âˆ T
t=1 s=1

+ lim

T
T
ÂµÎ²04 X X
Cov {us us+p (Xs âˆ’ Âµ)(Xs+p âˆ’ Âµ), ut+p (ut âˆ’ 1)(Xt+p âˆ’ Âµ)}
T â†’âˆ T
t=1 s=1

+ lim

T
T
ÂµÎ²04 X X
Cov {ut ut+p (Xt âˆ’ Âµ)(Xt+p âˆ’ Âµ), us (us+p âˆ’ 1)(Xs âˆ’ Âµ)}
T â†’âˆ T
t=1 s=1

+ lim

T
T
ÂµÎ²04 X X
Cov {ut ut+p (Xt âˆ’ Âµ)(Xt+p âˆ’ Âµ), us+p (us âˆ’ 1)(Xs+p âˆ’ Âµ)}
T â†’âˆ T
t=1 s=1

+ lim

T
T
Âµ2 Î²04 X X
Cov {ut (ut+p âˆ’ 1)(Xt âˆ’ Âµ), us (us+p âˆ’ 1)(Xs âˆ’ Âµ)}
T â†’âˆ T
t=1 s=1

+ lim

T
T
Âµ2 Î²04 X X
Cov {ut (ut+p âˆ’ 1)(Xt âˆ’ Âµ), us+p (us âˆ’ 1)(Xs+p âˆ’ Âµ)}
T â†’âˆ T
t=1 s=1

+ lim

T
T
Âµ2 Î²04 X X
Cov {ut+p (ut âˆ’ 1)(Xt+p âˆ’ Âµ), us (us+p âˆ’ 1)(Xs âˆ’ Âµ)}
T â†’âˆ T
t=1 s=1

+ lim

T
T
Âµ2 Î²04 X X
Cov {ut+p (ut âˆ’ 1)(Xt+p âˆ’ Âµ), us+p (us âˆ’ 1)(Xs+p âˆ’ Âµ)}
T â†’âˆ T
t=1 s=1

+ lim

T
T
Âµ4 Î²04 X X
Cov {(ut âˆ’ 1)(ut+p âˆ’ 1), (us âˆ’ 1)(us+p âˆ’ 1)} ,
T â†’âˆ T
t=1 s=1

+ lim

= Î²04 qpp + Î²04 (Ïƒu4 + 2Ïƒu2 )Var{(Xt âˆ’ Âµ)(Xt+p âˆ’ Âµ)} + 2Î²04 E{(Xt âˆ’ Âµ)(Xt+p âˆ’ Âµ)2 (Xt+2p âˆ’ Âµ)} (S.55)


+ ÂµÎ²04 Ïƒu2 E{(Xt âˆ’ Âµ)(Xt+p âˆ’ Âµ)2 } + 2E{(Xt âˆ’ Âµ)(Xt+p âˆ’ Âµ)(Xt+2p âˆ’ Âµ)} + E{(Xt âˆ’ Âµ)2 (Xt+p âˆ’ Âµ)}
+ 2Âµ2 Î²04 Ïƒu4 Î³p + 2Âµ2 Î²04 Ïƒu2 (Î³0 + Î³2p ) + Âµ4 Î²04 Ïƒu4 ,

where the second step is by (S.34), the third step is because (S.39) and (S.50), and the last
step is because (S.38), (S.48), (S.49) and (S.52).
32

B

Tables
Supplementary Table 4: The results of the augmented Dickey-Fuller test
British Columbia

Definition
Definition 1

Definition 2

Definition 3

Transformation

TSV

Ontario

p-value

TSV

Quebec

p-value

TSV

Alberta

p-value

TSV

p-value

Xt

-8.346

<0.01

-1.527

0.755

-1.813

0.645

-2.850

0.245

Xt+1 âˆ’ Xt

-6.974

<0.01

-5.522

<0.01

-3.880

0.027

-3.516

0.059

Xt

-1.208

0.878

-4.294

<0.01

-2.018

0.566

-1.768

0.662

Xt+1 âˆ’ Xt

-3.336

0.084

-2.599

0.342

-3.340

0.084

-3.296

0.090

Xt

-1.325

0.833

-2.264

0.471

0.098

0.999

-2.688

0.307

Xt+1 âˆ’ Xt

-3.590

0.048

-4.584

<0.01

-2.209

0.492

-2.008

0.569

Supplementary Table 5: The results of the augmented Dickey-Fuller test
British Columbia
Definition

Ontario

Quebec

Alberta

Differencing

lag p

Differencing

lag p

Differencing

lag p

Differencing

lag p

1 degree

1

1 degree

1

1 degree

1

1 degree

1

no differencing

2

-

-

-

-

-

-

Definition 2

1 degree

2

no differencing

2

1 degree

2

1 degree

1

Definition 3

1 degree

1

1 degree

4

-

-

-

-

Definition 1

33

34

0.1
0.01

0.05
0.2

0.03
0.3

Additive (Ïƒe2 )
Multiplicative (Ïƒu2 )

Additive (Ïƒe2 )
Multiplicative (Ïƒu2 )

Additive (Ïƒe2 )
Multiplicative (Ïƒu2 )

0.3

Multiplicative (Ïƒu2 )

0.6

0.06

AR(2)

0.5

0.1

AR(2)

0.02

0.2

AR(2)*

0.6

0.2

-

-

-

1

1

0.01

0.1

0.1

0.02

0.2

0.05

AR(4)

0.005

0.05

AR(2)*

-

-

0.5

0.5

AR(1)

AR(1)
0.1

Ontario

British Columbia

Additive (Ïƒe2 )

Error Model

* The time series with no differencing

Definition 3

Definition 2

Definition 1

Definition

that are used for sensitivity analyses.

-

-

-

1

1

-

-

0.3

0.1

-

-

-

0.6

0.2

AR(2)

-

-

0.5

0.5

AR(1)

Quebec

-

-

-

0.8

0.3

-

-

0.4

0.05

-

-

-

0.8

0.1

AR(1)

-

-

0.4

0.1

AR(1)

Alberta

Supplementary Table 6: The parameter values of Ïƒe2 or Ïƒu2 for the measurement error model (7) or (9)

Supplementary Table 7: Definition 3: The parameter estimation under different measurement error models: the AR(1) model with â€œorder-1 differencingâ€ is used to fit the data of
British Columbia and the AR(4) model with â€œorder-1 differencingâ€ is used to fit the data of
Ontario.
British Columbia
Method

Ontario

Parameter

EST

SE

p-value

EST

SE

p-value

Ï†0

0.105

0.038

0.018

0.379

0.057

<0.001

Ï†1

-0.207

0.077

0.020

-0.086

0.099

0.391

Ï†2

-

-

-

-0.287

0.106

0.012

Ï†3

-

-

-

-0.301

0.094

0.004

Ï†4

-

-

-

-0.284

0.078

0.001

Ï†0

0.057

0.021

0.021

0.206

0.031

<0.001

Ï†1

-0.213

0.086

0.029

-0.088

0.100

0.383

Ï†2

-

-

-

-0.290

0.109

0.014

Ï†3

-

-

-

-0.303

0.094

0.003

The Proposed Method

Ï†4

-

-

-

-0.287

0.081

0.002

with Additive Error

Ï†0

0.058

0.021

0.017

0.212

0.036

<0.001

Ï†1

-0.234

0.147

0.137

-0.102

0.123

0.417

Ï†2

-

-

-

-0.306

0.139

0.037

Ï†3

-

-

-

-0.318

0.107

0.006

Ï†4

-

-

-

-0.308

0.093

0.003

Ï†0

0.058

0.023

0.027

0.210

0.033

<0.001

Ï†1

-0.244

0.090

0.019

-0.097

0.107

0.375

Ï†2

-

-

-

-0.300

0.117

0.016

Ï†3

-

-

-

-0.312

0.098

0.004

The Proposed Method

Ï†4

-

-

-

-0.300

0.087

0.002

with Multiplicative Error

Ï†0

0.066

0.035

0.087

0.230

0.058

0.001

Naive

Error Degree

-

2
Small (Ïƒe1
)

2
Large (Ïƒe2
)

2
Small (Ïƒu1
)

2
Large (Ïƒu2
)

Ï†1

-0.401

0.219

0.092

-0.139

0.183

0.454

Ï†2

-

-

-

-0.347

0.213

0.116

Ï†3

-

-

-

-0.354

0.159

0.035

Ï†4

-

-

-

-0.361

0.149

0.023

35

36

0

2

4

6

8

0

2

4

6

Apr 13

Apr 20

Apr 27

Additive

May 04

Day

Apr 13

Apr 20

Apr 27

Multiplicative

May 04

Moderate

Multiplicative

Additive

95% Prediction Interval

Naive

Multiplicative

Additive

Measurement Error Type

Reported Fatality

Fitted Fatality

Adjusted Fatality

Reference Time Series

reported mortality rates (in black) and the adjusted true mortality rate accounting for the asymptomatic cases (in green).

rate (May 5 - May 9) based on the additive (in blue) or multiplicative (in red) versus the naive model (in dark yellow); the

Supplementary Figure 4: British Columbia by Definition 1 (AR(2), no differencing): A 5-day forecasting of the true mortality

Fatality Rate (%)

8

Mild

37

0

2

4

6

0

2

4

Apr 13

Apr 20

Apr 27

May 04

Day

Apr 13

Apr 20

Apr 27

Multiplicative

May 04

Moderate

Multiplicative

Additive

95% Prediction Interval

Naive

Multiplicative

Additive

Measurement Error Type

Reported Fatality

Fitted Fatality

Adjusted Fatality

Reference Time Series

the reported mortality rates (in black) and the adjusted true mortality rate accounting for the asymptomatic cases (in green).

mortality rate (May 5 - May 9) based on the additive (in blue) or multiplicative (in red) versus the naive model (in dark yellow);

Supplementary Figure 5: British Columbia by Definition 1 (AR(1), order-1 differencing): A 5-day forecasting of the true

Fatality Rate (%)

6

Additive

Mild

38

1

2

3

4

5

6

1

2

3

4

5

Apr 13

Apr 20

Apr 27

May 04

Day

Apr 13

Apr 20

Apr 27

Multiplicative

May 04

Moderate

Multiplicative

Additive

95% Prediction Interval

Naive

Multiplicative

Additive

Measurement Error Type

Reported Fatality

Fitted Fatality

Adjusted Fatality

Reference Time Series

the reported mortality rates (in black) and the adjusted true mortality rate accounting for the asymptomatic cases (in green).

mortality rate (May 5 - May 9) based on the additive (in blue) or multiplicative (in red) versus the naive model (in dark yellow);

Supplementary Figure 6: British Columbia by Definition 2 (AR(3), order-1 differencing): A 5-day forecasting of the true

Fatality Rate (%)

6

Additive

Mild

39

0

10

20

âˆ’10

0

10

Apr 13

Apr 20

Apr 27

May 04

Day

Apr 13

Apr 20

Apr 27

Multiplicative

May 04

Moderate

Multiplicative

Additive

95% Prediction Interval

Naive

Multiplicative

Additive

Measurement Error Type

Reported Fatality

Fitted Fatality

Adjusted Fatality

Reference Time Series

mortality rates (in black) and the adjusted true mortality rate accounting for the asymptomatic cases (in green).

(May 5 - May 9) based on the additive (in blue) or multiplicative (in red) versus the naive model (in dark yellow); the reported

Supplementary Figure 7: Ontario by Definition 1 (AR(1), order-1 differencing): A 5-day forecasting of the true mortality rate

Fatality Rate (%)

20

Additive

Mild

40

2

4

6

8

10

2

4

6

8

Apr 13

Apr 20

Apr 27

May 04

Day

Apr 13

Apr 20

Apr 27

Multiplicative

May 04

Moderate

Multiplicative

Additive

95% Prediction Interval

Naive

Multiplicative

Additive

Measurement Error Type

Reported Fatality

Fitted Fatality

Adjusted Fatality

Reference Time Series

mortality rates (in black) and the adjusted true mortality rate accounting for the asymptomatic cases (in green).

5 - May 9) based on the additive (in blue) or multiplicative (in red) versus the naive model (in dark yellow); the reported

Supplementary Figure 8: Ontario by Definition 2 (AR(1), no differencing): A 5-day forecasting of the true mortality rate (May

Fatality Rate (%)

10

Additive

Mild

41

0

5

10

15

0

Apr 13

Apr 20

Apr 27

May 04

Day

Apr 13

Apr 20

Apr 27

Multiplicative

May 04

Moderate

Multiplicative

Additive

95% Prediction Interval

Naive

Multiplicative

Additive

Measurement Error Type

Reported Fatality

Fitted Fatality

Adjusted Fatality

Reference Time Series

mortality rates (in black) and the adjusted true mortality rate accounting for the asymptomatic cases (in green).

(May 5 - May 9) based on the additive (in blue) or multiplicative (in red) versus the naive model (in dark yellow); the reported

Supplementary Figure 9: Quebec by Definition 1 (AR(1), order-1 differencing): A 5-day forecasting of the true mortality rate

Fatality Rate (%)

10

Additive

Mild

42

2

4

6

8

10

2.5

5.0

7.5

Apr 13

Apr 20

Apr 27

May 04

Day

Apr 13

Apr 20

Apr 27

Multiplicative

May 04

Moderate

Multiplicative

Additive

95% Prediction Interval

Naive

Multiplicative

Additive

Measurement Error Type

Reported Fatality

Fitted Fatality

Adjusted Fatality

Reference Time Series

mortality rates (in black) and the adjusted true mortality rate accounting for the asymptomatic cases (in green).

(May 5 - May 9) based on the additive (in blue) or multiplicative (in red) versus the naive model (in dark yellow); the reported

Supplementary Figure 10: Quebec by Definition 2 (AR(2), order-1 differencing): A 5-day forecasting of the true mortality rate

Fatality Rate (%)

10.0

Additive

Mild

43

0

2

4

6

0

2

4

Apr 13

Apr 20

Apr 27

Additive

May 04

Day

Apr 13

Apr 20

Apr 27

Multiplicative

May 04

Moderate

Multiplicative

Additive

95% Prediction Interval

Naive

Multiplicative

Additive

Measurement Error Type

Reported Fatality

Fitted Fatality

Adjusted Fatality

Reference Time Series

mortality rates (in black) and the adjusted true mortality rate accounting for the asymptomatic cases (in green).

(May 5 - May 9) based on the additive (in blue) or multiplicative (in red) versus the naive model (in dark yellow); the reported

Supplementary Figure 11: Alberta by Definition 1 (AR(1), order-1 differencing): A 5-day forecasting of the true mortality rate

Fatality Rate (%)

6

Mild

44

0

1

2

3

4

0

1

2

3

Apr 13

Apr 20

Apr 27

Additive

May 04

Day

Apr 13

Apr 20

Apr 27

Multiplicative

May 04

Moderate

Multiplicative

Additive

95% Prediction Interval

Naive

Multiplicative

Additive

Measurement Error Type

Reported Fatality

Fitted Fatality

Adjusted Fatality

Reference Time Series

mortality rates (in black) and the adjusted true mortality rate accounting for the asymptomatic cases (in green).

(May 5 - May 9) based on the additive (in blue) or multiplicative (in red) versus the naive model (in dark yellow); the reported

Supplementary Figure 12: Alberta by Definition 2 (AR(1), order-1 differencing): A 5-day forecasting of the true mortality rate

Fatality Rate (%)

4

Mild

45

Multiplicative

Additive

Naive

Multiplicative

Additive

Naive

Multiplicative

Additive

Naive

Multiplicative

Additive
0.012
0.011
0.013

Moderate
Mild
Moderate

0.004
0.004
0.003

Moderate
Mild
Moderate

0.060
0.061
0.060

Moderate
Mild
Moderate

0.002
0.004
0.006
0.004
0.005

Mild
Moderate
Mild
Moderate

Alberta

0.163
0.061

Mild

Quebec

0.004

Mild

0.830

Ontario

0.017
0.011

-

Day 2

0.013

0.012

0.017

0.012

0.007

0.215

0.216

0.215

0.216

0.607

0.132

0.119

0.119

0.116

0.077

0.001

0.001

0.001

0.001

0.006

British Columbia

Mild

Naive

Day 1

-

2
2
Ïƒe
(or Ïƒu
)

Method

definition of death rates.

0.045

0.044

0.052

0.044

0.027

0.477

0.479

0.478

0.479

1.357

0.000

0.001

0.001

0.002

3.409

0.004

0.005

0.005

0.005

0.017

Day 3

0.089

0.087

0.098

0.087

0.055

0.776

0.778

0.776

0.778

2.289

0.225

0.171

0.172

0.161

0.360

0.029

0.027

0.027

0.027

0.058

Day 4

0.118

0.115

0.129

0.115

0.070

1.050

1.053

1.051

1.053

3.294

0.029

0.007

0.007

0.004

8.264

0.037

0.035

0.036

0.035

0.081

Day 5

Observed Prediction Error
OPE(h)

0.270

0.263

0.302

0.262

0.160

2.578

2.586

2.580

2.587

7.709

0.389

0.302

0.304

0.288

12.940

0.083

0.078

0.080

0.078

0.178

Definition 1

h=1

PH

0.022

0.031

0.035

0.115

0.125

0.205

0.399

0.811

1.561

1.811

0.169

0.176

0.591

0.607

0.612

0.015

0.020

0.057

0.066

0.069

Day 1

0.022

0.031

0.035

0.115

0.125

0.205

0.399

0.811

1.561

1.811

0.418

0.420

1.422

1.440

1.446

0.019

0.023

0.070

0.078

0.081

Day 2

0.022

0.031

0.035

0.115

0.125

0.205

0.399

0.811

1.561

1.811

0.630

0.602

2.040

2.046

2.048

0.018

0.023

0.070

0.078

0.081

Day 3

0.022

0.031

0.035

0.115

0.125

0.205

0.399

0.811

1.561

1.811

0.775

0.704

2.378

2.373

2.372

0.019

0.023

0.072

0.080

0.082

Day 4

0.022

0.031

0.035

0.115

0.125

0.205

0.399

0.811

1.561

1.811

0.888

0.754

2.531

2.517

2.514

0.019

0.023

0.073

0.080

0.083

Day 5

Expected Prediction Error
EPE(h)

0.109

0.157

0.177

0.577

0.627

1.025

1.995

4.057

7.807

9.057

2.879

2.655

8.963

8.983

8.991

0.090

0.111

0.342

0.382

0.396

h=1

PH

Supplementary Table 8: Definition 1: The observed prediction error and expected prediction error for different

46

Multiplicative

Additive

Naive

Multiplicative

Additive

Naive

Multiplicative

Additive

Naive

Multiplicative

Additive
0.010
0.010
0.010

Moderate
Mild
Moderate

0.004

Mild
Moderate

0.000
0.000
0.002

Moderate
Mild
Moderate

0.001
0.001
0.001
0.001
0.001

Mild
Moderate
Mild
Moderate

Alberta

0.013
0.000

Mild

Quebec

0.000
0.000

Moderate

0.001

Mild

0.020

Ontario

0.015
0.010

-

Day 2

0.001

0.001

0.001

0.001

0.000

0.003

0.002

0.002

0.001

0.044

0.010

0.002

0.000

0.004

0.087

0.005

0.005

0.005

0.005

0.015

British Columbia

Mild

Naive

Day 1

-

2
2
Ïƒe
(or Ïƒu
)

Method

definition of death rates.

0.003

0.003

0.003

0.003

0.002

0.004

0.003

0.003

0.003

0.086

0.014

0.003

0.000

0.007

0.196

0.011

0.011

0.011

0.011

0.032

Day 3

0.006

0.006

0.006

0.007

0.003

0.017

0.013

0.014

0.013

0.183

0.000

0.044

0.023

0.056

0.521

0.011

0.011

0.011

0.011

0.043

Day 4

0.019

0.019

0.019

0.019

0.012

0.073

0.066

0.068

0.065

0.413

0.035

0.152

0.110

0.175

1.059

0.000

0.000

0.000

0.000

0.020

Day 5

Observed Prediction Error
OPE(h)

0.030

0.031

0.031

0.031

0.019

0.098

0.084

0.087

0.081

0.739

0.063

0.201

0.134

0.243

1.884

0.037

0.037

0.037

0.037

0.126

Definition 2

h=1

PH

0.008

0.012

0.036

0.044

0.047

0.030

0.044

0.130

0.163

0.174

0.270

0.558

1.453

2.264

2.527

0.034

0.044

0.154

0.154

0.164

Day 1

0.008

0.012

0.037

0.045

0.047

0.030

0.045

0.133

0.165

0.176

0.331

0.599

1.626

2.391

2.643

0.035

0.044

0.157

0.157

0.167

Day 2

0.008

0.012

0.037

0.045

0.047

0.033

0.049

0.149

0.181

0.191

0.345

0.603

1.646

2.399

2.649

0.035

0.044

0.157

0.157

0.167

Day 3

0.008

0.012

0.037

0.045

0.047

0.034

0.049

0.151

0.182

0.192

0.348

0.603

1.649

2.399

2.649

0.035

0.044

0.157

0.157

0.167

Day 4

0.008

0.012

0.037

0.045

0.047

0.034

0.050

0.153

0.183

0.193

0.348

0.603

1.649

2.399

2.649

0.035

0.044

0.157

0.157

0.167

Day 5

Expected Prediction Error
EPE(h)

0.042

0.059

0.185

0.223

0.236

0.162

0.236

0.716

0.874

0.926

1.642

2.965

8.023

11.853

13.117

0.174

0.222

0.784

0.783

0.834

h=1

PH

Supplementary Table 9: Definition 2: The observed prediction error and expected prediction error for different

47

Multiplicative

Additive

Naive

Multiplicative

Additive

Day 2

0.001
0.001

Mild
Moderate

0.048
0.002
0.002
0.002
0.002

Mild
Moderate
Mild
Moderate

Ontario

0.001

Moderate

0.000

0.004

0.004

0.004

0.004

0.132

0.001

0.001

0.001

0.001

0.003

British Columbia

0.001

-

Naive

Day 1

Mild

2
2
Ïƒe
(or Ïƒu
)

Method

definition of death rates.

0.011

0.011

0.011

0.011

0.243

0.000

0.000

0.000

0.000

0.020

Day 3

0.015

0.016

0.016

0.017

0.333

0.006

0.005

0.005

0.005

0.057

Day 4

0.023

0.023

0.023

0.024

0.464

0.010

0.009

0.009

0.009

0.090

Day 5

Observed Prediction Error
OPE(h)

0.055

0.057

0.057

0.058

1.219

0.017

0.016

0.016

0.016

0.170

Definition 3

h=1

PH

0.009

0.011

0.036

0.039

0.039

0.005

0.007

0.026

0.029

0.030

Day 1

0.009

0.011

0.036

0.039

0.039

0.005

0.008

0.028

0.030

0.031

Day 2

0.009

0.011

0.036

0.039

0.039

0.005

0.008

0.028

0.030

0.031

Day 3

0.010

0.012

0.039

0.042

0.042

0.005

0.008

0.028

0.030

0.031

Day 4

0.010

0.012

0.039

0.042

0.042

0.005

0.008

0.028

0.030

0.031

Day 5

Expected Prediction Error
EPE(h)

0.048

0.056

0.187

0.200

0.202

0.023

0.038

0.137

0.151

0.155

h=1

PH

Supplementary Table 10: Definition 3: The observed prediction error and expected prediction error for different

