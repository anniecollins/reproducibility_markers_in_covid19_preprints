Algorithm is Experiment:
Machine Learning, Market Design, and Policy Eligibility Rules
Yusuke Narita

Kohei Yata‚àó

arXiv:2104.12909v2 [econ.EM] 28 Jun 2021

June 29, 2021

Abstract
Algorithms produce a growing portion of decisions and recommendations both in policy and business. Such algorithmic decisions are natural experiments (conditionally quasirandomly assigned instruments) since the algorithms make decisions based only on observable
input variables. We use this observation to develop a treatment-effect estimator for a class
of stochastic and deterministic decision-making algorithms. Our estimator is shown to be
consistent and asymptotically normal for well-defined causal effects. A key special case of
our estimator is a multidimensional regression discontinuity design. We apply our estimator
to evaluate the effect of the Coronavirus Aid, Relief, and Economic Security (CARES) Act,
where more than $175 billion worth of relief funding is allocated to hospitals via an algorithmic rule. Our estimates suggest that the relief funding has little effect on COVID-19-related
hospital activity levels. Naive OLS and IV estimates exhibit substantial selection bias.

‚àó

Narita: Yale University, email: yusuke.narita@yale.edu. Yata: Yale University, email: kohei.yata@yale.edu.
We are especially indebted to Aneesha Parvathaneni and Richard Liu for expert research assistance. For their
suggestions, we are grateful to Josh Angrist, Tim Armstrong, Pat Kline, Michal Koles√°r and seminar participants
at American Economic Association, Caltech, Columbia, CEMFI, Counterfactual Machine Learning Workshop,
Econometric Society, European Economic Association, Hitotsubashi, JSAI, Stanford, UC Irvine, the University
of Tokyo, and Yale.

1

Introduction

Today‚Äôs society increasingly resorts to algorithms for decision-making and resource allocation.
For example, judges in the US make legal decisions aided by predictions from supervised machine learning algorithms. Supervised learning is also used by governments to detect potential
criminals and terrorists, and by banks and insurance companies to screen potential customers.
Tech companies like Facebook, Microsoft, and Netflix allocate digital content by reinforcement
learning and bandit algorithms. Retailers and e-commerce platforms engage in algorithmic pricing. Similar algorithms are encroaching on high-stakes settings, such as in education, healthcare,
and the military.
Other types of algorithms also loom large. School districts, college admissions systems,
and labor markets use matching algorithms for position and seat allocations. Objects worth
astronomical sums of money change hands every day in algorithmically run auctions. Many
public policy domains like Medicaid often use algorithmic rules to decide who are eligible.
All of the above, diverse examples share a common trait: a decision-making algorithm makes
decisions based only on its observable input variables. Thus conditional on the observable variables, algorithmic treatment decisions are (quasi-)randomly assigned. That is, they are independent of any potential outcome or unobserved heterogeneity. This property turns algorithmbased treatment decisions into instrumental variables (IVs) that can be used for measuring the
causal effect of the final treatment assignment. The algorithm-based instrument may produce
stratified randomization (e.g., certain bandit and reinforcement learning algorithms), regressiondiscontinuity-style local variation (e.g., machine judges), or some combination of the two.
This paper shows how to use data obtained from algorithmic decision-making to identify and
estimate causal effects. In our framework, the analyst observes a random sample {(Yi , Xi , Di , Zi )}ni=1 ,
where Yi is the outcome of interest, Xi ‚àà Rp is a vector of pre-treatment covariates (algorithm‚Äôs
input variables), Di is the binary treatment assignment, possibly made by humans, and Zi is the
binary treatment recommendation made by a known algorithm. The algorithm takes Xi as input
and computes the probability of the treatment recommendation A(Xi ) = Pr(Zi = 1|Xi ). Zi is
then randomly determined based on the known probability A(Xi ) independently of everything
else conditional on Xi . The algorithm‚Äôs recommendation Zi may influence the final treatment
assignment Di , determined as Di = Zi Di (1) + (1 ‚àí Zi )Di (0), where Di (z) is the potential treatment assignment that would be realized if Zi = z. Finally, the observed outcome Yi is determined
as Yi = Di Yi (1) + (1 ‚àí Di )Yi (0), where Yi (1) and Yi (0) are potential outcomes that would be
realized if the individual were treated and not treated, respectively. This setup is an IV model
where the IV satisfies the conditional independence condition but may not satisfy the overlap
(full-support) condition. To our knowledge, there is no standard estimator for this setup.
Within this framework, we first characterize the sources of causal-effect identification for a
class of data-generating algorithms. This class includes both stochastic and deterministic algorithms.1 The sources of causal-effect identification turn out to be summarized by a suitable
1
This class includes all of the aforementioned examples, thus nesting existing insights on quasi-experimental
variation in particular algorithms, such as surge pricing (Cohen, Hahn, Hall, Levitt and Metcalfe, 2016), bandit
(Li, Chu, Langford and Schapire, 2010), reinforcement learning (Precup, 2000), supervised learning (Cowgill,
2018; Bundorf, Polyakova and Tai-Seale, 2019), and market-design algorithms (Abdulkadiroƒülu, Angrist, Narita

1

modification of the Propensity Score (Rosenbaum and Rubin, 1983). We call it the Approximate Propensity Score (APS). For each covariate value x, the Approximate Propensity Score is
the average probability of a treatment recommendation in a shrinking neighborhood around x,
defined as
R
‚àó
‚àó
B(x,Œ¥) A(x )dx
A
R
p (x) ‚â° lim
,
‚àó
Œ¥‚Üí0
B(x,Œ¥) dx
where B(x, Œ¥) is a p-dimensional ball with radius Œ¥ centered at x. The Approximate Propensity
Score provides an easy-to-check condition for what causal effects the data from an algorithm
allow us to identify. In particular, we show that the conditional local average treatment effect
(LATE; Imbens and Angrist, 1994) at covariate value x is identified if and only if the Approximate
Propensity Score is nondegenerate, i.e., pA (x) ‚àà (0, 1).
The identification analysis suggests a way of estimating treatment effects using the algorithmproduced data. The treatment effects can be estimated by two-stage least squares (2SLS) where
we regress the outcome on the treatment with the algorithm‚Äôs recommendation as an IV. To
make the algorithmic recommendation a conditionally independent IV, we propose to control for
the Approximate Propensity Score. Our proposed estimator is as follows.
1. For small bandwidth Œ¥ > 0 and a large number of simulation draws S, compute
S
1X
‚àó
A(Xi,s
),
p (Xi ; Œ¥) =
S
s

s=1

‚àó , ..., X ‚àó are S independent simulation draws from the uniform distribution on
where Xi,1
i,S
B(Xi , Œ¥).2 This ps (Xi ; Œ¥) is a simulation-based approximation to the Approximate Propensity Score pA (x).

2. Using the observations with ps (Xi ; Œ¥) ‚àà (0, 1), run the following 2SLS IV regression:
Di = Œ≥0 + Œ≥1 Zi + Œ≥2 ps (Xi ; Œ¥) + ŒΩi (First Stage)
Yi = Œ≤0 + Œ≤1 Di + Œ≤2 ps (Xi ; Œ¥) + i (Second Stage).
Let Œ≤ÃÇ1s be the estimated coefficient on Di .
and Pathak, 2017, Forthcoming; Abdulkadiroƒülu, 2013; Narita, 2020, 2021). Our framework also reveals new
sources of identification for algorithms that, at first sight, do not appear to produce a natural experiment.
2
To make common Œ¥ for all dimensions reasonable, we standardize each characteristic Xij to have mean zero
and variance one for each j = 1, ..., p, where p is the number of input characteristics. For the bandwidth Œ¥, we
suggest that the analyst considers several different values and check if the 2SLS estimates are robust to bandwidth
changes, as we often do in regression discontinuity design (RDD) applications. It is hard to pick Œ¥ in a data-driven
way. Common methods for bandwidth selection in univariate RDDs include Imbens and Kalyanaraman (2012)
and Calonico, Cattaneo and Titiunik (2014), who estimate the bandwidth that minimizes the asymptotic mean
squared error (AMSE). It is not straightforward to estimate the AMSE-optimal bandwidth in our setting with
many running variables and complex IV assignment, since it requires nonparametric estimation of functions on
the multidimensional covariate space such as conditional mean functions, their derivatives, the curvature of the
RDD boundary, etc.

2

As the main theoretical result, we prove the 2SLS estimator Œ≤ÃÇ1s is a consistent and asymptotically normal estimator of a well-defined causal effect (weighted average of conditional local average treatment effects). We also show that inference based on the conventional 2SLS
heteroskedasticity-robust standard errors is asymptotically valid as long as the bandwidth Œ¥
goes to zero at an appropriate rate. We prove the asymptotic properties by exploiting results
from differential geometry and geometric measure theory, which may be of independent interest. There appears to be no existing estimator with these asymptotic properties even for the
multidimensional RDD, a special case of our framework where the decision-making algorithm is
deterministic and uses multiple input (running) variables for assigning treatment recommendations. Moreover, our result applies to much more general settings with stochastic algorithms,
deterministic algorithms, and combinations of the two.
The practical performance of our estimator is demonstrated through simulation and an original application. We first conduct a Monte Carlo simulation mimicking real-world decision-making
based on machine learning algorithms. We consider a data-generating process combining stochastic and deterministic algorithms. Treatment recommendations are randomly assigned for a small
experimental segment of the population and are determined by a high-dimensional, deterministic machine learning algorithm for the rest of the population. Our estimator is shown to have
smaller mean squared errors relative to alternative estimators.
Our empirical application is an analysis of COVID-19 hospital relief funding. The Coronavirus
Aid, Relief, and Economic Security (CARES) Act and Paycheck Protection Program designated
$175 billion for COVID-19 response efforts and reimbursement to health care entities for expenses
or lost revenues (Kakani, Chandra, Mullainathan and Obermeyer, 2020). This policy intended
to help hospitals hit hard by the pandemic, as ‚Äúfinancially insecure hospitals may be less capable
of investing in COVID-19 response efforts‚Äù (Khullar, Bond and Schpero, 2020). We ask whether
this problem is alleviated by the relief funding to hospitals.
We identify the causal effects of the relief funding by exploiting the funding eligibility rule.
The government employs an algorithmic rule to decide which hospitals are eligible for funding.
This fact allows us to apply our method to estimate the effect of relief funding. Specifically, our
2SLS estimators use eligibility status as an instrumental variable for funding amounts, while controlling for the Approximate Propensity Score induced by the eligibility-determining algorithm.
The resulting estimates suggest that COVID-19 relief funding has little to no effect on outcomes, such as the number of hospitalized COVID-19 patients at each hospital. The estimated
causal effects of relief funding are much smaller and less significant than the naive ordinary least
squares (OLS) (with and without controls) or 2SLS estimates with no controls. Our finding
provides causal evidence for the concern that funding in the CARES Act was not well targeted
to the clinics and hospitals with the greatest needs.3
3
See, for example, Kakani et al. (2020) as well as Forbes‚Äôs article, ‚ÄúHospital Giant HCA To Return $6 Billion in
CARES Act Money,‚Äù at https://www.forbes.com/sites/brucejapsen/2020/10/08/hospital-giant-hca-toreturn-6-billion-in-cares-act-money, retrieved June 2021.

3

Related Literature
Theoretically, our framework integrates the classic propensity-score (selection-on-observables)
scenario with a multidimensional extension of the RDD. We analyze this integrated setup in
the IV world with noncompliance. This general setting appears to have no prior established
estimator. Armstrong and Koles√°r (2020) provide an estimator for a related setting with perfect
compliance.4 The Approximate Propensity Score developed in this paper shares its spirit with
the local random assignment interpretation of the RDD, discussed by Fr√∂lich (2007), Cattaneo,
Frandsen and Titiunik (2015), Cattaneo, Titiunik and Vazquez-Bare (2017), Frandsen (2017),
Sekhon and Titiunik (2017), Fr√∂lich and Huber (2019), Abdulkadiroƒülu et al. (Forthcoming) and
Eckles, Ignatiadis, Wager and Wu (2020). These papers consider settings that fit into this paper‚Äôs
framework.
When we adapt our estimator to the multidimensional RDD case, our estimator has three
features. First, it is a consistent and asymptotically normal estimator of a well-interpreted causal
effect (average of conditional treatment effects along the RDD boundary) even if treatment
effects are heterogeneous. Second, it uses observations near all the boundary points as opposed
to using only observations near one specific boundary point, thus avoiding variance explosion
even when Xi has many elements. Third, it can be easily implemented even in cases with many
covariates and complex algorithms (RDD boundaries). Our method circumvents the difficulty
of identifying the decision boundary from a complicated decision-making algorithm. No prior
estimator appears to have all of these properties (Papay, Willett and Murnane, 2011; Zajonc,
2012; Keele and Titiunik, 2015; Cattaneo, Titiunik, Vazquez-Bare and Keele, 2016; Imbens and
Wager, 2019). Appendix A.1 provides a detailed review of the most closely related papers on
the multidimensional RDD.
Our empirical application uses the proposed method to study hospitals receiving CARES Act
relief funding. Our empirical finding contributes to emerging work on how health care providers
respond to financial shocks (Duggan, 2000; Adelino, Lewellen and Sundaram, 2015; Dranove,
Garthwaite and Ody, 2017; Adelino, Lewellen and McCartney, 2021).
A focal group of decision-making algorithms are machine learning algorithms, as illustrated
in our machine-learning simulation. While we are interested in machine learning as a dataproduction tool, the existing literature (except the above mentioned strand) focuses on machine
learning as a data-analysis tool. For example, a set of predictive studies applies machine learning
to make predictions important for social policy questions (Kleinberg, Lakkaraju, Leskovec, Ludwig and Mullainathan, 2017; Einav, Finkelstein, Mullainathan and Obermeyer, 2018). Another
set of causal and structural work repurposes machine learning to aid with causal inference and
structural econometrics (Athey and Imbens, 2017; Belloni, Chernozhukov, Fern√°ndez-Val and
Hansen, 2017; Bonhomme, Lamadon and Manresa, 2019; Mullainathan and Spiess, 2017). We
supplement these studies by highlighting the role of machine learning as a data-production tool.5
4

Building on their prior work (Armstrong and Koles√°r, 2018), Armstrong and Koles√°r (2020) consider estimation and inference on average treatment effects under the assumption that the final treatment assignment is
independent of potential outcomes conditional on observables. Their estimator is not applicable to the IV world
we consider. Their method and our method also achieve different goals; their goal lies in finite-sample optimality
and asymptotically valid inference while our goal is to obtain consistency and asymptotic normality.
5
This paper also has a conceptual connection to the heated conversation about whether algorithmic decisions

4

2

Framework

Our framework is a mix of the conditional independence, multidimensional RDD, and instrumental variable scenarios. In the setup in the introduction, we are interested in the effect of
some binary treatment Di ‚àà {0, 1} on some outcome of interest Yi ‚àà R. As is standard in the
literature, we impose the exclusion restriction that the treatment recommendation Zi ‚àà {0, 1}
does not affect the observed outcome other than through the treatment assignment Di . This
allows us to define the potential outcomes indexed against the treatment assignment Di alone.6
We consider algorithms that make treatment recommendations based solely on individual i‚Äôs
predetermined, observable covariates Xi = (Xi1 , ..., Xip )0 ‚àà Rp . Let the function A : Rp ‚Üí [0, 1]
represent the decision algorithm, where A(Xi ) = Pr(Zi = 1|Xi ) is the probability that the treatment is recommended for individual i with covariates Xi .7 The central assumption is that the
analyst knows function A and is able to simulate it. That is, the analyst is able to compute the
recommendation probability A(x) given any input value x ‚àà Rp . The treatment recommendation Zi for individual i is then randomly determined with probability A(Xi ) independently of
everything else. Consequently, the following conditional independence property holds.
Property 1 (Conditional Independence). Zi ‚ä•
‚ä•(Yi (1), Yi (0), Di (1), Di (0))|Xi .
Note that the codomain of A contains 0 and 1, allowing for deterministic treatment assignments conditional on Xi . Our framework therefore nests the RDD as a special case.8 Another
special case of our framework is the classic conditional independence scenario with the common
support condition (A(Xi ) ‚àà (0, 1) almost surely). In addition to these simple settings, this
framework nests many other situations, such as multidimensional RDDs and complex machine
learning and market-design algorithms, as illustrated in Section 7.
In typical machine-learning scenarios, an algorithm first applies machine learning on Xi to
make some prediction and then uses the prediction to output the recommendation probability
A(Xi ), as in the following example.
Example. Automated disease detection algorithms (Gulshan et al., 2016) use machine learning, in particular deep learning, to detect various diseases and to identify patients at risk. Using
our framework described above, a detection algorithm predicts whether an individual i has a
certain disease (Zi = 1) or not (Zi = 0) based on a digital image Xi ‚àà Rp of a part of the
are better than human decisions. Here ‚Äúbetter‚Äù is in terms of fairness and efficiency (Hoffman, Kahn and Li,
2017; Horton, 2017; Kleinberg et al., 2017). In this study, we take a complementary perspective in that we take
a decision algorithm as given, no matter whether it is good or bad, and study how to use its produced data for
impact evaluation.
6
Formally, let Yi (d, z) denote the potential outcome that would be realized if i‚Äôs treatment assignment and
recommendation were d and z, respectively. The exclusion restriction assumes that Yi (d, 1) = Yi (d, 0) for d ‚àà {0, 1}
(Imbens and Angrist, 1994).
7
We assume that the function A is supported on Rp irrespective of the support of Xi .
8
Most of the existing studies on RDDs define the potential treatment assignment indexed against the running variable like Di (x), which represents the counterfactual treatment assignment the individual i would have
received if her running variable had been set to x. Unlike prior work, we define it indexed against the treatment
recommendation z.

5

individual‚Äôs body, where each Xij ‚àà R denotes the intensity value of a pixel in the image. The
algorithm uses training data to construct a binary classifier A : Rp ‚Üí {0, 1}. The classifier takes
an image of individual i as input and makes a binary prediction of whether the individual has
the disease:
Zi ‚â° A(Xi ).
The algorithm‚Äôs diagnosis Zi may influence the doctor‚Äôs treatment decision for the individual,
denoted by Di ‚àà {0, 1}. We are interested in how the treatment decision Di affects the individual‚Äôs outcome Yi .
Let Yzi be defined as Yzi ‚â° Di (z)Yi (1) + (1 ‚àí Di (z))Yi (0) for z ‚àà {0, 1}. Yzi is the potential outcome when the treatment recommendation is Zi = z. It follows from Property 1 that
Zi ‚ä•
‚ä•(Y1i , Y0i )|Xi .
We put a few assumptions on the covariates Xi and the algorithm A. To simplify the
exposition, the main text assumes that the distribution of Xi is absolutely continuous with
respect to the Lebesgue measure. In practice, Xi often include discrete variables. Appendix A.3
extends the analysis to the case where some covariates in Xi are discrete. Let X be the support
of Xi , X0 = {x ‚àà X : A(x) = 0}, X1 = {x ‚àà X : A(x) = 1}, Lp be the Lebesgue measure on Rp ,
and int(S) denote the interior of a set S ‚äÇ Rp .
Assumption 1.
(a) (Almost Everywhere Continuity of A) A is continuous almost everywhere with respect to
the Lebesgue measure.
(b) (Measure Zero Boundaries of X0 and X1 ) Lp (Xk ) = Lp (int(Xk )) for k = 0, 1.
Assumption 1 (a) allows the function A to be discontinuous on a set of points with the
Lebesgue measure zero. For example, A is allowed to be a discontinuous step function as long
as it is continuous almost everywhere. Assumption 1 (b) holds if the Lebesgue measures of the
boundaries of X0 and X1 are zero.

3

Identification

What causal effects can be learned from data (Yi , Xi , Di , Zi ) generated by the algorithm A? A
key step toward answering this question is what we call the Approximate Propensity Score (APS).
To define it, let:
R
‚àó
‚àó
B(x,Œ¥) A(x )dx
A
R
p (x; Œ¥) ‚â°
,
‚àó
B(x,Œ¥) dx
where B(x, Œ¥) = {x‚àó ‚àà Rp : kx ‚àí x‚àó k < Œ¥} is the (open) Œ¥-ball around x ‚àà X .9 Here, k ¬∑ k denotes
the Euclidean norm on Rp . To make common Œ¥ for all dimensions reasonable, we normalize Xij
9

Whether we use an open ball or closed ball does not affect pA (x; Œ¥). When we instead use a rectangle, ellipsoid,
or any standard kernel function to define pA (x; Œ¥), the limit limŒ¥‚Üí0 pA (x; Œ¥) may be different at some points (e.g.,
at discontinuity points of A), but the same identification results hold under suitable conditions. We use a ball for
simplicity and practicality.

6

to have mean zero and variance one for each j = 1, ..., p.10 We assume that A is a Lp -measurable
function so that the integrals exist. We then define APS as follows.
pA (x) ‚â° lim pA (x; Œ¥).
Œ¥‚Üí0

APS at x is the average probability of a treatment recommendation in a shrinking ball around x.
We call this the Approximate Propensity Score, since this score modifies the standard propensity
score A(Xi ) to incorporate local variation in the score. APS exists for most covariate points and
algorithms (see Appendix A.2).
Figure 1 illustrates APS. In the example, Xi is two dimensional, and the support of Xi is
divided into three sets depending on the value of A. For the interior points of each set, APS is
equal to A. On the border of any two sets, APS is the average of the A values in the two sets.
Thus, pA (x) = 21 (0+0.5) = 0.25 for any x in the open line segment AB, pA (x) = 12 (0.5+1) = 0.75
for any x in the open line segment BC, and pA (x) = 12 (0 + 1) = 0.5 for any x in the open line
segment BD.
We say that a causal effect is identified if it is uniquely determined by the joint distribution
of (Yi , Xi , Di , Zi ). Our identification analysis uses the following continuity condition.
Assumption 2 (Local Mean Continuity). For z ‚àà {0, 1}, the conditional expectation functions
E[Yzi |Xi ] and E[Di (z)|Xi ] are continuous at any point x ‚àà X such that pA (x) ‚àà (0, 1) and
A(x) ‚àà {0, 1}.
Assumption 2 is a multivariate extension of the local mean continuity condition that is
frequently assumed in the RDD.11 A(x) ‚àà {0, 1} means that the treatment recommendation Zi
is deterministic conditional on Xi = x. If APS at the point x is nondegenerate (pA (x) ‚àà (0, 1)),
however, there exists a point close to x that has a different value of A from x‚Äôs, which creates
variation in the treatment recommendation near x. For any such point x, Assumption 2 requires
that the points close to x have similar conditional means of the outcome Yzi and treatment
assignment Di (z).12 Note that Assumption 2 does not require continuity of the conditional
means at x for which A(x) ‚àà (0, 1), since the identification of the conditional means at such
points follows from Property 1 without continuity.
Under the above assumptions, APS provides an easy-to-check condition for whether an algorithm allows us to identify causal effects.
This normalization is without loss of generality in the following sense. Take a vector Xi‚àó of any continuous
random variables and A‚àó : Rp ‚Üí [0, 1]. The normalization induces the random vector Xi = T (Xi‚àó ‚àí E[Xi‚àó ]),
where T is a diagonal matrix with diagonal entries Var(X1‚àó )1/2 , ..., Var(X1‚àó )1/2 . Let A(x) = A‚àó (T ‚àí1 x + E[Xi‚àó ]).
10

i1

ip

Then (Xi‚àó , A‚àó ) is equivalent to (Xi , A) in the sense that A(Xi ) = A‚àó (Xi‚àó ) for any individual i.
11
In the RDD with a single running variable, the point x for which pA (x) ‚àà (0, 1) and A(x) ‚àà {0, 1} is the cutoff
point at which the treatment probability discontinuously changes.
12
In the context of the RDD with a single running variable, one sufficient condition for continuity of E[Yzi |Xi ] is
a local independence condition in the spirit of Hahn, Todd and van der Klaauw (2001): (Yi (1), Yi (0), Di (1), Di (0))
is independent of Xi near x. A weaker sufficient condition, which allows such dependence, is that E[Yi (d)|Di (1) =
d1 , Di (0) = d0 , Xi ] and Pr(Di (1) = d1 , Di (0) = d0 |Xi ) are continuous at x for every d ‚àà {0, 1} and (d1 , d0 ) ‚àà
{0, 1}2 (Dong, 2018). This assumes that the conditional means of the potential outcomes for each of the four
types determined based on the potential treatment assignment Di (z) and the conditional probabilities of those
types are continuous at the cutoff. These two sets of conditions are sufficient for continuity of E[Yzi |Xi ] regardless
of the dimension of Xi , accommodating multidimensional RDDs.

7

Proposition 1 (Identification). Under Assumptions 1 and 2:
(a) E[Y1i ‚àí Y0i |Xi = x] and E[Di (1) ‚àí Di (0)|Xi = x] are identified for every x ‚àà int(X ) such
that pA (x) ‚àà (0, 1).13
(b) Let S be any open subset of X such that pA (x) exists for all x ‚àà S. Then either E[Y1i ‚àí
Y0i |Xi ‚àà S] or E[Di (1) ‚àí Di (0)|Xi ‚àà S] or both are identified only if pA (x) ‚àà (0, 1) for
almost every x ‚àà S (with respect to the Lebesgue measure).14
Proof. See Appendix C.1.
Proposition 1 characterizes a necessary and sufficient condition for identification. Part (a)
says that the average effects of the treatment recommendation Zi on the outcome Yi and on
the treatment assignment Di for the individuals with Xi = x are both identified if APS at x is
neither 0 nor 1. Non-degeneracy of APS at x implies that there are both types of individuals
who receive Zi = 1 and Zi = 0 among those whose Xi is close to x. Assumption 2 ensures that
these individuals are similar in terms of average potential outcomes and treatment assignments.
We can therefore identify the average effects conditional on Xi = x. In Figure 1, pA (x) ‚àà (0, 1)
holds for any x in the shaded region (the union of the minor circular segment made by the chord
AC and the line segment BD).
Part (b) provides a necessary condition for identification. It says that if the average effect
of the treatment recommendation conditional on Xi being in some open set S is identified, then
we must have pA (x) ‚àà (0, 1) for almost every x ‚àà S. If, to the contrary, there is a subset of S
of nonzero measure for which pA (x) = 1 (or pA (x) = 0), then Zi has no variation in the subset,
which makes it impossible to identify the average effect for the subset.
Proposition 1 concerns causal effects of treatment recommendation, not of treatment assignment. The proposition implies that the conditional average treatment effects and the conditional
local average treatment effects (LATEs) are identified under additional assumptions.
Corollary 1 (Perfect and Imperfect Compliance). Under Assumptions 1 and 2:
(a) The average treatment effect conditional on Xi = x, E[Yi (1) ‚àí Yi (0)|Xi = x], is identified
for every x ‚àà int(X ) such that pA (x) ‚àà (0, 1) and Pr(Di (1) > Di (0)|Xi = x) = 1 (perfect
compliance).
(b) The local average treatment effect conditional on Xi = x, E[Yi (1)‚àíYi (0)|Di (1) 6= Di (0), Xi =
x], is identified for every x ‚àà int(X ) such that pA (x) ‚àà (0, 1), Pr(Di (1) ‚â• Di (0)|Xi = x) =
1 (monotonicity), and Pr(Di (1) 6= Di (0)|Xi = x) > 0 (existence of compliers).
Proof. See Appendix C.2.
13

The causal effects may not be identified at a boundary point x of X for which pA (x) ‚àà (0, 1). For example,
if A(x‚àó ) = 1 for all x‚àó ‚àà B(x, Œ¥) ‚à© X and A(x‚àó ) = 0 for all x‚àó ‚àà B(x, Œ¥) \ X for any sufficiently small Œ¥ > 0,
pA (x) ‚àà (0, 1) but the causal effects are not identified at x since Pr(Zi = 0|Xi ‚àà B(x, Œ¥)) = 0.
14
We assume that pA is a Lp -measurable function so that {x ‚àà S : pA (x) = 0} and {x ‚àà S : pA (x) = 1} are
p
L -measurable.

8

Non-degeneracy of APS pA (x) therefore summarizes what causal effects the data from A
identify. Note that the key condition (pA (x) ‚àà (0, 1)) holds for some points x for every standard
algorithm except trivial algorithms that always recommend a treatment with probability 0 or 1.
Therefore, the data from almost every algorithm identify some causal effect.

4

Estimation

The sources of quasi-random assignment characterized in Proposition 1 suggest a way of estimating causal effects of the treatment. In view of Proposition 1, it is possible to nonparametrically
estimate conditional average causal effects E[Y1i ‚àí Y0i |Xi = x] and E[Di (1) ‚àí Di (0)|Xi = x] for
points x such that pA (x) ‚àà (0, 1). This approach is hard to use in practice, however, when Xi
has many elements.
We instead seek an estimator that aggregates conditional effects at different points into a single average causal effect. Proposition 1 suggests that conditioning on APS makes algorithm-based
treatment recommendation quasi-randomly assigned. This motivates the use of an algorithm‚Äôs
recommendation as an instrument conditional on APS, which we operationalize as follows.

4.1

Two-Stage Least Squares Meets APS

Suppose that we observe a random sample {(Yi , Xi , Di , Zi )}ni=1 of size n from the population
whose data generating process is as described in the introduction and Section 2. Consider the
following 2SLS regression using the observations with pA (Xi ; Œ¥n ) ‚àà (0, 1):
Di = Œ≥0 + Œ≥1 Zi + Œ≥2 pA (Xi ; Œ¥n ) + ŒΩi

(1)

Yi = Œ≤0 + Œ≤1 Di + Œ≤2 pA (Xi ; Œ¥n ) + i ,

(2)

where bandwidth Œ¥n shrinks toward zero as the sample size n increases. Let Ii,n = 1{pA (Xi ; Œ¥n ) ‚àà
(0, 1)}, Di,n = (1, Di , pA (Xi ; Œ¥n ))0 , and Zi,n = (1, Zi , pA (Xi ; Œ¥n ))0 . The 2SLS estimator Œ≤ÃÇ is then
given by
n
n
X
X
0
‚àí1
Œ≤ÃÇ = (
Zi,n Di,n Ii,n )
Zi,n Yi Ii,n .
i=1

i=1

Let Œ≤ÃÇ1 denote the 2SLS estimator of Œ≤1 in the above regression.
The above regression uses true APS pA (Xi ; Œ¥n ), but it may be difficult to analytically compute if A is complex. In such a case, we propose to approximate pA (Xi ; Œ¥n ) using brute force
simulation. We draw a value of x from the uniform distribution on B(Xi , Œ¥n ) a number of
times, compute A(x) for each draw, and take the average of A(x) over the draws.15 Formally, let
‚àó , ..., X ‚àó
Xi,1
i,Sn be Sn independent draws from the uniform distribution on B(Xi , Œ¥n ), and calculate
Sn
1 X
‚àó
p (Xi ; Œ¥n ) =
A(Xi,s
).
Sn
s

s=1

15

See Appendix A.5 for how to efficiently sample from the uniform distribution on a p-dimensional ball.

9

We compute ps (Xi ; Œ¥n ) for each i = 1, ..., n independently across i so that ps (X1 ; Œ¥n ), ..., ps (Xn ; Œ¥n )
are independent of each other. For fixed n and Xi , the approximation error relative to true
‚àö
pA (Xi ; Œ¥n ) has a 1/ Sn rate of convergence.16 This rate does not depend on the dimension of
Xi , so the simulation error can be made negligible even when Xi has many elements.
Now consider the following simulation version of the 2SLS regression using the observations
with ps (Xi ; Œ¥n ) ‚àà (0, 1):
Di = Œ≥0 + Œ≥1 Zi + Œ≥2 ps (Xi ; Œ¥n ) + ŒΩi
s

Yi = Œ≤0 + Œ≤1 Di + Œ≤2 p (Xi ; Œ¥n ) + i .

(3)
(4)

Let Œ≤ÃÇ1s denote the 2SLS estimator of Œ≤1 in the simulation-based regression. This regression is
the same as the 2SLS regression (1) and (2) except that it uses the simulated APS ps (Xi ; Œ¥n ) in
place of pA (Xi ; Œ¥n ).17

4.2

Consistency and Asymptotic Normality

We establish the consistency and asymptotic normality of the 2SLS estimators Œ≤ÃÇ1 and Œ≤ÃÇ1s . Our
consistency and asymptotic normality result uses the following assumptions.
Assumption 3.
(a) (Finite Moment) E[Yi4 ] < ‚àû.
(b) (Nonzero First Stage) There exists a constant c > 0 such that E[Di (1) ‚àí Di (0)|Xi = x] > c
for every x ‚àà X such that pA (x) ‚àà (0, 1).
(c) (Nonzero Conditional Variance) If Pr(A(Xi ) ‚àà (0, 1)) > 0, then Var(A(Xi )|A(Xi ) ‚àà
(0, 1)) > 0.
If Pr(A(Xi ) ‚àà (0, 1)) = 0, then the following conditions (d)‚Äì(g) hold.
(d) (Nonzero Variance) Var(A(Xi )) > 0.
For a set S ‚äÇ Rp , let cl(S) denote the closure of S and let ‚àÇS denote the boundary of S,
i.e., ‚àÇS = cl(S) \ int(S).
(e) (C 2 Boundary of ‚Ñ¶‚àó ) There exists a partition {‚Ñ¶‚àó1 , ..., ‚Ñ¶‚àóM } of ‚Ñ¶‚àó = {x ‚àà Rp : A(x) = 1}
(the set of the covariate points whose A value is one) such that
‚àö
More precisely, we have |ps (Xi ; Œ¥n ) ‚àí pA (Xi ; Œ¥n )| = Ops (1/ Sn ), where Ops indicates the stochastic boundedness in terms of the probability distribution of the Sn simulation draws.
17
In many industry and policy applications, the analyst is only able to change the algorithm‚Äôs recommendation
Zi by redesigning the algorithm. In this case, the effect of recommendation Zi on outcome Yi may also be of
interest. We can estimate the effect of recommendation by running the following ordinary least squares (OLS)
regression using the observations with ps (Xi ; Œ¥) ‚àà (0, 1):
16

Yi = Œ±0 + Œ±1 Zi + Œ±2 ps (Xi ; Œ¥) + ui .
The estimated coefficient on Zi , Œ±ÃÇ1s , is our preferred estimator of the recommendation effect.

10

(i) dist(‚Ñ¶‚àóm , ‚Ñ¶‚àóm0 ) > 0 for any m, m0 ‚àà {1, ..., M } such that m 6= m0 . Here dist(S, T ) =
inf x‚ààS,y‚ààT kx ‚àí yk is the distance between two sets S and T ‚äÇ Rp ;
(ii) ‚Ñ¶‚àóm is nonempty, bounded, open, connected and twice continuously differentiable for
each m ‚àà {1, ..., M }. Here we say that a bounded open set S ‚äÇ Rp is twice continuously
differentiable if for every x ‚àà S, there exists a ball B(x, ) and a one-to-one mapping
œà from B(x, ) onto an open set D ‚äÇ Rp such that œà and œà ‚àí1 are twice continuously
differentiable, œà(B(x, ) ‚à© S) ‚äÇ {(x1 , ..., xp ) ‚àà Rp : xp > 0} and œà(B(x, ) ‚à© ‚àÇS) ‚äÇ
{(x1 , ..., xp ) ‚àà Rp : xp = 0}.
Let fX denote the probability density function of Xi and let Hk denote the k-dimensional
Hausdorff measure on Rp .18
(f ) (Regularity of Deterministic A)
R
(i) Hp‚àí1 (‚àÇ‚Ñ¶‚àó ) < ‚àû, and ‚àÇ‚Ñ¶‚àó fX (x)dHp‚àí1 (x) > 0.
(ii) There exists Œ¥ > 0 such that A(x) = 0 for almost every x ‚àà N (X , Œ¥) \ ‚Ñ¶‚àó , where
N (S, Œ¥) = {x ‚àà Rp : kx ‚àí yk < Œ¥ for some y ‚àà S} for a set S ‚äÇ Rp and Œ¥ > 0.
(g) (Conditional Moments and Density near ‚àÇ‚Ñ¶‚àó ) There exists Œ¥ > 0 such that
(i) E[Y1i |Xi ], E[Y0i |Xi ], E[Di (1)|Xi ], E[Di (0)|Xi ] and fX are continuously differentiable
and have bounded partial derivatives on N (‚àÇ‚Ñ¶‚àó , Œ¥);
(ii) E[Y1i2 |Xi ], E[Y0i2 |Xi ], E[Y1i Di (1)|Xi ] and E[Y0i Di (0)|Xi ] are continuous on N (‚àÇ‚Ñ¶‚àó , Œ¥);
(iii) E[Yi4 |Xi ] is bounded on N (‚àÇ‚Ñ¶‚àó , Œ¥).
Assumption 3 is a set of conditions for establishing consistency. Assumption 3 (b) assumes
that, conditional on each value of Xi for which APS is nondegenerate, more individuals would
change their treatment assignment status from 0 to 1 in response to treatment recommendation
than would change it from 1 to 0.19 Under this assumption, the estimated first-stage coefficient on
Zi converges to a positive quantity. Note that, if there exists c < 0 such that E[Di (1)‚àíDi (0)|Xi =
x] < c for every x ‚àà X with pA (x) ‚àà (0, 1), changing the labels of treatment recommendation
makes Assumption 3 (b) hold.
Assumption 3 (c) rules out potential multicollinearity. If the support of A(Xi ) contains only
one value in (0, 1), pA (Xi ; Œ¥n ) is asymptotically constant and equal to A(Xi ) conditional on
pA (Xi ; Œ¥n ) ‚àà (0, 1), resulting in the multicollinearity between pA (Xi ; Œ¥n ) and the constant term.
Although dropping the constant term from the 2SLS regression solves this issue, Assumption 3
(c) allows us to only consider the regression with a constant for the purpose of simplifying the presentation. In Appendix C.3, we provide 2SLS estimators that are consistent and asymptotically
normal even if we do not know whether Assumption 3 (c) holds.
18

The k-dimensional Hausdorff measure on Rp is defined as follows. Let Œ£ be the Lebesgue œÉ-algebra on Rp
P
k
(the set of all Lebesgue measurable sets on Rp ). For S ‚àà Œ£ and Œ¥ > 0, let HŒ¥k (S) = inf{ ‚àû
j=1 d(Ej ) : S ‚äÇ
‚àû
p
‚à™j=1 Ej , d(Ej ) < Œ¥, Ej ‚äÇ R for all j}, where d(E) = sup{kx ‚àí yk : x, y ‚àà E}. The k-dimensional Hausdorff
measure of S on Rp is Hk (S) = limŒ¥‚Üí0 HŒ¥k (S).
19
At the cost of making the presentation more complex, the assumption can be relaxed so that the sign of
E[Di (1) ‚àí Di (0)|Xi = x] is allowed to vary over x with pA (x) ‚àà (0, 1).

11

Assumption 3 (d)‚Äì(g) are a set of conditions we require for proving consistency and asymptotic normality of Œ≤ÃÇ1 when A is deterministic and produces only multidimensional regressiondiscontinuity variation. Assumption 3 (d) says that A produces variation in the treatment
recommendation.
Assumption 3 (e) imposes the differentiability of the boundary of ‚Ñ¶‚àó = {x ‚àà Rp : A(x) =
1}. The conditions are satisfied if, for example, ‚Ñ¶‚àó = {x ‚àà Rp : f (x) ‚â• 0} for some twice
, ..., ‚àÇf‚àÇx(x)
)0 6= 0 for all
continuously differentiable function f : Rp ‚Üí R such that ‚àáf (x) = ( ‚àÇf‚àÇx(x)
p
1
x ‚àà Rp with f (x) = 0. ‚Ñ¶‚àó takes this form, for example, when the conditional treatment effect
E[Yi (1) ‚àí Yi (0)|X] is predicted by supervised learning based on smooth models such as lasso and
ridge regressions, and treatment is recommended to individuals who are estimated to experience
nonnegative treatment effects.
In general, the differentiability of ‚Ñ¶‚àó may not hold. For example, if tree-based algorithms
such as Classification And Regression Tree (CART) and random forests are used to predict the
conditional treatment effect, the predicted conditional treatment effect function is not differentiable at some points. Although the resulting ‚Ñ¶‚àó does not exactly satisfy Assumption 3 (e), the
assumptions approximately hold in that ‚Ñ¶‚àó is arbitrarily well approximated by a set that satisfies
the differentiability condition.20
Part (i) of Assumption 3 (f) says that the boundary of ‚Ñ¶‚àó is (p ‚àí 1) dimensional and that
the boundary has nonzero density. Part (ii) puts a weak restriction on the values A takes on
outside the support of Xi . It requires that A(x) = 0 for almost every x ‚àà
/ ‚Ñ¶‚àó that is outside
X but is in the neighborhood of X . A(x) may take on any value if x is not close to X . These
conditions hold in practice.21 Assumption 3 (g) imposes continuity, continuous differentiability
and boundedness on the conditional moments of potential outcomes and the probability density
near the boundary of ‚Ñ¶‚àó .
When A is stochastic, asymptotic normality requires additional assumptions. Let
C ‚àó = {x ‚àà Rp : A is continuously differentiable at x},
and let D‚àó = Rp \ C ‚àó be the set of points at which A is not continuously differentiable.
Assumption 4. If Pr(A(Xi ) ‚àà (0, 1)) > 0, then the following conditions (a)‚Äì(c) hold.
(a) (Probability of Neighborhood of D‚àó ) Pr(Xi ‚àà N (D‚àó , Œ¥)) = O(Œ¥).
(b) (Bounded Partial Derivatives of A) The partial derivatives of A are bounded on C ‚àó .
(c) (Bounded Conditional Mean) E[Yi |Xi ] is bounded on X .
20

For example, suppose that p = 2, A(x) = 1 if x1 > 0 and x2 > 0, and A(x) = 0 otherwise. In this case,
2
2
‚Ñ¶ = {x ‚àà R2 : x1 > 0, x2 > 0}. Let {‚Ñ¶k }‚àû
k=1 be a sequence of subsets of R , where ‚Ñ¶k = {x ‚àà R : x2 ‚â•
‚àó
1
, x1 > 0} for each k. ‚Ñ¶k is twice continuously differentiable for all k, and well approximates ‚Ñ¶ for a large k
kx1
in that dH (‚Ñ¶‚àó , ‚Ñ¶k ) ‚Üí 0 as k ‚Üí ‚àû, where dH (S, T ) = max{supx‚ààS inf y‚ààT kx ‚àí yk, supy‚ààT inf x‚ààS kx ‚àí yk} is the
Hausdorff distance between two sets S and T ‚äÇ Rp .
21
The boundary of ‚Ñ¶‚àó fails to be (p ‚àí 1) dimensional, for example, when the covariate space is three dimensional
(p = 3) and ‚Ñ¶‚àó is a straight line, not a set with nonzero volume nor even a plane. In this example, the boundary
is the same as ‚Ñ¶‚àó , and its two-dimensional Hausdorff measure is zero.
‚àó

12

Assumption 4 is required for proving asymptotic normality of Œ≤ÃÇ1 when A is stochastic. To
explain the role of Assumption 4 (a), consider a path of covariate points xŒ¥ ‚àà N (D‚àó , Œ¥) ‚à©
C ‚àó indexed by Œ¥ > 0. Since A is continuous at xŒ¥ , pA (xŒ¥ ) = A(xŒ¥ ) (as formally implied by
Proposition A.2 in Appendix A.2). However, pA (xŒ¥ ; Œ¥) does not necessarily get sufficiently close
to A(xŒ¥ ) even as Œ¥ ‚Üí 0, since xŒ¥ is in the Œ¥-neighborhood of D‚àó and hence A may discontinuously
change within the Œ¥-ball B(xŒ¥ , Œ¥). Assumption 4 (a) requires that the probability of Xi being
in the Œ¥-neighborhood of D‚àó shrink to zero at the rate of Œ¥, which makes the points in the
neighborhood negligible.
Assumption 4 (a) often holds in practice. If A is continuously differentiable on X , then
‚àó
D ‚à© X = ‚àÖ, so this condition holds. If, for example, the treatment recommendation is randomly
assigned based on a stratified randomized experiment or on the -Greedy algorithm (see Example
A.1 (a) in Appendix A.6), D‚àó is the boundary at which the recommendation probability changes
discontinuously. For any boundary of standard shape, the probability of Xi being in the Œ¥neighborhood of the boundary vanishes at the rate of Œ¥, and the required condition is satisfied.
We provide a sufficient condition for this condition in Appendix A.4.
Assumption 4 (b) and (c) are regularity conditions, imposing the boundedness of the partial
derivatives of A and of the conditional mean of the outcome.
Assumption 5 (The Number of Simulation Draws). n‚àí1/2 Sn ‚Üí ‚àû, and Pr(pA (Xi ; Œ¥n ) ‚àà
n
log n
‚àí1/2 Œ¥ 1/2 ) for some Œ≥ > 1 .
(0, Œ≥ log
n
Sn ) ‚à™ (1 ‚àí Œ≥ Sn , 1)) = o(n
2
Assumption 5 is the key to proving asymptotic normality of the simulation-based estimator
Assumption 5 says that we need to choose the number of simulation draws Sn so that
it grows to infinity faster than n1/2 , and that the probability that pA (Xi ; Œ¥n ) lies on the tails
n
log n
‚àí1/2 Œ¥ 1/2 . This condition makes the bias caused
(0, Œ≥ log
n
Sn ) ‚à™ (1 ‚àí Œ≥ Sn , 1) vanishes faster than n
by using ps (Xi ; Œ¥n ) instead of pA (Xi ; Œ¥n ) asymptotically negligible. To illustrate how the second
part of this assumption restricts the rate at which Sn goes to infinity, consider an example where
Pr(pA (Xi ; Œ¥n ) ‚àà (0, 1)) = O(Œ¥n ), and pA (Xi ; Œ¥n ) is approximately uniformly distributed on the
log n
log n
log n
n
A
tails (0, Œ≥ log
Sn ) ‚à™ (1 ‚àí Œ≥ Sn , 1). In this case, Pr(p (Xi ; Œ¥n ) ‚àà (0, Œ≥ Sn ) ‚à™ (1 ‚àí Œ≥ Sn , 1)) =
n
O(Œ¥n log
Sn ), and the second part of Assumption 5 requires that Sn grow sufficiently fast so that
Œ≤ÃÇ1s .

1/2

n1/2 Œ¥n log n
Sn

1/2

= o(1). One choice of Sn satisfying this is Sn = Œ±nŒ∫ Œ¥n

for some Œ± > 0 and Œ∫ > 12 ,

1/2 1/2

n
in which case n Œ¥nSn log n = Œ±nlog
Œ∫‚àí1/2 = o(1).
Under the above conditions, the 2SLS estimators Œ≤ÃÇ1 and Œ≤ÃÇ1s are consistent and asymptotically
normal estimators of a weighted average treatment effect.

Theorem 1 (Consistency and Asymptotic Normality). Suppose that Assumptions 1 and 3 hold,
and that Œ¥n ‚Üí 0, nŒ¥n ‚Üí ‚àû and Sn ‚Üí ‚àû as n ‚Üí ‚àû. Then the 2SLS estimators Œ≤ÃÇ1 and Œ≤ÃÇ1s
converge in probability to
Œ≤1 ‚â° lim E[œâi (Œ¥)(Yi (1) ‚àí Yi (0))],
Œ¥‚Üí0

where
œâi (Œ¥) =

pA (Xi ; Œ¥)(1 ‚àí pA (Xi ; Œ¥))(Di (1) ‚àí Di (0))
.
E[pA (Xi ; Œ¥)(1 ‚àí pA (Xi ; Œ¥))(Di (1) ‚àí Di (0))]
13

Suppose, in addition, that Assumptions 4 and 5 hold and that nŒ¥n2 ‚Üí 0 as n ‚Üí ‚àû. Then
d

œÉÃÇn‚àí1 (Œ≤ÃÇ1 ‚àí Œ≤1 ) ‚àí‚Üí N (0, 1),
d

(œÉÃÇns )‚àí1 (Œ≤ÃÇ1s ‚àí Œ≤1 ) ‚àí‚Üí N (0, 1),
where we define œÉÃÇn‚àí1 and (œÉÃÇns )‚àí1 as follows. Let
n
n
n
X
X
X
0
‚àí1
2
0
Œ£ÃÇn = (
Zi,n Di,n Ii,n ) (
ÀÜi,n Zi,n Zi,n Ii,n )(
Di,n Z0i,n Ii,n )‚àí1 ,
i=1

i=1

i=1

where
ÀÜi,n = Yi ‚àí D0i,n Œ≤ÃÇ.
Œ£ÃÇn is the conventional heteroskedasticity-robust estimator for the variance of the 2SLS estimator.
œÉÃÇn2 is the second diagonal element of Œ£ÃÇn . (œÉÃÇns )2 is the analogously-defined estimator for the
variance of Œ≤ÃÇ1s from the simulation-based regression.
Proof. See Appendix C.3.
Theorem 1 says that the 2SLS estimators converge to a weighted average of causal effects
for the subpopulation whose APS is nondegenerate (pA (Xi ; Œ¥) ‚àà (0, 1)) and who would switch
their treatment status in response to the treatment recommendation (Di (1) 6= Di (0)).22 The
limit limŒ¥‚Üí0 E[œâi (Œ¥)(Yi (1) ‚àí Yi (0))] always exists under the assumptions of Theorem 1. It also
shows that inference based on the conventional 2SLS heteroskedasticity-robust standard errors
is asymptotically valid if Œ¥n goes to zero at an appropriate rate. The convergence rate of Œ≤ÃÇ1 is
‚àö
‚àö
Op (1/ n) if Pr(A(Xi ) ‚àà (0, 1)) > 0 and is Op (1/ nŒ¥n ) if Pr(A(Xi ) ‚àà (0, 1)) = 0.23
Our consistency result requires that Œ¥n go to zero slower than n‚àí1 . The rate condition
ensures that, when Pr(A(Xi ) ‚àà (0, 1)) = 0, we have sufficiently many observations in the Œ¥n neighborhood of the boundary of ‚Ñ¶‚àó . Importantly, the rate condition does not depend on the
dimension of Xi , unlike other bandwidth-based estimation methods such as kernel methods. This
is because we use all the observations in the Œ¥-neighborhood of the boundary, and the number
of those observations is of order nŒ¥n regardless of the dimension of Xi if the dimension of the
boundary is one less than the dimension of Xi , i.e., (p ‚àí 1).
The asymptotic normality result requires that Œ¥n go to zero sufficiently quickly so that nŒ¥n2 ‚Üí
0. When Pr(A(Xi ) ‚àà (0, 1)) > 0, we need to use a small enough Œ¥n so that pA (Xi ; Œ¥n ) converges
to pA (Xi ) at a fast rate and Œ¥n -neighborhood of D‚àó is asymptotically small enough. When
Pr(A(Xi ) ‚àà (0, 1)) = 0, the asymptotic normality is based on undersmoothing, which eliminates
the asymptotic bias by using the observations sufficiently close to the boundary of ‚Ñ¶‚àó . In both
22

In principle, it is possible to estimate other weighted averages and the unweighted average by reweighting
different observations appropriately. For example, we can estimate the unweighted average treatment effect by
weighting observations by the inverse of APS.
23
In the standard RDD with a single running variable Xi and cutoff c, pA (Xi ; Œ¥n ) = X2Œ¥i ‚àíc
+ 12 if Xi ‚àà [c‚àíŒ¥n , c+Œ¥n ]
n
A
A
and p (Xi ; Œ¥n ) ‚àà {0, 1} otherwise. Since p (Xi ; Œ¥n ) is linear in the running variable Xi if pA (Xi ; Œ¥n ) ‚àà (0, 1), the
estimator Œ≤ÃÇ1 becomes a local regression estimator with the box kernel that places the same slope coefficient of Xi
on both sides of the cutoff. Under our assumptions, Œ≤ÃÇ1 and standard local linear estimators are shown to have
the same fastest possible convergence rate.

14

‚àö
cases, the bias of our estimator is O(Œ¥n ). On the other hand, the standard deviation is O(1/ n)
‚àö
when Pr(A(Xi ) ‚àà (0, 1)) > 0 and is O(1/ nŒ¥n ) when Pr(A(Xi ) ‚àà (0, 1)) = 0. The condition
that nŒ¥n2 ‚Üí 0 ensures that the bias converges to zero faster than the standard deviation in either
case.
Whether or not Pr(A(Xi ) ‚àà (0, 1)) = 0, when we use simulated APS, the consistency result
requires that the number of simulation draws Sn go to infinity as n increases. The asymptotic
normality result requires a sufficiently fast growth rate of Sn given by Assumption 5 to make the
bias caused by using ps (Xi ; Œ¥n ) negligible.
Finally, note that the weight œâi (Œ¥) given in Theorem 1 is negative if Di (1) < Di (0), so
E[œâi (Œ¥)(Yi (1) ‚àí Yi (0))] may not be a causally interpretable convex combination of treatment
effects Yi (1) ‚àí Yi (0). This can happen because the treatment effect of those whose treatment
assignment switches from 1 to 0 in response to the treatment recommendation (defiers) negatively
contributes to E[œâi (Œ¥)(Yi (1) ‚àí Yi (0))]. Additional assumptions prevent this problem. If the
treatment effect is constant, for example, the 2SLS estimators are consistent for the treatment
effect.
Corollary 2. Suppose that Assumptions 1 and 3 hold, that the treatment effect is constant, i.e.,
Yi (1) ‚àí Yi (0) = b for some constant b, and that Œ¥n ‚Üí 0, nŒ¥n ‚Üí ‚àû, and Sn ‚Üí ‚àû as n ‚Üí ‚àû.
Then the 2SLS estimators Œ≤ÃÇ1 and Œ≤ÃÇ1s converge in probability to b.
Another approach is to impose monotonicity (Imbens and Angrist, 1994). Let LAT E(x) =
E[Yi (1) ‚àí Yi (0)|Di (1) 6= Di (0), Xi = x] be the local average treatment effect (LATE) conditional
on Xi = x.
Corollary 3. Suppose that Assumptions 1 and 3 hold, that Pr(Di (1) ‚â• Di (0)|Xi = x) = 1 for
any x ‚àà X with pA (x) ‚àà (0, 1), and that Œ¥n ‚Üí 0, nŒ¥n ‚Üí ‚àû and Sn ‚Üí ‚àû as n ‚Üí ‚àû. Then the
2SLS estimators Œ≤ÃÇ1 and Œ≤ÃÇ1s converge in probability to
lim E[œâ(Xi ; Œ¥)LAT E(Xi )],

Œ¥‚Üí0

where
œâ(x; Œ¥) =

pA (x; Œ¥)(1 ‚àí pA (x; Œ¥))E[Di (1) ‚àí Di (0)|Xi = x]
.
E[pA (Xi ; Œ¥)(1 ‚àí pA (Xi ; Œ¥))(Di (1) ‚àí Di (0))]

The 2SLS estimators are consistent for a weighted average of conditional LATEs over all
values of Xi with nondegenerate APS pA (Xi ; Œ¥n ). The weights are proportional to pA (Xi ; Œ¥n )(1‚àí
pA (Xi ; Œ¥n )), and to the proportion of compliers, E[Di (1) ‚àí Di (0)|Xi ].

4.3

Special Cases

The result in Theorem 1 holds whether A is stochastic (Pr(A(Xi ) ‚àà (0, 1)) > 0) or deterministic
(Pr(A(Xi ) ‚àà (0, 1)) = 0). As shown in the proof of Theorem 1 in Appendix C.3, if we consider
these two underlying cases separately, the probability limit of the 2SLS estimators has a more
specific expression. If Pr(A(Xi ) ‚àà (0, 1)) > 0,
plim Œ≤ÃÇ1 = plim Œ≤ÃÇ1s =

E[A(Xi )(1 ‚àí A(Xi ))(Di (1) ‚àí Di (0))(Yi (1) ‚àí Yi (0))]
.
E[A(Xi )(1 ‚àí A(Xi ))(Di (1) ‚àí Di (0))]
15

(5)

The 2SLS estimators converge to a weighted average of treatment effects for the subpopulation
with nondegenerate A(Xi ). To relate this result to existing work, consider the following 2SLS
regression with the (standard) propensity score A(Xi ) control:
Di = Œ≥0 + Œ≥1 Zi + Œ≥2 A(Xi ) + ŒΩi

(6)

Yi = Œ≤0 + Œ≤1 Di + Œ≤2 A(Xi ) + i .

(7)

Existing results show that under conditional independence, the 2SLS estimator from this
regression converges in probability to the treatment-variance weighted average of treatment effects in (5) (Angrist and Pischke, 2008; Hull, 2018).24 Not surprisingly, for this selection-onobservables case, our result shows that the 2SLS estimator is consistent for the same treatment
effect whether we use as a control the propensity score, APS, or simulated APS.
Importantly, using APS as a control allows us to consistently estimate a causal effect even if A
is deterministic and produces multidimensional regression-discontinuity variation. If Pr(A(Xi ) ‚àà
(0, 1)) = 0,
R
p‚àí1 (x)
‚àó E[(Di (1) ‚àí Di (0))(Yi (1) ‚àí Yi (0))|Xi = x]fX (x)dH
s
R
plim Œ≤ÃÇ1 = plim Œ≤ÃÇ1 = ‚àÇ‚Ñ¶
. (9)
p‚àí1 (x)
‚àÇ‚Ñ¶‚àó E[Di (1) ‚àí Di (0)|Xi = x]fX (x)dH
The 2SLS estimators converge to a weighted average of treatment effects for the subpopulation
who are on the boundary of the treated region.
Our proof exploits a technique that may be useful for other problems. Recall that the 2SLS
regression uses the observations with pA (Xi ; Œ¥n ) ‚àà (0, 1) (or ps (Xi ; Œ¥n ) ‚àà (0, 1) when we use
simulated APS) only. By definition, if pA (Xi ; Œ¥) ‚àà (0, 1), Xi must be in the Œ¥-neighborhood of
the boundary of ‚Ñ¶‚àó . Therefore, to derive the probability limit of Œ≤ÃÇ1 , it is necessary to derive
R
the limits of the integrals of relevant variables over the Œ¥-neighborhood (e.g., N (‚àÇ‚Ñ¶‚àó ,Œ¥) E[Yi |Xi =
x]fX (x)dx) as Œ¥ shrinks to zero. We take an approach drawing on change of variables techniques
from differential geometry and geometric measure theory.25 In this approach, we first use the
coarea formula (Lemma B.3 in Appendix B.3) to write the integral of an integrable function g
24

Precisely speaking, Angrist and Pischke (2008) consider the OLS regression of Yi (or Di ) on Zi controlling a
dummy variable for every value taken on by Xi (i.e., the model is saturated in Xi ) when Xi is a discrete variable:
X
Yi = Œ±1 Zi +
Œ±2,x 1{Xi = x} + ui .
(8)
x‚ààX
i ‚àíE[Zi |Xi ])Yi ]
By the Frisch-Waugh Theorem, the population coefficient on Zi from (8) is given by Œ±1 = E[(Z
.
E[(Zi ‚àíE[Zi |Xi ])2 ]
Angrist and Pischke (2008) show that this expression is reduced to the treatment-variance weighted average
i )(1‚àíA(Xi ))(Y1i ‚àíY0i )]
of treatment effects E[A(X
under the conditional independence assumption. Their derivation
E[A(Xi )(1‚àíA(Xi ))
follows even when Xi is continuous and we control the propensity score linearly.
25
Our approach using geometric theory shows that Œ≤ÃÇ1 converges to an integral of the conditional treatment
effect over boundary points with respect to the Hausdorff measure. In constrast, prior studies on multidimensional
RDDs express treatment effect estimands in terms of expectations conditional on Xi being in the boundary like
E[Y1i ‚àí Y0i |Xi ‚àà ‚àÇ‚Ñ¶‚àó ] (Zajonc, 2012). However, those conditional expectations are, formally, not well-defined,
since Lp (‚àÇ‚Ñ¶‚àó ) = 0 and hence Pr(Xi ‚àà ‚àÇ‚Ñ¶‚àó ) = 0. We therefore prefer our expression in terms of an integral with
respect to the Hausdorff measure to any expressions in terms of conditional expectations on the boundary. Arias,
Rubio-Ram√≠rez and Waggoner (2018), Bornn, Shephard and Solgi (2019), and Qiao (2021) use similar tools from
differential geometry and geometric measure theory, but for different purposes.

16

over N (‚àÇ‚Ñ¶‚àó , Œ¥) in terms of the iterated integral over the levels sets of the signed distance function
of ‚Ñ¶‚àó :
Z Œ¥Z
Z
g(x)dHp‚àí1 (x)dŒª,
(10)
g(x)dx =
N (‚àÇ‚Ñ¶‚àó ,Œ¥)

‚àíŒ¥

{x0 ‚ààRp :ds‚Ñ¶‚àó (x0 )=Œª}

where ds‚Ñ¶‚àó is the signed distance function of ‚Ñ¶‚àó (see Appendix B.2 for the definition). The set
{x0 ‚àà Rp : ds‚Ñ¶‚àó (x0 ) = Œª} is a level set of ds‚Ñ¶‚àó , which collects the points in ‚Ñ¶‚àó when Œª > 0 and
the points in Rp \ ‚Ñ¶‚àó when Œª < 0 whose distance to the boundary ‚àÇ‚Ñ¶‚àó is |Œª|. Figure 2a shows a
visual illustration.
We then use the area formula (Lemma B.4 in Appendix B.3) to write the integral over each
level set in terms of the integral over the boundary ‚àÇ‚Ñ¶‚àó :
Z
Z
‚àÇ‚Ñ¶‚àó
p‚àí1
g(x‚àó + ŒªŒΩ‚Ñ¶‚àó (x‚àó ))Jp‚àí1
œà‚Ñ¶‚àó (x‚àó , Œª)dHp‚àí1 (x‚àó ),
(11)
g(x)dH (x) =
{x0 ‚ààRp :ds‚Ñ¶‚àó (x0 )=Œª}

‚àÇ‚Ñ¶‚àó

where ŒΩ‚Ñ¶‚àó (x‚àó ) is the inward unit normal vector of ‚àÇ‚Ñ¶‚àó at x‚àó (the unit vector orthogonal to all
‚àÇ‚Ñ¶‚àó œà ‚àó (x‚àó , Œª)
vectors in the tangent space of ‚àÇ‚Ñ¶‚àó at x‚àó that points toward the inside of ‚Ñ¶‚àó ), and Jp‚àí1
‚Ñ¶
is the Jacobian of the transformation œà‚Ñ¶‚àó (x‚àó , Œª) = x‚àó +ŒªŒΩ‚Ñ¶‚àó (x‚àó ). Figure 2b illustrates this change
of variables formula. Finally, combining (10) and (11) and proceeding with further analysis, we
prove in Appendix C.3.3 that when g is continuous,
Z

Z
g(x)dx = Œ¥
g(x)dHp‚àí1 (x) + o(1) .
N (‚àÇ‚Ñ¶‚àó ,Œ¥)

‚àÇ‚Ñ¶‚àó

Thus, the integral over the Œ¥-neighborhood of ‚àÇ‚Ñ¶‚àó scaled up by Œ¥ ‚àí1 converges to the integral
over boundary points with respect to the (p ‚àí 1)-dimensional Hausdorff measure. This result is
used to derive the expression of the probability limit of Œ≤ÃÇ1 given by (9).

5

Machine Learning Simulation

This section conducts a Monte Carlo experiment to assess the feasibility and performance of our
method. Consider a tech company that conducts a randomized controlled trial (RCT) using a
small segment of the population. At the same time, the company applies a deterministic decision
algorithm to the rest of the population. This mimics a situation in which the decision maker
first conducts an experiment that randomly assigns Zi to predict the conditional average effect
of Zi and then constructs an algorithm that greedily chooses the treatment predicted to perform
better based on the predicted effect.
We simulate 1, 000 hypothetical samples from the following data-generating process. Each
sample {(Yi , Xi , Di , Zi )}ni=1 is of size n = 10, 000. There are 100 covariates (p = 100), and Xi ‚àº
N (0, Œ£). Yi (0) is generated as Yi (0) = 0.75Xi0 Œ±0 + 0.250i , where Œ±0 ‚àà R100 , and 0i ‚àº N (0, 1).
We consider two models for Yi (1), one in which the treatment effect Yi (1)‚àíYi (0) does not depend
on Xi and one in which the treatment effect depends on Xi .
Model A. Yi (1) = Yi (0) + 1i , where 1i ‚àº N (0, 1).
17

Model B. Yi (1) = Yi (0) + Xi0 Œ±1 , where Œ±1 ‚àà R100 .
The choice of parameters Œ£, Œ±0 and Œ±1 is explained in Appendix D. Di (0) and Di (1) are generated
as Di (0) = 0 and Di (1) = 1{Yi (1) ‚àí Yi (0) > ui }, where ui ‚àº N (0, 1). To generate Zi , let q0.495
and q0.505 be the 49.5th and 50.5th (empirical) quantiles of the first covariate Xi1 .
Let œÑpred (Xi ) be a real-valued function of Xi , which we regard as a prediction of the effect
of recommendation on the outcome for individual i obtained from past data. We construct
œÑpred using an independent sample {(YÃÉi , XÃÉi , DÃÉi , ZÃÉi )}nÃÉi=1 of size nÃÉ = 2, 000. The distribution
of (YÃÉi , XÃÉi , DÃÉi , ZÃÉi ) is the same as that of (Yi , Xi , Di , Zi ) except (1) that YÃÉi (1) is generated as
YÃÉi (1) = YÃÉi (0) + 0.5XÃÉi0 Œ±1 + 0.51i , where 1i ‚àº N (0, 1) and (2) that ZÃÉi ‚àº Bernoulli(0.5). This can
be viewed as data from a past randomized experiment conducted to construct the algorithm.
We then use random forests separately for the subsamples with ZÃÉi = 1 and ZÃÉi = 0 to predict YÃÉi
from XÃÉi . Let ¬µz (x) be the trained prediction model. Set œÑpred (x) = ¬µ1 (x) ‚àí ¬µ0 (x). We generate
the sample {(YÃÉi , XÃÉi , DÃÉi , ZÃÉi )}nÃÉi=1 and construct œÑpred only once, and we use it for all of the 1, 000
samples. The distribution of the sample {(Yi , Xi , Di , Zi )}ni=1 is thus held fixed for all simulations.
Zi is then generated as
Ô£±
‚àó
Ô£¥
Ô£¥
Ô£≤Zi ‚àº Bernoulli(0.5) if Xi1 ‚àà [q0.495 , q0.505 ]
Zi =

if Xi1 ‚àà
/ [q0.495 , q0.505 ] and œÑpred (Xi ) ‚â• 0

1
Ô£¥
Ô£¥
Ô£≥0

if Xi1 ‚àà
/ [q0.495 , q0.505 ] and œÑpred (Xi ) < 0.

The first case corresponds to the RCT segment while the latter two cases to the deterministic
algorithm segment. The function A is given by
Ô£±
Ô£¥
Ô£¥0.5 if x1 ‚àà [q0.495 , q0.505 ]
Ô£≤
A(x) = 1
if x1 ‚àà
/ [q0.495 , q0.505 ] and œÑpred (x) ‚â• 0
Ô£¥
Ô£¥
Ô£≥0
if x ‚àà
/ [q
,q
] and œÑ
(x) < 0.
1

0.495

0.505

pred

Finally, Di and Yi are generated as Di = Zi Di (1)+(1‚àíZi )Di (0) and Yi = Di Yi (1)+(1‚àíDi )Yi (0),
respectively.
Estimators and Estimands. We use the data {(Yi , Xi , Di , Zi )}ni=1 to estimate treatment
effect parameters. Our main approach is 2SLS with APS controls in Theorem 1. To compute
APS, we use S = 400 simulation draws for each observation.
We compare our approach with two alternatives. The first alternative is OLS of Yi on a
constant and Di (i.e., the difference in the sample mean of Yi between the treated group and
untreated group) using all observations. The second alternative is 2SLS with A controls. This
method uses the observations with A(Xi ) ‚àà (0, 1) to run the 2SLS regression of Yi on a constant,
Di , and A(Xi ) using Zi as an instrument for Di (see (6) and (7) in Section 4.3) and reports the
coefficient on Di .
We consider four parameters as target estimands: ATE ‚â° E[Yi (1) ‚àí Yi (0)], ATE(RCT) ‚â°
E[Yi (1)‚àíYi (0)|Xi1 ‚àà [q0.495 , q0.505 ]], LATE ‚â° E[Yi (1)‚àíYi (0)|Di (1) 6= Di (0)], and LATE(RCT) ‚â°
E[Yi (1) ‚àí Yi (0)|Di (1) 6= Di (0), Xi1 ‚àà [q0.495 , q0.505 ]]. In the case where the treatment effect does
not depend on Xi (Model A), the conditional effects are homogeneous, so that ATE and LATE
18

are the same as ATE(RCT) and LATE(RCT), respectively. In the case where the treatment
effect depends on Xi (Model B), the conditional effects are heterogeneous. However, since the
RCT segment consists of those in the middle of the distribution of Xi1 , the average effect for
the RCT segment is close to the unconditional average effect. As a result, ATE is equal to
ATE(RCT) and LATE is similar to LATE(RCT) under this data-generating process.
For both models, the 2SLS estimator converges in probability to LATE(RCT) whether we
control for APS or A.26 However, 2SLS with A controls uses only the individuals for the RCT segment while 2SLS with APS controls additionally uses the individuals near the decision boundary
of the deterministic algorithm (i.e., the boundary of the region for which œÑpred (x) ‚â• 0). Therefore, 2SLS with APS controls is expected to produce a more precise estimate than 2SLS with
A controls if the conditional effects for those near the boundary are not far from the target
estimand.
Results. Table 1 reports the bias, standard deviation (SD), and root mean squared error
(RMSE) of each estimator. Panels A and B present the results for the cases where the conditional
effects are homogeneous and heterogeneous, respectively. Note first that OLS with no controls
is significantly biased, showing the importance of correcting for omitted variable bias. 2SLS
with APS achieves this goal, as demonstrated by its smaller biases across all possible treatment
effect models, target parameters, and values of the bandwidth Œ¥. 2SLS with APS controls shows
a consistent pattern; as the bandwidth Œ¥ grows, the bias increases while the variance declines.
For several values of Œ¥, 2SLS with APS controls outperforms 2SLS with A controls in terms of
the RMSE. This finding implies that exploiting individuals near the multidimensional decision
boundary of the deterministic algorithm can lead to better performance than using only the
individuals in the RCT segment.
We also evaluate our inference procedure based on Theorem 1. Table 1 reports the coverage
probabilities of the 95% confidence intervals for LATE(RCT) constructed from the 2SLS estimates and their heteroskedasticity-robust standard errors. The confidence intervals offer nearly
correct coverage when Œ¥ is small, which supports the implication of Theorem 1 that the inference
procedure is valid when we use a sufficiently small Œ¥. Overall, Table 1 shows that our estimator
is feasible in a high-dimensional setting and performs better than altenrative estimators.

6

Empirical Policy Application

6.1

Hospital Relief Funding during the COVID-19 Pandemic

Here we provide our real-world empirical application. As part of the 3-phase Coronavirus Aid,
Relief, and Economic Security (CARES) Act, the government has distributed tens of billions of
dollars of relief funding to hospitals since April 2020. This funding intended to help health care
providers hit hardest by the COVID-19 outbreak and at a high risk of closing. The bill specified
that providers may (but are not required to) use the funds for COVID-19-related expenses, such
as construction of temporary structures, leasing of properties, purchasing medical supplies and
26

The 2SLS estimators converge in probability to the right-hand side of equation (5), which is the same as
LATE(RCT) under the data-generating process of this simulation.

19

equipment (including personal protective equipment and testing supplies), increased workforce
utilization and training, establishing emergency operation centers, retrofitting facilities and managing the surge in capacity, among others. We are interested in whether this funding had a causal
impact on hospital operation and activities in dealing with COVID-19 cases.
We focus on an initial portion of this funding ($10 billion), which was allocated to hospitals
that qualified as ‚Äúsafety net hospitals‚Äù according to a specific eligibility criterion. This eligibility
criterion intends to direct funding towards hospitals that ‚Äúdisproportionately provide care to the
most vulnerable, and operate on thin margins.‚Äù Specifically, an acute care hospital was deemed
eligible for funding if the following conditions hold:
‚Ä¢ Medicare Disproportionate Patient Percentage (DPP) of 20.2% or greater. DPP is equal
to the sum of the percentage of Medicare inpatient days attributable to patients eligible
for both Medicare Part A and Supplemental Security Income (SSI), and the percentage of
total inpatient days attributable to patients eligible for Medicaid but not Medicare Part
A.27
‚Ä¢ Annual Uncompensated Care (UCC) of at least $25, 000 per bed. UCC is a measure of
hospital care provided for which no payment was received from the patient or insurer. It
is the sum of a hospital‚Äôs bad debt and the financial assistance it provides.28
‚Ä¢ Profit Margin (Net income/(Net patient revenue + Total other income)) of 3.0% or less.
Hospitals that do not qualify on any of the three dimensions are funding ineligible. Figure 4 visualizes how the three dimensions determine safety net eligibility. As the bottom two-dimensional
planes show, eligibility discontinuously changes as hospitals cross the eligibility boundary in the
space of the three characteristics. This setting is a three-dimensional RDD, falling under our
framework.
The final funding amount is calculated as follows. Each eligible hospital is assigned an
individual facility score, which is calculated as the product of DPP and the number of beds in
that hospital. This facility score determines the share of funding allocated to the hospital, out
of the total $10 billion. The share received by each hospital is determined by the ratio of the
hospital‚Äôs facility score to the sum of facility scores across all eligible hospitals. The amount of
funding that can be received is bounded below at $5 million and capped above at $50 million.
Figure 3 shows the distribution of funding amounts received by eligible hospitals. A majority of
eligible hospitals receive the minimum amount of $5 million.
We use publicly available data from the Healthcare Cost Report Information System (HCRIS)
for the 2018 financial year, to replicate the funding eligibility status as well as the amount
of funding received.29 To obtain outcome measures of interest, we use the publicly available
COVID-19 Reported Patient Impact and Hospital Capacity by Facility dataset. This provides
27

Source: https://www.cms.gov/Medicare/Medicare-Fee-for-Service-Payment/AcuteInpatientPPS/dsh
Source:
https://www.aha.org/fact-sheets/2020-01-06-fact-sheet-uncompensated-hospital-carecost
29
We use the methodology detailed in the CARES ACT website to project funding based on 2018 financial
year cost reports. We use the RAND cleaned version of the dataset which can be accessed at https://www.
hospitaldatasets.org/
28

20

facility-level data on hospital utilization aggregated on a weekly basis, from July 31st 2020
onwards.30 Summary statistics about hospital outcomes and characteristics are documented
in Table 2. Safety net hospitals have higher fractions of inpatient and ICU beds occupied by
COVID-19 patients. Safety net hospitals also have a higher disproportionate patient percentage,
higher uncompensated care per bed, lower profit margins, more employees and beds, and shorter
lengths of inpatient stay.

6.2

Covariate Balance Estimates

Using the above data, we study the effect of safety net funding on relevant hospital outcomes,
such as the total number of inpatient beds and the number of staffed ICU beds occupied by adult
COVID patients reported between July 31st 2020 and August 6th 2020.
We first evaluate the balancing property of APS conditioning using APS-controlled differences
in covariate means for hospitals who are and are not deemed eligible for safety net funding.
Specifically, we run the following OLS regression of hospital-level characteristics on the eligibility
status using observations with ps (Xi ; Œ¥n ) ‚àà (0, 1):
Wi = Œ≥0 + Œ≥1 Zi + Œ≥2 ps (Xi ; Œ¥n ) + Œ∑i ,
where Wi is one of the predetermined characteristics of the hospital, Zi is a funding eligibility
dummy, Xi is a vector of three input variables (DPP, UCC, and profit margin) that determine
the funding eligibility, and ps (Xi ; Œ¥n ) is the simulated APS. We compute APS using S = 10, 000
simulation draws.31 The estimated coefficient on Zi is the APS-controlled difference in the mean
of the covariate between eligible and ineligible hospitals. For comparison, we also run the OLS
regression of hospital characteristics on the eligibility status with no controls using the whole
sample.
Table 3 reports the covariate balance estimates. Column 1 shows that, without controlling
for APS, eligible hospitals are significantly different from ineligible hospitals. We find that all
the relevant hospital eligibility characteristics are strongly associated with eligibility. Once we
control for APS with small enough bandwidth Œ¥, eligible and ineligible hospitals have similar
financial and utilization characteristics, as reported in columns 2‚Äì6 of Table 3. These estimates
are consistent with our theoretical results, establishing the empirical relevance of APS controls.

6.3

2SLS Estimates

The balancing performance of APS motivates us to estimate causal effects of safety net funding
by 2SLS using funding eligibility as an instrument for the amount of funding received. We run
the following 2SLS regression on four different hospital-level outcome variables, using hospitals
30

Source:
https://healthdata.gov/Hospital/COVID-19-Reported-Patient-Impact-and-Hospital-Capa/
anag-cw7u
31
Figure A.2 in Appendix E.4 reports APS for several hospitals with varying numbers of simulation draws. We
find that S = 10, 000 is sufficient for well stabilizing APS simulation.

21

with ps (Xi ; Œ¥) ‚àà (0, 1):
Di = Œ≥0 + Œ≥1 Zi + Œ≥2 ps (Xi ; Œ¥) + vi
Yi = Œ≤0 + Œ≤1 Di + Œ≤2 ps (Xi ; Œ¥) + i ,
where Yi is a hospital-level outcome and Di is the amount of relief funding received.32 We also
run the OLS and 2SLS regressions with no controls, as well as OLS regression controlling for
the three eligibility determinants (disproportionate patient percentage, uncompensated care per
bed and profit margin).33 These alternative regressions are computed using the sample of all
hospitals, as benchmark estimators.
The first stage effects of safety net eligibility on funding amount (in millions), shown in
columns 3‚Äì10 of Table 4, suggest that safety net eligibility boosts funding significantly. For
example, in column 3 of Table 4, we can see that funding eligibility increases the safety net
funding by approximately 15 million dollars.
OLS estimates of funding effects, reported as the benchmark in column 1 of Table 4, indicate
that safety net funding is associated with a higher number of adult inpatient beds and higher
number of staffed ICU beds utilized by patients who have lab-confirmed or suspected COVID.
The estimates indicate that a million dollar increase in funding is associated with 5.58 more adult
inpatient beds occupied by patients with lab-confirmed or suspected COVID. The corresponding
increase in total adult inpatient beds occupied by those who have lab-confirmed COVID is 4.53
and the increase in staffed ICU beds occupied by those who have lab-confirmed or suspected
COVID is 1.67. The estimated increase in staffed ICU beds occupied by lab-confirmed COVID
patients is 1.51. Naive 2SLS estimates with no controls and OLS with covariate controls produce
similar significantly positive results.
In contrast with the OLS or uncontrolled 2SLS estimates, the 2SLS estimates with APS
controls in columns 4‚Äì10 show a different picture. The gains in number of inpatient beds and
staffed ICU beds occupied by suspected and lab-confirmed COVID patients become much smaller
and lose significance across all bandwidth specifications. These results suggest that APS reveals
important selection bias in the estimated effects of safety net funding. Once we control for APS
32

This specification uses a continuous treatment, unlike our theoretical framework with a binary treatment.
We obtain similar results when the treatment is a binary transformation of the amount of relief funding received
(e.g., a dummy indicating whether the amount exceeds a certain value). Results are available upon request.
33
Precisely speaking, we run the following specification for each hospital-level outcome variable Yi . For the
OLS regression without any controls, we estimate:
Yi = Œ≤0 + Œ≤1 Di + i .
For the 2SLS regression without any controls, we run:
Di = Œ≥0 + Œ≥1 Zi + vi
Yi = Œ≤0 + Œ≤1 Di + i .
For the OLS regression controlling for disproportionate patient percentage, uncompensated care per bed and
profit margin, we estimate:
Yi = Œ≤0 + Œ≤1 Di + Œ≤2 Xi1 + Œ≤3 Xi2 + Œ≤4 Xi3 + i ,
where Xi1 is disproportionate patient percentage, Xi2 is uncompensated care per bed, and Xi3 is profit margin.

22

to eliminate the bias, the safety net relief funding has little to no effect on the hospital utilization
level by COVID-19 patients.34
The above analysis looks at the immediate effects of relief funding. However, the effects of
relief funding might kick in after a time lag, given that expansion in capacity and staff takes
time. To investigate the relevance of this concern, we finally measure the evolving effects of
relief funding. We estimate our main 2SLS specification on the 7-day average of each hospital
outcome for each week from July 31st, 2020 to April 2nd, 2021. We plot the results in Figure
5. The estimated effects are similar to the initial null effects in Table 4, even several months
after the distribution of relief funding. The overall insignificance of the estimates suggests that
CARES funding had largely no effect on hospital utilization trends during the pandemic. This
finding is consistent with policy and media arguments that CARES Act funding was not well
targeted toward needy providers. Unlike the previous arguments, the analysis here provide causal
evidence.

7

Other Examples

Here we give real-world examples of other algorithms and discuss the applicability of our framework.
Example 1 (Bandit Algorithms). We are constantly exposed to digital information (movie,
music, news, search results, advertisements, and recommendations) through a variety of devices
and platforms. Tech companies allocate these pieces of content by using bandit algorithms.
Our method is applicable to many popular bandit algorithms. For simplicity, assume a perfectcompliance scenario where the company perfectly controls the treatment assignment (Di = Zi ).
The algorithms below first use past data and supervised learning to estimate the conditional
means and variances of potential outcomes, E[Yi (z)|Xi ] and Var(Yi (z)|Xi ), for each z ‚àà {0, 1}.
Let ¬µz and œÉz2 denote the estimated functions. The algorithms use ¬µz (Xi ) and œÉz2 (Xi ) to determine the treatment assignment for individual i.
(a) (Thompson Sampling Using Gaussian Priors) The algorithm first samples potential outcomes from the normal distribution with mean (¬µ0 (Xi ), ¬µ1 (Xi )) and variance-covariance
matrix diag(œÉ02 (Xi ), œÉ12 (Xi )). The algorithm then chooses the treatment with the highest
sampled potential outcome:
ZiT S ‚â° arg max y(z), AT S (Xi ) = E[arg max y(z)|Xi ],
z‚àà{0,1}

z‚àà{0,1}

where y(z) ‚àº N (¬µz (Xi ), œÉz2 (Xi )) independently across z. These algorithms often induce
quasi-experimental variation in treatment assignment, as a strand of the computer science
literature has observed (Precup, 2000; Li et al., 2010; Narita, Yasui and Yata, 2019; Saito,
34
Estimates reported in Table A.1 in Appendix E.4 show little difference in outcome availability rates between
eligible and ineligible hospitals once we control for APS. The 2SLS estimates in Table 4 are therefore unlikely to
be compromised by differential attrition.

23

Aihara, Matsutani and Narita, 2021). The function A has an analytical expression:
!
¬µ0 (x) ‚àí ¬µ1 (x)
TS
,
A (x) = 1 ‚àí Œ¶ p 2
œÉ0 (x) + œÉ12 (x)
where Œ¶ is the cumulative distribution function of a standard normal distribution. Suppose
that the functions ¬µ0 , ¬µ1 , œÉ02 and œÉ12 are continuous. APS for this case is given by
!
¬µ
(x)
‚àí
¬µ
(x)
0
1
pT S (x) = 1 ‚àí Œ¶ p 2
.
œÉ0 (x) + œÉ12 (x)
This APS is nondegenerate, meaning that the data from the algorithm allow for causaleffect identification.
(b) (Upper Confidence Bound, UCB) Unlike the above stochastic algorithm, the UCB algorithm (Li et al., 2010) is a deterministic algorithm, producing a less obvious example of
our framework. This algorithm chooses the treatment with the highest upper confidence
bound for the potential outcome:
ZiU CB ‚â° arg max{¬µz (Xi ) + Œ±œÉz (Xi )}, AU CB (x) = arg max{¬µz (x) + Œ±œÉz (x)},
z=0,1

z=0,1

where Œ± is chosen so that |¬µz (x)‚àíE[Yi (z)|Xi = x]| ‚â§ Œ±œÉz (x) at least with some probability,
for example, 0.95, for every x. Suppose that the function g = ¬µ1 ‚àí ¬µ0 + Œ±(œÉ1 ‚àí œÉ0 ) is
continuous on X and is continuously differentiable in a neighborhood of x with ‚àág(x) 6= 0
for any x ‚àà X such that g(x) = 0. APS for this case is given by
Ô£±
Ô£¥
if ¬µ1 (x) + Œ±œÉ1 (x) < ¬µ0 (x) + Œ±œÉ0 (x)
Ô£¥
Ô£≤0
U CB
p
(x) = 0.5
if ¬µ1 (x) + Œ±œÉ1 (x) = ¬µ0 (x) + Œ±œÉ0 (x)
Ô£¥
Ô£¥
Ô£≥1
if ¬µ (x) + Œ±œÉ (x) > ¬µ (x) + Œ±œÉ (x).
1

1

0

0

This means that the UCB algorithm produces potentially complicated quasi-experimental
variation along the boundary in the covariate space where the algorithm‚Äôs treatment recommendation changes from one to the other. It is possible to identify and estimate causal
effects across the boundary.
Example 2 (Unsupervised Learning). Customer segmentation is a core marketing practice that
divides a company‚Äôs customers into groups based on their characteristics and behavior so that
the company can effectively target marketing activities at each group. Many businesses today
use unsupervised learning algorithms, clustering algorithms in particular, to perform customer
segmentation. Using our notation, assume that a company decides whether it targets a campaign
at customer i (Zi = 1) or not (Zi = 0). The company first uses a clustering algorithm such as
K-means clustering or Gaussian mixture model clustering to divide customers into K groups,
making a partition {S1 , ..., SK } of the covariate space Rp . The company then conducts the
campaign targeted at some of the groups:
ZiCL ‚â° 1{Xi ‚àà ‚à™k‚ààT Sk }, ACL (x) = 1{x ‚àà ‚à™k‚ààT Sk },
24

where T ‚äÇ {1, .., K} is the set of the indices of the target groups.
For example, suppose that the company uses K-means clustering, which creates a partition
in which a covariate value x belongs to the group with the nearest centroid. Let c1 , ..., cK be the
centroids of the K groups. Define a set-valued function C : Rp ‚Üí 2{1,...,K} , where 2{1,...,K} is the
power set of {1, ..., K}, as C(x) ‚â° arg mink‚àà{1,...,K} kx ‚àí ck k. If C(x) is a singleton, x belongs to
the unique group in C(x). If C(x) contains more than one indices, the group to which x belongs
is arbitrarily determined. APS for this case is given by
Ô£±
Ô£¥
if C(x) ‚à© T = ‚àÖ
Ô£¥
Ô£≤0
CL
p (x) = 0.5
if |C(x)| = 2, x ‚àà ‚àÇ(‚à™k‚ààT Sk )
Ô£¥
Ô£¥
Ô£≥1
if C(x) ‚äÇ T
and pCL (x) ‚àà (0, 1) if |C(x)| ‚â• 3 and x ‚àà ‚àÇ(‚à™k‚ààT Sk ), where |C(x)| is the number of elements in
C(x).35 Thus, it is possible to identify causal effects across the boundary ‚àÇ(‚à™k‚ààT Sk ).
Example 3 (Supervised Learning). Millions of times each year, judges make jail-or-release
decisions that hinge on a prediction of what a defendant would do if released. Many judges
now use proprietary algorithms (like COMPAS criminal risk score) to make such predictions
and use the predictions to support jail-or-release decisions. Using our notation, assume that a
criminal risk algorithm recommends jailing (Zi = 1) or releasing (Zi = 0) for each defendant i.
The algorithm uses defendant i‚Äôs observable characteristics Xi , including criminal history and
demographics. The algorithm first translates Xi into a risk score r(Xi ), where r : Rp ‚Üí R is
a function estimated by supervised leaning based on past data and assumed to be fixed. For
example, Kleinberg et al. (2017) construct a version of r(Xi ) using gradient boosted decision
trees. The algorithm then uses the risk score to make the final recommendation:
ZiSL ‚â° 1{r(Xi ) > c}, ASL (x) = 1{r(x) > c},
where c ‚àà R is a constant threshold that is set ex ante.36 A similar procedure applies to
the screening of potential borrowers by banks and insurance companies based on credit scores
estimated by supervised learning (Agarwal, Chomsisengphet, Mahoney and Stroebel, 2017).
A widely-used approach to identifying and estimating treatment effects in these settings is
to use the score r(Xi ) as a continuous univariate running variable and apply a univariate RDD
method (Cowgill, 2018). However, whether r(Xi ) is continuously distributed or not depends on
how the function r is constructed. For example, suppose that r is constructed by a tree-based
algorithm and is the following simple regression tree with three terminal nodes:
Ô£±
Ô£¥
if x1 ‚â§ 0
Ô£¥
Ô£≤r1
r(x) =

r2
Ô£¥
Ô£¥
Ô£≥r
3

if x1 > 0, x2 ‚â§ 0

if x1 > 0, x2 > 0,

35

If |C(x)| = 2 and x ‚àà ‚àÇ(‚à™k‚ààT Sk ), x is on a linear boundary between one target group and one non-target
group, and hence APS is 0.5. If |C(x)| ‚â• 3 and x ‚àà ‚àÇ(‚à™k‚ààT Sk ), x is a common endpoint of several group
boundaries, and APS is determined by the angles at which the boundaries intersect.
36
The algorithm sometimes discretizes the original risk score r(Xi ) into d(r(Xi )), where d : R ‚Üí N (Cowgill,
2018). In this case, the algorithm uses the discretized risk score to make the final recommendation: ZiSL ‚â°
1{d(r(Xi )) > c}.

25

where r1 < r2 < c < r3 . In this case, the score r(Xi ) is a discrete variable, and hence it may not
be suitable to apply a standard univariate RDD method.
Our approach is applicable to this case as long as at least one of the original multi-dimensional
covariates Xi are continuously distributed. Since ASL (x) = 1{r(x) > c} = 1{x1 > 0, x2 > 0},
APS for this case is given by
Ô£±
Ô£¥
0
if x1 < 0 or x2 < 0
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≤0.25
if x1 = x2 = 0
pSL (x) =
Ô£¥
0.5
if (x1 = 0, x2 > 0) or (x1 > 0, x2 = 0)
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≥1
if x1 > 0, x2 > 0.
It is therefore possible to identify causal effects across the boundary {x ‚àà X : (x1 = 0, x2 ‚â•
0) or (x1 > 0, x2 = 0)}.
Example 4 (Policy Eligibility Rules). Medicaid and other welfare policies often decide who
are eligible based on algorithmic rules, as studied by Currie and Gruber (1996) and Brown,
Kowalski and Lurie (2020).37 Using our notation, the state government determines whether each
individual i is eligible (Zi = 1) or not (Zi = 0) for Medicare. The state government‚Äôs eligibility
rule AM edicaid maps individual characteristics Xi (e.g. income, family composition) into an
eligibility decision ZiM edicare . A similar procedure also applies to bankruptcy laws (Mahoney,
2015). These policy eligibility rules produce quasi-experimental variation as in Example 3.
Example 5 (Mechanism Design: Matching and Auction). Centralized economic mechanisms
such as matching and auction are also suitable examples, as summarized below:

i
Xi
Zi
Di
Yi

Matching (e.g., School Choice)

Auction

Student
Preference/Priority/Tie-breaker
Whether student i is
assigned treatment school
Whether student i
attends treatment school
Student i‚Äôs
future test score

Bidder
Bid
Whether bidder i
wins the good
Same as Zi
Bidder i‚Äôs future
economic performance

In mechanism design and other algorithms with capacity constraints, the treatment recommendation for individual i may depend not only on Xi but also on the characteristics of others.
These interactive situations can be accommodated by our framework if we consider the following
large market setting.38 Suppose that there is a continuum of individuals i ‚àà [0, 1] and that the
37

These papers estimate the effect of Medicaid eligibility by exploiting variation in the eligibility rule across
states and over time (simulated instrumental variable method). In contrast, our method exploits local variation
in the eligibility status across different individuals given a fixed eligibility rule.
38
The approach proposed by Borusyak and Hull (2020) is applicable to finite-sample settings if the treatment
recommendation probability, which may depend on all individuals‚Äô characteristics, is nondegenerate for multiple
individuals.

26

recommendation probability for individual i with covariate Xi is determined by a function M as
follows:
Pr(Zi = 1|Xi ; FX‚àíi ) = M (Xi ; FX‚àíi ).
Here FX‚àíi = Pr({j ‚àà [0, 1] \ {i} : Xj ‚â§ x}) is the distribution of X among all individuals
j ‚àà [0, 1] \ {i}. The function M : Rp √ó F ‚Üí [0, 1], where F is a set of distributions on Rp ,
gives the recommendation probability for each individual in the market. With a continuum of
individuals, for any i ‚àà [0, 1], FX‚àíi is the same as the distribution of X in the whole market,
denoted by FX . Therefore, the data generated by the mechanism M are equivalent to the data
generated by the algorithm A : Rp ‚Üí [0, 1] such that A(x) ‚â° M (x; FX ) for all x ‚àà Rp . Our
framework is applicable to this large-market interactive setting.
The above discussions can be summarized as follows.
Corollary 4. In all the above examples, there exists x ‚àà int(X ) such that pA (x) ‚àà (0, 1).
Therefore, a causal effect is identified under Assumptions 1 and 2.

8

Conclusion

As algorithmic decisions become the new norm, the world becomes a mountain of natural experiments and instruments. We develop a general method to use these algorithm-produced
instruments to identify and estimate causal treatment effects. Our analysis of the CARES Act
hospital relief funding uses the proposed method to find that relief funding has little effect on
COVID-19-related hospital activities. OLS or uncontrolled 2SLS estimates, by contrast, show
considerably larger and more significant effects. The large estimates appear to be an artifact
of selection bias; relief funding just went to hospitals with more COVID-19 patients, without
helping hospitals accommodate additional patients.
Our analysis clarifies a few implications for policy and management practices around algorithmic decision-making. It is important to record the implementation of algorithms in a replicable,
simulatable way, including what input variables Xi are used to make algorithmic recommendation Zi . Another key lesson is the importance of recording an algorithm‚Äôs recommendation Zi
even if they are superseded by a human decision Di . These data retention efforts would go a
long way to exploit the full potential of algorithms as natural experiments.
Our agenda for future research includes a formalization of such optimal policy (algorithm)
learning. Another important topic is estimation details, such as data-driven bandwidth selection.
This work needs to extend Imbens and Kalyanaraman (2012) and Calonico et al. (2014)‚Äôs bandwidth selection methods in the univariate RDD to our setting. Inference on treatment effects
in our framework relies on conventional large sample reasoning. It seems natural to additionally consider permutation or randomization inference as in Imbens and Rosenbaum (2005). It
will also be challenging but interesting to develop finite-sample optimal estimation and inference
strategies such as those recently introduced by Armstrong and Koles√°r (2018, 2020) and Imbens
and Wager (2019). Finally, we look forward to empirical applications of our method in a variety
of business, policy, and scientific domains.

27

References
Abdulkadiroƒülu, A. (2013). Instrumental Variable Estimation in School Choice. Private Communication.
Abdulkadiroƒülu, A., Angrist, J. D., Narita, Y. and Pathak, P. A. (2017). Research
Design Meets Market Design: Using Centralized Assignment for Impact Evaluation. Econometrica, 85 (5), 1373‚Äì1432.
‚Äî, ‚Äî, ‚Äî and Pathak, P. A. (Forthcoming). Breaking Ties: Regression Discontinuity Design
Meets Market Design. Econometrica.
Adelino, M., Lewellen, K. and McCartney, W. B. (2021). Hospital Financial Health and
Clinical Choices: Evidence from the Financial Crisis. Management Science.
‚Äî, ‚Äî and Sundaram, A. (2015). Investment Decisions of Nonprofit Firms: Evidence from
Hospitals. Journal of Finance, 70 (4), 1583‚Äì1628.
Agarwal, S., Chomsisengphet, S., Mahoney, N. and Stroebel, J. (2017). Do Banks
Pass Through Credit Expansions to Consumers Who Want to Borrow? Quarterly Journal of
Economics, 133 (1), 129‚Äì190.
Angrist, J. D. and Pischke, J.-S. (2008). Mostly Harmless Econometrics: An Empiricist‚Äôs
Companion. Princeton University Press.
Arias, J. E., Rubio-Ram√≠rez, J. F. and Waggoner, D. F. (2018). Inference Based on
Structural Vector Autoregressions Identified with Sign and Zero Restrictions: Theory and
Applications. Econometrica, 86 (2), 685‚Äì720.
Armstrong, T. B. and Koles√°r, M. (2018). Optimal Inference in a Class of Regression
Models. Econometrica, 86 (2), 655‚Äì683.
‚Äî and Koles√°r, M. (2020). Finite-Sample Optimal Estimation and Inference on Average Treatment Effects Under Unconfoundedness. Econometrica.
Athey, S. and Imbens, G. W. (2017). The State of Applied Econometrics: Causality and
Policy Evaluation. Journal of Economic Perspectives, 31 (2), 3‚Äì32.
Belloni, A., Chernozhukov, V., Fern√°ndez-Val, I. and Hansen, C. (2017). Program
Evaluation and Causal Inference with High-Dimensional Data. Econometrica, 85 (1), 233‚Äì
298.
Bonhomme, S., Lamadon, T. and Manresa, E. (2019). Discretizing Unobserved Heterogeneity. University of Chicago, Becker Friedman Institute for Economics Working Paper No.
2019-16, unpublished Manuscript, University of Chicago.
Bornn, L., Shephard, N. and Solgi, R. (2019). Moment conditions and bayesian nonparametrics. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 81 (1),
5‚Äì43.
28

Borusyak, K. and Hull, P. (2020). Non-Random Exposure to Exogenous Shocks: Theory
and Applications. NBER Working Paper No. 27845, unpublished Manuscript, University of
Chicago.
Brown, D., Kowalski, A. E. and Lurie, I. Z. (2020). Long-Term Impacts of Childhood
Medicaid Expansions on Outcomes in Adulthood. The Review of Economic Studies, 87 (2),
729‚Äì821.
Bundorf, K., Polyakova, M. and Tai-Seale, M. (2019). How Do Humans Interact with
Algorithms? Experimental Evidence from Health Insurance. NBER Working Paper No. 25976.
Calonico, S., Cattaneo, M. D. and Titiunik, R. (2014). Robust Nonparametric Confidence
Intervals for Regression-Discontinuity Designs. Econometrica, 82 (6), 2295‚Äì2326.
Cattaneo, M. D., Frandsen, B. R. and Titiunik, R. (2015). Randomization Inference in
the Regression Discontinuity Design: An Application to Party Advantages in the US Senate.
Journal of Causal Inference, 3 (1), 1‚Äì24.
‚Äî, Titiunik, R. and Vazquez-Bare, G. (2017). Comparing Inference Approaches for RD
Designs: A Reexamination of the Effect of Head Start on Child Mortality. Journal of Policy
Analysis and Management, 36 (3), 643‚Äì681.
‚Äî, ‚Äî, ‚Äî and Keele, L. (2016). Interpreting Regression Discontinuity Designs with Multiple
Cutoffs. Journal of Politics, 78 (4), 1229‚Äì1248.
Cohen, P., Hahn, R., Hall, J., Levitt, S. and Metcalfe, R. (2016). Using Big Data to
Estimate Consumer Surplus: The Case of Uber. NBER Working Paper No. 22627, unpublished
Manuscript, University of Chicago.
Cowgill, B. (2018). The Impact of Algorithms on Judicial Discretion: Evidence from Regression
Discontinuities. Unpublished Manuscript, Columbia Business School.
Crasta, G. and Malusa, A. (2007). The Distance Function from the Boundary in a Minkowski
Space. Transactions of the American Mathematical Society, 359, 5725‚Äì5759.
Currie, J. and Gruber, J. (1996). Health Insurance Eligibility, Utilization of Medical Care,
and Child Health. Quarterly Journal of Economics, 111 (2), 431‚Äì466.
Dong, Y. (2018). Alternative Assumptions to Identify LATE in Fuzzy Regression Discontinuity
Designs. Oxford Bulletin of Economics and Statistics, 80 (5), 1020‚Äì1027.
Dranove, D., Garthwaite, C. and Ody, C. (2017). How Do Nonprofits Respond to Negative
Wealth Shocks? The Impact of the 2008 Stock Market Collapse on Hospitals. RAND Journal
of Economics, 48 (2), 485‚Äì525.
Duggan, M. G. (2000). Hospital Ownership and Public Medical Spending. Quarterly Journal
of Economics, 115 (4), 1343‚Äì1373.

29

Eckles, D., Ignatiadis, N., Wager, S. and Wu, H. (2020). Noise-induced Randomization
in Regression Discontinuity Designs. arXiv preprint arXiv:2004.09458.
Einav, L., Finkelstein, A., Mullainathan, S. and Obermeyer, Z. (2018). Predictive
Modeling of U.S. Health Care Spending in Late Life. Science, 360 (6396), 1462‚Äì1465.
Ernst, D., Geurts, P. and Wehenkel, L. (2005). Tree-Based Batch Mode Reinforcement
Learning. Journal of Machine Learning Research, 6, 503‚Äì556.
Frandsen, B. R. (2017). Party Bias in Union Representation Elections: Testing for Manipulation in the Regression Discontinuity Design When the Running Variable is Discrete. In
Regression Discontinuity Designs: Theory and Applications, Emerald Publishing Limited, pp.
281‚Äì315.
Fr√∂lich, M. (2007). Regression Discontinuity Design with Covariates. IZA Discussion Paper
No. 3024, unpublished Manuscript, University of St. Gallen.
Fr√∂lich, M. and Huber, M. (2019). Including Covariates in the Regression Discontinuity
Design. Journal of Business and Economic Statistics, 37 (4), 736‚Äì748.
Gulshan, V. et al. (2016). Development and Validation of a Deep Learning Algorithm for
Detection of Diabetic Retinopathy in Retinal Fundus Photographs. Journal of the American
Medical Association, 316 (22), 2402‚Äì2410.
Hahn, J., Todd, P. and van der Klaauw, W. (2001). Identification and Estimation of
Treatment Effects with a Regression-Discontinuity Design. Econometrica, 69 (1), 201‚Äì209.
Hoffman, M., Kahn, L. B. and Li, D. (2017). Discretion in Hiring. Quarterly Journal of
Economics, 133 (2), 765‚Äì800.
Horton, J. J. (2017). The Effects of Algorithmic Labor Market Recommendations: Evidence
from a Field Experiment. Journal of Labor Economics, 35 (2), 345‚Äì385.
Hull, P. (2018). Subtracting the Propensity Score in Linear Models. Unpublished Manuscript,
MIT.
Imbens, G. and Kalyanaraman, K. (2012). Optimal Bandwidth Choice for the Regression
Discontinuity Estimator. The Review of Economic Studies, 79 (3), 933‚Äì959.
‚Äî and Wager, S. (2019). Optimized Regression Discontinuity Designs. Review of Economics
and Statistics, 101 (2), 264‚Äì278.
Imbens, G. W. and Angrist, J. D. (1994). Identification and Estimation of Local Average
Treatment Effects. Econometrica, 62 (2), 467‚Äì475.
‚Äî and Rosenbaum, P. R. (2005). Robust, Accurate Confidence Intervals with a Weak Instrument: Quarter of Birth and Education. Journal of the Royal Statistical Society: Series A
(Statistics in Society), 168 (1), 109‚Äì126.
30

Kakani, P., Chandra, A., Mullainathan, S. and Obermeyer, Z. (2020). Allocation of
COVID-19 Relief Funding to Disproportionately Black Counties. Journal of the American
Medical Association (JAMA), 324 (10), 1000‚Äì1003.
Keele, L. J. and Titiunik, R. (2015). Geographic Boundaries as Regression Discontinuities.
Political Analysis, 23 (1), 127‚Äì155.
Khullar, D., Bond, A. M. and Schpero, W. L. (2020). COVID-19 and the Financial Health
of US Hospitals. Journal of the American Medical Association (JAMA), 323 (21), 2127‚Äì2128.
Kleinberg, J., Lakkaraju, H., Leskovec, J., Ludwig, J. and Mullainathan, S. (2017).
Human Decisions and Machine Predictions. Quarterly Journal of Economics, 133 (1), 237‚Äì293.
Krantz, S. G. and Parks, H. R. (2008). Geometric Integration Theory. Birkh√§user Basel.
Li, L., Chu, W., Langford, J. and Schapire, R. E. (2010). A Contextual-Bandit Approach
to Personalized News Article Recommendation. Proceedings of the 19th international conference on World Wide Web (WWW), pp. 661‚Äì670.
Li, S. (2011). Concise Formulas for the Area and Volume of a Hyperspherical Cap. Asian Journal
of Mathematics and Statistics, 4, 66‚Äì70.
Mahoney, N. (2015). Bankruptcy as Implicit Health Insurance. American Economic Review,
105 (2), 710‚Äì46.
Mullainathan, S. and Spiess, J. (2017). Machine Learning: An Applied Econometric Approach. Journal of Economic Perspectives, 31 (2), 87‚Äì106.
Narita, Y. (2020). A Theory of Quasi-Experimental Evaluation of School Quality. Management
Science.
‚Äî (2021). Incorporating ethics and welfare into randomized experiments. Proceedings of the
National Academy of Sciences, 118 (1).
‚Äî, Yasui, S. and Yata, K. (2019). Efficient Counterfactual Learning from Bandit Feedback.
Proceedings of the 33rd AAAI Conference on Artificial Intelligence, pp. 4634‚Äì4641.
Papay, J. P., Willett, J. B. and Murnane, R. J. (2011). Extending the RegressionDiscontinuity Approach to Multiple Assignment Variables. Journal of Econometrics, 161 (2),
203‚Äì207.
Precup, D. (2000). Eligibility Traces for Off-Policy Policy Evaluation. Proceedings of the Seventeenth International Conference on Machine Learning, pp. 759‚Äì766.
Qiao, W. (2021). Nonparametric Estimation of Surface Integrals on Level Sets. Bernoulli, 27 (1),
155‚Äì191.
Rosenbaum, P. R. and Rubin, D. B. (1983). The Central Role of the Propensity Score in
Observational Studies for Causal Effects. Biometrika, 70 (1), 41‚Äì55.
31

Saito, Y., Aihara, S., Matsutani, M. and Narita, Y. (2021). Open bandit dataset and
pipeline: Towards realistic and reproducible off-policy evaluation. Unpublished Manuscript,
Tokyo Institute of Technology.
Sekhon, J. S. and Titiunik, R. (2017). On Interpreting the Regression Discontinuity Design as
a Local Experiment. In Regression Discontinuity Designs: Theory and Applications, Emerald
Publishing Limited, pp. 1‚Äì28.
Stein, E. M. and Shakarchi, R. (2005). Real Analysis: Measure Theory, Integration, and
Hilbert Spaces. Princeton Lectures in Analysis, Princeton, NJ: Princeton Univ. Press.
Voelker, A. R., Gosmann, J. and Stewart, T. C. (2017). Efficiently Sampling Vectors and
Coordinates from the n-Sphere and n-Ball. Centre for Theoretical Neuroscience - Technical
Report.
Zajonc, T. (2012). Regression Discontinuity Design with Multiple Forcing Variables. Essays on
Causal Inference for Public Policy, pp. 45‚Äì81.

32

Figure 1: Example of the Approximate Propensity Score

Figure 2: Illustration of the Change of Variables Techniques
(a)

(b)

33

Table 1: Bias, SD, and RMSE of Estimators and Coverage of 95% Confidence Intervals
Our Method: 2SLS with Approximate Propensity Score Controls
Œ¥ = 0.01
Œ¥ = 0.05
Œ¥ = 0.1
Œ¥ = 0.25
Œ¥ = 0.5
Œ¥=1

2SLS
with A
Controls

OLS
with No
Controls

Panel A: Homogeneous Conditional Effects
Estimand: ATE = ATE(RCT) = 0
Bias
.603
.634
SD
.304
.205
RMSE
.675
.667

.644
.157
.663

.659
.110
.668

.684
.078
.689

.740
.061
.842

.572
.372
.683

.754
.024
.754

Estimand: LATE = LATE(RCT) = 0.564
Bias
.039
.070
.080
SD
.304
.205
.157
RMSE
.306
.217
.176

.095
.110
.145

.120
.078
.143

.176
.061
.186

.008
.372
.372

.190
.024
.191

Coverage
Avg N

94.8%

92.8%

92.9%

84.6%

69.6%

18.6%

‚Äî

‚Äî

235

727

1275

2567

3995

5561

100

10000

Panel B: Heterogeneous Conditional Effects
Estimand: ATE = ATE(RCT) = 0
Bias
.568
.587
SD
.331
.222
RMSE
.657
.628

.589
.170
.613

.604
.118
.615

.636
.083
.642

.709
.063
.712

.545
.399
.676

1.192
.025
1.193

Estimand: LATE = 0.564
Bias
.004
SD
.331
RMSE
.331

.023
.222
.223

.025
.170
.172

.040
.118
.125

.072
.083
.110

.145
.063
.158

‚àí.019
.399
.399

.628
.025
.629

Estimand: LATE(RCT) = 0.559
Bias
.009
.028
SD
.331
.222
RMSE
.331
.224

.030
.170
.173

.045
.118
.127

.077
.083
.114

.150
.063
.163

‚àí.014
.399
.399

.633
.025
.634

Coverage
Avg N

95.9%

94.8%

95.0%

93.2%

87.1%

37.4%

‚Äî

‚Äî

235

723

1274

2567

3993

5561

100

10000

Notes: This table shows the bias, the standard deviation (SD) and the root mean squared error (RMSE) of 2SLS with
Approximate Propensity Score controls, 2SLS with A controls, and OLS with no controls. These statistics are computed
with the estimand set to ATE, ATE(RCT), LATE, or LATE(RCT). The row ‚ÄúCoverage‚Äù in each panel shows the
s , Œ≤ÃÇ s + 1.96œÉÃÇ s ] contains LATE(RCT), where Œ≤ÃÇ s is
probabilities that the 95% confidence intervals of the form [Œ≤ÃÇ1s ‚àí 1.96œÉÃÇn
n
1
1
s is its heteroskedasticity-robust standard error. We
the 2SLS estimate with Approximate Propensity Score controls and œÉÃÇn
use 1, 000 replications of a size 10, 000 simulated sample to compute these statistics. We use several possible values of Œ¥ to
compute the Approximate Propensity Score. All Approximate Propensity Scores are computed by averaging 400
simulation draws of the A value. Panel A reports the results under the model in which the treatment effect does not
depend on Xi . Panel B reports the results under the model in which the treatment effect depends on Xi . The bottom row
‚ÄúAvg N ‚Äù in each panel shows the average number of observations used for estimation (i.e., the average number of
observations for which the Approximate Propensity Score or the A value is strictly between 0 and 1).

34

Table 2: Hospital Characteristics and Outcomes
All

Ineligible

Eligible

Hospitals

Hospitals

Hospitals
with APS
APS ‚àà (0,1)

136.61
107.83
42.1
36.56
715

125.19
86.78
36.84
31.41
429

188.35
.11
154.66
.18
.23
.59
4.66
1351.57
.38
76,096.31
-.07
781

206.47
.09
170.49
.15
.16
.68
4.38
1525.06
.36
45,996.48
-.03
485

Panel A: Outcome Variable Means
# Confirmed/Suspected Covid Patients
# Confirmed Covid Patients
# Confirmed/Suspected Covid Patients in ICU
# Confirmed Covid Patients in ICU
Observations

105.59
80.1
31.37
26.62
4,008

98.41
73.86
28.92
24.41
3,293

Panel B: Hospital Characteristics
Beds
Interns and residents (full-time equivalents) per bed
Adult and pediatric hospital beds
Ownership: Proprietary (for-profit)
Ownership: Governmental
Ownership: Voluntary (non-profit)
Inpatient length of stay
Employees on payroll (full-time equivalents)
Disproportionate patient percentage
Uncompensated care per bed
Profit margin
Observations

143.66
.06
120.26
.19
.22
.58
9.21
973.9
.21
59,850
.02
4,633

134.6
.05
113.29
.2
.22
.58
10.14
897.31
.18
56,556.03
.04
3,852

Notes: This table reports averages of outcome variables and hospital characteristics by safety net eligibility. A
safety net hospital is defined as any acute care hospital with disproportionate patient percentage of 20.2% or
greater, annual uncompensated care of at least $25,000 per bed and profit margin of 3.0% or less. Panel A
reports the outcome variable means. Outcome variable estimates are 7 day sums for the week spanning July
31st 2020 to August 6th 2020. Confirmed or Suspected COVID patients refer to the sum of patients in inpatient
beds with lab-confirmed/suspected COVID-19. Confirmed COVID patients refer to the sum of patients in
inpatient beds with lab-confirmed COVID-19, including those with both lab-confirmed COVID-19 and influenza.
Inpatient bed totals also include observation beds. Similarly, Confirmed/ Suspected COVID patients in ICU
refer to the sum of patients in ICU beds with lab-confirmed or suspected COVID-19. Confirmed COVID
patients in ICU refers to the sum of patients in ICU beds with lab-confirmed COVID-19, including those with
both lab-confirmed COVID-19 and influenza. Panel B reports the means for hospital characteristics for the
financial year 2018. Column 1 shows the means for All Hospitals. Columns 2 and 3 show the means for hospitals
that are ineligible and eligible to receive safety net funding respectively. Column 4 shows the means for the
hospitals with non-degenerate Approximate Propensity Score with bandwidth Œ¥ = 0.05. Approximate
Propensity Score is computed by averaging 10,000 simulation draws.

35

Figure 3: Funding Distribution for Eligible Hospitals
400
350

Frequency

300
250
200
150
100
50
0

5

10

15

20

25

30

35

Funding (in $millions)

40

45

50

Notes: The figure shows the distribution of funding amount for eligible hospitals. Each eligible hospital is assigned an
individual facility score, which is the product of Disproportionate Patient Percentage and number of beds in the hospital.
The share of $10 billion received by an eligible hospital is determined by the ratio of the individual facility score of that
hospital to the sum of facility scores across all eligible hospitals. The amount of funding that can be received by an eligible
hospital is calculated as the product of this ratio and $10 billion, and is bounded below at $5 million and bounded above
at $50 million.

36

Figure 4: Three-dimensional Regression Discontinuity in Hospital Funding Eligibility

Notes: The top figure visualizes the three dimensions that determine safety net eligibility. The bottom figures show the
data points plotted along 2 out of 3 dimensions. The bottom left panel plots disproportionate patient percentage against
profit margin, while the bottom right panel plots uncompensated care per bed against profit margin. We remove hospitals
above the 99th percentile of disproportionate patient percentage and uncompensated care per bed, for visibility purposes.

37

Table 3: Covariate Balance Regressions

Beds

Costs per discharge
(in thousands)
Disproportionate
patient percentage
Full time employees

Medicare net revenue
(in millions)
Occupancy

Operating margin

Profit margin

Uncompensated
care per bed
p-value for joint significance

Our Method: 2SLS with Approximate Propensity Score Controls

No
Controls

Œ¥ = 0.01

Œ¥ = 0.025

Œ¥ = 0.05

Œ¥ = 0.075

Œ¥ = 0.1

Œ¥ = 0.25

Œ¥ = 0.5

(1)

(2)

(3)

(4)

(5)

(6)

(7)

(8)

7.02
(39.24)
N=671
-6.42
(8.12)
N=671
-0.08
(0.06)
N=671
90.96
(515.72)
N=670
-1.47
(12.09)
N=667
0.01
(0.04)
N=671
0.02
(0.03)
N=661
0.01
(0.03)
N=671
-9,599.98
(7,651.29)
N=671
.745

11.93
(33.06)
N=879
-0.88
(2.58)
N=879
-0.08
(0.05)
N=879
64.41
(414.25)
N=878
2.41
(10.81)
N=875
0.01
(0.03)
N=879
0.03
(0.03)
N=868
0.02
(0.02)
N=879
-11,001.3
(6,976.02)
N=879
.286

17.47
(20.05)
N=1726
6.06
(4.63)
N=1726
-0.06*
(0.02)
N=1726
214.79
(218.77)
N=1723
4.70
(6.63)
N=1684
0.03
(0.02)
N=1726
0.06***
(0.02)
N=1676
0.05**
(0.01)
N=1726
-8,005.95
(4,498.41)
N=1726
0

8.92
(14.46)
N=2368
6.76
(5.09)
N=2368
-0.07***
(0.02)
N=2368
115.94
(143.35)
N=2365
-0.47
(4.68)
N=2323
0.04**
(0.01)
N=2368
0.07***
(0.01)
N=2314
0.06***
(0.01)
N=2368
-6,121.90
(3,660.04)
N=2368
0

53.75***
198.67
35.86
2.93
(7.05)
(105.74)
(66.42)
(47.75)
N=4633
N=89
N=239
N=485
-49.95**
3.83
3.37*
1.65
(17.93)
(2.18)
(1.49)
(1.23)
N=3539
N=89
N=239
N=485
0.21***
-0.09
-0.09
-0.09
(0.01)
(0.09)
(0.07)
(0.07)
N=4633
N=89
N=239
N=485
454.26***
2,670.64
387.75
37.55
(69.23)
(1,652.55)
(997.64)
(668.32)
N=4626
N=89
N=238
N=484
18.36***
35.34
-7.92
-6.16
(2.39)
(29.74)
(18.36)
(14.34)
N=4511
N=88
N=238
N=483
0.07***
0.19*
0.07
-0.00
(0.01)
(0.09)
(0.06)
(0.04)
N=4624
N=89
N=239
N=485
-0.11***
-0.03
0.00
0.02
(0.01)
(0.06)
(0.04)
(0.03)
N=4541
N=88
N=238
N=477
-0.11***
-0.03
-0.00
0.02
(0.01)
(0.06)
(0.04)
(0.03)
N=4633
N=89
N=239
N=485
19,540.28*** 4,905.22
10,760.57
-4,229.12
(3,827.22)
(12,161.31) (10,355.61) (8,611.29)
N=4633
N=89
N=239
N=485
0
.74
.457
.87

Mean
(Ineligible)
(9)
134.6

66.28

.18

897.32

20.04

.44

.02

.04

56,556.02

Notes: This table shows the results of the covariate balance regressions at the hospital level. The dependent variables for these regressions are drawn
from the Healthcare Cost Report Information System for the financial year 2018. Disproportionate patient percentage, profit margin and
uncompensated care per bed are used to determine the hospital‚Äôs safety net funding eligibility. Other dependent variables shown indicate the
financial health and utilization of the hospitals. In column 1, we regress the dependent variables on the safety net eligibility of the hospital with no
controls. In columns 2‚Äì8, we regress the dependent variables on funding eligibility controlling for the Approximate Propensity Score with different
values of bandwidth Œ¥. All Approximate Propensity Scores are computed by averaging 10,000 simulation draws. Column 9 shows the mean of
dependent variables for hospitals that are ineligible to receive safety net funding. Robust standard errors are reported in the parenthesis and number
of observations are reported separately for each regression. The last row reports the p-value of the joint significance test.

38

Table 4: Estimated Effects of Funding on Hospital Utilization
OLS
OLS
2SLS
with
with
with
No
Covariate No
Controls Controls Controls
(1)

(2)

(3)

Our Method: 2SLS with Approximate Propensity Score Controls

Œ¥=
0.01
(4)

Œ¥=
0.025
(5)

Œ¥=
0.05
(6)

Œ¥=
0.075
(7)

Œ¥=
0.1
(8)

Œ¥=
0.25
(9)

Œ¥=
0.5
(10)

# Confirmed/Suspected COVID Patients
First stage
(in millions)
$1mm of funding
Observations

5.58***
(0.68)
3532

3.25***
(0.89)
3532

13.78*** 15.11*
(0.49)
(5.83)
2.77***
-1.03
(0.58)
(5.64)
3532
73

13.34*** 14.28*** 14.19*** 13.89*** 13.96*** 13.06***
(3.54)
(2.27)
(1.87)
(1.61)
(1.03)
(0.74)
-1.86
-3.10
-4.08
-2.91
0.15
-0.31
(5.40)
(4.99)
(4.57)
(3.58)
(1.59)
(1.21)
195
392
547
719
1389
1947

2.50**
(0.79)
3558

13.90*** 16.55**
(0.50)
(6.11)
2.44***
0.05
(0.50)
(4.33)
3558
70

14.37*** 15.05*** 14.81*** 14.42*** 14.10*** 13.19***
(3.66)
(2.33)
(1.91)
(1.64)
(1.04)
(0.75)
-2.14
1.42
0.13
-0.03
-0.09
-0.63
(3.97)
(2.17)
(1.97)
(1.74)
(1.12)
(0.96)
191
385
539
709
1366
1923

# Confirmed COVID Patients
First stage
(in millions)
$1mm of funding
Observations

4.53***
(0.63)
3558

# Confirmed/Suspected COVID Patients in ICU
First stage
(in millions)
$1mm of funding
Observations

1.67***
(0.21)
3445

0.91**
(0.28)
3445

13.88*** 14.67*
(0.51)
(5.59)
0.95***
0.93
(0.18)
(1.47)
3445
72

13.42*** 15.75*** 15.29*** 14.74*** 14.31*** 13.18***
(3.49)
(2.32)
(1.93)
(1.67)
(1.06)
(0.76)
0.71
0.36
-0.05
0.16
-0.03
-0.32
(1.27)
(0.74)
(0.70)
(0.60)
(0.40)
(0.36)
186
374
520
678
1314
1846

13.89*** 15.80*
(0.50)
(6.15)
0.88***
0.50
(0.17)
(1.54)
3503
67

13.79*** 15.78*** 15.53*** 15.08*** 14.43*** 13.40***
(3.73)
(2.41)
(2.02)
(1.73)
(1.09)
(0.77)
-0.11
0.18
0.04
0.12
-0.13
-0.35
(1.37)
(0.70)
(0.64)
(0.56)
(0.39)
(0.34)
181
370
514
671
1321
1868

# Confirmed COVID Patients in ICU
First stage
(in millions)
$1mm of funding
Observations

1.51***
(0.21)
3503

0.82**
(0.27)
3503

Notes: In this table we regress relevant outcomes at the hospital level on safety net funding. Column 1 presents the results
of OLS regression of the outcome variables on safety net funding without any controls. Column 2 presents the results of
OLS regression of the outcome variables on safety net funding controlling for disproportionate patient percentage,
uncompensated care per bed and profit margin. In columns 3‚Äì10, we instrument safety net funding with eligibility to
receive this funding and present the results of 2SLS regressions. In columns 3‚Äì10, the first stage shows the effect of being
deemed eligible on the amount of relief funding received by hospitals, in millions of dollars. Column 3 shows the results of
a 2SLS regression with no controls. In columns 4‚Äì10, we run this regression controlling for the Approximate Propensity
Score with different values of bandwidth Œ¥ on the sample with nondegenerate Approximate Propensity Score. All
Approximate Propensity Scores are computed by averaging 10,000 simulation draws. The outcome variables are the 7 day
totals for the week spanning July 31st, 2020 to August 6th, 2020. Confirmed or Suspected COVID patients refer to the
sum of patients in inpatient beds with lab-confirmed/suspected COVID-19. Confirmed COVID patients refer to the sum
of patients in inpatient beds with lab-confirmed COVID-19, including those with both lab-confirmed COVID-19 and
influenza. Inpatient bed totals also include observation beds. Similarly, Confirmed/ Suspected COVID patients in ICU
refer to the sum of patients in ICU beds with lab-confirmed or suspected COVID-19. Confirmed COVID patients in ICU
refers to the sum of patients in ICU beds with lab-confirmed COVID-19, including those with both lab-confirmed
COVID-19 and influenza. Robust standard errors are reported in parentheses.

39

2SLS w/ APS ( = 0.05)
OLS
2SLS w/o Controls

15
10

Confirmed Patients

Confirmed/Suspected Patients

Figure 5: Dynamic Effects of Funding on Weekly Hospital Outcomes

5
0
5
10
15

Aug

Sep

Oct

Nov

Dec

Date

Jan

Feb

r

Ma

15
10
5
0
5
10

Apr

Aug

3
2
1
0
1
2
3

Aug

Sep

Oct

Nov

Dec

Date

Jan

Feb

r

Ma

Oct

Nov

Dec

Date

Jan

Feb

r

Ma

Apr

(b) # Confirmed COVID Patients

Confirmed ICU Patients

Confirmed/Suspected ICU Patients

(a) # Confirmed/Suspected COVID Patients

Sep

Apr

3
2
1
0
1
2
3

Aug

(c) # Confirmed/Suspected COVID Patients in ICU

Sep

Oct

Nov

Dec

Date

Jan

Feb

r
Ma

Apr

(d) # Confirmed COVID Patients in ICU

Notes: The figure shows the results of estimating our main 2SLS specification about the effect of $1mm of relief funding on
weekly hospital outcomes from 07/31/2020 to 04/02/2021. The outcomes record the 7-day sum of the number of hospitalized
patients with the specified condition. We compute the Approximate Propensity Score with S = 10, 000 and Œ¥ = 0.05. The
estimates from the uncontrolled OLS, uncontrolled 2SLS, and 2SLS with the Approximate Propensity Score controls are
plotted on the y-axis. Standard error ribbons are given in grey.

40

A
A.1

Extensions and Discussions
Related Literature: Details

In this section, we discuss the related methodological literature on the multidimensional RDD in
detail. Imbens and Wager (2019) propose the finite-sample-minimax linear estimator of the form
Pn
i=1 Œ≥i Yi and uniform confidence intervals for treatment effects in the multidimensional RDD.
One version of their approach constructs a linear estimator by choosing the weight (Œ≥i )ni=1 greedily
to make the inference as precise as possible. Although their estimator is favorable in terms of
precision, it is not obvious what estimand the estimator estimates, without assuming a constant
treatment effect. The other version of Imbens and Wager (2019)‚Äôs approach and some other
existing approaches (Zajonc, 2012; Keele and Titiunik, 2015) consider nonparametric estimation
of the conditional average treatment effect E[Yi (1) ‚àí Yi (0)|Xi = x] for a specified boundary point
x. The estimand has a clear interpretation, but ‚Äúwhen curvature is nonnegligible, equation (6)
can effectively make use of only data near the specified focal point c, thus resulting in relatively
long confidence intervals‚Äù (Imbens and Wager, 2019, p. 268), where equation (6) defines their
estimator.
To obtain more precise estimates while keeping interpretability, several papers studying a twodimensional RDD, including Zajonc (2012) and Keele and Titiunik (2015), propose to estimate
an integral of conditional average treatment effects over the boundary. Their approach first
nonparametrically estimates E[Yi (1) ‚àí Yi (0)|Xi = x] and the density of Xi for a large number of
points x in the boundary and then computes the weighted average of the estimated conditional
average treatment effects with the weight set to the estimated density.
The above approach is difficult to implement, however, when Xi is high dimensional or the
decision algorithm is a complex, black box function of Xi , for the following reasons. First, it
is computationally demanding to estimate E[Yi (1) ‚àí Yi (0)|Xi = x] for numerous points in the
boundary such that the weighted average well approximates the integral of E[Yi (1)‚àíYi (0)|Xi = x]
over the boundary. Second, identifying boundary points from a general decision algorithm itself
is hard unless it has a known analytical form. By contrast, we develop an estimator that uses
observations near all the boundary points without tracing out the boundary or knowing its
analytical form, thus alleviating the limitations of existing estimators.

A.2

Existence of the Approximate Propensity Score

Proposition 1 assumes that APS exists, but is it fair to assume so? In general, APS may fail to
exist. Figure A.1 shows such an example. In this example, Xi is two dimensional, and
(
1 if 3( 21 )k‚àí1 < kxk ‚â§ 4( 12 )k‚àí1 for some k = 1, 2, ¬∑ ¬∑ ¬∑
A(x) =
0 if 2( 12 )k‚àí1 < kxk ‚â§ 3( 12 )k‚àí1 for some k = 1, 2, ¬∑ ¬∑ ¬∑ .

41

Figure A.1: Example of Algorithm A for which Approximate Propensity Score Does Not Exist

It is shown that
(
A

p (0; Œ¥) =

7
12
7
27

if Œ¥ = 4( 12 )k‚àí1 for some k = 1, 2, ¬∑ ¬∑ ¬∑
if Œ¥ = 3( 12 )k‚àí1 for some k = 1, 2, ¬∑ ¬∑ ¬∑ .

Therefore, limŒ¥‚Üí0 pA (0; Œ¥) does not exist.
Nevertheless, APS exists for almost every x, as shown in the following proposition.
Proposition A.1. pA (x) exists and is equal to A(x) for almost every x ‚àà X (with respect to the
Lebesgue measure).
Proof. See Appendix C.6.
Does APS exist at a specific point x? What is the value of APS at x if it is not equal to
A(x)? We show that APS exists and is of a particular form for most covariate points and typical
algorithms. For each x ‚àà X and each q ‚àà Supp(A(Xi )), define
Ux,q ‚â° {u ‚àà B(0, 1) : lim A(x + Œ¥u) = q}.
Œ¥‚Üí0

Ux,q is the set of vectors in B(0, 1) such that the value of A approaches q as we approach x from
the direction of the vector. With this notation, we obtain a sufficient condition for the existence
of APS at a point x.
Proposition A.2. Take any x ‚àà X . If there exists a countable set Q ‚äÇ Supp(A(Xi )) such that
Lp (‚à™q‚ààQ Ux,q ) = Lp (B(0, 1)) and Ux,q is Lp -measurable for all q ‚àà Q, then pA (x) exists and is
given by
P
p
q‚ààQ qL (Ux,q )
A
p (x) =
.
Lp (B(0, 1))
Proof. See Appendix C.4.
42

If almost every point in B(0, 1) is contained by one of countably many Ux,q ‚Äôs, therefore, APS
exists and is equal to the weighted average of the values of q with the weight proportional to the
hypervolume of Ux,q . This result implies that APS exists in practically important cases.
Corollary A.1.
1. (Continuity points) If A is continuous at x ‚àà X , then pA (x) exists and pA (x) = A(x).
2. (Interior points) Let Xq = {x ‚àà X : A(x) = q} for some q ‚àà [0, 1]. Then, for any interior
point x ‚àà int(Xq ), pA (x) exists and pA (x) = q.
3. (Smooth boundary points) Suppose that {x ‚àà X : A(x) = q1 } = {x ‚àà X : f (x) ‚â• 0} and
{x ‚àà X : A(x) = q2 } = {x ‚àà X : f (x) < 0} for some q1 , q2 ‚àà [0, 1], where f : Rp ‚Üí R.
Let x ‚àà X be a boundary point such that f (x) = 0, and suppose that f is continuously
differentiable in a neighborhood of x with ‚àáf (x) 6= 0. In this case, pA (x) exists and
pA (x) = 21 (q1 + q2 ).
4. (Intersection points under CART and random forests) Let p = 2, and suppose that {x ‚àà X :
A(x) = q1 } = {(x1 , x2 )0 ‚àà X : x1 ‚â§ 0 or x2 ‚â§ 0}, {x ‚àà X : A(x) = q2 } = {(x1 , x2 )0 ‚àà X :
x1 > 0, x2 > 0}, and 0 = (0, 0)0 ‚àà X . This is an example in which tree-based algorithms
such as Classification And Regression Tree (CART) and random forests are used to create
A. In this case, pA (0) exists and pA (0) = 43 q1 + 14 q2 .
Proof. See Appendix C.5.

A.3

Discrete Covariates

In this section, we provide the definition of APS and identification and consistency results when
Xi includes discrete covariates. Suppose that Xi = (Xdi , Xci ), where Xdi ‚àà Rpd is a vector
of discrete covariates, and Xci ‚àà Rpc is a vector of continuous covariates. Let Xd denote the
support of Xdi and be assumed to be finite. We also assume that Xci is continuously distributed
conditional on Xdi , and let Xc (xd ) denote the support of Xci conditional on Xdi = xd for each
xd ‚àà Xd . Let Xc,0 (xd ) = {xc ‚àà Xc (xd ) : A(xd , xc ) = 0} and Xc,1 (xd ) = {xc ‚àà Xc (xd ) : A(xd , xc ) =
1}.
Define APS as follows: for each x = (xd , xc ) ‚àà X ,
R
‚àó
‚àó
B(xc ,Œ¥) A(xd , xc )dxc
A
R
p (x; Œ¥) ‚â°
,
‚àó
B(xc ,Œ¥) dxc
pA (x) ‚â° lim pA (x; Œ¥),
Œ¥‚Üí0

where B(xc , Œ¥) = {x‚àóc ‚àà Rpc : kxc ‚àí x‚àóc k ‚â§ Œ¥} is the Œ¥-ball around xc ‚àà Rpc . In other words, we
take the average of the A(xd , x‚àóc ) values when x‚àóc is uniformly distributed on B(xc , Œ¥) holding xd
fixed, and let Œ¥ ‚Üí 0. Below, we assume that Assumptions 1, 2, 3 and 4 hold conditional on Xdi .
Assumption A.1 (Almost Everywhere Continuity of A).

43

(a) For every xd ‚àà Xd , A(xd , ¬∑) is continuous almost everywhere with respect to the Lebesgue
measure Lpc .
(b) For every xd ‚àà Xd , Lpc (Xc,k (xd )) = Lpc (int(Xc,k (xd ))) for k = 0, 1.
A.3.1

Identification

Assumption A.2 (Local Mean Continuity). For every xd ‚àà Xd and z ‚àà {0, 1}, the conditional
expectation functions E[Yzi |Xi = (xd , xc )] and E[Di (z)|Xi = (xd , xc )] are continuous in xc at
any point xc ‚àà Xc (xd ) such that pA (xd , xc ) ‚àà (0, 1) and A(xd , xc ) ‚àà {0, 1}.
Let intc (X ) = {(xd , xc ) ‚àà X : xc ‚àà int(Xc (xd ))}. We say that a set S ‚äÇ Rp is open
relative to X if there exists an open set U ‚äÇ Rp such that S = U ‚à© X . For a set S ‚äÇ Rp , let
XdS = {xd ‚àà Xd : (xd , xc ) ‚àà S for some xc ‚àà Rpc } and XcS (xd ) = {xc ‚àà Xc : (xd , xc ) ‚àà S} for
each xd ‚àà XdS .
Proposition A.3. Under Assumptions A.1 and A.2:
(a) E[Y1i ‚àí Y0i |Xi = x] and E[Di (1) ‚àí Di (0)|Xi = x] are identified for every x ‚àà intc (X ) such
that pA (x) ‚àà (0, 1).
(b) Let S be any subset of X open relative to X such that pA (x) exists for all x ‚àà S. Then either
E[Y1i ‚àí Y0i |Xi ‚àà S] or E[Di (1) ‚àí Di (0)|Xi ‚àà S], or both are identified only if pA (x) ‚àà (0, 1)
for almost every xc ‚àà XcS (xd ) for every xd ‚àà XdS .
Proof. See Appendix C.7.
A.3.2

Estimation

For each xd ‚àà Xd , let ‚Ñ¶‚àó (xd ) = {xc ‚àà Rpc : A(xd , xc ) = 1}. Also, let Xd‚àó = {xd ‚àà Xd :
Var(A(Xi )|Xdi = xd ) > 0}, and let fXc |Xd denote the probability density function of Xci conditional on Xdi . In addition, for each xd ‚àà Xd , let
C ‚àó (xd ) = {xc ‚àà Rpc : A(xd , ¬∑) is continuously differentiable at xc },
and let D‚àó (xd ) = Rpc \ C ‚àó (xd ).
Assumption A.3.
(a) (Finite Moments) E[Yi4 ] < ‚àû.
(b) (Nonzero First Stage) There exists a constant c > 0 such that E[Di (1) ‚àí Di (0)|Xi = x] > c
for every x ‚àà X such that pA (x) ‚àà (0, 1).
(c) (Nonzero Conditional Variance) If Pr(A(Xi ) ‚àà (0, 1)) > 0, then Var(A(Xi )|A(Xi ) ‚àà
(0, 1)) > 0.
If Pr(A(Xi ) ‚àà (0, 1)) = 0, then the following conditions (d)‚Äì(g) hold.
44

(d) (Nonzero Variance) Xd‚àó 6= ‚àÖ.
(e) (C 2 Boundary of ‚Ñ¶‚àó (xd )) For each xd ‚àà Xd‚àó , there exists a partition {‚Ñ¶‚àó1 (xd ), ..., ‚Ñ¶‚àóM (xd )}
of ‚Ñ¶‚àó (xd ) such that
(i) dist(‚Ñ¶‚àóm (xd ), ‚Ñ¶‚àóm0 (xd )) > 0 for any m, m0 ‚àà {1, ..., M } such that m 6= m0 ;
(ii) ‚Ñ¶‚àóm (xd ) is nonempty, bounded, open, connected and twice continuously differentiable
for each m ‚àà {1, ..., M }.
(f ) (Regularity of Deterministic A)
(i) For each xd ‚àà Xd‚àó , Hpc ‚àí1 (‚àÇ‚Ñ¶‚àó (xd )) < ‚àû, and

R

‚àÇ‚Ñ¶‚àó (xd ) fXc |Xd (xc |xd )dH

pc ‚àí1 (x )
c

(ii) There exists Œ¥ > 0 such that A(xd , xc ) = 0 for almost every xc ‚àà N (Xc (xd

> 0.

), Œ¥)\‚Ñ¶‚àó (x

d ).

(g) (Conditional Means and Density near ‚àÇ‚Ñ¶‚àó (xd )) For each xd ‚àà Xd‚àó , there exists Œ¥ > 0 such
that
(i) E[Y1i |Xi = (xd , ¬∑)], E[Y0i |Xi = (xd , ¬∑)], E[Di (1)|Xi = (xd , ¬∑)], E[Di (0)|Xi = (xd , ¬∑)]
and fXc |Xd (¬∑|xd ) are continuously differentiable and have bounded partial derivatives
on N (‚àÇ‚Ñ¶‚àó (xd ), Œ¥);
(ii) E[Y1i2 |Xi = (xd , ¬∑)], E[Y0i2 |Xi = (xd , ¬∑)], E[Y1i Di (1)|Xi = (xd , ¬∑)] and E[Y0i Di (0)|Xi =
(xd , ¬∑)] are continuous on N (‚àÇ‚Ñ¶‚àó (xd ), Œ¥);
(iii) E[Yi4 |Xi = (xd , ¬∑)] is bounded on N (‚àÇ‚Ñ¶‚àó (xd ), Œ¥).
Assumption A.4. If Pr(A(Xi ) ‚àà (0, 1)) > 0, then the following conditions (a)‚Äì(c) hold.
(a) (Probability of Neighborhood of D‚àó (xd )) For each xd ‚àà Xd‚àó , Pr(Xi ‚àà N (D‚àó (xd ), Œ¥)) =
O(Œ¥).
(b) (Bounded Partial Derivatives of A) For each xd ‚àà Xd‚àó , the partial derivatives of A(xd , ¬∑)
are bounded on C ‚àó (xd ).
(c) (Bounded Conditional Mean) For each xd ‚àà Xd‚àó , E[Yi |Xi = (xd , ¬∑)] is bounded on Xc (xd ).
Theorem A.1. Suppose that Assumptions A.1 and A.3 hold, and that Œ¥n ‚Üí 0, nŒ¥n ‚Üí ‚àû and
Sn ‚Üí ‚àû as n ‚Üí ‚àû. Then the 2SLS estimators Œ≤ÃÇ1 and Œ≤ÃÇ1s converge in probability to
Œ≤1 ‚â° lim E[œâi (Œ¥)(Yi (1) ‚àí Yi (0))],
Œ¥‚Üí0

where
œâi (Œ¥) =

pA (Xi ; Œ¥)(1 ‚àí pA (Xi ; Œ¥))(Di (1) ‚àí Di (0))
.
E[pA (Xi ; Œ¥)(1 ‚àí pA (Xi ; Œ¥))(Di (1) ‚àí Di (0))]

Suppose, in addition, that Assumptions A.4 and 5 hold and that nŒ¥n2 ‚Üí 0 as n ‚Üí ‚àû. Then
d

œÉÃÇn‚àí1 (Œ≤ÃÇ1 ‚àí Œ≤1 ) ‚àí‚Üí N (0, 1),
d

(œÉÃÇns )‚àí1 (Œ≤ÃÇ1s ‚àí Œ≤1 ) ‚àí‚Üí N (0, 1).
45

Proof. See Appendix C.8.
As in the case in which all covariates are continuous, the probability limit of the 2SLS
estimators has more specific expressions depending on whether Pr(A(Xi ) ‚àà (0, 1)) > 0 or not. If
Pr(A(Xi ) ‚àà (0, 1)) > 0,
plim Œ≤ÃÇ1 = plim Œ≤ÃÇ1s =

E[A(Xi )(1 ‚àí A(Xi ))(Di (1) ‚àí Di (0))(Yi (1) ‚àí Yi (0))]
.
E[A(Xi )(1 ‚àí A(Xi ))(Di (1) ‚àí Di (0))]

If Pr(A(Xi ) ‚àà (0, 1)) = 0,
plim Œ≤ÃÇ1
= plim Œ≤ÃÇ1s
P
=

xd ‚ààXd‚àó

R
Pr(Xdi = xd ) ‚àÇ‚Ñ¶‚àó (xd ) E[(Di (1) ‚àí Di (0))(Yi (1) ‚àí Yi (0))|Xi = x]fXc |Xd (xc |xd )dHpc ‚àí1 (xc )
R
P
.
pc ‚àí1 (x )
c
xd ‚ààX ‚àó Pr(Xdi = xd ) ‚àÇ‚Ñ¶‚àó (xd ) E[Di (1) ‚àí Di (0)|Xi = x]fXc |Xd (xc |xd )dH
d

A.4

A Sufficient Condition for Assumption 4 (a)

We provide a sufficient condition for Assumption 4 (a).
Assumption A.5.
‚àó ‚äÇ Rp such that
(a) (Twice Continuous Differentiability of D‚àó ) There exist C1‚àó , ..., CM
‚àó
(i) ‚àÇ(CÃÉ ‚àó ) = D‚àó , where CÃÉ ‚àó ‚â° ‚à™M
m=1 Cm ;
‚àó , C ‚àó ) > 0 for any m, m0 ‚àà {1, ..., M } such that m 6= m0 ;
(ii) dist(Cm
m0
‚àó is nonempty, bounded, open, connected and twice continuously differentiable for
(iii) Cm
each m ‚àà {1, ..., M }.

(b) (Regularity of D‚àó ) Hp‚àí1 (D‚àó ) < ‚àû.
(c) (Bounded Density near D‚àó ) There exists Œ¥ > 0 such that fX is bounded on N (D‚àó , Œ¥).
The key condition is the twice continuous differentiability of D‚àó . This condition holds if,
for example, the -Greedy algorithm described in Example A.1 (a) in Appendix A.6 uses an
estimated Q-function that is twice continuously differentiable in x.
Under Assumption A.5 (a), by Lemma B.4 in Appendix B.3 and with change of variables
v = ŒªŒ¥ , for any sufficiently small Œ¥ > 0,
Pr(Xi ‚àà N (D‚àó , Œ¥)) =

Z

Œ¥

Z

‚àíŒ¥

Z

‚àó

D‚àó
1

D
fX (u + ŒªŒΩCÃÉ ‚àó (u))Jp‚àí1
œàCÃÉ ‚àó (u, Œª)dHp‚àí1 (u)dŒª

Z

=Œ¥
‚àí1

D‚àó

‚àó

D
fX (u + Œ¥vŒΩCÃÉ ‚àó (u))Jp‚àí1
œàCÃÉ ‚àó (u, Œ¥v)dHp‚àí1 (u)dv.

(See Appendix B for the notation.) If fX is bounded on N (D‚àó , Œ¥) and Hp‚àí1 (D‚àó ) < ‚àû, the
right-hand side is O(Œ¥).
46

A.5

Sampling from Uniform Distribution on p-Dimensional Ball

When we calculate APS by simulation, we need to uniformly sample from B(Xi , Œ¥). We introduce
three existing methods to uniformly sample from a p-dimensional unit ball B(0, 1). By multiplying the sampled vector by Œ¥ and adding Xi to it, we can sample from a uniform distribution
on B(Xi , Œ¥).
Method 1.
1. Sample x1 , ..., xp independently from the uniform distribution on [‚àí1, 1].
P
2. Accept the vector x = (x1 , ..., xp ) if pk=1 x2k ‚â§ 1 and reject it otherwise.
Method 1 is a practical choice when p is small (e.g. p = 2, 3), but is inefficient for higher
dimensions, since the acceptance rate decreases to zero quickly as p increases. The conventional
method used for higher dimensions is the following.
Method 2.
1. Sample x‚àó1 , ..., x‚àóp independently from the standard normal distribution, and compute the
qP
p
‚àó 2
vector s = (x‚àó1 , ..., x‚àóp )/
k=1 (xk ) .
2. Sample u from the uniform distribution on [0, 1].
3. Return the vector x = u1/p s.
There is yet another method efficient for higher dimensions, which is recently proposed by
Voelker, Gosmann and Stewart (2017).
Method 3.
1. Sample x‚àó1 , ..., x‚àóp+2 independently from the standard normal distribution, and compute the
qP
p+2 ‚àó 2
‚àó
‚àó
vector s = (x1 , ..., xp+2 )/
k=1 (xk ) .
2. Return the vector x = (s1 , ..., sp ).

A.6

Additional Examples

Example A.1 (Reinforcement Learning Algorithms). Extending bandit algorithms to dynamically changing environments, reinforcement learning algorithms optimize decisions in dynamic
environments, where the state (the set of observables that the agent receives from the environment) and action in the current period can affect the future states and outcomes. Let
{(Xti , Zti , Yti )}‚àû
t=0 denote the trajectory of the states, treatment assignments, and outcomes
in periods t = 0, 1, 2, ¬∑ ¬∑ ¬∑ for individual i. For simplicity, we assume that the trajectory follows
a Markov decision process.39 Let Yti (1) and Yti (0) represent the potential outcomes in period t.
Let Q : X √ó {0, 1} ‚Üí R be the optimal state-action value function, called the Q-function: for
(x, z) ‚àà X √ó {0, 1},
"‚àû
#
X
t
Q(x, z) ‚â° max E
Œ≥ (Yti (1)œÄ(Xti ) + Yti (0)(1 ‚àí œÄ(Xti ))|X0i = x, Z0i = z ,
œÄ:X ‚Üí[0,1]

t=0

39

Under a Markov decision process, the distribution of the state Xti only depends on the last state and treatment
assignment (Xt‚àí1,i , Zt‚àí1,i ), the distribution of the outcome Yti only depends on the current state and treatment
assignment (Xti , Zti ), and these distributions are stationary over periods.

47

where Œ≥ ‚àà [0, 1) is a discount factor, and œÄ is a policy function that assigns the probability of
treatment to each possible state.
(a) (-Greedy) This algorithm first uses past data to yield QÃÇ, an estimate of the Q-function.
For example, the fitted Q iteration (Ernst, Geurts and Wehenkel, 2005) is used to estimate
Q.40

B

Notation and Lemmas

B.1

Basic Notations

For a scalar-valued differentiable function f : S ‚äÇ Rn ‚Üí R, let ‚àáf : S ‚Üí Rn be a gradient of f :
for every x ‚àà S,


‚àÇf (x)
‚àÇf (x) 0
.
‚àáf (x) =
,¬∑¬∑¬∑ ,
‚àÇx1
‚àÇxn
Also, when the second-order partial derivatives of f exist, let D2 f (x) be the Hessian matrix:
Ô£Æ ‚àÇ 2 f (x)
..
.

¬∑¬∑¬∑
..
.

‚àÇ 2 f (x)
‚àÇxn ‚àÇx1

¬∑¬∑¬∑

‚àÇx21

Ô£Ø
D2 f (x) = Ô£Ø
Ô£∞

‚àÇ 2 f (x)
‚àÇx1 ‚àÇxn

..
.

‚àÇ 2 f (x)
‚àÇx2n

Ô£π
Ô£∫
Ô£∫
Ô£ª

for each x ‚àà S.
Let f : S ‚äÇ Rm ‚Üí Rn be a function such that its first-order partial derivatives exist. For
each x ‚àà S, let Jf (x) be the Jacobian matrix of f at x:
Ô£π
Ô£Æ
‚àÇf1 (x)
‚àÇf1 (x)
¬∑
¬∑
¬∑
‚àÇxm
Ô£Ø ‚àÇx. 1
.. Ô£∫
..
..
Jf (x) = Ô£Ø
.
. Ô£∫
Ô£ª.
Ô£∞
‚àÇfn (x)
‚àÇfn (x)
¬∑ ¬∑ ¬∑ ‚àÇxm
‚àÇx1
For a positive integer n, let In denote the n √ó n identity matrix.

B.2

Differential Geometry

We provide some concepts and facts from differential geometry of twice continuously differentiable
sets, following Crasta and Malusa (2007). Let S ‚äÇ Rp be a twice continuously differentiable set.
For each x ‚àà ‚àÇS, we denote by ŒΩS (x) ‚àà Rp the inward unit normal vector of ‚àÇS at x, that is,
40

Suppose that we have collected a set of L four-tuples {(xltl , ztll , ytll , xltl +1 )}L
l=1 as a result of the agent interacting
with the dynamic environment. Given the dataset and an initial approximation QÃÇ of Q (e.g., QÃÇ(x, z) = 0 for all
(x, z)), we repeat the following steps until some stopping condition is reached: 1. For each l = 1, ..., L, calculate
q l = ytll + Œ≥ maxz‚àà{0,1} QÃÇ(xltl +1 , z); 2. Use {(xltl , ztll , q l )}L
l=1 and a supervised learning method to train a model
that predicts q from (x, z). Let the model be a new approximation QÃÇ of Q. Possible supervised learning methods
used in the second step include tree-based methods, neural networks (Neural Fitted Q Iteration) and deep neural
networks (Deep Fitted Q Iteration).

48

the unit vector orthogonal to all vectors in the tangent space of ‚àÇS at x that points toward the
inside of A. For a set S ‚äÇ Rp , let dsS : Rp ‚Üí R be the signed distance function of S, defined by
dsS (x) =

(
d(x, ‚àÇS)

if x ‚àà cl(S)

‚àíd(x, ‚àÇS)

if x ‚àà Rp \ cl(S),

where d(x, B) = inf y‚ààB ky ‚àí xk for any x ‚àà Rp for a set B ‚äÇ Rp . Note that we can write
N (‚àÇS, Œ¥) = {x ‚àà Rp : ‚àíŒ¥ < dsS (x) < Œ¥} for Œ¥ > 0. Lastly, let Œ†‚àÇS (x) = {y ‚àà ‚àÇS : ky ‚àí xk =
d(x, ‚àÇS)} be the set of projections of x on ‚àÇS.
Lemma B.1 (Corollary of Theorem 4.16, Crasta and Malusa (2007)). Let S ‚äÇ Rp be nonempty,
bounded, open, connected and twice continuously differentiable. Then the function dsS is twice
continuously differentiable on N (‚àÇS, ¬µ) for some ¬µ > 0. In addition, for every x0 ‚àà ‚àÇS, Œ†‚àÇS (x0 +
tŒΩS (x0 )) = {x0 } for every t ‚àà (‚àí¬µ, ¬µ). Furthermore, for every x ‚àà N (‚àÇS, ¬µ), Œ†‚àÇS (x) is a
singleton, ‚àádsS (x) = ŒΩS (y) and x = y + dsS (x)ŒΩS (y) for y ‚àà Œ†‚àÇS (x), and k‚àádsS (x)k = 1.
Proof. We apply results from Crasta and Malusa (2007). Let K = {x ‚àà Rp : kxk ‚â§ 1}. K is
nonempty, compact, convex subset of Rp with the origin as an interior point. The polar body
of K, defined as K0 = {y ‚àà Rp : y ¬∑ x ‚â§ 1 for all x ‚àà K}, is K itself. The gauge functions
œÅK , œÅK0 : Rp ‚Üí [0, ‚àû] of K and K0 are given by
œÅK (x) ‚â° inf{t ‚â• 0 : x ‚àà tK} = kxk,
œÅK0 (x) ‚â° inf{t ‚â• 0 : x ‚àà tK0 } = kxk.
Given œÅK0 , the Minkowski distance from a set S ‚äÇ Rp is defined as
Œ¥S (x) ‚â° inf œÅK0 (x ‚àí y),
y‚ààS

x ‚àà Rp .

Note that we can write
dsS (x) =

(
Œ¥‚àÇS (x)

if x ‚àà cl(S)
if x ‚àà Rp \ cl(S).

‚àíŒ¥‚àÇS (x)

It then follows from Theorem 4.16 of Crasta and Malusa (2007) that dsS is twice continuously
differentiable on N (‚àÇS, ¬µ) for some ¬µ > 0, and for every x0 ‚àà ‚àÇS,
‚àádsS (x0 ) =

ŒΩS (x0 )
ŒΩS (x0 )
=
= ŒΩS (x0 ),
œÅK (ŒΩS (x0 ))
kŒΩS (x0 )k

where the last equality follows since ŒΩS (x0 ) is a unit vector. It then follows that k‚àádsS (x0 )k =
kŒΩS (x0 )k = 1 for every x0 ‚àà ‚àÇS. Also, it is obvious that, for every x0 ‚àà ‚àÇS, Œ†‚àÇS (x0 ) = {x0 } and
x0 = x0 + dsS (x0 )ŒΩS (x0 ), since dsS (x0 ) = 0. In addition, as stated in the proof of Theorem 4.16
of Crasta and Malusa (2007), ¬µ is chosen so that (4.7) in Proposition 4.6 of Crasta and Malusa
(2007) holds for every x0 ‚àà ‚àÇS and every t ‚àà (‚àí¬µ, ¬µ). That is, Œ†‚àÇS (x0 + t‚àáœÅK (ŒΩS (x0 ))) =
(x0 )
{x0 } for every x0 ‚àà ‚àÇS and every t ‚àà (‚àí¬µ, ¬µ). Since ‚àáœÅK (ŒΩS (x0 )) = kŒΩŒΩSS (x
= ŒΩS (x0 ),
0 )k
Œ†‚àÇS (x0 + tŒΩS (x0 )) = {x0 } for every x0 ‚àà ‚àÇS and every t ‚àà (‚àí¬µ, ¬µ).
49

Furthermore, for every x ‚àà N (‚àÇS, ¬µ) \ ‚àÇS, Œ†‚àÇS (x) is a singleton as shown in the proof of
Theorem 4.16 of Crasta and Malusa (2007). Let œÄ‚àÇS (x) be the unique element in Œ†‚àÇS (x). By
Lemma 4.3 of Crasta and Malusa (2007), for every x ‚àà N (‚àÇS, ¬µ) \ ‚àÇS,
‚àádsS (x) =

ŒΩS (œÄ‚àÇS (x))
ŒΩS (œÄ‚àÇS (x))
=
= ŒΩS (œÄ‚àÇS (x)),
œÅK (ŒΩS (œÄ‚àÇS (x)))
kŒΩS (œÄ‚àÇS (x))k

where the last equality follows since ŒΩS (œÄ‚àÇS (x)) is a unit vector. It then follows that k‚àádsS (x)k =
kŒΩS (œÄ‚àÇS (x))k = 1 for every x ‚àà N (‚àÇS, ¬µ) \ ‚àÇS.
Lastly, note that
(
dsS (x)
if x ‚àà N (‚àÇS, ¬µ) ‚à© int(S)
Œ¥‚àÇS (x) =
‚àídsS (x)
if x ‚àà N (‚àÇS, ¬µ) \ cl(S),
and
‚àáŒ¥‚àÇS (x) =

(
‚àádsS (x)

if x ‚àà N (‚àÇS, ¬µ) ‚à© int(S)
if x ‚àà N (‚àÇS, ¬µ) \ cl(S),

‚àí‚àádsS (x)

so Œ¥‚àÇS (x)‚àáŒ¥‚àÇS (x) = dsS (x)‚àádsS (x) = dsS (x)ŒΩS (œÄ‚àÇS (x)) for every x ‚àà N (‚àÇS, ¬µ) \ ‚àÇS. By Proposition 3.3 (i) of Crasta and Malusa (2007), for every x ‚àà N (‚àÇS, ¬µ) \ ‚àÇS,
‚àáœÅK (‚àáŒ¥‚àÇS (x)) =

x ‚àí œÄ‚àÇS (x)
,
Œ¥‚àÇS (x)

which implies that
x = œÄ‚àÇS (x) + Œ¥‚àÇS (x)‚àáœÅK (‚àáŒ¥‚àÇS (x))
= œÄ‚àÇS (x) + Œ¥‚àÇS (x)

‚àáŒ¥‚àÇS (x)
= œÄ‚àÇS (x) + dsS (x)ŒΩS (œÄ‚àÇS (x)).
k‚àáŒ¥‚àÇS (x)k

We say that a set S ‚äÇ Rn is a m-dimensional C 1 submanifold of Rn if for every point x ‚àà S,
there exist an open neighborhood V ‚äÇ Rn of x and a one-to-one continuously differentiable
function œÜ from an open set U ‚äÇ Rm to Rn such that the Jacobian matrix JœÜ(u) is of rank m
for all u ‚àà U , and œÜ(U ) = V ‚à© S.
Lemma B.2. Let S ‚äÇ Rp be nonempty, bounded, open, connected and twice continuously differentiable. Then ‚àÇS is a (p ‚àí 1)-dimensional C 1 submanifold of Rp ,
Proof. Fix any x‚àó ‚àà ‚àÇS. By Lemma B.1, ‚àádsS (x‚àó ) is nonzero. Without loss of generality, let
‚àÇdsS (x‚àó )
6= 0. Let œà : Rp ‚Üí Rp be the function such that œà(x) = (x1 , ..., xp‚àí1 , dsS (x)). œà is
‚àÇxp
continuously differentiable, and the Jacobian matrix of œà at x‚àó is given by
Ô£´
Ô£∂
Ô£´ ‚àÇœà
Ô£∂
0
‚àÇœà1
‚àó
‚àó
1
Ô£¨
‚àÇx (x ) ¬∑ ¬∑ ¬∑ ‚àÇxp (x )
.. Ô£∑
Ô£¨ 1.
Ô£∑ Ô£¨
I
. Ô£∑
.
p‚àí1
.
‚àó
Ô£∑=Ô£¨
Ô£∑.
..
..
..
Jœà(x ) = Ô£¨
Ô£∑
Ô£≠
Ô£∏ Ô£¨
0
Ô£≠
Ô£∏
‚àÇœàp
‚àÇœà
p
‚àó
‚àó
s
‚àó
s
‚àó
s
‚àó
‚àÇdS (x )
‚àÇdS (x )
‚àÇdS (x )
‚àÇx1 (x ) ¬∑ ¬∑ ¬∑ ‚àÇxp (x )
¬∑
¬∑
¬∑
‚àÇx1
‚àÇxp‚àí1
‚àÇxp
50

‚àÇds (x‚àó )

S
6= 0, the Jacobian matrix is invertible. By the Inverse Function Theorem, there
Since ‚àÇx
p
exist an open set V containing x‚àó and an open set W containing œà(x‚àó ) such that œà : V ‚Üí W
has an inverse function œà ‚àí1 : W ‚Üí V that is continuously differentiable. We make V small
‚àÇdsS (x)
enough so that ‚àÇx
6= 0 for every x ‚àà V . The Jacobian matrix of œà ‚àí1 is given by Jœà ‚àí1 (y) =
p
Jœà(œà ‚àí1 (y))‚àí1 for all y ‚àà W .
Now note that œà(x) = (x1 , ..., xp‚àí1 , 0) for all x ‚àà V ‚à© ‚àÇS by the definition of dsS . Let U =
{(x1 , ..., xp‚àí1 ) ‚àà Rp‚àí1 : x ‚àà V ‚à© ‚àÇS} and œÜ : U ‚Üí Rp be a function such that œÜ(u) = œà ‚àí1 ((u, 0))
for all u ‚àà U . Below we verify that œÜ is one-to-one and continously differentiable, that JœÜ(u) is
of rank p ‚àí 1 for all u ‚àà U , that œÜ(U ) = V ‚à© ‚àÇS, and that U is open.
First, œÜ is one-to-one, since œà ‚àí1 is one-to-one, and (u, 0) 6= (u0 , 0) if u 6= u0 . Second, œÜ is
continuously differentiable, since œà ‚àí1 is so. The Jacobian matrix of œÜ at u ‚àà U is by definition
Ô£´ ‚àí1
Ô£∂
‚àÇœà1‚àí1
‚àÇœà1
((u,
0))
¬∑
¬∑
¬∑
((u,
0))
‚àÇyp‚àí1
Ô£¨ ‚àÇy1
Ô£∑
Ô£¨
Ô£∑
..
..
.
.
JœÜ(u) = Ô£¨
Ô£∑.
.
.
Ô£≠ ‚Äò .
Ô£∏
‚àÇœàp ‚àí1
‚àÇœàp‚àí1
‚àÇy1 ((u, 0)) ¬∑ ¬∑ ¬∑ ‚àÇyp‚àí1 ((u, 0))

Note that this is the left p √ó (p ‚àí 1) submatrix of Jœà ‚àí1 ((u, 0)). Since Jœà ‚àí1 ((u, 0)) has full rank,
JœÜ(u) is of rank p ‚àí 1. Moreover,
œÜ(U ) = {œà ‚àí1 ((u, 0)) : u ‚àà U }
= {œà ‚àí1 ((x1 , ..., xp‚àí1 , 0)) : x ‚àà V ‚à© ‚àÇS}
= {œà ‚àí1 (œà(x)) : x ‚àà V ‚à© ‚àÇS}
= V ‚à© ‚àÇS.
Lastly, we show that U is open. Pick any uÃÑ ‚àà U . Then, there exists xÃÑp ‚àà R such that
‚àÇds ((uÃÑ,xÃÑ ))
(uÃÑ, xÃÑp ) ‚àà V ‚à© ‚àÇS. As (uÃÑ, xÃÑp ) ‚àà V ‚à© ‚àÇS, dsS ((uÃÑ, xÃÑp )) = 0. Since S ‚àÇxp p 6= 0, it follows by the
Implicit Function Theorem that there exist an open set S ‚äÇ Rp‚àí1 containing uÃÑ and a continuously
differentiable function g : S ‚Üí R such that g(uÃÑ) = xÃÑp and dsS (u, g(u)) = 0 for all u ‚àà S. Since
g is continuous, (uÃÑ, g(uÃÑ)) ‚àà V and V is open, there exists an open set S 0 ‚äÇ S containing uÃÑ such
that (u, g(u)) ‚àà V for all u ‚àà S 0 . By the definition of dsS , dsS (x) = 0 if and only if x ‚àà ‚àÇS.
Therefore, if u ‚àà S 0 , (u, g(u)) must be contained by ‚àÇS, for otherwise dsS (u, g(u)) 6= 0, which is
a contradiction. Thus, (u, g(u)) ‚àà V ‚à© ‚àÇS and hence u ‚àà U for all u ‚àà S 0 . This implies that S 0
is an open subset of U containing uÃÑ, which proves that U is open.

B.3

Geometric Measure Theory

We provide some concepts and facts from geometric measure theory, following Krantz and Parks
(2008). Recall that for a function f : S ‚äÇ Rm ‚Üí Rn and a point x ‚àà S at which f is differentiable,
Jf (x) denotes the Jacobian matrix of f at x.
Lemma B.3 (Coarea Formula, Lemma 5.1.4 and Corollary 5.2.6 of Krantz and Parks (2008)).
If f : Rm ‚Üí Rn is a Lipschitz function and m ‚â• n, then
Z
Z Z
g(x)Jn f (x)dLm (x) =
g(x)dHm‚àín (x)dLn (y)
S

Rn

{x0 ‚ààS:f (x0 )=y}

51

for every Lebesgue measurable subset S of Rm and every Lm -measurable function g : S ‚Üí R,
where for each x ‚àà Rm at which f is differentiable,
p
Jn f (x) = det((Jf (x))(Jf (x))0 ).
Let S be an m-dimensional C 1 submanifold of Rn . Let x ‚àà S and let œÜ : U ‚äÇ Rm ‚Üí Rn be
as in the definition of m-dimensional C 1 submanifold. We denote by TS (x) the tangent space of
S at x, {JœÜ(u)v : v ‚àà Rm }, where u = œÜ‚àí1 (x).
Lemma B.4 (Area Formula, Lemma 5.3.5 and Theorem 5.3.7 of Krantz and Parks (2008)).
Suppose m ‚â§ ŒΩ and f : Rn ‚Üí RŒΩ is Lipschitz. If S is an m-dimensional C 1 submanifold of Rn ,
then
Z
Z
X
S
m
g(x)Jm f (x)dH (x) =
g(x)dHm (y)
RŒΩ x‚ààS:f (x)=y

S

for every Hm -measurable function g : S ‚Üí R, where for each x ‚àà Rn at which f is differentiable,
S
Jm
f (x) =

Hm ({Jf (x)y : y ‚àà P })
Hm (P )

for an arbitrary m-dimensional parallelepiped P contained in TS (x).
Let S ‚äÇ Rp . For each x ‚àà Rp at which dsS is differentiable and for each Œª ‚àà R, let œàS (x, Œª) =
x + Œª‚àádsS (x).
Lemma B.5. Let ‚Ñ¶ ‚äÇ Rp , and suppose that there exists a partition {‚Ñ¶1 , ..., ‚Ñ¶M } of ‚Ñ¶ such that
(i) dist(‚Ñ¶m , ‚Ñ¶m0 ) > 0 for any m, m0 ‚àà {1, ..., M } such that m 6= m0 ;
(ii) ‚Ñ¶m is nonempty, bounded, open, connected and twice continuously differentiable for each
m ‚àà {1, ..., M }.
Then there exists ¬µ > 0 such that ds‚Ñ¶ is twice continuously differentiable on N (‚àÇ‚Ñ¶, ¬µ) and that
Z

Z

Œ¥

Z

g(x)dx =
N (‚àÇ‚Ñ¶,Œ¥)

‚àíŒ¥

‚àÇ‚Ñ¶
g(u + ŒªŒΩ‚Ñ¶ (u))Jp‚àí1
œà‚Ñ¶ (u, Œª)dHp‚àí1 (u)dŒª

‚àÇ‚Ñ¶

for every Œ¥ ‚àà (0, ¬µ) and every function g : Rp ‚Üí R that is integrable on N (‚àÇ‚Ñ¶, Œ¥), where for
‚àÇ‚Ñ¶ œà (¬∑, Œª) is calculated by applying the operation J ‚àÇ‚Ñ¶ to the function
each fixed Œª ‚àà (‚àí¬µ, ¬µ), Jp‚àí1
‚Ñ¶
p‚àí1
‚àÇ‚Ñ¶
‚àÇ‚Ñ¶ œà (x, 0) = 1 for
œà‚Ñ¶ (¬∑, Œª). Futhermore, Jp‚àí1 œà‚Ñ¶ (x, ¬∑) is continuously differentiable in Œª and Jp‚àí1
‚Ñ¶
‚àÇ‚Ñ¶ œà (¬∑, ¬∑) and
every x ‚àà ‚àÇ‚Ñ¶, and Jp‚àí1
‚Ñ¶

‚àÇ‚Ñ¶ œà (¬∑,¬∑)
‚àÇJp‚àí1
‚Ñ¶
‚àÇŒª

are bounded on ‚àÇ‚Ñ¶ √ó (‚àí¬µ, ¬µ).

Proof. Let ¬µÃÑ = 21 minm,m0 ‚àà{1,...,M },m6=m0 dist(‚Ñ¶‚àóm , ‚Ñ¶m0 ) so that {N (‚àÇ‚Ñ¶m , ¬µÃÑ)}M
m=1 is a partition of
s
s
N (‚àÇ‚Ñ¶, ¬µÃÑ). Note that for every m ‚àà {1, ..., M }, d‚Ñ¶ (x) = d‚Ñ¶m (x) for every x ‚àà N (‚àÇ‚Ñ¶m , ¬µÃÑ). By
Lemma B.1, for every m ‚àà {1, ..., M }, there exists ¬µÃÑm > 0 such that ds‚Ñ¶m is twice continuously
differentiable on N (‚àÇ‚Ñ¶m , ¬µÃÑm ). Letting ¬µ ‚àà (0, min{¬µÃÑ, ¬µÃÑ1 , ..., ¬µÃÑM }), we have that ds‚Ñ¶ is twice

52

continuously differentiable on N (‚àÇ‚Ñ¶, ¬µ). This implies that ds‚Ñ¶ is Lipschitz on N (‚àÇ‚Ñ¶, ¬µ). For
every Œ¥ ‚àà (0, ¬µ) and every function g : Rp ‚Üí R that is integrable on N (‚àÇ‚Ñ¶, Œ¥),
Z
Z
q
g(x) det(k‚àáds‚Ñ¶ (x)k)dx
g(x)dx =
{x0 ‚ààRp :ds‚Ñ¶ (x0 )‚àà(‚àíŒ¥,Œ¥)}

N (‚àÇ‚Ñ¶,Œ¥)

Z
=
{x0 ‚ààRp :ds‚Ñ¶ (x0 )‚àà(‚àíŒ¥,Œ¥)}

Z
=
{x0 ‚ààRp :ds‚Ñ¶ (x0 )‚àà(‚àíŒ¥,Œ¥)}

q
g(x) det(‚àáds‚Ñ¶ (x)0 ‚àáds‚Ñ¶ (x))dx
q
g(x) det((Jds‚Ñ¶ (x))(Jds‚Ñ¶ (x))0 )dx

Z Z

g(x)dHp‚àí1 (x)dŒª

=
R

Z

Œ¥

{x0 ‚ààRp :ds‚Ñ¶ (x0 )‚àà(‚àíŒ¥,Œ¥),ds‚Ñ¶ (x0 )=Œª}

Z

g(x)dHp‚àí1 (x)dŒª,

=
‚àíŒ¥

{x0 ‚ààRp :ds‚Ñ¶ (x0 )=Œª}

(12)

where the first equality follows since k‚àáds‚Ñ¶ (x)k = 1 for every x ‚àà N (‚àÇ‚Ñ¶, Œ¥) by Lemma B.1, the
third equality follows from the definition of the Jacobian matrix, and the fourth equality follows
from Lemma B.3.
Let Œì(Œª) = {x ‚àà Rp : ds‚Ñ¶ (x) = Œª} for each Œª ‚àà (‚àí¬µ, ¬µ). Since ‚àáds‚Ñ¶ is differentiable on
N (‚àÇ‚Ñ¶, ¬µ), œà‚Ñ¶ (x, Œª) is defined on N (‚àÇ‚Ñ¶, ¬µ) √ó R. We show that {œà‚Ñ¶ (x0 , Œª) : x0 ‚àà ‚àÇ‚Ñ¶} ‚äÇ Œì(Œª) for
every Œª ‚àà (‚àí¬µ, ¬µ). By Lemma B.1, for every x0 ‚àà ‚àÇ‚Ñ¶, œà‚Ñ¶ (x0 , Œª) = x0 + ŒªŒΩ‚Ñ¶ (x0 ) and
Œ†‚àÇ‚Ñ¶ (œà‚Ñ¶ (x0 , Œª)) = Œ†‚àÇ‚Ñ¶ (x0 + ŒªŒΩ‚Ñ¶ (x0 )) = {x0 }.
Hence,
d(œà‚Ñ¶ (x0 , Œª), ‚àÇ‚Ñ¶) = kœà‚Ñ¶ (x0 , Œª) ‚àí x0 k = kŒªŒΩ‚Ñ¶ (x0 )k = |Œª|.
Since ŒΩ‚Ñ¶ (x0 ) is an inward normal vector, œà‚Ñ¶ (x0 , Œª) ‚àà cl(‚Ñ¶) if 0 ‚â§ Œª < ¬µ, and œà‚Ñ¶ (x, Œª0 ) ‚àà
Rp \ cl(‚Ñ¶) if ‚àí¬µ < Œª < 0. It follows that
ds‚Ñ¶ (œà‚Ñ¶ (x0 , Œª)) =

(
|Œª|
‚àí|Œª|

if 0 ‚â§ Œª < ¬µ
if ¬µ < Œª < 0

= Œª,
so {œà‚Ñ¶ (x0 , Œª) : x0 ‚àà ‚àÇ‚Ñ¶} ‚äÇ Œì(Œª). It also holds that Œì(Œª) ‚äÇ {œà‚Ñ¶ (x0 , Œª) : x0 ‚àà ‚àÇ‚Ñ¶}, since by
Lemma B.1, for every x ‚àà Œì(Œª),
œà‚Ñ¶ (œÄ‚àÇ‚Ñ¶ (x), Œª) = œÄ‚àÇ‚Ñ¶ (x) + Œª‚àáds‚Ñ¶ (œÄ‚àÇ‚Ñ¶ (x)) = œÄ‚àÇ‚Ñ¶ (x) + ds‚Ñ¶ (x)ŒΩ‚Ñ¶ (œÄ‚àÇ‚Ñ¶ (x)) = x,
where œÄ‚àÇ‚Ñ¶ (x) is the unique element in Œ†‚àÇ‚Ñ¶ (x). Thus, {œà‚Ñ¶ (x0 , Œª) : x0 ‚àà ‚àÇ‚Ñ¶} = Œì(Œª).
0
Now note that {‚àÇ‚Ñ¶m }M
m=1 is a partition of ‚àÇ‚Ñ¶, since dist(‚Ñ¶m , ‚Ñ¶m0 ) > 0 for any m, m ‚àà
{1, ..., M } such that m 6= m0 . By Lemma B.2, ‚àÇ‚Ñ¶m is a (p ‚àí 1)-dimensional C 1 submanifold
of Rp for every m ‚àà {1, ..., M }, and hence ‚àÇ‚Ñ¶ is a (p ‚àí 1)-dimensional C 1 submanifold of
Rp . Furthermore, since ‚àáds‚Ñ¶ is continuously differentiable on N (‚àÇ‚Ñ¶, ¬µ), œà‚Ñ¶ (¬∑, Œª) is continuously
53

differentiable on N (‚àÇ‚Ñ¶, ¬µ), which implies that œà‚Ñ¶ (¬∑, Œª) is Lipschitz on N (‚àÇ‚Ñ¶, ¬µ) for every Œª ‚àà R.
Applying Lemma B.4, we have that for every Œª ‚àà (‚àí¬µ, ¬µ),
Z
Z
‚àÇ‚Ñ¶
‚àÇ‚Ñ¶
p‚àí1
g(œà‚Ñ¶ (u, Œª))Jp‚àí1
œà‚Ñ¶ (u, Œª)dHp‚àí1 (u)
g(u + ŒªŒΩ‚Ñ¶ (u))Jp‚àí1 œà‚Ñ¶ (u, Œª)dH (u) =
‚àÇ‚Ñ¶
‚àÇ‚Ñ¶
Z
X
g(œà‚Ñ¶ (u, Œª))dHp‚àí1 (x).
(13)
=
Rp u‚àà‚àÇ‚Ñ¶:œà (u,Œª)=x
‚Ñ¶

If x ‚àà
/ {œà‚Ñ¶ (u, Œª) : u ‚àà ‚àÇ‚Ñ¶}, {u ‚àà ‚àÇ‚Ñ¶ : œà‚Ñ¶ (u, Œª) = x} = ‚àÖ. If x ‚àà {œà‚Ñ¶ (u, Œª) : u ‚àà ‚àÇ‚Ñ¶}, there exists
u ‚àà ‚àÇ‚Ñ¶ such that x = œà‚Ñ¶ (u, Œª). Since Œ†‚àÇ‚Ñ¶ (x) = Œ†‚àÇ‚Ñ¶ (u + Œª‚àáds‚Ñ¶ (u)) = Œ†‚àÇ‚Ñ¶ (u + ŒªŒΩ‚Ñ¶ (u)) = {u}
by Lemma B.1, such u is unique, and hence {u ‚àà ‚àÇ‚Ñ¶ : œà‚Ñ¶ (u, Œª) = x} is a singleton. It follow
that
Z
Z
X
p‚àí1
g(x)dHp‚àí1 (x)
g(œà‚Ñ¶ (u, Œª))dH (x) =
Rp u‚àà‚àÇ‚Ñ¶:œà (u,Œª)=x
‚Ñ¶

{œà‚Ñ¶ (u,Œª):u‚àà‚àÇ‚Ñ¶}

Z
=

g(x)dHp‚àí1 (x),

(14)

Œì(Œª)

where the last equality holds since {œà‚Ñ¶ (u, Œª) : u ‚àà ‚àÇ‚Ñ¶} = Œì(Œª). Combining (12), (13) and (14),
we obtain
Z
Z Œ¥Z
‚àÇ‚Ñ¶
g(x)dx =
g(u + ŒªŒΩ‚Ñ¶ (u))Jp‚àí1
œà‚Ñ¶ (u, Œª)dHp‚àí1 (u)dŒª.
N (‚àÇ‚Ñ¶,Œ¥)

‚àíŒ¥

‚àÇ‚Ñ¶

‚àÇ‚Ñ¶ œà (x, ¬∑) is continuously differentiable in Œª and J ‚àÇ‚Ñ¶ œà (x, 0) = 1
We next show that Jp‚àí1
‚Ñ¶
p‚àí1 ‚Ñ¶
for every x ‚àà ‚àÇ‚Ñ¶. Fix an x ‚àà ‚àÇ‚Ñ¶, and let V‚Ñ¶ (x) be an arbitrary p √ó (p ‚àí 1) matrix whose
columns v1 (x), ..., vp‚àí1 (x) ‚àà Rp form an orthonormal basis of T‚àÇ‚Ñ¶ (x). Let P (x) ‚äÇ T‚àÇ‚Ñ¶ (x)
P
be a parallelepiped determined by v1 (x), ..., vp‚àí1 (x), that is, let P (x) = { p‚àí1
k=1 ck vk (x) : 0 ‚â§
ck ‚â§ 1 for k = 1, ..., p ‚àí 1}. Since v1 (x), ..., vp‚àí1 (x) are linearly independent, P (x) is a (p ‚àí 1)dimensional parallelepiped. It follows that for each fixed Œª ‚àà R,

{Jœà‚Ñ¶ (x, Œª)y : y ‚àà P (x)} = {Jœà‚Ñ¶ (x, Œª)

p‚àí1
X

ck vk (x) : 0 ‚â§ ck ‚â§ 1 for k = 1, ..., p ‚àí 1}

k=1

={

={

p‚àí1
X
k=1
p‚àí1
X

ck Jœà‚Ñ¶ (x, Œª)vk (x) : 0 ‚â§ ck ‚â§ 1 for k = 1, ..., p ‚àí 1}
ck wk (x, Œª) : 0 ‚â§ ck ‚â§ 1 for k = 1, ..., p ‚àí 1},

k=1

where wk (x, Œª) = Jœà‚Ñ¶ (x, Œª)vk (x) for k = 1, ..., p ‚àí 1. Since Jœà‚Ñ¶ (x, Œª)vk (x) is the k-th column
of Jœà‚Ñ¶ (x, Œª)V‚Ñ¶ (x), {Jœà‚Ñ¶ (x, Œª)y : y ‚àà P (x)} is the parallelepiped determined by the columns of

54

Jœà‚Ñ¶ (x, Œª)V‚Ñ¶ (x). By Proposition 5.1.2 of Krantz and Parks (2008), we have that
‚àÇ‚Ñ¶
œà‚Ñ¶ (x, Œª)
Jp‚àí1

=

Hp‚àí1 ({

Pp‚àí1

: 0 ‚â§ ck ‚â§ 1 for k = 1, ..., p ‚àí 1})
Hp‚àí1 (P (x))

k=1 ck wk (x, Œª)

p
det((Jœà‚Ñ¶ (x, Œª)V‚Ñ¶ (x))0 (Jœà‚Ñ¶ (x, Œª)V‚Ñ¶ (x)))
p
=
det(V‚Ñ¶ (x)0 V‚Ñ¶ (x))
p
det((V‚Ñ¶ (x) + ŒªD2 ds‚Ñ¶ (x)V‚Ñ¶ (x))0 (V‚Ñ¶ (x) + ŒªD2 ds‚Ñ¶ (x)V‚Ñ¶ (x)))
p
=
det(Ip‚àí1 )
q
= det(V‚Ñ¶ (x)0 V‚Ñ¶ (x) + 2V‚Ñ¶ (x)0 ŒªD2 ds‚Ñ¶ (x)V‚Ñ¶ (x) + V‚Ñ¶ (x)0 (ŒªD2 ds‚Ñ¶ (x))2 V‚Ñ¶ (x))
q
= det(Ip‚àí1 + ŒªV‚Ñ¶ (x)0 (2D2 ds‚Ñ¶ (x) + Œª(D2 ds‚Ñ¶ (x))2 )V‚Ñ¶ (x)))
q
= det(Ip + ŒªV‚Ñ¶ (x)V‚Ñ¶ (x)0 (2D2 ds‚Ñ¶ (x) + Œª(D2 ds‚Ñ¶ (x))2 )),
where we use the fact that V‚Ñ¶ (x)0 V‚Ñ¶ (x) = Ip‚àí1 and the fact that det(Im + AB) = det(In + BA)
for an m √ó n matrix A and an n √ó m matrix B (the Weinstein-Aronszajn identity). For every
p
‚àÇ‚Ñ¶ œà (x, ¬∑) is continuously differentiable in Œª, and J ‚àÇ‚Ñ¶ œà (x, 0) =
det(Ip ) = 1.
x ‚àà ‚àÇ‚Ñ¶, Jp‚àí1
‚Ñ¶
p‚àí1 ‚Ñ¶
‚àÇJ ‚àÇ‚Ñ¶ œà‚Ñ¶ (¬∑,¬∑)

p‚àí1
‚àÇ‚Ñ¶ œà (¬∑, ¬∑) and
Lastly, we show that Jp‚àí1
‚Ñ¶
‚àÇŒª
‚àÇ‚Ñ¶ √ó Rp√ó(p‚àí1) ‚Üí Rp√óp be functions such that

are bounded on ‚àÇ‚Ñ¶ √ó (‚àí¬µ, ¬µ). Let f, h :

f (x, A) = 2AA0 D2 ds‚Ñ¶ (x),
h(x, A) = AA0 (D2 ds‚Ñ¶ (x))2 .
Also, let k : ‚àÇ‚Ñ¶ √ó R √ó Rp√ó(p‚àí1) ‚Üí R be a function such that
q
k(x, Œª, A) = det(Ip + Œªf (x, A) + Œª2 h(x, A)).
Observe that
‚àÇ‚Ñ¶
Jp‚àí1
œà‚Ñ¶ (x, Œª) = k(x, Œª, V‚Ñ¶ (x))

and that
‚àÇ‚Ñ¶ œà (x, Œª)
‚àÇJp‚àí1
‚Ñ¶
‚àÇŒª
‚àÇk(x, Œª, A)
=
‚àÇŒª
A=V‚Ñ¶ (x)

=

X ‚àÇdet(Ip + Œªf (x, A) + Œª2 h(x, A))
1
(fij (x, A) + 2Œªhij (x, A))
2k(x, Œª, A)
‚àÇbij
i,j

,
A=V‚Ñ¶ (x)

where ‚àÇdet(B)
denotes the partial derivative of the function det : Rp√óp ‚Üí R with respect to the
‚àÇbij
(i, j) entry of B.
Note that k(¬∑, ¬∑, ¬∑) and ‚àÇk(¬∑,¬∑,¬∑)
are continuous on ‚àÇ‚Ñ¶ √ó R √ó Rp√ó(p‚àí1) (except at the points
‚àÇŒª
for which k(x, Œª, A) = 0), since det is infinitely differentiable, and f and h are continuous on
55

‚àÇ‚Ñ¶ √ó Rp√ó(p‚àí1) . Let S = {(x, Œª, A) ‚àà ‚àÇ‚Ñ¶ √ó [‚àí¬µ, ¬µ] √ó Rp√ó(p‚àí1) : kaj k = 1 for k = 1, ..., p ‚àí 1},
where aj denotes the jth column of A. Since k(¬∑, ¬∑, ¬∑) and ‚àÇk(¬∑,¬∑,¬∑)
are continuous and S is
‚àÇŒª
0
closed and bounded, kÃÑ = max(x,Œª,A)‚ààS |k(x, Œª, A)| and kÃÑ = max(x,Œª,A)‚ààS | ‚àÇk(x,Œª,A)
| exist. Since
‚àÇŒª
‚àÇ‚Ñ¶ œà (x, Œª)| ‚â§ kÃÑ and
(x, Œª, V‚Ñ¶ (x)) ‚àà S for every (x, Œª) ‚àà ‚àÇ‚Ñ¶ √ó (‚àí¬µ, ¬µ), it follows that |Jp‚àí1
‚Ñ¶
|

‚àÇ‚Ñ¶ œà (x,Œª)
‚àÇJp‚àí1
‚Ñ¶
|
‚àÇŒª

B.4

‚â§ kÃÑ 0 for every (x, Œª) ‚àà ‚àÇ‚Ñ¶ √ó (‚àí¬µ, ¬µ).

Other Lemmas

2
Lemma B.6. Let {Vi }‚àû
i=1 be i.i.d. random variables such that E[Vi ] < ‚àû. If Assumption 1
holds, then for l ‚â• 0 and m = 0, 1,

E[Vi pA (Xi ; Œ¥)l 1{pA (Xi ; Œ¥) ‚àà (0, 1)}m ] ‚Üí E[Vi A(Xi )l 1{A(Xi ) ‚àà (0, 1)}m ]
as Œ¥ ‚Üí 0. Moreover, if, in addition, Œ¥n ‚Üí 0 as n ‚Üí ‚àû, then for l ‚â• 0,
n

1X
p
Vi pA (Xi ; Œ¥n )l Ii,n ‚àí‚Üí E[Vi A(Xi )l 1{A(Xi ) ‚àà (0, 1)}]
n
i=1

as n ‚Üí ‚àû.
Proof. Note that E[ n1
that

Pn

A
l
i=1 Vi p (Xi ; Œ¥n ) Ii,n ]

= E[Vi pA (Xi ; Œ¥n )l 1{pA (Xi ; Œ¥n ) ‚àà (0, 1)}]. We show

E[Vi pA (Xi ; Œ¥)l 1{pA (Xi ; Œ¥) ‚àà (0, 1)}m ] ‚Üí E[Vi A(Xi )l 1{A(Xi ) ‚àà (0, 1)}m ]
for l ‚â• 0 and m = 0, 1 as Œ¥ ‚Üí 0, and that
n

Var(

1X
Vi pA (Xi ; Œ¥n )l Ii,n ) ‚Üí 0
n
i=1

for l ‚â• 0 as n ‚Üí ‚àû. For the first part, we have
Z
A
l
A
m
E[Vi p (Xi ; Œ¥) 1{p (Xi ; Œ¥) ‚àà (0, 1)} ] =
E[Vi |Xi = x]pA (x; Œ¥)l 1{pA (x; Œ¥) ‚àà (0, 1)}m fX (x)dx.
X

Suppose A is continuous at x and A(x) ‚àà (0, 1). Then limŒ¥‚Üí0 pA (x; Œ¥) = A(x) by Part 1 of
Corollary A.1, and hence pA (x; Œ¥) ‚àà (0, 1) for sufficiently small Œ¥ > 0. It follows that 1{pA (x; Œ¥) ‚àà
(0, 1)} ‚Üí 1 = 1{A(x) ‚àà (0, 1)} as Œ¥ ‚Üí 0. Suppose x ‚àà int(X0 ) ‚à™ int(X1 ). Then B(x, Œ¥) ‚äÇ X0 or
B(x, Œ¥) ‚äÇ X1 for sufficiently small Œ¥ > 0 by the fact that int(X0 ) and int(X1 ) are open, and hence
1{pA (x; Œ¥) ‚àà (0, 1)} ‚Üí 0 = 1{A(x) ‚àà (0, 1)} as Œ¥ ‚Üí 0. Therefore, limŒ¥‚Üí0 pA (x; Œ¥) = A(x) and
limŒ¥‚Üí0 1{pA (x; Œ¥) ‚àà (0, 1)} = 1{A(x) ‚àà (0, 1)} for almost every x ‚àà X , since A is continuous at
x for almost every x ‚àà X by Assumption 1 (a), and either A(x) ‚àà (0, 1) or x ‚àà int(X0 ) ‚à™ int(X1 )
for almost every x ‚àà X by Assumption 1 (b). By the Dominated Convergence Theorem,
Z
A
l
A
m
E[Vi p (Xi ; Œ¥) 1{p (Xi ; Œ¥) ‚àà (0, 1)} ] ‚Üí
E[Vi |Xi = x]A(x)l 1{A(x) ‚àà (0, 1)}m fX (x)dx
X

= E[Vi A(Xi )l 1{A(Xi ) ‚àà (0, 1)}m ]
56

as Œ¥ ‚Üí 0. As for variance,
n

Var(

1X
1
Vi pA (Xi ; Œ¥n )l Ii,n ) ‚â§ E[Vi2 pA (Xi ; Œ¥n )2l (Ii,n )2 ]
n
n
i=1

1
E[Vi2 ]
n
‚Üí0
‚â§

as n ‚Üí ‚àû.
Lemma B.7. Let {(Œ¥n , Sn )}‚àû
n=1 be any sequence of positive numbers and positive integers. Fix
x ‚àà X , and let X1‚àó , ..., XS‚àón be Sn independent draws from the uniform distribution on B(x, Œ¥n )
so that
Sn
1 X
A(Xs‚àó ).
ps (x; Œ¥n ) =
Sn
s=1

Then,
E[ps (x; Œ¥n ) ‚àí pA (x; Œ¥n )] = 0,
1
E[(ps (x; Œ¥n ) ‚àí pA (x; Œ¥n ))2 ] ‚â§
,
Sn
1
|E[ps (x; Œ¥n )2 ‚àí pA (x; Œ¥n )2 ]| ‚â§
,
Sn
4
E[(ps (x; Œ¥n )2 ‚àí pA (x; Œ¥n )2 )2 ] ‚â§
,
Sn
Pr(ps (x; Œ¥n ) ‚àà {0, 1}) ‚â§ (1 ‚àí pA (x; Œ¥n ))Sn + pA (x; Œ¥n )Sn .
Moreover, for any  > 0,
E[|ps (x; Œ¥n ) ‚àí pA (x; Œ¥n )|] ‚â§

1
+ ,
Sn 2

and if Sn ‚Üí ‚àû, then
E[|ps (x; Œ¥n ) ‚àí pA (x; Œ¥n )|] ‚Üí 0
as n ‚Üí ‚àû.
Proof. By construction, E[A(Xs‚àó )] = pA (x; Œ¥n ), so
Sn
1 X
E[p (x; Œ¥n ) ‚àí p (x; Œ¥n )] = E[
A(Xs‚àó )] ‚àí pA (x; Œ¥n )
Sn
s

A

=

s=1
E[A(Xs‚àó )]

= 0.

57

‚àí pA (x; Œ¥n )

We have
E[(ps (x; Œ¥n ) ‚àí pA (x; Œ¥n ))2 ] = Var(ps (x; Œ¥n ))
= Var(

Sn
1 X
A(Xs‚àó ))
Sn
s=1

1
Var(A(Xs‚àó ))
=
Sn
1
‚â§
E[A(Xs‚àó )2 ]
Sn
1
‚â§
,
Sn
|E[ps (x; Œ¥n )2 ‚àí pA (x; Œ¥n )2 ]| = |Var(ps (x; Œ¥n )) + (E[ps (x; Œ¥n )])2 ‚àí pA (x; Œ¥n )2 |
1
‚â§
+ |(pA (x; Œ¥n ))2 ‚àí pA (x; Œ¥n )2 |
Sn
1
=
,
Sn
and
E[(ps (x; Œ¥n )2 ‚àí pA (x; Œ¥n )2 )2 ]
= E[(ps (x; Œ¥n ) + pA (x; Œ¥n ))2 (ps (x; Œ¥n ) ‚àí pA (x; Œ¥n ))2 ]
‚â§ 4E[(ps (x; Œ¥n ) ‚àí pA (x; Œ¥n ))2 ]
4
.
‚â§
Sn
Now note that we have the following bounds on Pr(A(Xs‚àó ) = 0) and Pr(A(Xs‚àó ) = 1):
0 ‚â§ Pr(A(Xs‚àó ) = 0) ‚â§ 1 ‚àí pA (x; Œ¥n ),
0 ‚â§ Pr(A(Xs‚àó ) = 1) ‚â§ pA (x; Œ¥n ).
It follows that
0 ‚â§ Pr(ps (x; Œ¥n ) ‚àà {0, 1})
= Pr(A(Xs‚àó ) = 0)Sn + Pr(A(Xs‚àó ) = 1)Sn
‚â§ (1 ‚àí pA (x; Œ¥n ))Sn + pA (x; Œ¥n )Sn .
Lastly, for any  > 0,
E[|ps (x; Œ¥n ) ‚àí pA (x; Œ¥n )|]
= E[|ps (x; Œ¥n ) ‚àí pA (x; Œ¥n )|||ps (x; Œ¥n ) ‚àí pA (x; Œ¥n )| ‚â• ] Pr(|ps (x; Œ¥n ) ‚àí pA (x; Œ¥n )| ‚â• )
+ E[|ps (x; Œ¥n ) ‚àí pA (x; Œ¥n )|||ps (x; Œ¥n ) ‚àí pA (x; Œ¥n )| < ] Pr(|ps (x; Œ¥n ) ‚àí pA (x; Œ¥n )| < )
Var(ps (x; Œ¥n ))
+¬∑1
2
1
‚â§
+ ,
Sn 2
< 1¬∑

58

where we use Chebyshev‚Äôs inequality for the first inequality. We can make E[|ps (x; Œ¥n ) ‚àí
pA (x; Œ¥n )|] arbitrarily close to zero by taking sufficiently small  > 0 and sufficiently large Sn ,
which implies that E[|ps (x; Œ¥n ) ‚àí pA (x; Œ¥n )|] = o(1) if Sn ‚Üí ‚àû.
s = 1{ps (X ; Œ¥ ) ‚àà (0, 1)}, and let {V }‚àû be i.i.d. random variables such
Lemma B.8. Let Ii,n
i n
i i=1
2
that E[Vi ] < ‚àû. If Assumption 1 holds, Sn ‚Üí ‚àû, and Œ¥n ‚Üí 0, then
n

n

i=1

i=1

1X
1X
s
Vi ps (Xi ; Œ¥n )l Ii,n
‚àí
Vi pA (Xi ; Œ¥n )l Ii,n = op (1)
n
n
for l = 0, 1, 2, 3, 4. If, in addition, Assumption 5 holds, and E[Vi |Xi ] is bounded, then
n

n

i=1

i=1

1 X
1 X
s
‚àö
Vi ps (Xi ; Œ¥n )l Ii,n
‚àí‚àö
Vi pA (Xi ; Œ¥n )l Ii,n = op (1)
n
n
for l = 0, 1, 2.
Proof. We have
n

n

i=1

i=1

1X
1X
s
Vi ps (Xi ; Œ¥n )l Ii,n
‚àí
Vi pA (Xi ; Œ¥n )l Ii,n
n
n
n
n
1X
1X
s
=
Vi ps (Xi ; Œ¥n )l (Ii,n
‚àí Ii,n ) +
Vi (ps (Xi ; Œ¥n )l ‚àí pA (Xi ; Œ¥n )l )Ii,n .
n
n
i=1

We first consider

i=1

1
n

Pn

s
l
i=1 Vi (p (Xi ; Œ¥n )

‚àí pA (Xi ; Œ¥n )l )Ii,n . By Lemma B.7, for l = 0, 1, 2,

n

|E[

1X
Vi (ps (Xi ; Œ¥n )l ‚àí pA (Xi ; Œ¥n )l )Ii,n ]|
n
i=1
s

= |E[Vi (p (Xi ; Œ¥n )l ‚àí pA (Xi ; Œ¥n )l )Ii,n ]|
= E[|E[Vi |Xi ]||E[ps (Xi ; Œ¥n )l ‚àí pA (Xi ; Œ¥n )l |Xi ]|Ii,n ]
1
‚â§
E[|E[Vi |Xi ]|Ii,n ]
Sn
= O(Sn‚àí1 ).
Also, by Lemma B.7,
n

|E[

1X
Vi (ps (Xi ; Œ¥n )3 ‚àí pA (Xi ; Œ¥n )3 )Ii,n ]|
n
i=1
s

= |E[Vi (p (Xi ; Œ¥n ) ‚àí pA (Xi ; Œ¥n ))(ps (Xi ; Œ¥n )2 + ps (Xi ; Œ¥n )pA (Xi ; Œ¥n ) + pA (Xi ; Œ¥n )2 )Ii,n ]|
‚â§ E[|E[Vi |Xi ]||E[(ps (Xi ; Œ¥n ) ‚àí pA (Xi ; Œ¥n ))(ps (Xi ; Œ¥n )2 + ps (Xi ; Œ¥n )pA (Xi ; Œ¥n ) + pA (Xi ; Œ¥n )2 )|Xi ]|Ii,n ]
‚â§ 3E[|E[Vi |Xi ]|E[|ps (Xi ; Œ¥n ) ‚àí pA (Xi ; Œ¥n )||Xi ]Ii,n ]
= o(1),
59

and
n

1X
|E[
Vi (ps (Xi ; Œ¥n )4 ‚àí pA (Xi ; Œ¥n )4 )Ii,n ]|
n
i=1
s

= |E[Vi (p (Xi ; Œ¥n )2 + pA (Xi ; Œ¥n )2 )(ps (Xi ; Œ¥n ) + pA (Xi ; Œ¥n ))(ps (Xi ; Œ¥n ) ‚àí pA (Xi ; Œ¥n ))Ii,n ]|
‚â§ E[|E[Vi |Xi ]||E[(ps (Xi ; Œ¥n )2 + pA (Xi ; Œ¥n )2 )(ps (Xi ; Œ¥n ) + pA (Xi ; Œ¥n ))(ps (Xi ; Œ¥n ) ‚àí pA (Xi ; Œ¥n ))|Xi ]|Ii,n ]
‚â§ 4E[|E[Vi |Xi ]|E[|ps (Xi ; Œ¥n ) ‚àí pA (Xi ; Œ¥n )||Xi ]Ii,n ]
= o(1).
As for variance, for l = 0, 1, 2,
n

Var(

1X
1
Vi (ps (Xi ; Œ¥n )l ‚àí pA (Xi ; Œ¥n )l )Ii,n ) ‚â§ E[Vi2 (ps (Xi ; Œ¥n )l ‚àí pA (Xi ; Œ¥n )l )2 Ii,n ]
n
n
i=1

1
E[E[Vi2 |Xi ]E[(ps (Xi ; Œ¥n )l ‚àí pA (Xi ; Œ¥n )l )2 |Xi ]Ii,n ]
n
4
E[E[Vi2 |Xi ]Ii,n ]
‚â§
nSn
= O((nSn )‚àí1 ),
‚â§

and for l = 3, 4,
n

Var(

1X
1
Vi (ps (Xi ; Œ¥n )l ‚àí pA (Xi ; Œ¥n )l )Ii,n ) ‚â§ E[Vi2 (ps (Xi ; Œ¥n )l ‚àí pA (Xi ; Œ¥n )l )2 Ii,n ]
n
n
i=1

1
E[Vi2 Ii,n ]
n
= o(1).

‚â§

P
Therefore, n1 ni=1 Vi (ps (Xi ; Œ¥n )l ‚àí pA (Xi ; Œ¥n )l )Ii,n = op (1) if Sn ‚Üí ‚àû for l = 0, 1, 2, 3, 4, and
Pn
s
l
A
l
‚àí1/2 S ‚Üí ‚àû for l = 0, 1, 2.
‚àö1
n
i=1 Vi (p (Xi ; Œ¥n ) ‚àí p (Xi ; Œ¥n ) )Ii,n = op (1) if n
n
P
n
1
s
l
s
We next show that n i=1 Vi p (Xi ; Œ¥n ) (Ii,n ‚àí Ii,n ) = op (1) if Sn ‚Üí ‚àû and Œ¥n ‚Üí 0 for l ‚â• 0.
We have
n

|E[

1X
s
s
Vi ps (Xi ; Œ¥n )l (Ii,n
‚àí Ii,n )]| = |E[Vi ps (Xi ; Œ¥n )l (Ii,n
‚àí Ii,n )]|
n
i=1

s
‚â§ E[|E[Vi |Xi ]||E[ps (Xi ; Œ¥n )l (Ii,n
‚àí Ii,n )|Xi ]|]
s
‚â§ E[|E[Vi |Xi ]|E[|Ii,n
‚àí Ii,n ||Xi ]].

Note that by construction, 1{ps (Xi ; Œ¥n ) ‚àà (0, 1)} ‚â§ 1{pA (Xi ; Œ¥n ) ‚àà (0, 1)} with probability one
conditional on Xi = x, so that
s
s
E[|Ii,n
‚àí Ii,n ||Xi = x] = ‚àíE[Ii,n
‚àí Ii,n |Xi = x].

Suppose A is continuous at x and A(x) ‚àà (0, 1). Then limŒ¥‚Üí0 pA (x; Œ¥) = A(x) ‚àà (0, 1) by Part 1
of Corollary A.1, and hence pA (x; Œ¥n ) ‚àà [, 1 ‚àí ] for sufficiently small Œ¥n > 0 for some constant
60

 ‚àà (0, 1/2). It follows that
s
E[Ii,n
|Xi = x] = 1 ‚àí Pr(ps (x; Œ¥n ) ‚àà {0, 1})

‚â• 1 ‚àí (1 ‚àí pA (x; Œ¥n ))Sn ‚àí pA (x; Œ¥n )Sn
‚â• 1 ‚àí 2(1 ‚àí )Sn
‚Üí1
s ‚àí
as Sn ‚Üí ‚àû, where the first inequality follows from Lemma B.7. This implies that E[Ii,n
Ii,n |Xi = x] ‚Üí 0 as n ‚Üí ‚àû. Suppose x ‚àà int(X0 )‚à™int(X1 ). Then B(x, Œ¥n ) ‚äÇ X0 or B(x, Œ¥n ) ‚äÇ X1
for sufficiently small Œ¥n > 0 by the fact that int(X0 ) and int(X1 ) are open, and hence pA (x; Œ¥n ) ‚àà
s ‚àí I |X = x] ‚Üí 0 as
{0, 1} and ps (x; Œ¥n ) ‚àà {0, 1} for sufficiently small Œ¥n > 0, so that E[Ii,n
i,n
i
s
n ‚Üí ‚àû. Therefore, E[Ii,n ‚àí Ii,n |Xi = x] ‚Üí 0 for almost every x ‚àà X , since A is continuous at x
for almost every x ‚àà X by Assumption 1 (a), and either A(x) ‚àà (0, 1) or x ‚àà int(X0 ) ‚à™ int(X1 )
for almost every x ‚àà X by Assumption 1 (b). By the Dominated Convergence Theorem,
s
‚àíE[|E[Vi |Xi ]|E[Ii,n
‚àí Ii,n |Xi ]] ‚Üí 0

as n ‚Üí ‚àû.
As for variance,
n

Var(

1
1X
s
s
Vi ps (Xi ; Œ¥n )l (Ii,n
‚àí Ii,n )) ‚â§ E[Vi2 ps (Xi ; Œ¥n )2l (Ii,n
‚àí Ii,n )2 ]
n
n
i=1

1
E[Vi2 ]
n
‚Üí 0.
‚â§

Lastly, we show that, for l ‚â• 0,
holds, and E[Vi |Xi ] is bounded. Let
We have

‚àö1
n

Pn

s
l s
i=1 Vi p (Xi ; Œ¥n ) (Ii,n ‚àí Ii,n )
n
Œ∑n = Œ≥ log
Sn , where Œ≥ is the one

= op (1) if Assumption 5
satisfying Assumption 5.

n

1 X
s
|E[ ‚àö
Vi ps (Xi ; Œ¥n )l (Ii,n
‚àí Ii,n )]|
n
i=1
‚àö
s
‚àí Ii,n ||Xi ]]
‚â§ nE[|E[Vi |Xi ]|E[|Ii,n
‚àö
s
= ‚àí nE[|E[Vi |Xi ]|E[Ii,n ‚àí 1|Xi ]Ii,n ]
‚àö
‚â§ nE[|E[Vi |Xi ]|((1 ‚àí pA (Xi ; Œ¥n ))Sn + pA (Xi ; Œ¥n )Sn ))Ii,n ]
‚àö
= nE[|E[Vi |Xi ]|((1 ‚àí pA (Xi ; Œ¥n ))Sn + pA (Xi ; Œ¥n )Sn ))1{pA (Xi ; Œ¥n ) ‚àà (0, Œ∑n ) ‚à™ (1 ‚àí Œ∑n , 1)}]
‚àö
+ nE[|E[Vi |Xi ]|((1 ‚àí pA (Xi ; Œ¥n ))Sn + pA (Xi ; Œ¥n )Sn ))1{pA (Xi ; Œ¥n ) ‚àà [Œ∑n , 1 ‚àí Œ∑n ]}]
‚àö
‚àö
‚â§ (sup |E[Vi |Xi = x]|)( n Pr(pA (Xi ; Œ¥n ) ‚àà (0, Œ∑n ) ‚à™ (1 ‚àí Œ∑n , 1)) + 2 n(1 ‚àí Œ∑n )Sn ),
x‚ààX
s ‚â§ I
where the second equality follows from the fact that Ii,n
i,n with strict inequality only if
‚àö
‚àö
A
Ii,n = 1. By Assumption 5, n Pr(p (Xi ; Œ¥n ) ‚àà (0, Œ∑n ) ‚à™ (1 ‚àí Œ∑n , 1)) = o(1). As for n(1 ‚àí Œ∑n )Sn ,
n
log n
1
‚àí1/2 S ‚Üí ‚àû and log n ‚Üí 0. Using the
first observe that Œ∑n = Œ≥ log
n
Sn = Œ≥ n1/2 n‚àí1/2 S ‚Üí 0, since n
n1/2
n

61

fact that et ‚â• 1 + t for every t ‚àà R, we have
‚àö
‚àö
n(1 ‚àí Œ∑n )Sn ‚â§ n(e‚àíŒ∑n )Sn
‚àö
= ne‚àíŒ∑n Sn
‚àö
= ne‚àíŒ≥ log n
‚àö
= nn‚àíŒ≥
= n1/2‚àíŒ≥
‚Üí 0,
since Œ≥ > 1/2. As for variance,
n

1 X
s
s
Var( ‚àö
Vi ps (Xi ; Œ¥n )l (Ii,n
‚àí Ii,n )) ‚â§ E[Vi2 ps (Xi ; Œ¥n )2l (Ii,n
‚àí Ii,n )2 ]
n
i=1

s
‚â§ E[Vi2 |Ii,n
‚àí Ii,n |]
s
= E[E[Vi2 |Xi ]E[|Ii,n
‚àí Ii,n ||Xi ]]

= o(1).

C
C.1

Proofs
Proof of Proposition 1

Suppose that Assumptions 1 and 2 hold. Here, we only show that
(a) E[Y1i ‚àí Y0i |Xi = x] is identified for every x ‚àà int(X ) such that pA (x) ‚àà (0, 1).
(b) Let S be any open subset of X such that pA (x) exists for all x ‚àà S. Then E[Y1i ‚àíY0i |Xi ‚àà S]
is identified only if pA (x) ‚àà (0, 1) for almost every x ‚àà S.
The results for E[Di (1) ‚àí Di (0)|Xi = x] and E[Di (1) ‚àí Di (0)|Xi ‚àà S] are obtained by a similar
argument.
Proof of Part (a). Pick an x ‚àà int(X ) such that pA (x) ‚àà (0, 1). If A(x) ‚àà (0, 1), E[Y1i ‚àí
Y0i |Xi = x] and E[Di (1) ‚àí Di (0)|Xi = x] are trivially identified by Property 1:
E[Yi |Xi = x, Zi = 1] ‚àí E[Yi |Xi = x, Zi = 0] = E[Y1i ‚àí Y0i |Xi = x].
We next consider the case where A(x) ‚àà {0, 1}. Since x ‚àà int(X ), B(x, Œ¥) ‚äÇ X for any
sufficiently small Œ¥ > 0. Moreover, since pA (x) = limŒ¥‚Üí0 pA (x; Œ¥) ‚àà (0, 1), pA (x; Œ¥) ‚àà (0, 1) for
any sufficiently small Œ¥ > 0. This implies that we can find points x0,Œ¥ , x1,Œ¥ ‚àà B(x, Œ¥)(‚äÇ X ) such
that A(x0,Œ¥ ) < 1 and A(x1,Œ¥ ) > 0 for any sufficiently small Œ¥ > 0, for otherwise pA (x; Œ¥) ‚àà {0, 1}.
Noting that x0,Œ¥ ‚Üí x and x1,Œ¥ ‚Üí x as Œ¥ ‚Üí 0,
lim (E[Yi |Xi = x1,Œ¥ , Zi = 1] ‚àí E[Yi |Xi = x0,Œ¥ , Zi = 0]) = lim (E[Yi1 |Xi = x1,Œ¥ ] ‚àí E[Yi0 |Xi = x0,Œ¥ ])

Œ¥‚Üí0

Œ¥‚Üí0

= E[Y1i ‚àí Y0i |Xi = x],
62

where the first equality follows from Property 1, and the second from Assumption 2.
Proof of Part (b).
Suppose to the contrary that Lp ({x ‚àà S : pA (x) ‚àà {0, 1}}) > 0. Without loss of generality,
assume Lp ({x ‚àà S : pA (x) = 1}) > 0. The proof proceeds in four steps.
Step C.1.1. Lp (S ‚à© X1 ) > 0.
Proof. By Assumption 1, A is continuous almost everywhere. Part 1 of Cororally A.1 then
implies that pA (x) = A(x) for almost every x ‚àà {x‚àó ‚àà S : pA (x‚àó ) = 1}. Since Lp ({x ‚àà S :
pA (x) = 1}) > 0, Lp ({x ‚àà S : pA (x) = 1, pA (x) = A(x)}) > 0, and hence Lp (S ‚à© X1 ) > 0.
Step C.1.2. S ‚à© int(X1 ) 6= ‚àÖ.
Proof. Suppose that S ‚à© int(X1 ) = ‚àÖ. Then, we must have that S ‚à© X1 ‚äÇ X1 \ int(X1 ). It then
follows that Lp (S ‚à© X1 ) ‚â§ Lp (X1 \ int(X1 )) = Lp (X1 ) ‚àí Lp (int(X1 )) = 0, where the last equality
holds by Assumption 1. But this is a contradiction to the result from Step C.1.1.
Step C.1.3. pA (x) = 1 for any x ‚àà int(X1 ).
Proof. Pick any x ‚àà int(X1 ). By the definition of interior, B(x, Œ¥) ‚äÇ X1 for any sufficiently small
Œ¥ > 0. Therefore, pA (x; Œ¥) = 1 for any sufficiently small Œ¥ > 0.
Step C.1.4. E[Y1i ‚àí Y0i |Xi ‚àà S] is not identified.
Proof. We first introduce some notation. Let Q be the set of all distributions of (Y1i , Y0i , Xi , Zi )
satisfying Property 1 and Assumptions 1 and 2. Let P be the set of all distributions of (Yi , Xi , Zi ).
Let T : Q ‚Üí P be a function such that, for Q ‚àà Q, T (Q) is the distribution of (Zi Y1i + (1 ‚àí
Zi )Y0i , Xi , Zi ), where the distribution of (Y1i , Y0i , Xi , Zi ) is Q. Let Q0 and P0 denote the true
distributions of (Y1i , Y0i , Xi , Zi ) and (Yi , Xi , Zi ), respectively. Given P0 , the identified set of
E[Y1i ‚àí Y0i |Xi ‚àà S] is given by {EQ [Y1i ‚àí Y0i |Xi ‚àà S] : P0 = T (Q), Q ‚àà Q}, where EQ [¬∑] is the
expectation operator under distribution Q. We show that this set contains two distinct values.
In what follows, Pr(¬∑) and E[¬∑] without a subscript denote the probability and expectation under
the true distributions Q0 and P0 as up until now.
Now pick any x‚àó ‚àà S ‚à© int(X1 ). Since A and int(X1 ) are open, there is some Œ¥ > 0 such
that B(x‚àó , Œ¥) ‚äÇ S ‚à© int(X1 ). Let  = 2Œ¥ , and consider a function f : X ‚Üí R such that f (x) =
E[Y0i |X = x] for all x ‚àà X \ B(x‚àó , ) and f (x) = E[Y0i |X = x] ‚àí 1 for all x ‚àà B(x‚àó , ). Below,
we show that f is continuous at any point x ‚àà X such that pA (x) ‚àà (0, 1) and A(x) ‚àà {0, 1}.
Pick any x ‚àà X such that pA (x) ‚àà (0, 1) and A(x) ‚àà {0, 1}. Since B(x‚àó , Œ¥) ‚äÇ int(X1 ) and
int(X1 ) ‚äÇ {x0 ‚àà X : pA (x0 ) = 1} by Step C.1.3, x ‚àà
/ B(x‚àó , Œ¥). Hence, B(x, ) ‚äÇ X \ B(x‚àó , ). By
Assumption 2 and the definition of f , f is continuous at x.
Now take any random vector (Y1i‚àó , Y0i‚àó , Xi‚àó , Zi‚àó ) that is distributed according to the true distribution Q0 . Let Q be the distribution of (Y1iQ , Y0iQ , XiQ , ZiQ ), where (Y1iQ , XiQ , ZiQ ) = (Y1i‚àó , Xi‚àó , Zi‚àó ),
and
(
if Xi‚àó ‚àà X \ B(x‚àó , )
Y0i‚àó
Q
Y0i =
Y0i‚àó ‚àí 1
if Xi‚àó ‚àà B(x‚àó , )
63

Note first that Q ‚àà Q, since EQ [Y1iQ |XiQ = x] = E[Y1i‚àó |Xi‚àó = x] and EQ [Y0iQ |XiQ = x] = f (x),
where E[Y1i‚àó |Xi‚àó ] and f are both continuous at any point x ‚àà X such that pA (x) ‚àà (0, 1) and
A(x) ‚àà {0, 1}. Also, ZiQ = Zi‚àó = 1 if Xi‚àó ‚àà B(x‚àó , ). It then follows that
YiQ = ZiQ Y1iQ + (1 ‚àí ZiQ )Y0iQ
(
Zi‚àó Y1i‚àó + (1 ‚àí Zi‚àó )Y0i‚àó
=
Zi‚àó Y1i‚àó

if Xi‚àó ‚àà X \ B(x‚àó , )

Yi‚àó = Zi‚àó Y1i‚àó + (1 ‚àí Zi‚àó )Y0i‚àó
(
Zi‚àó Y1i‚àó + (1 ‚àí Zi‚àó )Y0i‚àó
=
Zi‚àó Y1i‚àó

if Xi‚àó ‚àà X \ B(x‚àó , )

if Xi‚àó ‚àà B(x‚àó , )

and

if Xi‚àó ‚àà B(x‚àó , ).

Thus, YiQ = Yi‚àó , and hence T (Q) = T (Q0 ) = P0 .
Using EQ [Y1iQ |XiQ = x] = E[Y1i‚àó |Xi‚àó = x] and EQ [Y0iQ |XiQ = x] = f (x), we have
EQ [Y1iQ ‚àí Y0iQ |XiQ ‚àà S]
= EQ [EQ [Y1iQ |XiQ ]|XiQ ‚àà S]
‚àí EQ [EQ [Y0iQ |XiQ ]|XiQ ‚àà S, XiQ ‚àà
/ B(x‚àó , )]PrQ (XiQ ‚àà
/ B(x‚àó , )|XiQ ‚àà S)
‚àí EQ [EQ [Y0iQ |XiQ ]|XiQ ‚àà B(x‚àó , )]PrQ (XiQ ‚àà B(x‚àó , )|XiQ ‚àà S)
= E[E[Y1i‚àó |Xi‚àó ]|Xi‚àó ‚àà S] ‚àí E[f (Xi‚àó )|Xi‚àó ‚àà S, Xi‚àó ‚àà
/ B(x‚àó , )] Pr(Xi‚àó ‚àà
/ B(x‚àó , )|Xi‚àó ‚àà S)
‚àí E[f (Xi‚àó )|Xi‚àó ‚àà B(x‚àó , )] Pr(Xi‚àó ‚àà B(x‚àó , )|Xi‚àó ‚àà S)
= E[Y1i‚àó |Xi‚àó ‚àà S] ‚àí E[Y0i‚àó |Xi‚àó ‚àà S, Xi‚àó ‚àà
/ B(x‚àó , )] Pr(Xi‚àó ‚àà
/ B(x‚àó , )|Xi‚àó ‚àà S)
‚àí E[Y0i‚àó ‚àí 1|Xi‚àó ‚àà B(x‚àó , )] Pr(Xi‚àó ‚àà B(x‚àó , )|Xi‚àó ‚àà S)
= E[Y1i‚àó ‚àí Y0i‚àó |Xi‚àó ‚àà S] + Pr(Xi‚àó ‚àà B(x‚àó , )|Xi‚àó ‚àà S).
By the definition of support, Pr(Xi‚àó ‚àà B(x‚àó , )) > 0. Since T (Q) = T (Q0 ) = P0 but EQ [Y1iQ ‚àí
Y0iQ |XiQ ‚àà S] 6= E[Y1i‚àó ‚àí Y0i‚àó |Xi‚àó ‚àà S], E[Y1i ‚àí Y0i |Xi ‚àà S] is not identified.

C.2

Proof of Corollary 1

If Pr(Di (1) ‚àí Di (0) = 1|Xi = x) = 1, Pr(Y1i ‚àí Y0i = Yi (1) ‚àí Yi (0)|Xi = x) = 1, and hence
E[Y1i ‚àí Y0i |Xi = x] = E[Yi (1) ‚àí Yi (0)|Xi = x]. Then, Part (a) follows from Proposition 1 (a). If
Pr(Di (1) ‚â• Di (0)|Xi = x) = 1, we have
E[Y1i ‚àí Y0i |Xi = x] = E[(Di (1) ‚àí Di (0))(Yi (1) ‚àí Yi (0))|Xi = x]
= Pr(Di (1) 6= Di (0)|Xi = x)E[Yi (1) ‚àí Yi (0)|Di (1) 6= Di (0), Xi = x].
64

If in addition Pr(Di (1) 6= Di (0)|Xi = x) > 0, we obtain
E[Y1i ‚àí Y0i |Xi = x]
Pr(Di (1) 6= Di (0)|Xi = x)
E[Y1i ‚àí Y0i |Xi = x]
=
.
E[Di (1) ‚àí Di (0)|Xi = x]

E[Yi (1) ‚àí Yi (0)|Di (1) 6= Di (0), Xi = x] =

Then, Part (b) follows from Proposition 1 (a).

C.3

Proof of Theorem 1

We prove consistency and asymptotic normality of the following estimators without imposing
Assumption 3 (c). These estimators are aymptotically equivalent to the estimators defined in
Section 4.1 if Assumption 3 (c) holds.
First, consider the following 2SLS regression using the observations with pA (Xi ; Œ¥n ) ‚àà (0, 1):
Di = Œ≥0 (1 ‚àí In ) + Œ≥1 Zi + Œ≥2 pA (Xi ; Œ¥n ) + ŒΩi
A

Yi = Œ≤0 (1 ‚àí In ) + Œ≤1 Di + Œ≤2 p (Xi ; Œ¥n ) + i .

(15)
(16)

Here In is a dummy random variable which equals one if there exists a constant q ‚àà (0, 1) such
that A(Xi ) ‚àà {0, q, 1} for all i ‚àà {1, ..., n}. In is the indicator that A(Xi ) takes on only one
nondegenerate value in the sample. If the support of A(Xi ) (in the population) contains only
one value in (0, 1), pA (Xi ; Œ¥n ) is asymptotically constant conditional on pA (Xi ; Œ¥n ) ‚àà (0, 1). To
avoid the multicollinearity between asymptotically constant pA (Xi ; Œ¥n ) and a constant, we do not
include the constant term if In = 1. Let Ii,n = 1{pA (Xi ; Œ¥n ) ‚àà (0, 1)}, Di,n = (1, Di , pA (Xi ; Œ¥n ))0 ,
A
0
nc
A
0
Zi,n = (1, Zi , pA (Xi ; Œ¥n ))0 , Dnc
i,n = (Di , p (Xi ; Œ¥n )) , and Zi,n = (Zi , p (Xi ; Œ¥n )) . The 2SLS
estimator Œ≤ÃÇ from this regression is then given by
(P
P
if In =0
( ni=1 Zi,n D0i,n Ii,n )‚àí1 ni=1 Zi,n Yi Ii,n
Œ≤ÃÇ = Pn
Pn
nc
nc
nc
0
‚àí1
if In =1.
( i=1 Zi,n (Di,n ) Ii,n )
i=1 Zi,n Yi Ii,n
Let Œ≤ÃÇ1 denote the 2SLS estimator of Œ≤1 in the above regression.
Similarly, consider the following simulation version of the 2SLS regression using the observations with ps (Xi ; Œ¥n ) ‚àà (0, 1):
Di = Œ≥0 (1 ‚àí In ) + Œ≥1 Zi + Œ≥2 ps (Xi ; Œ¥n ) + ŒΩi

(17)

Yi = Œ≤0 (1 ‚àí In ) + Œ≤1 Di + Œ≤2 ps (Xi ; Œ¥n ) + i .

(18)

Let Œ≤ÃÇ1s denote the 2SLS estimator of Œ≤1 in the simulation-based regression.
Below, we prove the following result.
Theorem C.1. Suppose that Assumptions 1 and 3 hold except Assumption 3 (c), and that
Œ¥n ‚Üí 0, nŒ¥n ‚Üí ‚àû and Sn ‚Üí ‚àû as n ‚Üí ‚àû. Then the 2SLS estimators Œ≤ÃÇ1 and Œ≤ÃÇ1s converge in
probability to
Œ≤1 ‚â° lim E[œâi (Œ¥)(Yi (1) ‚àí Yi (0))],
Œ¥‚Üí0

65

where
œâi (Œ¥) =

pA (Xi ; Œ¥)(1 ‚àí pA (Xi ; Œ¥))(Di (1) ‚àí Di (0))
.
E[pA (Xi ; Œ¥)(1 ‚àí pA (Xi ; Œ¥))(Di (1) ‚àí Di (0))]

Suppose, in addition, that Assumptions 4 and 5 hold and that nŒ¥n2 ‚Üí 0 as n ‚Üí ‚àû. Then
d

œÉÃÇn‚àí1 (Œ≤ÃÇ1 ‚àí Œ≤1 ) ‚àí‚Üí N (0, 1),
d

(œÉÃÇns )‚àí1 (Œ≤ÃÇ1s ‚àí Œ≤1 ) ‚àí‚Üí N (0, 1).
where we define œÉÃÇn‚àí1 and (œÉÃÇns )‚àí1 as follows: let
(P
P
P
( ni=1 Zi,n D0i,n Ii,n )‚àí1 ( ni=1 ÀÜ2i,n Zi,n Z0i,n Ii,n )( ni=1 Di,n Z0i,n Ii,n )‚àí1
Œ£ÃÇn = Pn
Pn 2 nc nc 0
P
nc 0
‚àí1
nc 0
‚àí1
( i=1 Znc
ÀÜi,n Zi,n (Zi,n ) Ii,n )( ni=1 Dnc
i,n (Di,n ) Ii,n ) ( i=1 
i,n (Zi,n ) Ii,n )

if In = 0
if In = 1,

where
ÀÜi,n =

(
Yi ‚àí D0i,n Œ≤ÃÇ

if In = 0

0
Yi ‚àí (Dnc
i,n ) Œ≤ÃÇ

if In = 1.

Let œÉÃÇn2 denote the estimator for the variance of Œ≤ÃÇ1 . That is, œÉÃÇn2 is the second diagonal element of
Œ£ÃÇn when In = 0 and is the first diagonal element of Œ£ÃÇn when In = 1. (œÉÃÇns )2 is the analogouslydefined estimator for the variance of Œ≤ÃÇ1s from the simulation-based regression.
Throughout the proof, we omit the subscript n from Ii,n , Di,n , Zi,n , ÀÜi,n , Œ£ÃÇn , œÉÃÇn , etc. for
notational brevity. We provide proofs separately for the two cases, the case in which Pr(A(Xi ) ‚àà
(0, 1)) > 0 and the case in which Pr(A(Xi ) ‚àà (0, 1)) = 0. For each case, we first prove consistency
and asymptotic normality of Œ≤ÃÇ1 , and then prove consistency and asymptotic normality of Œ≤ÃÇ1s .
Consistency and Asymptotic Normality of Œ≤ÃÇ1 When Pr(A(Xi ) ‚àà (0, 1)) > 0

C.3.1

By Lemma B.6,
lim E[pA (Xi ; Œ¥)(1 ‚àí pA (Xi ; Œ¥))(Di (1) ‚àí Di (0))] = E[A(Xi )(1 ‚àí A(Xi ))(Di (1) ‚àí Di (0))].

Œ¥‚Üí0

When Pr(A(Xi ) ‚àà (0, 1)) > 0, E[A(Xi )(1‚àíA(Xi ))(Di (1)‚àíDi (0))] = E[pA (Xi )(1‚àípA (Xi ))(Di (1)‚àí
Di (0))], since pA (x) = A(x) for almost every x ‚àà X by Proposition A.1. Under Assumption 3
(b), E[pA (Xi )(1 ‚àí pA (Xi ))(Di (1) ‚àí Di (0))] > 0. Again by Lemma B.6,
lim E[œâi (Œ¥)(Yi (1) ‚àí Yi (0))] =

Œ¥‚Üí0

Let Œ≤1 =

E[A(Xi )(1 ‚àí A(Xi ))(Di (1) ‚àí Di (0))(Yi (1) ‚àí Yi (0)]
.
E[A(Xi )(1 ‚àí A(Xi ))(Di (1) ‚àí Di (0))]

E[A(Xi )(1‚àíA(Xi ))(Di (1)‚àíDi (0))(Yi (1)‚àíYi (0)]
.
E[A(Xi )(1‚àíA(Xi ))(Di (1)‚àíDi (0))]

Let

n
n
X
X
Œ≤ÃÇ c = (
Zi D0i Ii )‚àí1
Zi Yi Ii
i=1

Œ≤ÃÇ nc

i=1

n
n
X
X
nc 0
‚àí1
(D
)
I
)
Znc
=(
Znc
i
i Yi Ii ,
i
i
i=1

i=1

66

and let Œ≤ÃÇ1c = (0, 1, 0)Œ≤ÃÇ c and Œ≤ÃÇ1nc = (1, 0)Œ≤ÃÇ nc . Œ≤ÃÇ1 is given by
Œ≤ÃÇ1 = Œ≤ÃÇ1c (1 ‚àí In ) + Œ≤ÃÇ1nc In .
0
nc
0
Also, let DÃÉi = (1, Di , A(Xi ))0 , ZÃÉi = (1, Zi , A(Xi ))0 , DÃÉnc
i = (Di , A(Xi )) , ZÃÉi = (Zi , A(Xi )) , and
IiA = 1{A(Xi ) ‚àà (0, 1)}.
We claim that Pr(In = 1) ‚Üí 0 when Var(A(Xi )|IiA = 1) > 0, and that Pr(In = 1) ‚Üí 1 when
Var(A(Xi )|IiA = 1) = 0. To show the first claim, observe that In = 1 if and only if VÃÇn = 0, where
Pn
A(Xi )IiA 2 A
i=1
Pn
) Ii
A
i=1 Ii
A
i=1 Ii

Pn
VÃÇn =

i=1 (A(Xi ) ‚àí
Pn

is the sample variance of A(Xi ) conditional on IiA = 1. When Var(A(Xi )|IiA = 1) > 0,
Pr(In = 1) = Pr(VÃÇn = 0)
‚â§ Pr(|VÃÇn ‚àí Var(A(Xi )|IiA = 1)| ‚â• Var(A(Xi )|IiA = 1))
‚Üí 0,
p

where the convergence follows since VÃÇn ‚àí‚Üí Var(A(Xi )|IiA = 1) > 0.
To show the second claim, note that, when Var(A(Xi )|IiA = 1) = 0, there exists q ‚àà (0, 1)
such that Pr(A(Xi ) = q|IiA = 1) = 1. It follows that
Pr(In = 0) = Pr(A(Xi ) ‚àà {0, 1} for all i = 1, ..., n)
+ Pr(A(Xi ) = q 0 and A(Xj ) = q 00 for some q 0 , q 00 ‚àà (0, 1) with q 0 6= q 00
for some i, j ‚àà {1, ..., n})
= Pr(A(Xi ) ‚àà {0, 1} for all i = 1, ..., n)
= (1 ‚àí Pr(A(Xi ) ‚àà (0, 1)))n ,
which converges to zero as n ‚Üí ‚àû, since Pr(A(Xi ) ‚àà (0, 1)) > 0.
The above claims imply that Œ≤ÃÇ1 = Œ≤ÃÇ1c with probability approaching one when Var(A(Xi )|IiA =
1) > 0, and that Œ≤ÃÇ1 = Œ≤ÃÇ1nc with probability approaching one when Var(A(Xi )|IiA = 1) = 0.
Therefore, to prove consistency and asymptotic normality of Œ≤ÃÇ1 , it suffices to show those of Œ≤ÃÇ1c
when Var(A(Xi )|IiA = 1) > 0 and those of Œ≤ÃÇ1nc when Var(A(Xi )|IiA = 1) = 0.
Below we first show that, if Assumptions 1 and 3 hold and Œ¥n ‚Üí 0 as n ‚Üí ‚àû, then
p
Œ≤ÃÇ1 ‚àí‚Üí Œ≤1 . We then show that, if, in addition, Assumption 4 holds and nŒ¥n2 ‚Üí 0 as n ‚Üí ‚àû, then
d

œÉÃÇ ‚àí1 (Œ≤ÃÇ1 ‚àí Œ≤1 ) ‚àí‚Üí N (0, 1).
p

Proof of Consistency. To prove consistency of Œ≤ÃÇ1 , we first show that Œ≤ÃÇ1c ‚àí‚Üí Œ≤1 when
p
Var(A(Xi )|IiA = 1) > 0. We then show that Œ≤ÃÇ1nc ‚àí‚Üí Œ≤1 whether or not Var(A(Xi )|IiA = 1) > 0.
By Lemma B.6,
n
n
X
X
p
Œ≤ÃÇ c = (
Zi D0i Ii )‚àí1
Zi Yi Ii ‚àí‚Üí (E[ZÃÉi DÃÉ0i IiA ])‚àí1 E[ZÃÉi Yi IiA ]
i=1

i=1

67

provided that E[ZÃÉi DÃÉ0i IiA ] is invertible. After a few lines of algebra, we have
det(E[ZÃÉi DÃÉ0i IiA ])
= Pr(IiA = 1)2 Var(A(Xi )|IiA = 1)E[Di (Zi ‚àí A(Xi ))IiA ]
= Pr(IiA = 1)2 Var(A(Xi )|IiA = 1)E[(Zi Di (1) + (1 ‚àí Zi )Di (0))(Zi ‚àí A(Xi ))IiA ]
= Pr(IiA = 1)2 Var(A(Xi )|IiA = 1)E[((Zi ‚àí Zi A(Xi ))Di (1) ‚àí (1 ‚àí Zi )A(Xi )Di (0))IiA ]
= Pr(IiA = 1)2 Var(A(Xi )|IiA = 1)E[((A(Xi ) ‚àí A(Xi )2 )Di (1) ‚àí (1 ‚àí A(Xi ))A(Xi )Di (0))IiA ]
= Pr(IiA = 1)2 Var(A(Xi )|IiA = 1)E[A(Xi )(1 ‚àí A(Xi ))(Di (1) ‚àí Di (0))IiA ]
= Pr(IiA = 1)2 Var(A(Xi )|IiA = 1)E[A(Xi )(1 ‚àí A(Xi ))(Di (1) ‚àí Di (0))],
where the fourth equality follows from Property 1. Therefore, E[ZÃÉi DÃÉ0i IiA ] is invertible when
Var(A(Xi )|IiA = 1) > 0. Another few lines of algebra gives
Ô£Æ
Ô£π
‚àó ‚àó ‚àó
1
Ô£Ø
Ô£∫
(E[ZÃÉi DÃÉ0i IiA ])‚àí1 =
Ô£∞0 1 ‚àí1Ô£ª
E[A(Xi )(1 ‚àí A(Xi ))(Di (1) ‚àí Di (0))]
‚àó ‚àó ‚àó
when Var(A(Xi )|IiA = 1) > 0. Therefore, when Var(A(Xi )|IiA = 1) > 0,
p

Œ≤ÃÇ1c ‚àí‚Üí
=
=
=
=

E[Zi Yi IiA ] ‚àí E[A(Xi )Yi IiA ]
E[A(Xi )(1 ‚àí A(Xi ))(Di (1) ‚àí Di (0))]
E[Zi Y1i IiA ] ‚àí E[A(Xi )(Zi Y1i + (1 ‚àí Zi )Y0i )IiA ]
E[A(Xi )(1 ‚àí A(Xi ))(Di (1) ‚àí Di (0))]
E[A(Xi )Y1i IiA ] ‚àí E[A(Xi )(A(Xi )Y1i + (1 ‚àí A(Xi ))Y0i )IiA ]
E[A(Xi )(1 ‚àí A(Xi ))(Di (1) ‚àí Di (0))]
E[A(Xi )(1 ‚àí A(Xi ))(Y1i ‚àí Y0i )IiA ]
E[A(Xi )(1 ‚àí A(Xi ))(Di (1) ‚àí Di (0))]
E[A(Xi )(1 ‚àí A(Xi ))((Di (1) ‚àí Di (0))(Yi (1) ‚àí Yi (0))]
E[A(Xi )(1 ‚àí A(Xi ))(Di (1) ‚àí Di (0))]

= Œ≤1 ,
where the third line follows from Property 1, and the second last follows from the definitions of
Y1i and Y0i .
We next consider Œ≤ÃÇ1nc . By Lemma B.6,
Œ≤ÃÇ

nc

n
n
X
X
p
nc
nc 0
‚àí1
nc
nc 0 A ‚àí1
nc
A
=(
Zi (Di ) Ii )
Znc
i Yi Ii ‚àí‚Üí (E[ZÃÉi (DÃÉi ) Ii ]) E[ZÃÉi Yi Ii ]
i=1

i=1

nc 0 A
provided that E[ZÃÉnc
i (DÃÉi ) Ii ] is invertible. After a few lines of algebra, we have
nc 0 A
2 A
A
det(E[ZÃÉnc
i (DÃÉi ) Ii ]) = E[A(Xi ) Ii ]E[Di (Zi ‚àí A(Xi ))Ii ]

= E[A(Xi )2 IiA ]E[A(Xi )(1 ‚àí A(Xi ))(Di (1) ‚àí Di (0))]
> 0.
68

Another few lines of algebra gives
nc 0 A ‚àí1
(E[ZÃÉnc
i (DÃÉi ) Ii ])

"
#
1
1 ‚àí1
=
.
E[A(Xi )(1 ‚àí A(Xi ))(Di (1) ‚àí Di (0))] ‚àó ‚àó

Therefore,
p

Œ≤ÃÇ1nc ‚àí‚Üí

E[Zi Yi IiA ] ‚àí E[A(Xi )Yi IiA ]
= Œ≤1 .
E[A(Xi )(1 ‚àí A(Xi ))(Di (1) ‚àí Di (0))]

Proof of Asymptotic Normality. Let (œÉÃÇ c )2 be the second diagonal element of
n
n
n
X
X
X
Œ£ÃÇc = (
Zi D0i Ii )‚àí1 (
ÀÜ2i Zi Z0i Ii )(
Di Z0i Ii )‚àí1
i=1

i=1

i=1

and (œÉÃÇ nc )2 be the first diagonal element of
nc

Œ£ÃÇ

n
n
n
X
X
X
nc
nc 0
‚àí1
2
nc
nc 0
nc 0
‚àí1
=(
Zi,n (Di,n ) Ii ) (
ÀÜi,n Zi,n (Zi,n ) Ii )(
Dnc
i,n (Zi,n ) Ii ) .
i=1

i=1

i=1

d

We only show that (œÉÃÇ c )‚àí1 (Œ≤ÃÇ1c ‚àí Œ≤1 ) ‚àí‚Üí N (0, 1) when Var(A(Xi )|IiA = 1) > 0. We can show
d

that (œÉÃÇ nc )‚àí1 (Œ≤ÃÇ1nc ‚àí Œ≤1 ) ‚àí‚Üí N (0, 1) by an analogous argument. The proof proceeds in six steps.
Step C.3.1.1. Let Œ≤ÃÉn = (E[ZÃÉi DÃÉ0i Ii ])‚àí1 E[ZÃÉi Yi Ii ], and let Œ≤ÃÉ1,n denote the second element of Œ≤ÃÉn .
Then Œ≤ÃÉ1,n = Œ≤1 for any choice of Œ¥n > 0.
Proof. Note first that, for every Œ¥ > 0, pA (x; Œ¥) ‚àà (0, 1) for almost every x ‚àà {x0 ‚àà X : A(x0 ) ‚àà
(0, 1)}, since by almost everywhere continuity of A, for almost every x ‚àà {x0 ‚àà X : A(x0 ) ‚àà (0, 1)},
there exists an open ball B ‚äÇ B(x, Œ¥) such that A(x0 ) ‚àà (0, 1) for every x0 ‚àà B. After a few lines
of algebra, we have
det(E[ZÃÉi DÃÉ0i Ii ]) = Pr(Ii = 1)2 Var(A(Xi )|Ii = 1)E[Di (Zi ‚àí A(Xi ))Ii ]
= Pr(Ii = 1)2 Var(A(Xi )|Ii = 1)E[A(Xi )(1 ‚àí A(Xi ))(Di (1) ‚àí Di (0))Ii ]
= Pr(Ii = 1)2 Var(A(Xi )|Ii = 1)E[A(Xi )(1 ‚àí A(Xi ))(Di (1) ‚àí Di (0))],
where the last equality holds since pA (x; Œ¥) ‚àà (0, 1) for almost every x ‚àà {x0 ‚àà X : A(x0 ) ‚àà (0, 1)}.
By the law of total conditional variance,
Var(A(Xi )|Ii = 1)
= E[Var(A(Xi )|Ii = 1, IiA )|Ii = 1] + Var(E[A(Xi )|Ii = 1, IiA ]|Ii = 1)
X
‚â•
Var(A(Xi )|Ii = 1, IiA = t) Pr(IiA = t|Ii = 1)
t‚àà{0,1}

‚â• Var(A(Xi )|Ii = 1, IiA = 1) Pr(IiA = 1|Ii = 1)
= Var(A(Xi )|IiA = 1) Pr(IiA = 1|Ii = 1)
> 0.
69

Therefore, E[ZÃÉi DÃÉ0i Ii ] is invertible. Another few lines of algebra gives
Ô£Æ
‚àó
1
Ô£Ø
(E[ZÃÉi DÃÉ0i Ii ])‚àí1 =
Ô£∞0
E[A(Xi )(1 ‚àí A(Xi ))(Di (1) ‚àí Di (0))]
‚àó

Ô£π
‚àó ‚àó
Ô£∫
1 ‚àí1Ô£ª .
‚àó ‚àó

It follows that
E[Zi Yi Ii ] ‚àí E[A(Xi )Yi Ii ]
E[A(Xi )(1 ‚àí A(Xi ))(Di (1) ‚àí Di (0))]
E[A(Xi )(1 ‚àí A(Xi ))(Di (1) ‚àí Di (0))(Yi (1) ‚àí Yi (0))Ii ]
=
E[A(Xi )(1 ‚àí A(Xi ))(Di (1) ‚àí Di (0))]
E[A(Xi )(1 ‚àí A(Xi ))(Di (1) ‚àí Di (0))(Yi (1) ‚àí Yi (0))]
=
E[A(Xi )(1 ‚àí A(Xi ))(Di (1) ‚àí Di (0))]

Œ≤ÃÉ1,n =

= Œ≤1 .

We can write
n
n
n
n
‚àö
1X
1 X
1X
1 X
n(Œ≤ÃÇ c ‚àí Œ≤ÃÉn ) = (
Zi D0i Ii )‚àí1 ‚àö
ZÃÉi DÃÉ0i Ii )‚àí1 ‚àö
Zi Yi Ii ‚àí (
ZÃÉi Yi Ii
n
n
n
n
i=1
i=1
i=1
i=1
{z
}
|
=(A)

+(

1
n

n
X
i=1

1
ZÃÉi DÃÉ0i Ii )‚àí1 ‚àö
n

n
X

‚àö
ZÃÉi Yi Ii ‚àí (E[ZÃÉi DÃÉ0i Ii ])‚àí1 nE[ZÃÉi Yi Ii ] .

i=1

{z

|

=(B)

}

We first consider (B). Let Àúi,n = Yi ‚àí DÃÉ0i Œ≤ÃÉn so that
E[ZÃÉi Àúi,n Ii ] = E[ZÃÉi (Yi ‚àí DÃÉ0i Œ≤ÃÉn )Ii ] = E[ZÃÉi Yi Ii ] ‚àí E[ZÃÉi DÃÉ0i Ii ]Œ≤ÃÉn = 0.
Then
n

n

i=1

i=1

‚àö
1X
1 X
(B) = (
ZÃÉi DÃÉ0i Ii )‚àí1 ‚àö
ZÃÉi (DÃÉ0i Œ≤ÃÉn + Àúi,n )Ii ‚àí (E[ZÃÉi DÃÉ0i Ii ])‚àí1 nE[ZÃÉi (DÃÉ0i Œ≤ÃÉn + Àúi,n )Ii ]
n
n
n
n
X
‚àö
‚àö
1X
0
‚àí1 1
= n(Œ≤ÃÉn ‚àí Œ≤ÃÉn ) + (
ZÃÉi DÃÉi Ii ) ‚àö
ZÃÉi Àúi,n Ii ‚àí (E[ZÃÉi DÃÉ0i Ii ])‚àí1 nE[ZÃÉi Àúi,n Ii ]
n
n
i=1

i=1

n
n
X
1X
0
‚àí1 1
=(
ZÃÉi DÃÉi Ii ) ‚àö
ZÃÉi Àúi,n Ii .
n
n
i=1

Step C.3.1.2.

i=1

n

1 X
d
‚àö
ZÃÉi Àúi,n Ii ‚àí‚Üí N (0, E[Àú
2i ZÃÉi ZÃÉ0i IiA ]).
n
i=1

70

Proof. We use the triangular-array Lyapunov CLT and the Cram√©r-Wold device. Pick a nonzero
Œª ‚àà Rp , and let Vi,n = ‚àö1n Œª0 ZÃÉi Àúi,n Ii . First, we have
n
X

2
E[Vi,n
] = Œª0 E[Àú
2i,n ZÃÉi ZÃÉ0i Ii ]Œª.

i=1

By Lemma B.6,
Œ≤ÃÉn ‚Üí (E[ZÃÉi DÃÉ0i IiA ])‚àí1 E[ZÃÉi Yi IiA ]
as n ‚Üí ‚àû. Let Œ≤ = (E[ZÃÉi DÃÉ0i IiA ])‚àí1 E[ZÃÉi Yi IiA ] and Àúi = Yi ‚àí DÃÉ0i Œ≤. We have
E[Àú
2i,n ZÃÉi ZÃÉ0i Ii ] = E[(Yi ‚àí DÃÉ0i Œ≤ÃÉn )2 ZÃÉi ZÃÉ0i Ii ]
= E[(Àú
i ‚àí DÃÉ0i (Œ≤ÃÉn ‚àí Œ≤))2 ZÃÉi ZÃÉ0i Ii ]
= E[Àú
2i ZÃÉi ZÃÉ0i Ii ] ‚àí 2E[Àú
i ((Œ≤ÃÉ0,n ‚àí Œ≤0 ) + Di (Œ≤ÃÉ1,n ‚àí Œ≤1 ) + A(Xi )(Œ≤ÃÉ2,n ‚àí Œ≤2 ))ZÃÉi ZÃÉ0i Ii ]
+ E[((Œ≤ÃÉ0,n ‚àí Œ≤0 ) + Di (Œ≤ÃÉ1,n ‚àí Œ≤1 ) + A(Xi )(Œ≤ÃÉ2,n ‚àí Œ≤2 ))2 ZÃÉi ZÃÉ0i Ii ]
‚Üí E[Àú
2i ZÃÉi ZÃÉ0i IiA ]
as n ‚Üí ‚àû, where the convergence follows from Lemma B.6 and from the fact that Œ≤ÃÉn ‚Üí Œ≤.
Therefore,
n
X
2
E[Vi,n
] ‚Üí Œª0 E[Àú
2i ZÃÉi ZÃÉ0i IiA ]Œª.
i=1

We next verify the Lyapunov condition: for some t > 0,
n
X

E[|Vi,n |2+t ] ‚Üí 0.

i=1

We have
n
X

E[|Vi,n |4 ] =

i=1

1
E[|Œª0 ZÃÉi Àúi,n Ii |4 ].
n

We use the cr -inequality: E[|X + Y |r ] ‚â§ 2r‚àí1 E[|X|r + |Y |r ] for r ‚â• 1. Repeating using the
cr -inequality gives
E[|Œª0 ZÃÉi Àúi,n Ii |4 ] = E[|Œª0 ZÃÉi (Yi ‚àí Œ≤ÃÉ0,n ‚àí Œ≤ÃÉ1,n Di ‚àí Œ≤ÃÉ2,n A(Xi ))|4 Ii ]
‚â§ 23c E[(|Œª0 ZÃÉi |4 )(|Yi |4 + |Œ≤ÃÉ0,n |4 + |Œ≤ÃÉ1,n |4 Di + |Œ≤ÃÉ2,n |4 A(Xi )4 )Ii ]
4
4
4
‚â§ 23c (Œª1 + Œª2 + Œª3 )4 (E[Yi4 ] + Œ≤ÃÉ0,n
+ Œ≤ÃÉ1,n
+ Œ≤ÃÉ2,n
)

for some finite constant c, and the right-hand side converges to
23c (Œª1 + Œª2 + Œª3 )4 (E[Yi4 ] + Œ≤ÃÉ04 + Œ≤ÃÉ14 + Œ≤ÃÉ24 ),
which is finite under Assumption 3 (a). Therefore,
n
X

E[|Vi,n |4 ] ‚Üí 0,

i=1

and the conclusion follows from the Lyapunov CLT and the Cram√©r-Wold device.
71

We next consider (A). We can write
(A) = (

n

n

i=1

i=1

1X
1 X
(Zi Yi Ii ‚àí ZÃÉi Yi Ii )
Zi D0i Ii )‚àí1 ‚àö
n
n

‚àí(

1
n

n
X
i=1

n

n

n

i=1

i=1

i=1

1X
1 X
1X
(Zi D0i Ii ‚àí ZÃÉi DÃÉ0i Ii )](
Zi D0i Ii )‚àí1 [ ‚àö
ZÃÉi DÃÉ0i Ii )‚àí1
ZÃÉi Yi Ii .
n
n
n

Step C.3.1.3. Let {Vi }‚àû
i=1 be i.i.d. random variables
‚àó
0
is bounded on N (D , Œ¥ ) ‚à© X for some Œ¥ 0 > 0. Then,

such that E[|Vi |] < ‚àû and that E[Vi |Xi ]

E[Vi pA (Xi ; Œ¥)l (pA (Xi ; Œ¥) ‚àí A(Xi ))1{pA (Xi ; Œ¥) ‚àà (0, 1)}] = O(Œ¥)
for l = 0, 1.
Proof. For every x ‚àà
/ N (D‚àó , Œ¥), B(x, Œ¥) ‚à© D‚àó = ‚àÖ, so A is continuously differentiable on B(x, Œ¥).
By the mean value theorem, for every x ‚àà
/ N (D‚àó , Œ¥) and a ‚àà B(0, Œ¥),
A(x + a) = A(x) + ‚àáA(y(x, a))0 a
for some point y(x, a) on the line segment connecting x and x + a. For every x ‚àà
/ N (D‚àó , Œ¥),
R
B(0,1) A(x + Œ¥u)du
R
pA (x; Œ¥) =
B(0,1) du
R
0
B(0,1) (A(x) + Œ¥‚àáA(y(x, Œ¥u)) u)du
R
=
B(0,1) du
R
0
B(0,1) ‚àáA(y(x, Œ¥u)) udu
R
= A(x) + Œ¥
.
B(0,1) du
Now, we can write
E[Vi pA (Xi ; Œ¥)l (pA (Xi ; Œ¥) ‚àí A(Xi ))1{pA (Xi ; Œ¥) ‚àà (0, 1)}]
= E[Vi pA (Xi ; Œ¥)l (pA (Xi ; Œ¥) ‚àí A(Xi ))1{pA (Xi ; Œ¥) ‚àà (0, 1)}1{Xi ‚àà
/ N (D‚àó , Œ¥)}]
+ E[Vi pA (Xi ; Œ¥)l (pA (Xi ; Œ¥) ‚àí A(Xi ))1{pA (Xi ; Œ¥) ‚àà (0, 1)}1{Xi ‚àà N (D‚àó , Œ¥)}].
For the first term,
|E[Vi pA (Xi ; Œ¥)l (pA (Xi ; Œ¥) ‚àí A(Xi ))1{pA (Xi ; Œ¥) ‚àà (0, 1)}1{Xi ‚àà
/ N (D‚àó , Œ¥)}]|
R
‚àáA(y(Xi , Œ¥u))0 udu
A
l B(0,1) R
1{pA (Xi ; Œ¥) ‚àà (0, 1)}1{Xi ‚àà
/ N (D‚àó , Œ¥)}]|
= Œ¥|E[Vi p (Xi ; Œ¥)
du
B(0,1)
R
Pp
‚àÇA(y(Xi ,Œ¥u))
||uk |du
k=1 |
‚àÇxk
A
l B(0,1)
R
‚â§ Œ¥E[|Vi |p (Xi ; Œ¥)
1{pA (Xi ; Œ¥) ‚àà (0, 1)}1{Xi ‚àà
/ N (D‚àó , Œ¥)}]
du
B(0,1)
R
p
X
‚àÇA(x) B(0,1) |uk |du
R
‚â§ Œ¥E[|Vi |]
sup
‚àÇxk
x‚ààC ‚àó
B(0,1) du
k=1

= O(Œ¥),
72

where we use the assumption that the partial derivatives of A is bounded on C ‚àó . For the second
term, for sufficiently small Œ¥ > 0,
|E[Vi pA (Xi ; Œ¥)l (pA (Xi ; Œ¥) ‚àí A(Xi ))1{pA (Xi ; Œ¥) ‚àà (0, 1)}1{Xi ‚àà N (D‚àó , Œ¥)}]|
‚â§ E[|E[Vi |Xi ]|1{Xi ‚àà N (D‚àó , Œ¥)}]
‚â§ CE[1{Xi ‚àà N (D‚àó , Œ¥)}]
= C Pr(Xi ‚àà N (D‚àó , Œ¥))
= O(Œ¥),
where C is some constant, the second inequality follows from the assumption that E[Vi |Xi ] is
bounded on N (D‚àó , Œ¥ 0 ) ‚à© X for some Œ¥ 0 > 0, and the last equality follows from Assumption 4
(a).
P
P
Step C.3.1.4. ‚àö1n ni=1 (Zi Yi Ii ‚àí ZÃÉi Yi Ii ) = op (1) and ‚àö1n ni=1 (Zi D0i Ii ‚àí ZÃÉi DÃÉ0i Ii ) = op (1).
P
Proof. We only show that ‚àö1n ni=1 (pA (Xi ; Œ¥n )2 ‚àí A(Xi )2 )Ii = op (1). The proofs for the other
elements are similar. As for bias,
n

1 X A
E[ ‚àö
(p (Xi ; Œ¥n )2 ‚àí A(Xi )2 )Ii ]
n
i=1
‚àö
= nE[(pA (Xi ; Œ¥n )2 ‚àí A(Xi )2 )Ii ]
‚àö
= nE[(pA (Xi ; Œ¥n ) + A(Xi ))(pA (Xi ; Œ¥n ) ‚àí A(Xi ))Ii ]
‚àö
= nO(Œ¥n )
= 0,
where the third equality follows from Step C.3.1.3 and the last from the assumption that nŒ¥n2 ‚Üí 0.
As for variance, by Lemma B.6,
n

1 X A
(p (Xi ; Œ¥n )2 ‚àí A(Xi )2 )Ii )
Var( ‚àö
n
i=1

A

‚â§ E[(p (Xi ; Œ¥n )2 ‚àí A(Xi )2 )2 Ii ]
= E[(pA (Xi ; Œ¥n )4 ‚àí 2pA (Xi ; Œ¥n )2 A(Xi )2 + A(Xi )4 )Ii ]
‚Üí E[(A(Xi )4 ‚àí 2A(Xi )2 A(Xi )2 + A(Xi )4 )IiA ]
= 0.

p

2i ZÃÉi ZÃÉ0i IiA ](E[DÃÉi ZÃÉ0i IiA ])‚àí1 .
Step C.3.1.5. nŒ£ÃÇc ‚àí‚Üí (E[ZÃÉi DÃÉ0i IiA ])‚àí1 E[Àú

73

Proof. Let i = Yi ‚àí D0i Œ≤. We have
n

n

1X 2
1X
ÀÜi Zi Z0i Ii =
(Yi ‚àí D0i Œ≤ÃÇ c )2 Zi Z0i Ii
n
n
i=1

i=1

n
1X
(i ‚àí D0i (Œ≤ÃÇ c ‚àí Œ≤))2 Zi Z0i Ii
=
n
i=1

n
1X 2
=
i Zi Z0i Ii
n
i=1

n

2X
(Yi ‚àí D0i Œ≤)((Œ≤ÃÇ0c ‚àí Œ≤0 ) + Di (Œ≤ÃÇ1c ‚àí Œ≤1 ) + pA (Xi ; Œ¥n )(Œ≤ÃÇ2c ‚àí Œ≤2 ))Zi Z0i Ii
‚àí
n
i=1

n
1X c
+
((Œ≤ÃÇ0 ‚àí Œ≤0 ) + Di (Œ≤ÃÇ1c ‚àí Œ≤1 ) + pA (Xi ; Œ¥n )(Œ≤ÃÇ2c ‚àí Œ≤2 ))2 Zi Z0i Ii
n
i=1

n
1X 2
=
i Zi Z0i Ii + op (1)Op (1),
n
i=1

where the last equality follows from the result that Œ≤ÃÇ c ‚àí Œ≤ = op (1) and from Lemma B.6. Again
by Lemma B.6,
n

n

1X 2
1X 2
i Zi Z0i Ii =
(Yi ‚àí 2Yi D0i Œ≤ + Œ≤ 0 Di D0i Œ≤)Zi Z0i Ii
n
n
i=1

i=1

p

‚àí‚Üí E[(Yi2 ‚àí 2Yi DÃÉ0i Œ≤ + Œ≤ 0 DÃÉi DÃÉ0i Œ≤)ZÃÉi ZÃÉ0i IiA ]
= E[Àú
2i ZÃÉi ZÃÉ0i IiA ],
and

n

1X
p
Zi D0i Ii ‚àí‚Üí E[ZÃÉi DÃÉ0i IiA ].
n
i=1

The conclusion then follows.
d

Step C.3.1.6. (œÉÃÇ c )‚àí1 (Œ≤ÃÇ1c ‚àí Œ≤1 ) ‚àí‚Üí N (0, 1).
Proof. By combining the results from Steps C.3.1.2‚ÄìC.3.1.4 and by Lemma B.6,
p

(A) ‚àí‚Üí 0,
d

(B) ‚àí‚Üí N (0, (E[ZÃÉi DÃÉ0i IiA ])‚àí1 E[Àú
2i ZÃÉi ZÃÉ0i IiA ](E[DÃÉi ZÃÉ0i IiA ])‚àí1 ),
and therefore,
‚àö

d

n(Œ≤ÃÇ c ‚àí Œ≤ÃÉn ) ‚àí‚Üí N (0, (E[ZÃÉi DÃÉ0i IiA ])‚àí1 E[Àú
2i ZÃÉi ZÃÉ0i IiA ](E[DÃÉi ZÃÉ0i IiA ])‚àí1 ).

The conclusion then follows from Steps C.3.1.1 and C.3.1.5.

74

C.3.2

Consistency and Asymptotic Normality of Œ≤ÃÇ1s When Pr(A(Xi ) ‚àà (0, 1)) > 0

Let Iis = 1{ps (Xi ; Œ¥n ) ‚àà (0, 1)}, Dsi = (1, Di , ps (Xi ; Œ¥n ))0 and Zsi = (1, Zi , ps (Xi ; Œ¥n ))0 . Let
Œ≤ÃÇ

c,s

n
n
X
X
s
s 0 s ‚àí1
=(
Zi (Di ) Ii )
Zsi Yi Iis
i=1

and
c,s

Œ£ÃÇ

i=1

n
n
n
X
X
X
s
s 0 s ‚àí1
s 2 s
s 0 s
=(
Zi (Di ) Ii ) ( (ÀÜ
i ) Zi (Zi ) Ii )(
Dsi (Zsi )0 Iis )‚àí1 ,
i=1

i=1

i=1

where ÀÜsi = Yi ‚àí (Dsi )0 Œ≤ÃÇ c,s . Here, we only show that

Œ≤ÃÇ1c,s

p

‚àí‚Üí Œ≤1 if Sn ‚Üí ‚àû and that (œÉÃÇ s )‚àí1 (Œ≤ÃÇ1c,s ‚àí

d

Œ≤1 ) ‚àí‚Üí N (0, 1) if Assumption 5 holds when Var(A(Xi )|IiA = 1) > 0. For that, it suffices to
show that
Œ≤ÃÇ c,s ‚àí Œ≤ÃÇ c = op (1)
if Sn ‚Üí ‚àû and that
‚àö

n(Œ≤ÃÇ c,s ‚àí Œ≤ÃÇ c ) = op (1),
p

nŒ£ÃÇc,s ‚àí‚Üí (E[ZÃÉi DÃÉ0i IiA ])‚àí1 E[Àú
2i ZÃÉi ZÃÉ0i IiA ](E[DÃÉi ZÃÉ0i IiA ])‚àí1
if Assumption 5 holds. We have
n

Œ≤ÃÇ c,s ‚àí Œ≤ÃÇ c = (
=(

n

n

n

i=1
n
X

i=1

1X
1X
1 X s s 0 s ‚àí1 1 X s s
Zi (Di ) Ii )
Zi Yi Ii ‚àí (
Zi D0i Ii )‚àí1
Zi Yi Ii
n
n
n
n
1
n

‚àí(

i=1
n
X

Zsi (Dsi )0 Iis )‚àí1 (

i=1

i=1
n
X

1
n

Zsi Yi Iis ‚àí

i=1

n

n

i=1

i=1

1
n

Zi Yi Ii )

i=1
n

n

n

i=1

i=1

i=1

1 X s s 0 s ‚àí1 1 X s s 0 s 1 X
1X
1X
Zi (Di ) Ii ) (
Zi (Di ) Ii ‚àí
Zi D0i Ii )(
Zi D0i Ii )‚àí1
Zi Yi Ii .
n
n
n
n
n
‚àö

By Lemma B.8, Œ≤ÃÇ c,s ‚àí Œ≤ÃÇ c = op (1) if Sn ‚Üí ‚àû, and n(Œ≤ÃÇ c,s ‚àí Œ≤ÃÇ c ) = op (1) under the boundedness
imposed by Assumption 4 (c) if Assumption 5 holds.
By proceeding as in Step C.3.1.5 in Section C.3.1, we have
n

n

i=1

i=1

1X s 2 s s 0 s
1X s 2 s s 0 s
(ÀÜ
i ) Zi (Zi ) Ii =
(i ) Zi (Zi ) Ii + op (1),
n
n
where si = Yi ‚àí (Dsi )0 Œ≤. Then, by Lemma B.8,
n

n

i=1

i=1

1X s 2 s s 0 s 1X 2
(ÀÜ
i ) Zi (Zi ) Ii ‚àí
i Zi Z0i Ii
n
n
n
n
1X 2
1X 2
s 0
0 s
s 0
s
s 0 s
=
(Yi ‚àí 2Yi (Di ) Œ≤ + Œ≤ Di (Di ) Œ≤)Zi (Zi ) Ii ‚àí
(Yi ‚àí 2Yi D0i Œ≤ + Œ≤ 0 Di D0i Œ≤)Zi Z0i Ii + op (1)
n
n
i=1

i=1

= op (1)
75

so that

n

1X s 2 s s 0 s p
(ÀÜ
i ) Zi (Zi ) Ii ‚àí‚Üí E[Àú
2i ZÃÉi ZÃÉ0i IiA ].
n
i=1

Also,
C.3.3

1
n

Pn

s
s 0 s
i=1 Zi (Di ) Ii

p

‚àí‚Üí E[ZÃÉi DÃÉ0i IiA ] by using Lemma B.8. The conclusion then follows.

Consistency and Asymptotic Normality of Œ≤ÃÇ1 When Pr(A(Xi ) ‚àà (0, 1)) = 0

Since Pr(A(Xi ) ‚àà (0, 1)) = 0, In = 0 with probability one. Hence,
n
n
X
X
Œ≤ÃÇ = (
Zi D0i Ii )‚àí1
Zi Yi Ii
i=1

i=1

with probability one. We use the notation and results provided in Appendix B. By Lemma B.5,
under Assumption 3 (e), there exists ¬µ > 0 such that ds‚Ñ¶‚àó is twice continuously differentiable on
N (‚àÇ‚Ñ¶‚àó , ¬µ) and that
Z
Z Œ¥Z
‚àÇ‚Ñ¶‚àó
g(x)dx =
g(u + ŒªŒΩ‚Ñ¶‚àó (u))Jp‚àí1
œà‚Ñ¶‚àó (u, Œª)dHp‚àí1 (u)dŒª
N (‚àÇ‚Ñ¶‚àó ,Œ¥)

‚àíŒ¥

‚àÇ‚Ñ¶‚àó

for every Œ¥ ‚àà (0, ¬µ) and every function g : Rp ‚Üí R that is integrable on N (‚àÇ‚Ñ¶‚àó , Œ¥).
p

d

Below we show that Œ≤ÃÇ1 ‚àí‚Üí Œ≤1 if nŒ¥n ‚Üí ‚àû and Œ¥n ‚Üí 0 and that œÉÃÇ ‚àí1 (Œ≤ÃÇ1 ‚àí Œ≤1 ) ‚àí‚Üí N (0, 1) if
nŒ¥n3 ‚Üí 0 in addition. The proof proceeds in eight steps.
Step C.3.3.1. There exist Œ¥ÃÑ > 0 and a bounded function r : ‚àÇ‚Ñ¶‚àó ‚à© N (X , Œ¥ÃÑ) √ó (‚àí1, 1) √ó (0, Œ¥ÃÑ) ‚Üí R
such that
pA (u + Œ¥vŒΩ‚Ñ¶‚àó (u); Œ¥) = k(v) + Œ¥r(u, v, Œ¥)
for every (u, v, Œ¥) ‚àà ‚àÇ‚Ñ¶‚àó ‚à© N (X , Œ¥ÃÑ) √ó (‚àí1, 1) √ó (0, Œ¥ÃÑ), where
(
1
for v ‚àà [0, 1)
1 ‚àí 21 I(1‚àív2 ) ( p+1
2 , 2)
k(v) =
p+1 1
1
for v ‚àà (‚àí1, 0).
2 I(1‚àív 2 ) ( 2 , 2 )
Here Ix (Œ±, Œ≤) is the regularized incomplete beta function (the cumulative distribution function of
the beta distribution with shape parameters Œ± and Œ≤).
Proof. By Assumption 3 (f) (ii), there exists Œ¥ÃÑ ‚àà (0, ¬µ2 ) such that A(x) = 0 for almost every
x ‚àà N (X , 3Œ¥ÃÑ) \ ‚Ñ¶‚àó . By Taylor‚Äôs theorem, for every u ‚àà ‚àÇ‚Ñ¶‚àó ‚à© N (X , Œ¥ÃÑ) and a ‚àà B(0, 2Œ¥ÃÑ),
ds‚Ñ¶‚àó (u + a) = ds‚Ñ¶‚àó (u) + ‚àáds‚Ñ¶‚àó (u)0 a + a0 R(u, a)a,
where

Z
R(u, a) =

1

(1 ‚àí t)D2 ds‚Ñ¶‚àó (u + ta)dt.

0

Since D2 ds‚Ñ¶‚àó is
cl(N (‚àÇ‚Ñ¶‚àó , 2Œ¥ÃÑ)).

continuous and cl(N (‚àÇ‚Ñ¶‚àó , 2Œ¥ÃÑ)) is bounded and closed, D2 ds‚Ñ¶‚àó is bounded on
Therefore, R(¬∑, ¬∑) is bounded on ‚àÇ‚Ñ¶‚àó ‚à© N (X , Œ¥ÃÑ) √ó B(0, 2Œ¥ÃÑ). It also follows that
ds‚Ñ¶‚àó (u + a) = ŒΩ‚Ñ¶‚àó (u)0 a + a0 R(u, a)a,
76

since ds‚Ñ¶‚àó (u) = 0 and ‚àáds‚Ñ¶‚àó (u) = ŒΩ‚Ñ¶‚àó (u) for every u ‚àà ‚àÇ‚Ñ¶‚àó ‚à© N (X , 2Œ¥ÃÑ) by Lemma B.1. For
(u, v, Œ¥) ‚àà ‚àÇ‚Ñ¶‚àó ‚à© N (X , Œ¥ÃÑ) √ó (‚àí1, 1) √ó (0, Œ¥ÃÑ),
pA (u + Œ¥vŒΩ‚Ñ¶‚àó (u); Œ¥)
R
‚àó
B(0,1) A(u + Œ¥vŒΩ‚Ñ¶ (u) + Œ¥w)dw
R
=
B(0,1) dw
R
‚àó
‚àó
B(0,1) 1{u + Œ¥vŒΩ‚Ñ¶ (u) + Œ¥w ‚àà ‚Ñ¶ }dw
=
Volp
R
s
‚àó
B(0,1) 1{d‚Ñ¶‚àó (u + Œ¥(vŒΩ‚Ñ¶ (u) + w)) ‚â• 0)}dw
=
Volp
R
0
2
0
‚àó
‚àó
‚àó
‚àó
‚àó
B(0,1) 1{Œ¥ŒΩ‚Ñ¶ (u) (vŒΩ‚Ñ¶ (u) + w) + Œ¥ (vŒΩ‚Ñ¶ (u) + w) R(u, Œ¥(vŒΩ‚Ñ¶ (u) + w))(vŒΩ‚Ñ¶ (u) + w) ‚â• 0}dw
=
,
Volp
where Volp denotes the volume of the p-dimensional unit ball, and the second equality follows
since u + Œ¥vŒΩ‚Ñ¶‚àó (u) + Œ¥w ‚àà N (X , 3Œ¥ÃÑ) and hence A(u + Œ¥vŒΩ‚Ñ¶‚àó (u) + Œ¥w) = 0 for almost every
w ‚àà B(0, 1) such that u + Œ¥vŒΩ‚Ñ¶‚àó (u) + Œ¥w ‚àà
/ ‚Ñ¶‚àó . Observe that
1{Œ¥ŒΩ‚Ñ¶‚àó (u)0 (vŒΩ‚Ñ¶‚àó (u) + w) + Œ¥ 2 (vŒΩ‚Ñ¶‚àó (u) + w)0 R(u, Œ¥(vŒΩ‚Ñ¶‚àó (u) + w))(vŒΩ‚Ñ¶‚àó (u) + w) ‚â• 0}
= 1{v + ŒΩ‚Ñ¶‚àó (u) ¬∑ w + Œ¥(vŒΩ‚Ñ¶‚àó (u) + w)0 R(u, Œ¥(vŒΩ‚Ñ¶‚àó (u) + w))(vŒΩ‚Ñ¶‚àó (u) + w) ‚â• 0}
= 1{v + ŒΩ‚Ñ¶‚àó (u) ¬∑ w ‚â• 0}
‚àí 1{v + ŒΩ‚Ñ¶‚àó (u) ¬∑ w ‚â• 0, v + ŒΩ‚Ñ¶‚àó (u) ¬∑ w + Œ¥(vŒΩ‚Ñ¶‚àó (u) + w)0 R(u, Œ¥(vŒΩ‚Ñ¶‚àó (u) + w))(vŒΩ‚Ñ¶‚àó (u) + w) < 0}
|
{z
}
=a(u,v,w,Œ¥)

+ 1{v + ŒΩ‚Ñ¶‚àó (u) ¬∑ w < 0, v + ŒΩ‚Ñ¶‚àó (u) ¬∑ w + Œ¥(vŒΩ‚Ñ¶‚àó (u) + w)0 R(u, Œ¥(vŒΩ‚Ñ¶‚àó (u) + w))(vŒΩ‚Ñ¶‚àó (u) + w) ‚â• 0} .
|
{z
}
=b(u,v,w,Œ¥)

Note that the set {w ‚àà B(0, 1) : v + ŒΩ(u) ¬∑ w ‚â• 0} is a region of the p-dimensional unit ball cut
off by the plane {w ‚àà Rp : v + ŒΩ(u) ¬∑ w = 0}. The distance from the center of the unit ball to
the plane is |v|. Using the formula for the volume of a hyperspherical cap (see e.g. Li (2011)),
we have
(
Z
1
Volp ‚àí 21 Volp I(2(1‚àív)‚àí(1‚àív)2 ) ( p+1
for v ‚àà [0, 1)
2 , 2)
1{v + ŒΩ(u) ¬∑ w ‚â• 0}dw =
p+1 1
1
for v ‚àà (‚àí1, 0).
B(0,1)
2 Volp I(2(1+v)‚àí(1+v)2 ) ( 2 , 2 )
Therefore, for every (u, v, Œ¥) ‚àà ‚àÇ‚Ñ¶‚àó ‚à© N (X , Œ¥ÃÑ) √ó (‚àí1, 1) √ó (0, Œ¥ÃÑ),
R
B(0,1) (‚àía(u, v, w, Œ¥) + b(u, v, w, Œ¥))dw
A
p (u + Œ¥vŒΩ‚Ñ¶‚àó (u); Œ¥) = k(v) +
.
Volp
Now let r(u, v, Œ¥) = Œ¥ ‚àí1 (pA (u + Œ¥vŒΩ‚Ñ¶‚àó (u); Œ¥) ‚àí k(v)). Since R(¬∑, ¬∑) is bounded on ‚àÇ‚Ñ¶‚àó ‚à©
N (X , Œ¥ÃÑ) √ó B(0, 2Œ¥ÃÑ) and kŒΩ‚Ñ¶‚àó (u)k = 1, there exists rÃÑ > 0 such that
|(vŒΩ‚Ñ¶‚àó (u) + w)0 R(u, Œ¥(vŒΩ‚Ñ¶‚àó (u) + w))(vŒΩ‚Ñ¶‚àó (u) + w)| ‚â§ rÃÑ

77

for every (u, v, w, Œ¥) ‚àà ‚àÇ‚Ñ¶‚àó ‚à© N (X , Œ¥ÃÑ) √ó (‚àí1, 1) √ó B(0, 1) √ó (0, Œ¥ÃÑ). Therefore,
0 ‚â§ a(u, v, w, Œ¥) ‚â§ 1{0 ‚â§ v + ŒΩ‚Ñ¶‚àó (u) ¬∑ w < Œ¥rÃÑ}
and
0 ‚â§ b(u, v, w, Œ¥) ‚â§ 1{‚àíŒ¥rÃÑ ‚â§ v + ŒΩ‚Ñ¶‚àó (u) ¬∑ w < 0}.
It then follows that
R
R
‚àó
B(0,1) (‚àía(u, v, w, Œ¥) + b(u, v, w, Œ¥))dw
B(0,1) 1{0 ‚â§ v + ŒΩ‚Ñ¶ (u) ¬∑ w < Œ¥rÃÑ}dw
‚â§
‚àí
Volp
Volp
R
‚àó
B(0,1) 1{‚àíŒ¥rÃÑ ‚â§ v + ŒΩ‚Ñ¶ (u) ¬∑ w < 0}dw
‚â§
.
Volp
The set {w ‚àà B(0, 1) : 0 ‚â§ v + ŒΩ‚Ñ¶‚àó (u) ¬∑ w < Œ¥rÃÑ} is a region of the p-dimensional unit ball cut
off by the two planes {w ‚àà Rp : v + ŒΩ‚Ñ¶‚àó (u) ¬∑ w = 0} and {w ‚àà Rp : v + ŒΩ‚Ñ¶‚àó (u) ¬∑ w = Œ¥rÃÑ}. Its
Lebesgue measure is at most the volume of the (p ‚àí 1)-dimensional unit ball times the distance
between the two planes, so
Z
‚àíŒ¥Volp‚àí1 rÃÑ ‚â§ ‚àí
1{0 ‚â§ v + ŒΩ‚Ñ¶‚àó (u) ¬∑ w < Œ¥rÃÑ}dw.
B(0,1)

Likewise,

Z
1{‚àíŒ¥rÃÑ ‚â§ v + ŒΩ‚Ñ¶‚àó (u) ¬∑ w < 0}dw ‚â§ Œ¥Volp‚àí1 rÃÑ.
B(0,1)

Therefore,
Œ¥Volp‚àí1 rÃÑ
‚â§
‚àí
Volp

R

B(0,1) (‚àía(u, v, w, Œ¥)

+ b(u, v, w, Œ¥))dw

Volp

‚â§

Œ¥Volp‚àí1 rÃÑ
.
Volp

It follows that
R
r(u, v, Œ¥) = Œ¥ ‚àí1

B(0,1) (‚àía(u, v, w, Œ¥)

+ b(u, v, w, Œ¥))dw

Volp
Volp‚àí1 rÃÑ Volp‚àí1 rÃÑ
‚àà [‚àí
,
],
Volp
Volp

and hence r is bounded on ‚àÇ‚Ñ¶‚àó ‚à© N (X , Œ¥ÃÑ) √ó (‚àí1, 1) √ó (0, Œ¥ÃÑ).
Step C.3.3.2. For every (u, v, Œ¥) ‚àà ‚àÇ‚Ñ¶‚àó ‚à© N (X , Œ¥ÃÑ) √ó (‚àí1, 1) √ó (0, Œ¥ÃÑ), pA (u + Œ¥vŒΩ‚Ñ¶‚àó (u); Œ¥) ‚àà (0, 1).
Proof. Fix (u, v, Œ¥) ‚àà ‚àÇ‚Ñ¶‚àó ‚à© N (X , Œ¥ÃÑ) √ó (‚àí1, 1) √ó (0, Œ¥ÃÑ). Suppose v = 0. By Step C.3.3.1,
pA (u) = limŒ¥0 ‚Üí0 pA (u; Œ¥ 0 ) = k(0) = 21 . This implies that there exists Œ¥ 0 ‚àà (0, Œ¥) such that
pA (u; Œ¥ 0 ) ‚àà (0, 1). It then follows that 0 < Lp (B(u, Œ¥ 0 ) ‚à© ‚Ñ¶‚àó ) ‚â§ Lp (B(x, Œ¥) ‚à© ‚Ñ¶‚àó ) and that
p
‚àó)
0 < Lp (B(x, Œ¥ 0 ) \ ‚Ñ¶‚àó ) ‚â§ Lp (B(x, Œ¥) \ ‚Ñ¶‚àó ). Therefore, pA (u; Œ¥) = L L(B(u,Œ¥)‚à©‚Ñ¶
‚àà (0, 1).
p (B(u,Œ¥))
Suppose v 6= 0 and let  ‚àà (0, Œ¥(1 ‚àí |v|)). Note that B(u, ) ‚äÇ B(u + Œ¥vŒΩ‚Ñ¶‚àó (u), Œ¥), since for
any x ‚àà B(u, ), ku + Œ¥vŒΩ‚Ñ¶‚àó (u) ‚àí xk ‚â§ kŒ¥vŒΩ‚Ñ¶‚àó (u)k + ku ‚àí xk ‚â§ Œ¥|v| +  < Œ¥. Since pA (u) = 21 ,
there exists 0 ‚àà (0, ) such that pA (u; 0 ) ‚àà (0, 1). It then follows that 0 < Lp (B(u, 0 ) ‚à© ‚Ñ¶‚àó ) ‚â§
Lp (B(u, )‚à©‚Ñ¶‚àó ) ‚â§ Lp (B(u+Œ¥vŒΩ‚Ñ¶‚àó (u), Œ¥)‚à©‚Ñ¶‚àó ) and that 0 < Lp (B(x, 0 )\‚Ñ¶‚àó ) ‚â§ Lp (B(x, )\‚Ñ¶‚àó ) ‚â§
p
‚àó
‚Ñ¶‚àó (u),Œ¥)‚à©‚Ñ¶ )
Lp (B(u + Œ¥vŒΩ‚Ñ¶‚àó (u), Œ¥) \ ‚Ñ¶‚àó ). Therefore, pA (u + Œ¥vŒΩ‚Ñ¶‚àó (u); Œ¥) = L L(B(u+Œ¥vŒΩ
‚àà (0, 1).
p (B(u+Œ¥vŒΩ ‚àó (u),Œ¥))
‚Ñ¶

78

Step C.3.3.3. Let g : Rp ‚Üí R be a function that is bounded on N (‚àÇ‚Ñ¶‚àó , Œ¥ 0 ) ‚à© N (X , Œ¥ 0 ) for some
Œ¥ 0 > 0. Then, for l ‚â• 0, there exist Œ¥ÃÉ > 0 and constant C > 0 such that
|Œ¥ ‚àí1 E[pA (Xi ; Œ¥)l g(Xi )1{pA (Xi ; Œ¥) ‚àà (0, 1)}]| ‚â§ C
for every Œ¥ ‚àà (0, Œ¥ÃÉ). If g is continuous on N (‚àÇ‚Ñ¶‚àó , Œ¥ 0 ) ‚à© N (X , Œ¥ 0 ) for some Œ¥ 0 > 0, then
Z 1
Z
‚àí1
A
l
A
l
Œ¥ E[p (Xi ; Œ¥) g(Xi )1{p (Xi ; Œ¥) ‚àà (0, 1)}] =
k(v) dv
g(x)fX (x)dHp‚àí1 (x) + o(1)
Œ¥ ‚àí1 E[Zi pA (Xi ; Œ¥)l g(Xi )1{pA (Xi ; Œ¥) ‚àà (0, 1)}] =

‚àÇ‚Ñ¶‚àó

‚àí1
1

Z

k(v)l dv

Z
‚àÇ‚Ñ¶‚àó

0

g(x)fX (x)dHp‚àí1 (x) + o(1)

for l ‚â• 0. Furthermore, if g is continuously differentiable and ‚àág is bounded on N (‚àÇ‚Ñ¶‚àó , Œ¥ 0 ) ‚à©
N (X , Œ¥ 0 ) for some Œ¥ 0 > 0, then
Z
Z 1
g(x)fX (x)dHp‚àí1 (x) + O(Œ¥)
k(v)l dv
Œ¥ ‚àí1 E[pA (Xi ; Œ¥)l g(Xi )1{pA (Xi ; Œ¥) ‚àà (0, 1)}] =
Œ¥ ‚àí1 E[Zi pA (Xi ; Œ¥)l g(Xi )1{pA (Xi ; Œ¥) ‚àà (0, 1)}] =

‚àÇ‚Ñ¶‚àó

‚àí1
1

Z

0

k(v)l dv

Z
‚àÇ‚Ñ¶‚àó

g(x)fX (x)dHp‚àí1 (x) + O(Œ¥)

for l ‚â• 0.
Proof. Let Œ¥ÃÑ be given in Step C.3.3.1. Under Assumption 3 (g), there exists Œ¥ÃÉ ‚àà (0, Œ¥ÃÑ) such that
fX is bounded, is continuously differentiable, and has bounded partial derivatives on N (‚àÇ‚Ñ¶‚àó , 2Œ¥ÃÉ)‚à©
N (X , 2Œ¥ÃÉ). Let Œ¥ÃÉ ‚àà (0, Œ¥ÃÑ) be such that both g and fX are bounded on N (‚àÇ‚Ñ¶‚àó , 2Œ¥ÃÉ) ‚à© N (X , 2Œ¥ÃÉ).
We first show that pA (x; Œ¥) ‚àà {0, 1} for every x ‚àà X \ N (‚àÇ‚Ñ¶‚àó , Œ¥) for every Œ¥ ‚àà (0, Œ¥ÃÉ). Pick
x ‚àà X \ N (‚àÇ‚Ñ¶‚àó , Œ¥) and Œ¥ ‚àà (0, Œ¥ÃÉ). Since B(x, Œ¥) ‚à© ‚àÇ‚Ñ¶‚àó = ‚àÖ, either B(x, Œ¥) ‚äÇ int(‚Ñ¶‚àó ) or
B(x, Œ¥) ‚äÇ int(Rp \ ‚Ñ¶‚àó ). If B(x, Œ¥) ‚äÇ int(‚Ñ¶‚àó ), pA (x; Œ¥) = 1. If B(x, Œ¥) ‚äÇ int(Rp \ ‚Ñ¶‚àó ), pA (x; Œ¥) = 0,
since A(x0 ) = 0 for almost every x0 ‚àà B(x, Œ¥) ‚äÇ N (X , 3Œ¥ÃÑ) \ ‚Ñ¶‚àó by the choice of Œ¥ÃÑ. Therefore,
{x ‚àà X : pA (x; Œ¥) ‚àà (0, 1)} ‚äÇ N (‚àÇ‚Ñ¶‚àó , Œ¥) for every Œ¥ ‚àà (0, Œ¥ÃÉ). By this and Lemma B.5, for
Œ¥ ‚àà (0, Œ¥ÃÉ),
Œ¥ ‚àí1 E[pA (Xi ; Œ¥)l g(Xi )1{pA (Xi ; Œ¥) ‚àà (0, 1)}]
Z
= Œ¥ ‚àí1 pA (x; Œ¥)l g(x)1{pA (x; Œ¥) ‚àà (0, 1)}fX (x)dx
Z
= Œ¥ ‚àí1
pA (x; Œ¥)l g(x)1{pA (x; Œ¥) ‚àà (0, 1)}fX (x)dx
= Œ¥ ‚àí1

N (‚àÇ‚Ñ¶‚àó ,Œ¥)
Œ¥ Z

Z

‚àíŒ¥

‚àÇ‚Ñ¶‚àó

pA (u + ŒªŒΩ‚Ñ¶‚àó (u); Œ¥)l g(u + ŒªŒΩ‚Ñ¶‚àó (u))1{pA (u + ŒªŒΩ‚Ñ¶‚àó (u); Œ¥) ‚àà (0, 1)}
‚àó

‚àÇ‚Ñ¶
√ó fX (u + ŒªŒΩ‚Ñ¶‚àó (u))Jp‚àí1
œà‚Ñ¶‚àó (u, Œª)dHp‚àí1 (u)dŒª.

With change of variables v = ŒªŒ¥ , we have
Œ¥ ‚àí1 E[pA (Xi ; Œ¥)l g(Xi )1{pA (Xi ; Œ¥) ‚àà (0, 1)}]
Z 1Z
=
pA (u + Œ¥vŒΩ‚Ñ¶‚àó (u); Œ¥)l 1{pA (u + Œ¥vŒΩ‚Ñ¶‚àó (u); Œ¥) ‚àà (0, 1)}
‚àí1

‚àÇ‚Ñ¶‚àó

‚àó

‚àÇ‚Ñ¶
√ó g(u + Œ¥vŒΩ‚Ñ¶‚àó (u))fX (u + Œ¥vŒΩ‚Ñ¶‚àó (u))Jp‚àí1
œà‚Ñ¶‚àó (u, Œ¥v)dHp‚àí1 (u)dv.

79

For every (u, v, Œ¥) ‚àà ‚àÇ‚Ñ¶‚àó \ N (X , Œ¥ÃÉ) √ó (‚àí1, 1) √ó (0, Œ¥ÃÉ), u + Œ¥vŒΩ‚Ñ¶‚àó (u) ‚àà
/ X , so
Œ¥ ‚àí1 E[pA (Xi ; Œ¥)l g(Xi )1{pA (Xi ; Œ¥) ‚àà (0, 1)}]
Z 1Z
pA (u + Œ¥vŒΩ‚Ñ¶‚àó (u); Œ¥)l 1{pA (u + Œ¥vŒΩ‚Ñ¶‚àó (u); Œ¥) ‚àà (0, 1)}
=
‚àÇ‚Ñ¶‚àó ‚à©N (X ,Œ¥ÃÉ)

‚àí1

‚àó

‚àÇ‚Ñ¶
√ó g(u + Œ¥vŒΩ‚Ñ¶‚àó (u))fX (u + Œ¥vŒΩ‚Ñ¶‚àó (u))Jp‚àí1
œà‚Ñ¶‚àó (u, Œ¥v)dHp‚àí1 (u)dv
1

Z

Z

‚àó

=
‚àÇ‚Ñ¶‚àó ‚à©N (X ,Œ¥ÃÉ)

‚àí1

‚àÇ‚Ñ¶
(k(v) + Œ¥r(u, v, Œ¥))l g(u + Œ¥vŒΩ‚Ñ¶‚àó (u))fX (u + Œ¥vŒΩ‚Ñ¶‚àó (u))Jp‚àí1
œà‚Ñ¶‚àó (u, Œ¥v)dHp‚àí1 (u)dv,
‚àó

‚àÇ‚Ñ¶ œà ‚àó (¬∑, ¬∑)
where the second equality follows from Steps C.3.3.1 and C.3.3.2. By Lemma B.5, Jp‚àí1
‚Ñ¶
‚àó
is bounded on ‚àÇ‚Ñ¶ √ó (‚àíŒ¥ÃÉ, Œ¥ÃÉ). Since r, g and fX are also bounded, for some constant C > 0,

|Œ¥

‚àí1

A

l

Z

A

1

E[p (Xi ; Œ¥) g(Xi )1{p (Xi ; Œ¥) ‚àà (0, 1)}]| ‚â§ C
‚àí1

Z

dHp‚àí1 (u)dv,

‚àÇ‚Ñ¶‚àó ‚à©N (X ,Œ¥ÃÉ)

which is finite by Assumption 3 (f) (i). Moreover, if g and fX are continuous on N (‚àÇ‚Ñ¶‚àó , 2Œ¥ÃÉ) ‚à©
N (X , 2Œ¥ÃÉ), by the Dominated Convergence Theorem,
Œ¥

‚àí1

A

l

Z

A

1

E[p (Xi ; Œ¥) g(Xi )1{p (Xi ; Œ¥) ‚àà (0, 1)}] ‚Üí

l

Z

k(v) dv
‚àí1

‚àÇ‚Ñ¶‚àó

g(u)fX (u)dHp‚àí1 (u),

‚àó

‚àó

‚àÇ‚Ñ¶ œà ‚àó (u, Œª) is continuous in Œª and J ‚àÇ‚Ñ¶ œà ‚àó (u, 0) =
where we use the fact from Lemma B.5 that Jp‚àí1
‚Ñ¶
p‚àí1 ‚Ñ¶
1.
Note that A(x) = 1 for every x ‚àà ‚Ñ¶‚àó and A(x) = 0 for almost every x ‚àà N (X , 2Œ¥ÃÉ) \ ‚Ñ¶‚àó .
Also, for every (u, v, Œ¥) ‚àà ‚àÇ‚Ñ¶‚àó ‚à© N (X , Œ¥ÃÉ) √ó (‚àí1, 1) √ó (0, Œ¥ÃÉ), u + Œ¥vŒΩ‚Ñ¶‚àó (u) ‚àà ‚Ñ¶‚àó if v ‚àà (0, 1) and
u + Œ¥vŒΩ‚Ñ¶‚àó (u) ‚àà N (X , 2Œ¥ÃÉ) \ ‚Ñ¶‚àó if v ‚àà (‚àí1, 0]. Therefore,

Œ¥ ‚àí1 E[Zi pA (Xi ; Œ¥)l g(Xi )1{pA (Xi ; Œ¥) ‚àà (0, 1)}]
= Œ¥ ‚àí1 E[A(Xi )pA (Xi ; Œ¥)l g(Xi )1{pA (Xi ; Œ¥) ‚àà (0, 1)}]
Z 1Z
A(u + Œ¥vŒΩ‚Ñ¶‚àó (u))(k(v) + Œ¥r(u, v, Œ¥))l g(u + Œ¥vŒΩ‚Ñ¶‚àó (u))
=
‚àÇ‚Ñ¶‚àó ‚à©N (X ,Œ¥ÃÉ)

‚àí1

‚àó

‚àÇ‚Ñ¶
√ó fX (u + Œ¥vŒΩ‚Ñ¶‚àó (u))Jp‚àí1
œà‚Ñ¶‚àó (u, Œ¥v)dHp‚àí1 (u)dv

Z

1Z

‚àó

=
‚àÇ‚Ñ¶‚àó ‚à©N (X ,Œ¥ÃÉ)

0

Z
‚Üí

1
l

‚àÇ‚Ñ¶
(k(v) + Œ¥r(u, v, Œ¥))l g(u + Œ¥vŒΩ‚Ñ¶‚àó (u))fX (u + Œ¥vŒΩ‚Ñ¶‚àó (u))Jp‚àí1
œà‚Ñ¶‚àó (u, Œ¥v)dHp‚àí1 (u)dv

Z

k(v) dv
0

‚àÇ‚Ñ¶‚àó

g(u)fX (u)dHp‚àí1 (u).

Now suppose that g and fX are continuously differentiable on N (‚àÇ‚Ñ¶‚àó , 2Œ¥ÃÉ) ‚à© N (X , 2Œ¥ÃÉ) and
that ‚àág and ‚àáf are bounded on N (‚àÇ‚Ñ¶‚àó , 2Œ¥ÃÉ) ‚à© N (X , 2Œ¥ÃÉ). Using the mean-value theorem, we
obtain that, for any (u, v, Œ¥) ‚àà ‚àÇ‚Ñ¶‚àó ‚à© N (X , Œ¥ÃÉ) √ó (‚àí1, 1) √ó (0, Œ¥ÃÉ),
g(u + Œ¥vŒΩ‚Ñ¶‚àó (u)) = g(u) + ‚àág(yg (u, Œ¥vŒΩ‚Ñ¶‚àó (u)))0 Œ¥vŒΩ‚Ñ¶‚àó (u),
fX (u + Œ¥vŒΩ‚Ñ¶‚àó (u)) = fX (u) + ‚àáfX (yf (u, Œ¥vŒΩ‚Ñ¶‚àó (u)))0 Œ¥vŒΩ‚Ñ¶‚àó (u)
80

for some yg (u, Œ¥vŒΩ‚Ñ¶‚àó (u)) and yf (u, Œ¥vŒΩ‚Ñ¶‚àó (u)) that are on the line segment connecting u and
u + Œ¥vŒΩ‚Ñ¶‚àó (u). In addition,
‚àó

‚àÇ‚Ñ¶‚àó
Jp‚àí1
œà‚Ñ¶‚àó (u, Œ¥v)

‚àÇ‚Ñ¶ œà ‚àó (u, y (u, Œ¥v))
‚àÇJp‚àí1
‚Ñ¶
J
=
+
Œ¥v
‚àÇŒª
‚àó
‚àÇ‚Ñ¶ œà ‚àó (u, y (u, Œ¥v))
‚àÇJp‚àí1
‚Ñ¶
J
=1+
Œ¥v
‚àÇŒª
‚àÇ‚Ñ¶‚àó
Jp‚àí1
œà‚Ñ¶‚àó (u, 0)

‚àó

for some yJ (u, Œ¥v) that is on the line segment connecting 0 and Œ¥v. By Lemma B.5,
is bounded on ‚àÇ‚Ñ¶‚àó √ó (‚àíŒ¥ÃÉ, Œ¥ÃÉ). We then have

‚àÇ‚Ñ¶ œà ‚àó (¬∑,¬∑)
‚àÇJp‚àí1
‚Ñ¶
‚àÇŒª

Œ¥ ‚àí1 E[pA (Xi ; Œ¥)l g(Xi )1{pA (Xi ; Œ¥) ‚àà (0, 1)}]
Z 1Z
=
(k(v) + Œ¥r(u, v, Œ¥))l (g(u) + ‚àág(yg (u, Œ¥vŒΩ‚Ñ¶‚àó (u)))0 Œ¥vŒΩ‚Ñ¶‚àó (u))
‚àí1

‚àÇ‚Ñ¶‚àó ‚à©N (X ,Œ¥ÃÉ)

‚àó

‚àÇJ ‚àÇ‚Ñ¶ œà‚Ñ¶‚àó (u,yJ (u,Œ¥v))
Œ¥v)dHp‚àí1 (u)dv
‚àÇŒª

Z

1

=
‚àí1
1

Z

√ó (fX (u) + ‚àáfX (yf (u, Œ¥vŒΩ‚Ñ¶‚àó (u)))0 Œ¥vŒΩ‚Ñ¶‚àó (u))(1 + p‚àí1
Z
(k(v)l g(u)fX (u) + Œ¥h(u, v, Œ¥))dHp‚àí1 (u)dv
‚àÇ‚Ñ¶‚àó ‚à©N (X ,Œ¥ÃÉ)

k(v)l dv

=

Z
‚àÇ‚Ñ¶‚àó

‚àí1

g(u)fX (u)dHp‚àí1 (u) + Œ¥

Z

1

Z

‚àí1

h(u, v, Œ¥)dHp‚àí1 (u)dv

‚àÇ‚Ñ¶‚àó ‚à©N (X ,Œ¥ÃÉ)

for some function h bounded on ‚àÇ‚Ñ¶‚àó ‚à© N (X , Œ¥ÃÉ) √ó (‚àí1, 1) √ó (0, Œ¥ÃÉ). It then follows that
Œ¥

‚àí1

A

l

Z

A

1

E[p (Xi ; Œ¥) g(Xi )1{p (Xi ; Œ¥) ‚àà (0, 1)}] =

l

Z

k(v) dv
‚àí1

‚àÇ‚Ñ¶‚àó

g(u)fX (u)dHp‚àí1 (u) + O(Œ¥).

Also,
Œ¥ ‚àí1 E[Zi pA (Xi ; Œ¥)l g(Xi )1{pA (Xi ; Œ¥) ‚àà (0, 1)}]
Z 1Z
‚àÇ‚Ñ¶‚àó
=
(k(v) + Œ¥r(u, v, Œ¥))l g(u + Œ¥vŒΩ‚Ñ¶‚àó (u))fX (u + Œ¥vŒΩ‚Ñ¶‚àó (u))Jp‚àí1
œà‚Ñ¶‚àó (u, Œ¥v)dHp‚àí1 (u)dv
‚àÇ‚Ñ¶‚àó ‚à©N (X ,Œ¥ÃÉ)

0

Z
=
0

1

k(v)l dv

Z
‚àÇ‚Ñ¶‚àó

g(u)fX (u)dHp‚àí1 (u) + O(Œ¥).

Step C.3.3.4. Let SD = limŒ¥‚Üí0 Œ¥ ‚àí1 E[Zi D0i 1{pA (Xi ; Œ¥) ‚àà (0, 1)}] and SY = limŒ¥‚Üí0 Œ¥ ‚àí1 E[Zi Yi 1{pA (Xi ; Œ¥) ‚àà
‚àí1
(0, 1)}]. Then the second element of SD
SY is Œ≤1 .
Proof. Note that Di = Zi Di (1) + (1 ‚àí Zi )Di (0) and Yi = Zi Y1i + (1 ‚àí Zi )Y0i . By Step C.3.3.3,
SD
Ô£Æ

2f¬ØX
Ô£Ø
=Ô£∞
f¬ØX
R1
k(v)dv f¬ØX
‚àí1

Ô£π
R1
E[Di (1) + Di (0)|Xi = x]fX (x)dHp‚àí1 (x)
k(v)dv f¬ØX
‚àí1
R1
Ô£∫
E[Di (1)|Xi = x]fX (x)dHp‚àí1 (x)
k(v)dv f¬ØX Ô£ª ,
‚àÇ‚Ñ¶‚àó
0
R0
R
R
R1
1
( k(v)dvE[Di (1)|Xi = x] + ‚àí1 k(v)dvE[Di (0)|Xi = x])fX (x)dHp‚àí1 (x) ‚àí1 k(v)2 dv f¬ØX
‚àÇ‚Ñ¶‚àó 0
R

‚àÇ‚Ñ¶‚àóR

81

where f¬ØX =

fX (x)dHp‚àí1 (x), and
Ô£Æ
Ô£π
R
p‚àí1 (x)
‚àÇ‚Ñ¶R‚àó E[Y1i + Y0i |Xi = x]fX (x)dH
Ô£Ø
Ô£∫
p‚àí1 (x)
SY = Ô£∞
Ô£ª.
‚àó E[Y1i |Xi = x]fX (x)dH
‚àÇ‚Ñ¶
R
R1
R0
p‚àí1
(x)
‚àÇ‚Ñ¶‚àó ( 0 k(v)dvE[Y1i |Xi = x] + ‚àí1 k(v)dvE[Y0i |Xi = x])fX (x)dH
R

‚àÇ‚Ñ¶‚àó

After a few lines of algebra, we have
Z
‚àí2
¬Ø
det(SD ) =fX
E[Di (1) ‚àí Di (0)|Xi = x]fX (x)dHp‚àí1 (x)
‚àÇ‚Ñ¶‚àó
Z 1
Z 1
Z 0
Z 0
k(s)ds)2 dv),
(k(v) ‚àí
k(s)ds)2 dv +
(k(v) ‚àí
√ó(
‚àí1

‚àí1

0

0

which is nonzero under Assumption 3 (b) and (f) (i). After another few lines of algebra, we
‚àí1
obtain that the second element of SD
SY is
R
(1) ‚àí Di (0))(Yi (1) ‚àí Yi (0))|Xi = x]fX (x)dHp‚àí1 (x)
‚àÇ‚Ñ¶‚àó E[(D
Ri
.
p‚àí1 (x)
‚àÇ‚Ñ¶‚àó E[Di (1) ‚àí Di (0)|Xi = x]fX (x)dH
On the other hand, by Step C.3.3.3,
Œ≤1 = lim E[œâi (Œ¥)(Yi (1) ‚àí Yi (0))]
Œ¥‚Üí0

Œ¥ ‚àí1 E[pA (Xi ; Œ¥)(1 ‚àí pA (Xi ; Œ¥))(Di (1) ‚àí Di (0))(Yi (1) ‚àí Yi (0))1{pA (Xi ; Œ¥) ‚àà (0, 1)}]
Œ¥ ‚àí1 E[pA (Xi ; Œ¥)(1 ‚àí pA (Xi ; Œ¥))(Di (1) ‚àí Di (0))1{pA (Xi ; Œ¥) ‚àà (0, 1)}]
R
p‚àí1 (x)
‚àí1 k(v)(1 ‚àí k(v))dv ‚àÇ‚Ñ¶‚àó E[(Di (1) ‚àí Di (0))(Yi (1) ‚àí Yi (0))|Xi = x]fX (x)dH
=
R
R1
p‚àí1 (x)
‚àí1 k(v)(1 ‚àí k(v))dv ‚àÇ‚Ñ¶‚àó E[Di (1) ‚àí Di (0)|Xi = x]fX (x)dH
R
p‚àí1 (x)
‚àó E[(Di (1) ‚àí Di (0))(Yi (1) ‚àí Yi (0))|Xi = x]fX (x)dH
R
= ‚àÇ‚Ñ¶
.
p‚àí1 (x)
‚àÇ‚Ñ¶‚àó E[Di (1) ‚àí Di (0)|Xi = x]fX (x)dH
= lim
Œ¥‚Üí0
R1

p

Step C.3.3.5. If nŒ¥n ‚Üí ‚àû as n ‚Üí ‚àû, then Œ≤ÃÇ1 ‚àí‚Üí Œ≤1 .
P
P
Proof. It suffices to verify that the variance of each element of nŒ¥1n ni=1 Zi D0i Ii and nŒ¥1n ni=1 Zi Y Ii
P
is o(1). Here, we only verify that Var( nŒ¥1n ni=1 pA (Xi ; Œ¥n )Yi Ii ) = o(1). Note that
E[Yi2 |Xi ] = E[Zi Y1i2 + (1 ‚àí Zi )Y0i2 |Xi ] ‚â§ E[Y1i2 + Y0i2 |Xi ].
Under Assumption 3 (g), there exists Œ¥ 0 > 0 such that E[Y1i2 +Y0i2 |Xi ] is continuous on N (‚àÇ‚Ñ¶‚àó , Œ¥ 0 ).
Since cl(N (‚àÇ‚Ñ¶‚àó , 12 Œ¥ 0 )) is closed and bounded, E[Y1i2 + Y0i2 |Xi ] is bounded on cl(N (‚àÇ‚Ñ¶‚àó , 12 Œ¥ 0 )). We
have
n
1 X A
1 ‚àí1
p (Xi ; Œ¥n )Yi Ii ) ‚â§
Œ¥ E[pA (Xi ; Œ¥n )2 Yi2 Ii ]
Var(
nŒ¥n
nŒ¥n n
i=1

1 ‚àí1
Œ¥ E[pA (Xi ; Œ¥n )2 E[Yi2 |Xi ]Ii ]
nŒ¥n n
1
‚â§
C
nŒ¥n
for some C > 0, where the last inequality follows from Step C.3.3.3. The conclusion follows since
nŒ¥n ‚Üí ‚àû.
=

82

‚àí1
Now let Œ≤ = (Œ≤0 , Œ≤1 , Œ≤2 )0 = SD
SY and let i = Yi ‚àí D0i Œ≤. We can write
n
n
p
1 X
1 X
0
‚àí1
nŒ¥n (Œ≤ÃÇ ‚àí Œ≤) = (
Zi D i I i ) ‚àö
Zi i Ii
nŒ¥n
nŒ¥n
i=1

i=1

n
n
1 X
1 X
=(
Zi D0i Ii )‚àí1 ‚àö
{(Zi i Ii ‚àí E[Zi i Ii ]) + E[Zi i Ii ]}.
nŒ¥n
nŒ¥n
i=1

Step C.3.3.6.
‚àö

i=1

n
1 X
d
(Zi i Ii ‚àí E[Zi i Ii ]) ‚àí‚Üí N (0, V),
nŒ¥n i=1

where V = limn‚Üí‚àû Œ¥n‚àí1 E[2i Zi Zi Ii ].
Proof. We use the triangular-array Lyapunov CLT and the Cram√©r-Wold device. Pick a nonzero
1
Œª0 (Zi i Ii ‚àí E[Zi i Ii ]). First,
Œª ‚àà Rp , and let Vi,n = ‚àönŒ¥
n

n
X

2
E[Vi,n
] = Œ¥n‚àí1 Œª0 (E[2i Zi Z0i Ii ] ‚àí E[Zi i Ii ]E[Z0i i Ii ])Œª.

i=1

By Step C.3.3.3,
E[Zi i Ii ] = E[Zi (Yi ‚àí D0i Œ≤)Ii ] = O(Œ¥n ),
so
Œ¥n‚àí1 E[Zi i Ii ]E[Z0i i Ii ] = o(1).
We have
E[2i Zi Z0i Ii ] = E[(Yi ‚àí Œ≤0 ‚àí Œ≤1 Di ‚àí Œ≤2 pA (Xi ; Œ¥n ))2 Zi Z0i Ii ]
= E[Zi (Y1i ‚àí Œ≤0 ‚àí Œ≤1 Di (1) ‚àí Œ≤2 pA (Xi ; Œ¥n ))2 Zi Z0i Ii ]
+ E[(1 ‚àí Zi )(Y0i ‚àí Œ≤0 ‚àí Œ≤1 Di (0) ‚àí Œ≤2 pA (Xi ; Œ¥n ))2 Zi Z0i Ii ].
Since E[Y1i |Xi ], E[Y0i |Xi ], E[Di (1)|Xi ], E[Di (0)|Xi ], E[Y1i2 |Xi ], E[Y0i2 |Xi ], E[Y1i Di (1)|Xi ] and
E[Y0i Di (0)|Xi ] are continuous on N (‚àÇ‚Ñ¶‚àó , Œ¥ 0 ) for some Œ¥ 0 > 0 under Assumption 3 (g), limn‚Üí‚àû Œ¥n‚àí1 E[2i Zi Z0i Ii ]
exists and finite. Therefore,
n
X
2
E[Vi,n
] ‚Üí Œª0 VŒª < 0.
i=1

We next verify the Lyapunov condition: for some t > 0,
n
X

E[|Vi,n |2+t ] ‚Üí 0.

i=1

We have
n
X

E[|Vi,n |4 ] =

i=1

‚â§

1 ‚àí1
Œ¥ E[|Œª0 (Zi i Ii ‚àí E[Zi i Ii ])|4 ]
nŒ¥n n
1 3c ‚àí1
2 Œ¥n {E[|Œª0 Zi i Ii |4 ] + |Œª0 E[Zi i Ii ]|4 }
nŒ¥n
83

by the cr -inequality. Repeating using the cr -inequality gives
Œ¥n‚àí1 E[|Œª0 Zi i Ii |4 ] = Œ¥n‚àí1 E[|Œª0 Zi (Yi ‚àí Œ≤0 ‚àí Œ≤1 Di ‚àí Œ≤2 pA (Xi ; Œ¥n ))|4 Ii ]
‚â§ 23c Œ¥n‚àí1 E[(|Œª0 Zi |4 )(|Yi |4 + |Œ≤0 |4 + |Œ≤1 |4 Di + |Œ≤2 |4 pA (Xi ; Œ¥n )4 )Ii ]
‚â§ 23c (Œª1 + Œª2 + Œª3 )4 Œ¥n‚àí1 E[(Yi4 + Œ≤04 + Œ≤14 + Œ≤24 )Ii ]
= 23c O(1)
for some finite constant c, where the last equality holds by Step C.3.3.3 under Assumption 3 (g).
Moreover,
Œ¥n‚àí1 |Œª0 E[Zi i Ii ]|4 = Œ¥n3 |Œª0 Œ¥n‚àí1 E[Zi i Ii ]|4
= Œ¥n3 O(1)
= o(1).
Therefore, when nŒ¥n ‚Üí ‚àû,

n
X

E[|Vi,n |4 ] ‚Üí 0,

i=1

and the conclusion follows from the Lyapunov CLT and the Cram√©r-Wold device.
p

‚àí1
0 )‚àí1 .
Step C.3.3.7. nŒ¥n Œ£ÃÇ ‚àí‚Üí SD
V(SD

Proof. We have
n
n
1 X 2
1 X
ÀÜi Zi Z0i Ii =
(Yi ‚àí D0i Œ≤ÃÇ)2 Zi Z0i Ii
nŒ¥n
nŒ¥n
i=1

i=1

n
1 X
=
(i ‚àí D0i (Œ≤ÃÇ ‚àí Œ≤))2 Zi Z0i Ii
nŒ¥n

=

1
nŒ¥n
‚àí
+

i=1
n
X

2i Zi Z0i Ii

i=1

n
2 X
(Yi ‚àí D0i Œ≤)((Œ≤ÃÇ0 ‚àí Œ≤0 ) + Di (Œ≤ÃÇ1 ‚àí Œ≤1 ) + pA (Xi ; Œ¥n )(Œ≤ÃÇ2 ‚àí Œ≤2 ))Zi Z0i Ii
nŒ¥n

1
nŒ¥n

i=1
n
X

((Œ≤ÃÇ0 ‚àí Œ≤0 ) + Di (Œ≤ÃÇ1 ‚àí Œ≤1 ) + pA (Xi ; Œ¥n )(Œ≤ÃÇ2 ‚àí Œ≤2 ))2 Zi Z0i Ii

i=1

n
1 X 2
=
i Zi Z0i Ii + op (1)Op (1),
nŒ¥n
i=1

where the last equality follows from the result that Œ≤ÃÇ ‚àí Œ≤ = op (1) and from application of
P
p
Step C.3.3.3 as in Steps C.3.3.5 and C.3.3.6. To show nŒ¥1n ni=1 2i Zi Z0i Ii ‚àí‚Üí V, it suffices
P
to verify that the variance of each element of nŒ¥1n ni=1 2i Zi Z0i Ii is o(1). We only verify that

84

Var( nŒ¥1n

Pn

2 A
2
i=1 i p (Xi ; Œ¥n ) Ii )

Var(

= o(1). Using the cr -inequality, we have that for some constant c,

n
1 X 2 A
1 ‚àí1
Œ¥ E[4i Ii ]
i p (Xi ; Œ¥n )2 Ii ) ‚â§
nŒ¥n
nŒ¥n n
i=1

1 ‚àí1
Œ¥ E[(Yi ‚àí Œ≤0 ‚àí Œ≤1 Di ‚àí Œ≤2 pA (Xi ))4 Ii ]
nŒ¥n n
1 3c ‚àí1
‚â§
2 Œ¥n E[(Yi4 + Œ≤04 + Œ≤14 Di + Œ≤24 pA (Xi )4 )Ii ]
nŒ¥n
1 3c ‚àí1
‚â§
2 Œ¥n E[(Yi4 + Œ≤04 + Œ≤14 + Œ≤24 )Ii ]
nŒ¥n
1 3c
=
2 O(1)
nŒ¥n
= o(1),
=

where the second last equality holds by Step C.3.3.3 under Assumption 3 (g). Therefore,
n
1 X 2
p
ÀÜi Zi Z0i Ii ‚àí‚Üí V.
nŒ¥n
i=1

It follows that
n
n
n
X
1 X
1 X
p
‚àí1
0 ‚àí1
0
‚àí1 1
2
0
V(SD
) .
Zi Di Ii ) (
ÀÜi Zi Zi Ii )(
Di Z0i Ii )‚àí1 ‚àí‚Üí SD
nŒ¥n Œ£ÃÇ = (
nŒ¥n
nŒ¥n
nŒ¥n
i=1

i=1

i=1

d

Step C.3.3.8. œÉÃÇ ‚àí1 (Œ≤ÃÇ1 ‚àí Œ≤1 ) ‚àí‚Üí N (0, 1).
‚àí1 ‚àí1
Œ¥n E[Zi Yi Ii ]. We then have
Proof. Let Œ≤n = SD
n
p
1 X
‚àö
E[Zi i Ii ] = nŒ¥n Œ¥n‚àí1 E[Zi (Yi ‚àí D0 Œ≤)Ii ]
nŒ¥n i=1
p
= nŒ¥n Œ¥n‚àí1 E[Zi (Yi ‚àí D0i Œ≤n + D0i (Œ≤n ‚àí Œ≤))Ii ]
p
= nŒ¥n Œ¥n‚àí1 {E[Zi Yi Ii ] ‚àí E[Zi D0i Ii ]Œ≤n + E[Zi D0i Ii ](Œ≤n ‚àí Œ≤)}
p
‚àí1 ‚àí1
= nŒ¥n {(SD ‚àí Œ¥n‚àí1 E[Zi D0i Ii ])SD
Œ¥n E[Zi Yi Ii ]
‚àí1 ‚àí1
+ Œ¥n‚àí1 E[Zi D0i Ii ]SD
(Œ¥n E[Zi Yi Ii ] ‚àí SY )}

p
nŒ¥n (O(Œ¥n )O(1) + O(1)O(Œ¥n ))
p
= O( nŒ¥n Œ¥n ),

=

where we use Step C.3.3.3 for the second last equality. Thus, when nŒ¥n3 ‚Üí 0,
n
n
p
1 X
1 X
nŒ¥n (Œ≤ÃÇ ‚àí Œ≤) = (
Zi D0i Ii )‚àí1 ‚àö
{(Zi i Ii ‚àí E[Zi i Ii ]) + E[Zi i Ii ]}
nŒ¥n
nŒ¥n
i=1

d

‚àí‚Üí

i=1

‚àí1
0 ‚àí1
N (0, SD
V(SD
) ).

The conclusion then follows from Step C.3.3.7.

85

C.3.4

Consistency and Asymptotic Normality of Œ≤ÃÇ1s When Pr(A(Xi ) ‚àà (0, 1)) = 0

Let Iis = 1{ps (Xi ; Œ¥n ) ‚àà (0, 1)}, Dsi = (1, Di , ps (Xi ; Œ¥n ))0 and Zsi = (1, Zi , ps (Xi ; Œ¥n ))0 . Œ≤ÃÇ s and Œ£ÃÇs
are given by
n
n
X
X
Œ≤ÃÇ s = (
Zsi (Dsi )0 Iis )‚àí1
Zsi Yi Iis .
i=1

and

i=1

n
n
n
X
X
X
s
s 0 s ‚àí1
s 2 s
s 0 s
Œ£ÃÇ = (
Zi (Di ) Ii ) ( (ÀÜ
i ) Zi (Zi ) Ii )(
Dsi (Zsi )0 Iis )‚àí1 ,
s

i=1

i=1

i=1

where ÀÜsi = Yi ‚àí (Dsi )0 Œ≤ÃÇ s . It is sufficient to show that
Œ≤ÃÇ s ‚àí Œ≤ÃÇ = op (1),
if Sn ‚Üí ‚àû and that
p
nŒ¥n (Œ≤ÃÇ s ‚àí Œ≤ÃÇ) = op (1),
p

‚àí1
0 ‚àí1
nŒ¥n Œ£ÃÇs ‚àí‚Üí SD
V(SD
)

if Assumption 5 holds.
2
Step C.3.4.1. Let {Vi }‚àû
i=1 be i.i.d. random variables. If E[Vi |Xi ] and E[Vi |Xi ] are bounded on
N (‚àÇ‚Ñ¶‚àó , Œ¥ 0 ) ‚à© N (X , Œ¥ 0 ) for some Œ¥ 0 > 0, and Sn ‚Üí ‚àû, then
n
n
1 X
1 X
Vi ps (Xi ; Œ¥n )l Iis ‚àí
Vi pA (Xi ; Œ¥n )l Ii = op (1)
nŒ¥n
nŒ¥n
i=1

i=1

for l = 0, 1, 2, 3, 4. If, in addition, Assumption 5 holds, then
n
n
1 X
1 X
s
l s
‚àö
Vi p (Xi ; Œ¥n ) Ii ‚àí ‚àö
Vi pA (Xi ; Œ¥n )l Ii = op (1)
nŒ¥n i=1
nŒ¥n i=1

for l = 0, 1, 2.
Proof. We have
n
n
1 X
1 X
Vi ps (Xi ; Œ¥n )l Iis ‚àí
Vi pA (Xi ; Œ¥n )l Ii
nŒ¥n
nŒ¥n
i=1

i=1

n
n
1 X
1 X
=
Vi ps (Xi ; Œ¥n )l (Iis ‚àí Ii ) +
Vi (ps (Xi ; Œ¥n )l ‚àí pA (Xi ; Œ¥n )l )Ii .
nŒ¥n
nŒ¥n
i=1

i=1

86

P
We first consider nŒ¥1n ni=1 Vi (ps (Xi ; Œ¥n )l ‚àí pA (Xi ; Œ¥n )l )Ii . By using the argument in the proof of
Step C.3.3.3 in Section C.3.3, we have
|E[
=
‚â§

n
1 X
Vi (ps (Xi ; Œ¥n )l ‚àí pA (Xi ; Œ¥n )l )Ii ]|
nŒ¥n

i=1
‚àí1
Œ¥n |E[E[Vi |Xi ]E[ps (Xi ; Œ¥n )l ‚àí pA (Xi ; Œ¥n )l |Xi ]Ii ]|
Œ¥n‚àí1 E[|E[Vi |Xi ]||E[ps (Xi ; Œ¥n )l ‚àí pA (Xi ; Œ¥n )l |Xi ]|Ii ]

Z

1

Z

=
‚àÇ‚Ñ¶‚àó ‚à©N (X ,Œ¥ÃÉ)

‚àí1

|E[Vi |Xi = u + Œ¥n vŒΩ‚Ñ¶‚àó (u)]||E[ps (u + Œ¥n vŒΩ‚Ñ¶‚àó (u); Œ¥n )l ‚àí pA (u + Œ¥n vŒΩ‚Ñ¶‚àó (u); Œ¥n )l ]|
‚àó

‚àÇ‚Ñ¶
√ó fX (u + Œ¥n vŒΩ‚Ñ¶‚àó (u))Jp‚àí1
œà‚Ñ¶‚àó (u, Œ¥n v)dHp‚àí1 (u)dv,

where the choice of Œ¥ÃÉ is as in the proof of Step C.3.3.3. By Lemma B.7, for l = 0, 1, 2,
n
1 X
Vi (ps (Xi ; Œ¥n )l ‚àí pA (Xi ; Œ¥n )l )Ii ]|
nŒ¥n
i=1
Z 1Z
1
‚àÇ‚Ñ¶‚àó
|E[Vi |Xi = u + Œ¥n vŒΩ‚Ñ¶‚àó (u)]|fX (u + Œ¥n vŒΩ‚Ñ¶‚àó (u))Jp‚àí1
‚â§
œà‚Ñ¶‚àó (u, Œ¥n v)dHp‚àí1 (u)dv
Sn ‚àí1 ‚àÇ‚Ñ¶‚àó ‚à©N (X ,Œ¥ÃÉ)

|E[

= O(Sn‚àí1 ).
Also, by Lemma B.7,
|E[

n
1 X
Vi (ps (Xi ; Œ¥n )3 ‚àí pA (Xi ; Œ¥n )3 )Ii ]|
nŒ¥n

i=1
‚àí1
= |Œ¥n E[Vi (ps (Xi ; Œ¥n ) ‚àí pA (Xi ; Œ¥n ))(ps (Xi ; Œ¥n )2 + ps (Xi ; Œ¥n )pA (Xi ; Œ¥n ) + pA (Xi ; Œ¥n )2 )Ii ]|
‚â§ Œ¥n‚àí1 E[|E[Vi |Xi ]||E[(ps (Xi ; Œ¥n ) ‚àí pA (Xi ; Œ¥n ))(ps (Xi ; Œ¥n )2 + ps (Xi ; Œ¥n )pA (Xi ; Œ¥n ) + pA (Xi ; Œ¥n )2 )|Xi ]|Ii ]
‚â§ 3Œ¥n‚àí1 E[|E[Vi |Xi ]|E[|ps (Xi ; Œ¥n ) ‚àí pA (Xi ; Œ¥n )||Xi ]Ii ]
Z 1Z
=
|E[Vi |Xi = u + Œ¥n vŒΩ‚Ñ¶‚àó (u)]|E[|ps (u + Œ¥n vŒΩ‚Ñ¶‚àó (u); Œ¥n ) ‚àí pA (u + Œ¥n vŒΩ‚Ñ¶‚àó (u); Œ¥n )|]
‚àó
‚àí1 ‚àÇ‚Ñ¶ ‚à©N (X ,Œ¥ÃÉ)
‚àÇ‚Ñ¶‚àó
√ó fX (u + Œ¥n vŒΩ‚Ñ¶‚àó (u))Jp‚àí1
œà‚Ñ¶‚àó (u, Œ¥n v)dHp‚àí1 (u)dv

‚â§(

1
+ )O(1)
Sn 2

for every  > 0. We can make the right-hand side arbitrarily close to zero by taking suffiP
ciently small  > 0 and sufficiently large Sn , which implies that |E[ nŒ¥1n ni=1 Vi (ps (Xi ; Œ¥n )3 ‚àí
pA (Xi ; Œ¥n )3 )Ii ]| = o(1) if Sn ‚Üí ‚àû. Likewise,
|E[
=
‚â§
‚â§

n
1 X
Vi (ps (Xi ; Œ¥n )4 ‚àí pA (Xi ; Œ¥n )4 )Ii ]|
nŒ¥n

i=1
‚àí1
|Œ¥n E[Vi (ps (Xi ; Œ¥n )2 + pA (Xi ; Œ¥n )2 )(ps (Xi ; Œ¥n ) + pA (Xi ; Œ¥n ))(ps (Xi ; Œ¥n ) ‚àí pA (Xi ; Œ¥n ))Ii ]|
Œ¥n‚àí1 E[|E[Vi |Xi ]||E[(ps (Xi ; Œ¥n )2 + pA (Xi ; Œ¥n )2 )(ps (Xi ; Œ¥n ) + pA (Xi ; Œ¥n ))(ps (Xi ; Œ¥n ) ‚àí pA (Xi ; Œ¥n ))|Xi ]|Ii ]
8Œ¥n‚àí1 E[|E[Vi |Xi ]|E[|ps (Xi ; Œ¥n ) ‚àí pA (Xi ; Œ¥n )||Xi ]Ii ]

= o(1).
87

As for variance, for l = 0, 1, 2,
Var(

n
1 X
1 ‚àí1
Œ¥ E[Vi2 (ps (Xi ; Œ¥n )l ‚àí pA (Xi ; Œ¥n )l )2 Ii ]
Vi (ps (Xi ; Œ¥n )l ‚àí pA (Xi ; Œ¥n )l )Ii ) ‚â§
nŒ¥n
nŒ¥n n
i=1

1 ‚àí1
Œ¥ E[E[Vi2 |Xi ]E[(ps (Xi ; Œ¥n )l ‚àí pA (Xi ; Œ¥n )l )2 |Xi ]Ii ]
nŒ¥n n
4
‚â§
Œ¥ ‚àí1 E[E[Vi2 |Xi ]Ii ]
nŒ¥n Sn n
= O((nŒ¥n Sn )‚àí1 ),
‚â§

and for l = 3, 4,
Var(

n
1 X
1 ‚àí1
Œ¥ E[Vi2 (ps (Xi ; Œ¥n )l ‚àí pA (Xi ; Œ¥n )l )2 Ii ]
Vi (ps (Xi ; Œ¥n )l ‚àí pA (Xi ; Œ¥n )l )Ii ) ‚â§
nŒ¥n
nŒ¥n n
i=1

1 ‚àí1
Œ¥ E[Vi2 Ii ]
nŒ¥n n
= o(1).
‚â§

Pn

s
l
A
l
i=1 Vi (p (Xi ; Œ¥n ) ‚àí p (Xi ; Œ¥n ) )Ii = op (1) if Sn ‚Üí ‚àû for l = 0, 1, 2, 3, 4, and
s
l
A
l
‚àí1/2 S ‚Üí ‚àû for l = 0, 1, 2.
n
i=1 Vi (p (Xi ; Œ¥n ) ‚àí p (Xi ; Œ¥n ) )Ii = op (1) if n
1 Pn
s
l
s
next show that nŒ¥n i=1 Vi p (Xi ; Œ¥n ) (Ii ‚àí Ii ) = op (1) if Sn ‚Üí ‚àû for l ‚â• 0. We have

Therefore,
1 Pn
‚àö
nŒ¥n

We

1
nŒ¥n

|E[

n
1 X
Vi ps (Xi ; Œ¥n )l (Iis ‚àí Ii )]| = Œ¥n‚àí1 |E[Vi ps (Xi ; Œ¥n )l (Iis ‚àí Ii )]|
nŒ¥n
i=1

‚â§ Œ¥n‚àí1 E[|E[Vi |Xi ]||E[ps (Xi ; Œ¥n )l (Iis ‚àí Ii )|Xi ]|]
= Œ¥n‚àí1 E[|E[Vi |Xi ]|E[|Iis ‚àí Ii ||Xi ]].
Since Iis ‚àí Ii ‚â§ 0 with strict inequality only if Ii = 1,
E[|Iis ‚àí Ii ||Xi ] = ‚àíE[Iis ‚àí Ii |Xi ]Ii = (1 ‚àí E[Iis |Xi ])Ii = Pr(ps (Xi ; Œ¥n ) ‚àà {0, 1}|Xi )Ii .
We then have
n
1 X
|E[
Vi ps (Xi ; Œ¥n )l (Iis ‚àí Ii )]|
nŒ¥n
‚â§
‚â§

i=1
‚àí1
Œ¥n E[|E[Vi |Xi ]| Pr(ps (Xi ; Œ¥n ) ‚àà {0, 1}|Xi )Ii ]
Œ¥n‚àí1 E[|E[Vi |Xi ]|((1 ‚àí pA (Xi ; Œ¥n ))Sn + pA (Xi ; Œ¥n )Sn )Ii ]

Z

1

‚â§
‚àí1

Z

|E[Vi |Xi = u + Œ¥n vŒΩ‚Ñ¶‚àó (u)]|{(1 ‚àí pA (u + Œ¥n vŒΩ‚Ñ¶‚àó (u); Œ¥n ))Sn

‚àÇ‚Ñ¶‚àó ‚à©N (X ,Œ¥ÃÉ)
A

‚àó

‚àÇ‚Ñ¶
+ p (u + Œ¥n vŒΩ‚Ñ¶‚àó (u); Œ¥n )Sn }fX (u + Œ¥n vŒΩ‚Ñ¶‚àó (u))Jp‚àí1
œà‚Ñ¶‚àó (u, Œ¥n v)dHp‚àí1 (u)dv,

where the second inequality follows from Lemma B.7. Note that for every (u, v) ‚àà ‚àÇ‚Ñ¶‚àó ‚à©
N (X , Œ¥ÃÉ) √ó (‚àí1, 1), limŒ¥‚Üí0 pA (u + Œ¥n vŒΩ‚Ñ¶‚àó (u); Œ¥n ) = k(v) ‚àà (0, 1) by Step C.3.3.1 in Section C.3.3.
‚àÇ‚Ñ¶‚àó œà ‚àó are bounded, by the Bounded Convergence Theorem,
Since E[Vi |Xi ], fX and Jp‚àí1
‚Ñ¶
n
1 X
Vi ps (Xi ; Œ¥n )l (Iis ‚àí Ii )]| = o(1)
|E[
nŒ¥n
i=1

88

if Sn ‚Üí ‚àû.
As for variance,
Var(

n
1 X
1 ‚àí1
Œ¥ E[Vi2 ps (Xi ; Œ¥n )2l (Iis ‚àí Ii )2 ]
Vi ps (Xi ; Œ¥n )l (Iis ‚àí Ii )) ‚â§
nŒ¥n
nŒ¥n n
i=1

1 ‚àí1
Œ¥ E[Vi2 |Iis ‚àí Ii |]
nŒ¥n n
1 ‚àí1
=
Œ¥ E[E[Vi2 |Xi ]E[|Iis ‚àí Ii ||Xi ]]
nŒ¥n n
= o(1).
‚â§

Lastly, we show that, for l ‚â• 0,
holds. Let Œ∑n =

n
Œ≥ log
Sn ,

‚àö1
nŒ¥n

Pn

i=1 Vi p

s (X ; Œ¥ )l (I s
i n
i

‚àí Ii ) = op (1) if Assumption 5

where Œ≥ is the one satisfying Assumption 5. We have

n

1 X
|E[ ‚àö
Vi ps (Xi ; Œ¥n )l (Iis ‚àí Ii )]|
nŒ¥n i=1
q
‚â§ nŒ¥n‚àí1 E[|E[Vi |Xi ]|((1 ‚àí pA (Xi ; Œ¥n ))Sn + pA (Xi ; Œ¥n )Sn )Ii ]
q
= nŒ¥n‚àí1 E[|E[Vi |Xi ]|((1 ‚àí pA (Xi ; Œ¥n ))Sn + pA (Xi ; Œ¥n )Sn ))1{pA (Xi ; Œ¥n ) ‚àà (0, Œ∑n ) ‚à™ (1 ‚àí Œ∑n , 1)}]
q
+ nŒ¥n‚àí1 E[|E[Vi |Xi ]|((1 ‚àí pA (Xi ; Œ¥n ))Sn + pA (Xi ; Œ¥n )Sn ))1{pA (Xi ; Œ¥n ) ‚àà (Œ∑n , 1 ‚àí Œ∑n )}]
q
‚â§(
sup
|E[Vi |Xi = x]|)( nŒ¥n‚àí1 Pr(pA (Xi ; Œ¥n ) ‚àà (0, Œ∑n ) ‚à™ (1 ‚àí Œ∑n , 1))
x‚ààN (‚àÇ‚Ñ¶‚àó ,2Œ¥ÃÉ)‚à©N (X ,2Œ¥ÃÉ)

+2

p
nŒ¥n (1 ‚àí Œ∑n )Sn Œ¥n‚àí1 E[1{pA (Xi ; Œ¥n ) ‚àà (Œ∑n , 1 ‚àí Œ∑n )}]).

p
By Assumption 5, nŒ¥n‚àí1 Pr(pA (Xi ; Œ¥n ) ‚àà (0, Œ∑n ) ‚à™ (1 ‚àí Œ∑n , 1)) = o(1). For the second term,
p
p
2 nŒ¥n (1 ‚àí Œ∑n )Sn Œ¥n‚àí1 E[1{pA (Xi ; Œ¥n ) ‚àà (Œ∑n , 1 ‚àí Œ∑n )}] ‚â§ 2 nŒ¥n (1 ‚àí Œ∑n )Sn Œ¥n‚àí1 E[Ii ]
p
= 2 nŒ¥n (1 ‚àí Œ∑n )Sn O(1).
n
log n
1
‚àí1/2 S ‚Üí ‚àû and
Observe that Œ∑n = Œ≥ log
n
Sn = Œ≥ n1/2 n‚àí1/2 Sn ‚Üí 0, since n
t
that e ‚â• 1 + t for every t ‚àà R, we have
p
p
nŒ¥n (1 ‚àí Œ∑n )Sn ‚â§ nŒ¥n (e‚àíŒ∑n )Sn
p
= nŒ¥n e‚àíŒ∑n Sn
p
= nŒ¥n e‚àíŒ≥ log n
p
= nŒ¥n n‚àíŒ≥

= n1/2‚àíŒ≥ Œ¥n1/2
‚Üí 0,

89

log n
n1/2

‚Üí 0. Using the fact

since Œ≥ > 1/2. As for variance,
n
1 X
Var( ‚àö
Vi ps (Xi ; Œ¥n )l (Iis ‚àí Ii )) ‚â§ Œ¥n‚àí1 E[Vi2 ps (Xi ; Œ¥n )2l (Iis ‚àí Ii )2 ]
nŒ¥n i=1

‚â§ Œ¥n‚àí1 E[E[Vi2 |Xi ]E[|Iis ‚àí Ii ||Xi ]Ii ]
= o(1).

We have
Œ≤ÃÇ s ‚àí Œ≤ÃÇ
n
n
n
n
1 X
1 X
1 X s s 0 s ‚àí1 1 X s s
Zi (Di ) Ii )
Zi Yi Ii ‚àí (
Zi D0i Ii )‚àí1
Zi Yi Ii
=(
nŒ¥n
nŒ¥n
nŒ¥n
nŒ¥n
i=1

i=1

i=1

i=1

n
n
n
1 X s s 0 s ‚àí1 1 X s s
1 X
=(
Zi (Di ) Ii ) (
Zi Yi Ii ‚àí
Zi Yi Ii )
nŒ¥n
nŒ¥n
nŒ¥n
i=1

‚àí(

1
nŒ¥n

i=1

n
X

Zsi (Dsi )0 Iis )‚àí1 (

i=1

1
nŒ¥n

n
X

i=1

Zsi (Dsi )0 Iis ‚àí

i=1

n
n
n
1 X
1 X
1 X
Zi D0i Ii )(
Zi D0i Ii )‚àí1
Zi Yi Ii .
nŒ¥n
nŒ¥n
nŒ¥n
i=1

i=1

i=1

‚àö
By Step C.3.4.1, Œ≤ÃÇ s ‚àí Œ≤ÃÇ = op (1) if Sn ‚Üí ‚àû, and nŒ¥n (Œ≤ÃÇ s ‚àí Œ≤ÃÇ) = op (1) if Assumption 5 holds.
By proceeding as in Step C.3.3.7 in Section C.3.3, we have
n
n
1 X s 2 s s 0 s
1 X s 2 s s 0 s
(ÀÜ
i ) Zi (Zi ) Ii =
(i ) Zi (Zi ) Ii + op (1),
nŒ¥n
nŒ¥n
i=1

i=1

where si = Yi ‚àí (Dsi )0 Œ≤. Then, by Step C.3.4.1,
n
n
1 X s 2 s s 0 s
1 X 2
(ÀÜ
i ) Zi (Zi ) Ii ‚àí
i Zi Z0i Ii
nŒ¥n
nŒ¥n
i=1

i=1

n
n
1 X 2
1 X 2
=
(Yi ‚àí 2Yi (Dsi )0 Œ≤ + Œ≤ 0 Dsi (Dsi )0 Œ≤)Zsi (Zsi )0 Iis ‚àí
(Yi ‚àí 2Yi D0i Œ≤ + Œ≤ 0 Di D0i Œ≤)Zi Z0i Ii + op (1)
nŒ¥n
nŒ¥n
i=1

i=1

= op (1)
so that

n
1 X s 2 s s 0 s p
(ÀÜ
i ) Zi (Zi ) Ii ‚àí‚Üí V.
nŒ¥n
i=1

Also,

1
nŒ¥n

Pn

s
s 0 s
i=1 Zi (Di ) Ii

p

‚àí‚Üí SD by using Step C.3.4.1. The conclusion then follows.

90

C.4

Proof of Proposition A.2
x‚àó ‚àíx
Œ¥ ,

With change of variables u =
R

B(x,Œ¥) A(x

A

p (x; Œ¥) =

R
Œ¥

B(x,Œ¥) dx

‚àó

B(0,1) A(x + Œ¥u)du
R
Œ¥ p B(0,1) du

R
‚à™q‚ààQ Ux,q

A(x + Œ¥u)du +
R

R
B(0,1)\‚à™q‚ààQ Ux,q

A(x + Œ¥u)du

B(0,1) du

P
=

‚àó )dx‚àó

R
p

=
=

we have

R

q‚ààQ Ux,q

R

A(x + Œ¥u)du
,

B(0,1) du

where the last equality follows from the assumption that Lp (‚à™q‚ààQ Ux,q ) = Lp (B(0, 1)). By the
definition of Ux,q , for each q ‚àà Q, limŒ¥‚Üí0 A(x + Œ¥u) = q for any u ‚àà Ux,q . By the Dominated
Convergence Theorem,
pA (x) = lim pA (x; Œ¥)
Œ¥‚Üí0
P
p
q‚ààQ qL (Ux,q )
=
.
Lp (B(0, 1))
P
The numerator exists, since q ‚â§ 1 for all q ‚àà Q and q‚ààQ Lp (Ux,q ) = Lp (B(0, 1)).

C.5

Proof of Corollary A.1

1. Suppose that A is continuous at x ‚àà X , and let q = A(x). Then, by definition, Ux,q =
B(0, 1). By Proposition A.2, pA (x) exists, and pA (x) = q.
2. Pick any x ‚àà int(Xq ). A is continuous at x, since there exists Œ¥ > 0 such that B(x, Œ¥) ‚äÇ Xq
by the definition of interior. By the previous result, pA (x) exists, and pA (x) = q.
3. Let N be the neighborhood of x on which f is continuously differentiable. By the mean
value theorem, for any sufficiently small Œ¥ > 0,
f (x + Œ¥u) = f (x) + ‚àáf (xÃÉŒ¥ ) ¬∑ Œ¥u
= ‚àáf (xÃÉŒ¥ ) ¬∑ Œ¥u
for some xÃÉŒ¥ which is on the line segment connecting x and x + Œ¥u. Since xÃÉŒ¥ ‚Üí x as Œ¥ ‚Üí 0
and ‚àáf is continuous on N , ‚àáf (xÃÉŒ¥ ) ¬∑ u ‚Üí ‚àáf (x) ¬∑ u as Œ¥ ‚Üí 0. Therefore, if ‚àáf (x) ¬∑ u > 0,
then f (x + Œ¥u) = ‚àáf (xÃÉŒ¥ ) ¬∑ Œ¥u > 0 for any sufficiently small Œ¥ > 0, and if ‚àáf (x) ¬∑ u < 0,
then f (x + Œ¥u) = ‚àáf (xÃÉŒ¥ ) ¬∑ Œ¥u < 0 for any sufficiently small Œ¥ > 0. We then have
Ux+ ‚â° {u ‚àà B(0, 1) : ‚àáf (x) ¬∑ u > 0} ‚äÇ Ux,q1
Ux‚àí ‚â° {u ‚àà B(0, 1) : ‚àáf (x) ¬∑ u < 0} ‚äÇ Ux,q2 .
91

Let V be the Lebesgue measure of a half p-dimensional unit ball. Since V = Lp (Ux+ ) ‚â§
Lp (Ux,q1 ), V = Lp (Ux‚àí ) ‚â§ Lp (Ux,q2 ), and Lp (Ux,q1 ) + Lp (Ux,q2 ) ‚â§ Lp (B(0, 1)) = 2V , it
follows that Lp (Ux,q1 ) = Lp (Ux,q2 ) = V . By Proposition A.2, pA (x) exists, and pA (x) =
1
2 (q1 + q2 ).
4. We have that U0,q1 = {(u1 , u2 )0 ‚àà B(0, 1) : u1 ‚â§ 0 or u2 ‚â§ 0} and U0,q2 = {(u1 , u2 )0 ‚àà
B(0, 1) : u1 > 0, u2 > 0}. By Proposition A.2, pA (x) exists, and pA (x) =
3
4 q1

C.6

+

q1 L2 (U0,q1 )+q2 L2 (U0,q2 )
L2 (B(0,1))

1
4 q2 .

Proof of Proposition A.1

Since A is a Lp -measurable and bounded function, A is locally integrable with respect to the
R
Lebesgue measure, i.e., for every ball B ‚äÇ Rp , B A(x)dx exists. An application of the Lebesgue
differentiation theorem (see e.g. Theorem 1.4 in Chapter 3 of Stein and Shakarchi (2005)) to the
function A shows that
R
‚àó
‚àó
B(x,Œ¥) A(x )dx
R
lim
= A(x)
‚àó
Œ¥‚Üí0
B(x,Œ¥) dx
for almost every x ‚àà Rp .

C.7

Proof of Proposition A.3

We can prove Part (a) using the same argument in the proof of Proposition 1 (a). For Part (b),
suppose to the contrary that there exists xd ‚àà XdS such that Lpc ({xc ‚àà XcS (xd ) : pA (xd , xc ) ‚àà
{0, 1}}) > 0. Without loss of generality, assume Lpc ({xc ‚àà XcS (xd ) : pA (xd , xc ) = 1}) > 0. The
proof proceeds in five steps.
Step C.7.1. Lpc (XcS (xd ) ‚à© Xc,1 (xd )) > 0.
Step C.7.2. XcS (xd ) ‚à© int(Xc,1 (xd )) 6= ‚àÖ.
Step C.7.3. pA (xd , xc ) = 1 for any xc ‚àà int(Xc,1 (xd )).
Step C.7.4. For every x‚àóc ‚àà XcS (xd ) ‚à© int(Xc,1 (xd )), there exists Œ¥ > 0 such that B(x‚àóc , Œ¥) ‚äÇ
XcS (xd ) ‚à© int(Xc,1 (xd )).
Step C.7.5. E[Y1i ‚àí Y0i |Xi ‚àà S] is not identified.
Following the argument in the proof of Proposition 1 (b), we can prove Steps C.7.1‚ÄìC.7.3. Once
Step C.7.4 is established, we prove Step C.7.5 by following the proof of Step C.1.4 in Proposition
1 (b) with B(x‚àóc , Œ¥) and B(x‚àóc , ) in place of B(x‚àó , Œ¥) and B(x‚àó , ), respectively, using the fact
that Pr(Xci ‚àà B(x‚àóc , )|Xdi = xd ) > 0 by the definition of support. Here, we provide the proof
of Step C.7.4.
Proof of Step C.7.4. Pick an x‚àóc ‚àà XcS (xd ) ‚à© int(Xc,1 ). Then, x‚àó = (xd , x‚àóc ) ‚àà S. Since S is
open relative to X , there exists an open set U ‚àà Rp such that S = U ‚à© X . This implies that
for any sufficiently small Œ¥ > 0, B(x‚àó , Œ¥) ‚à© X ‚äÇ U ‚à© X = S. It then follows that {xc ‚àà Rpc :
92

=

(xd , xc ) ‚àà B(x‚àó , Œ¥) ‚à© X } ‚äÇ {xc ‚àà Rpc : (xd , xc ) ‚àà S}, equivalently, B(x‚àóc , Œ¥) ‚à© Xc (xd ) ‚äÇ XcS (xd ).
By choosing a sufficiently small Œ¥ > 0 so that B(x‚àóc , Œ¥) ‚äÇ int(Xc,1 (xd )) ‚äÇ Xc (xd ), we have
B(x‚àóc , Œ¥) ‚äÇ XcS (xd ) ‚à© int(Xc,1 (xd )).

C.8

Proof of Theorem A.1

The proof is analogous to the proof of Theorem 1. The only difference is that, when we prove the
convergence of expectations, we show the convergence of the expectations conditional on Xdi ,
and then take the expectations over Xdi .

D

Machine Learning Simulation: Details

Parameter Choice. For the variance-covariance matrix Œ£ of Xi , we first create a 100 √ó 100
symmetric matrix V such that the diagonal elements are one, Vij is nonzero and equal to
Vji for (i, j) ‚àà {2, 3, 4, 5, 6} √ó {35, 66, 78}, and everything else is zero. We draw values from
Unif(‚àí0.5, 0.5) independently for the nonzero off-diagonal elements of V. We then create matrix
Œ£ = V √ó V, which is a positive semidefinite matrix.
For Œ±0 and Œ±1 , we first draw Œ±ÃÉ0j , j = 51, ..., 100, from Unif(‚àí100, 100) independently
across j, and draw Œ±ÃÉ1j , j = 1, ..., 100, from Unif(‚àí150, 200) independently across j. We then
set Œ±ÃÉ0j = Œ±ÃÉ1j for j = 1, ..., 50, and calculate Œ±0 and Œ±1 by normalizing Œ±ÃÉ0 and Œ±ÃÉ1 so that
Var(Xi0 Œ±0 ) = Var(Xi0 Œ±1 ) = 1.
Training of Prediction Model. We first randomly split the sample {(YÃÉi , XÃÉi , DÃÉi , ZÃÉi )}nÃÉi=1 into
train (80%) and test datasets (20%). We use random forests on the training sample to obtain
the prediction model ¬µz and validate its performance on the test sample. The trained algorithm
has an accuracy of 97% on the test data.

E
E.1

Empirical Policy Application: Details
Hospital Cost Data

We use publicly available Healthcare Cost Report Information System (HCRIS) data,41 to
project42 safety net eligibility and funding amounts for all hospitals in the dataset. This data
set contains information on various hospital characteristics including utilization, number of employees, medicare cost data and financial statement data.
The data is available from financial year 1996 to 2019. As the coverage is higher for 2018
(compared to 2019), we utilize the data corresponding to the 2018 financial year. Hospitals are
uniquely identified in a financial year by their CMS (Center for Medicaid and Medicare Services)
41

We use the RAND cleaned version of this dataset, which can be accessed https://www.hospitaldatasets.
org/
42
We use the methodology detailed in the CARES ACT website to project funding based on 2018 financial year
cost reports.

93

Certification Number. We have data for 4,705 providers for the 2018 financial year. We focus
on 4,648 acute care and critical access hospitals that are either located in one of the 50 states or
Washington DC.

Disproportionate patient percentage
Disproportionate patient percentage is equal to the percentage of Medicare inpatient days attributable to patients eligible for both Medicare Part A and Supplemental Security Income (SSI)
summed with the percentage of total inpatient days attributable to patients eligible for Medicaid
but not Medicare Part A.43 In the data, this variable is missing for 1560 hospitals. We impute
the disproportionate patient percentage to 0 when it is missing.

Uncompensated care per bed
Cost of uncompensated care refers to the care provided by the hospital for which no compensation
was received from the patient or the insurer. It is the sum of a hospital‚Äôs bad debt and the
financial assistance it provides.44 The cost of uncompensated care is missing for 86 hospitals,
which we impute to 0. We divide the cost of uncompensated care by the number of beds in the
hospital to obtain the cost per bed. The data on bed count is missing for 15 hospitals, which we
drop from the analysis, leaving us with 4,633 hospitals in 2,473 counties.

Profit Margin
Hospital profit margins are indicative of the financial health of the hospitals. We calculate profit
margins as the ratio of net income to total revenue where total revenue is the sum of net patient
revenue and total other income. After the calculation, profit margins are missing for 92 hospitals,
which we impute to 0.

Funding
We calculate the projected funding using the formula on the CARES ACT website. Hospitals that
do not qualify on any of the three dimensions are not given any funding. Each eligible hospital
is assigned an individual facility score, which is calculated as the product of disproportionate
patient percentage and number of beds in that hospital. We calculate cumulative facility score
as the sum of all individual facility scores in the dataset. Each hospital receives a share of $10
billion, where the share is determined by the ratio of individual facility score of that hospital to
the cumulative facility score. The amount of funding received by hospitals is bounded below at
$5 million and capped above at $50 million.
43

For the precise definition, see https://www.cms.gov/Medicare/Medicare-Fee-for-Service-Payment/
AcuteInpatientPPS/dsh.
44
The precise definition can be found at https://www.aha.org/fact-sheets/2020-01-06-fact-sheetuncompensated-hospital-care-cost.

94

E.2

Hospital Utilization Data

We use the publicly available COVID-19 Reported Patient Impact and Hospital Capacity by
Facility dataset for our outcome variables. This provides facility level data on hospital utilization
aggregated on a weekly basis, from July 31st onwards. These reports are derived from two
main sources ‚Äì (1) HHS TeleTracking and (2) reporting provided directly to HHS Protect by
state/territorial health departments on behalf of health care facilities.45
The hospitals are uniquely identified for a given collection week (which goes from Friday to
Thursday) by their CMS Certification number. All hospitals that are registered with CMS by
June 1st 2020 are included in the population. We merge the hospital cost report data with the
utilization data using the CMS certification number. According to the terms and conditions of
the CARES Health Care Act, the recipients may use the relief funds only to ‚Äúprevent, prepare
for, and respond to coronavirus‚Äù and for ‚Äúhealth care related expenses or lost revenues that
are attributable to coronavirus‚Äù. Therefore, for our analysis we focus on 4 outcomes that were
directly affected by COVID-19, for the week spanning July 31st to August 6th 2020. The outcome
measures are described below.46
1. Total reports of patients currently hospitalized in an adult inpatient bed who have laboratoryconfirmed or suspected COVID-19, including those in observation beds reported during the
7-day period.
2. Total reports of patients currently hospitalized in an adult inpatient bed who have laboratoryconfirmed COVID-19 or influenza, including those in observation beds. Including patients
who have both laboratory-confirmed COVID-19 and laboratory confirmed influenza during
the 7-day period.
3. Total reports of patients currently hospitalized in a designated adult ICU bed who have
suspected or laboratory-confirmed COVID-19.
4. Total reports of patients currently hospitalized in a designated adult ICU bed who have
laboratory-confirmed COVID-19 or influenza, including patients who have both laboratoryconfirmed COVID-19 and laboratory-confirmed influenza.47
45

Source:
https://healthdata.gov/Hospital/COVID-19-Reported-Patient-Impact-and-Hospital-Capa/
anag-cw7u.
46
We conduct sanity checks and impute observations to missing if they fail our checks. For example, we impute
the value # Confirmed/ Suspected COVID Patients and # Confirmed COVID Patients to missing when the
latter is greater than the former. # Confirmed/ Suspected COVID Patients should be greater than or equal to
# Confirmed COVID Patients as the former includes the latter. Similarly, we impute # Confirmed/ Suspected
COVID Patients in ICU and # Confirmed COVID Patients in ICU to be missing when the latter is greater than
the former.
47
In the dataset, when the values of the 7 day sum are reported to be less than 4, they are replaced with
-999,999. We recode these values to be missing. The results in Table 4 remain almost the same even if we impute
the suppressed values (coded as -999,999) with 0s. Results are available upon request.

95

E.3

Computing Approximate Propensity Score

As the three determinants of safety net eligibility are continuous variables, we can think of this
setting as a multi-dimensional regression discontinuity design and a suitable setting to apply our
method. In this setting, the Xi are disproportionate patient percentage, uncompensated care
per bed and profit margin. Funding eligibility (Zi ) is determined algorithmically using these 3
dimensions. Di is the amount of funding received by hospital i, which depends on both safety net
eligibility status Zi , number of beds in the hospital, and disproportionate patient percentage.
Before calculating APS, we normalize each characteristic of Xi to have mean 0 and variance
1. For each hospital and every Œ¥ ‚àà {0.01, 0.025, 0.05, 0.075, 0.1, 0.25, 0.5}, we draw 10,000 times
from a Œ¥-ball around the normalized covariate space and calculate APS by averaging funding
eligibility Zi over these draws.

E.4

Additional Empirical Results
Figure A.2: APS Estimation with Varying Simulations S

Approximate Propensity Score by # of Simulations ( = 0.5)

Approximate Propensity Score

1.0
0.8
0.6
0.4
0.2
0.0
102

103

# Simulations (S)

104

105

Notes: The above figure plots the APS estimates for 10 randomly selected hospitals along the eligibility margin for varying
numbers of simulations S. Each line represents a different hospital. The dotted line at 104 indicates the number of
simulations we use for our main analysis.

96

Table A.1: Differential Attrition
Our Method with Approximate Propensity Score Controls
Ineligible No
Hospitals Controls
(1)
#Confirmed/Suspected
Covid Patients

.745

#Confirmed Covid Patients

.754

#Confirmed/Suspected
Covid Patients in ICU

.728

#Confirmed Covid Patients
in ICU

.744

(2)
38.19***
(8.55)
N=3532
33.97***
(7.44)
N=3558
13.18***
(2.74)
N=3445
12.16***
(2.58)
N=3503

Œ¥=
0.01
(3)

Œ¥=
0.025
(4)

Œ¥=
0.05
(5)

Œ¥=
0.075
(6)

Œ¥=
0.1
(7)

Œ¥=
0.25
(8)

Œ¥=
0.5
(9)

-15.51
(85.67)
N=73
0.85
(73.28)
N=70
13.68
(23.41)
N=72
7.97
(25.63)
N=67

-24.80
(70.81)
N=195
-30.81
(55.22)
N=191
9.54
(17.74)
N=186
-1.54
(18.89)
N=181

-44.34
(70.09)
N=392
21.32
(33.46)
N=385
5.71
(11.91)
N=374
2.79
(11.25)
N=370

-57.95
(63.06)
N=547
1.96
(29.41)
N=539
-0.83
(10.68)
N=520
0.65
(9.97)
N=514

-40.34
(48.58)
N=719
-0.39
(25.14)
N=709
2.34
(9.01)
N=678
1.87
(8.52)
N=671

2.05
(22.20)
N=1389
-1.28
(15.75)
N=1366
-0.46
(5.78)
N=1314
-1.94
(5.57)
N=1321

-4.08
(15.67)
N=1947
-8.25
(12.56)
N=1923
-4.21
(4.64)
N=1846
-4.66
(4.43)
N=1868

Notes: This table reports differential safety net eligibility effects on the availability of outcome data at the hospital level.
Column 1 presents the average of the availability indicators of the outcome variables for the ineligible hospitals. In column
2, we regress the availability indicator on dummy for safety net eligibility without any controls. In columns 3-9, we run
this regression controlling for the Approximate Propensity Score with different values of bandwidth Œ¥ on the sample with
nondegenerate Approximate Propensity Score. All Approximate Propensity Scores are computed by averaging 10,000
simulation draws. The outcome variables are the 7 day totals for the week spanning July 31st, 2020 to August 6th, 2020.
Confirmed or Suspected COVID patients refer to the sum of patients in inpatient beds with lab-confirmed/suspected
COVID-19. Confirmed COVID patients refer to the sum of patients in inpatient beds with lab-confirmed COVID-19,
including those with both lab-confirmed COVID-19 and influenza. Inpatient bed totals also include observation beds.
Similarly, Confirmed/Suspected COVID patients in ICU refer to the sum of patients in ICU beds with lab-confirmed or
suspected COVID-19. Confirmed COVID patients in ICU refers to the sum of patients in ICU beds with lab-confirmed
COVID-19, including those with both lab-confirmed COVID-19 and influenza. Robust standard errors are reported in
parenthesis.

97

