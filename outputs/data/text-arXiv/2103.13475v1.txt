Robust Stochastic Stability with Applications to Social Distancing in a Pandemic

arXiv:2103.13475v1 [cs.MA] 24 Mar 2021

Brandon C. Collins, Lisa Hines, Gia Barboza, and Philip N. Brown

Abstractâ€” The theory of learning in games has extensively
studied situations where agents respond dynamically to each
other in light of a fixed utility function. However, in many
settings of interest, agent utility functions themselves vary
as a result of past agent choices. The ongoing COVID-19
pandemic has highlighted the need to formulate and analyze
such models which feature game-environment feedback. For
instance, a highly prevalent virus may incentivize individuals
to wear masks, but extensive adoption of mask-wearing reduces
virus prevalence which in turn reduces individual incentives for
mask-wearing. What is the interplay between epidemic severity
and the behaviors of a victim population? For initial answers,
we develop a general framework using probabilistic coupling
methods that can be used to derive the stochastically stable
states of log-linear learning in certain games which feature such
game-environment feedback. We then apply this framework to
a simple dynamic game-theoretic model of social precautions
in an epidemic and give conditions under which maximallycautious social behavior in this model is stochastically stable.

I. I NTRODUCTION
In social systems and distributed engineered systems,
collective behavior is the result of many individuals making
intertwined self-interested choices. In many cases, the value
of a particular choice depends not only on the current choices
being made by others, but also on the history of past choices.
The recent COVID-19 pandemic exemplifies this. In particular, many social conventions such as wearing masks
and social distancing are known to be effective at reducing
the contagiousness of the disease [1], [2]. To effectively
deploy mitigation policies, it is critical to understand how
a populationâ€™s willingness to adopt preventative conventions
interacts dynamically with the severity of an epidemic. For
example, in the absence of an epidemic a population may
prefer not to practice social distancing, but rising case counts
may incentivize individuals to change their behavior to avoid
contracting the disease. However, an epidemicâ€™s severity at
any given time is not only a function of its own dynamics,
but also the behavioral history of the victim population. The
resulting feedback loop may lead to challenges in predicting
the effectiveness of mitigation policies.
In principle, these socio-environmental feedback loops
can be analyzed using techniques from game theory, which
has a long history of analyzing the society-scale effects of
self-interested behavior. For instance, game theory has long
been used to study the spread of social conventions [3]
using models such as the graphical coordination game [4]
This work was supported by the National Science Foundation under
Grants #DEB-2032465 and #ECCS-2013779.
The authors are with the University of Colorado at Colorado
Springs, CO 80918, USA {bcollin3,lhines,gbarboza,

philip.brown}@uccs.edu

with the stochastic learning algorithm log-linear learning [5].
However, traditional analysis techniques almost uniformly
assume that the gameâ€™s utility functions are fixed for all time,
so that the agentsâ€™ choices over time can be described by a
stationary Markov process. However, such analysis fails or
becomes unwieldy when utility functions themselves depend
on the history of play.
Analysis techniques for history-dependent games have
broad potential applications; for instance, game theoretic
methods are frequently proposed in the area of distributed
control of multiagent systems [6]â€“[9]. However, in a distributed control application, agentsâ€™ actions may directly
modify the strategic environment; for instance if a searchand-rescue UAV identifies a disaster victim, that victim may
be removed from the list of other UAVsâ€™ objectives. Other
applications that can be modeled by history-dependent games
are in machine learning [10]â€“[12] and biology [13], [14].
Owing in part to the challenges of modeling the complex
game-environment feedback inherent to history-dependent
games, general results on these games are elusive. Recent
work has focused on specific learning algorithms and strategic environments, such as zero-sum games under replicator
dynamics [15]. In [16] the authors characterize an oscillating
tragedy of the commons effect under certain environmental
feedback scenarios.
In this paper, we develop a general framework for analyzing the long-run behavior of binary-action history-dependent
games. In particular, we study the stochastically stable states
of the popular log-linear learning algorithm in such settings.
We show that if the utility functions of the history-dependent
game can be appropriately referenced to the utility functions
of a corresponding exact potential game, then the historydependent game of interest inherits the stochastically stable
states of the reference potential game. To accomplish this we
apply techniques from the theory of probabilistic couplings,
and derive a monotone coupling that relates play in the
history-dependent game with that in the reference potential
game. To showcase an application of the framework, we
present an epidemic model that intertwines the compartmental SIS disease model with the graphical coordination game
convention model. Using our analysis framework we provide
conditions under which the stochastically stable states may
be fully characterized, despite their history-dependence.
A. A Motivating Epidemic Example
The novel COVID-19 epidemic has led to a surge in
interest in understanding social responses to epidemics [17]â€“
[19]. To model such a scenario we adopt a game theoretic
model of behavior intertwined with a compartmental disease

|N |

1 action profile as ~1 = (1)i=1 and similarly for the all zero
profile, ~0. Further, let âˆ†(A) denote the standard probability
simplex over A.
Let Ui : A â†’ R be player iâ€™s utility function. We denote
U = {U }iâˆˆN as the collection of all players utility function.
Thus we may denote a game using tuple (N, A, U ), and let
G be the set of all such tuples.
A game g âˆˆ G is an exact potential game if there exists a
potential function Ï† such that
Ui (a0i , aâˆ’1 ) âˆ’ Ui (ai , aâˆ’i ) = Ï†(a0i , aâˆ’1 ) âˆ’ Ï†(ai , aâˆ’i ) (1)

Fig. 1. Two runs of the SISGCG model. The full description can be found in
Section IV however a brief description of the parameters is as follows. The
infectiousness of an agent using no precautions is Î²0 and using precautions
is Î²1 . The curing parameter, determining how fast agents recover, is Î³.

model. Specifically, we couple a graphical coordination game
with a compartmental SIS epidemic model. The coordination
game is a model for how social conventions spread in society
[3], using the assumption that one gains intrinsic value to
using the same conventions as others around them. In this
model, which we call SISGCG, agents choose whether or
not they take precautions by considering both a desire to
coordinate on a set of social distancing conventions and
a desire to practice safe conventions, influenced by the
severity of the epidemic. The dynamics and thus severity of
the epidemic are then in turn impacted by agentâ€™s decision
to adopt precautions. We present the complete details of
the SISGCG model in Section IV. The upper subplot of
Figure 1 showcases how SISGCG captures the complex
interplay between epidemic dynamics and social response.
In particular, epidemic peaks in this model occur due to
recent history of relatively many agents choosing not to
take precautions. In addition, both social conventions and the
epidemic take time to spread in SISGCG, so SISGCG models
a complex relationship between delayed game theorectic
social response, reactive epidemic dynamics, and the curing
time of the disease.
The lower subplot of Figure 1 showcases a scenario in
which our analysis framework can be used to characterize
agent behaviour using stochastic stability. Note that the lower
subplot depicts the case that social distancing is desirable
even when the disease is at a lower-prevalence state and thus
would also be desirable at any higher disease state.
II. M ODEL
A. Game Formulation
In this work we consider binary action games. Letting
N = {1, 2, 3, . . . , |N |} denote the player set, in a binary
action game player i âˆˆ N has action set Ai = {0, 1}.
The joint action space is then given by A = {0, 1}|N | . We
denote an action profile as a âˆˆ A and use ai to denote
player iâ€™s action and the actions of all other players by
aâˆ’i = (a1 , a2 , . . . , aiâˆ’1 , ai+1 , . . . , a|N | ). We refer to the all

for any a âˆˆ A, and ai , a0i âˆˆ Ai .
However, games g âˆˆ G cannot be used to model scenarios
where play interacts with a dynamic environment. That is,
the payoffs of such games cannot depend on the history of
play that may impact the playersâ€™ strategic environment. In
this work we generalize G by allowing each history of play
to have a unique utility function. We write AT to denote the
set of joint action histories of length T âˆˆ N, and the set of
all histories as A = âˆªT âˆˆN AT . We typically use Î± âˆˆ AT to
refer to a path and use superscripts to refer to time indices.
For example, Î±1 âˆˆ A is the first action profile in the history
and Î±T is the last, noting we use the convention that T is
the last time index even when Î± âˆˆ A. We also define A, AT
as partially ordered sets by first defining partial order â‰¥A ,
where a0 â‰¥A a whenever a, a0 âˆˆ A and a0i â‰¥ ai for all i âˆˆ
N , recalling that a0i , ai âˆˆ {0, 1}. Using this we define partial
order â‰¥AT as Î±Ì„ â‰¥ Î± whenever Î±, Î±Ì„ âˆˆ AT and Î±Ì„t â‰¥A Î±t
for all t âˆˆ {1, 2, ..., T }.
Using these notions we generalize the previously given
game formulation G to allow for the utility functions to
depend on the history of play. Similar to before, let UiÎ± :
A â†’ R, where this utility function is not only specific
player i but also to the history Î±. As before, let U Î± =
Î±
(U1Î± , U2Î± , ..., U|N
| ) denote each playerâ€™s utility function given
history Î± and let U A = {U Î± | Î± âˆˆ A} be the set of utility
functions across all paths. We denote a history-dependent
game as tuple (N, A, U A ) and let G A be the set of all
such tuples. We now present a class of games that combines
potential games and history dependence.
Definition 1: We call a tuple g = (N, A, U A ) âˆˆ G A
an aligned history-dependent game if there exists an exact
potential game gÌ‚ = (N, A, UÌ‚ ) âˆˆ G with potential function Ï†Ì‚
such that:
1) {~1} = arg maxzâˆˆA Ï†Ì‚(z)
T
2) UiÎ± (1, Î±âˆ’i
) â‰¥ UÌ‚i (1, aâˆ’i )
T
3) UÌ‚i (0, aâˆ’i ) â‰¥ UiÎ± (0, Î±âˆ’i
)
T
for any Î±, âˆˆ A, a, a0 âˆˆ A, T âˆˆ N such that Î±âˆ’i
â‰¥Aâˆ’i aâˆ’i
0
and a, a vary by only a unilateral deviation. For convenience
we denote ordering â‰¥Aâˆ’i over Aâˆ’1 = {0, 1}|N |âˆ’1 equivalently to â‰¥A .

B. Learning in Games
In this work we focus on the learning algorithm log-linear
learning, which is a discrete time asynchronous learning
algorithm [5], [20]. That is, for game g âˆˆ G A at each time

step log-linear learning will select a single agent to update
their action to ai with probability
We define the probability that agent i selects action ai
given history Î± âˆˆ AT as
Î±
T
1
e Ï„ Ui (ai ,Î±âˆ’i )
.
(2)
PÎ±
(a
)
=
i
P
1
T
Î±
i
Ï„ Ui (ai ,Î±âˆ’i )
a0i âˆˆAi e
where Ï„ , called the temperature is a parameter which governs
the rationality of agents. As Ï„ â†’ 0 agents will best respond
with high probability, and as Ï„ â†’ âˆ agents will choose
actions uniformly at random. Note that we take the last action
profile in the history Î±T as the behavior of the other agents.
We define the probability Î± âˆˆ AT transitions to a0 âˆˆ A
under log-linear
in a single transition as
ï£± 1learning
P
T
0
0
ï£´ |N | jâˆˆN PÎ±
j (aj ) Î± = a
ï£²
Î± 0
1
Î±
0
T
T
P (a ) = |N | Pi (ai )
Î±i 6= a0i , Î±âˆ’i
= a0âˆ’i (3)
ï£´
ï£³
0
else.
This can be interpreted as the probability that given history
Î± the next action profile Î±T +1 = a0 .
We say a âˆˆ A is strictly stochastically stable if the
following  definition [21] holds. For any  > 0 there exists
T > 0, T < âˆ such that such that
Pr(s(t; Ï„, Ï€, g) = ~1) > 1 âˆ’  whenever t > T, Ï„ < T . (4)
where s(Â·) is a random variable representing the action
profile at time t under log-linear learning, given temperature
Ï„ , initial distribution Ï€ âˆˆ âˆ†(A) and game g.
Traditionally, exact potential games under log-linear learning may be analyzed using a theory of resistance trees [3],
[5], [20], [22] to relate potential function maximizers to
stochastic stability. However, this analysis depends on the
fact that log-linear learning induces an ergodic markov chain
from any exact potential game. It is easy to show that when
log-linear learning is applied to game g âˆˆ G A the induced
random process is not Markovian in general. In particular,
the Markov property requires that at any time determining
the next state does not depend on the past, however the
underlying utility functions of g explicitly depend on history
Î±, and therefore the associated transition given in (2) violates
the Markov property. Thus the traditional technique for
showing stochastic stability in this context fails.
III. M AIN C ONTRIBUTION
We now present our main result, extending stochastic
stability to the class of games we term aligned historydependent games. In particular, we show that for such games
the ~1 state is strictly stochastically stable.
Theorem 1: If g âˆˆ G A is an aligned history-dependent
game then ~1 is strictly stochastically stable in g under loglinear learning.
The proof of Theorem 1 proceeds using Lemma 1, which
we present here and prove in Section V. The interpretation
of this lemma is that for an aligned history dependent game
g, the probability at any time step that g is in the ~1 action
profile is lower bounded by the probability its associated
exact potential game gÌ‚ is in the ~1 profile.
Lemma 1: If g âˆˆ G A is an aligned history-dependent
game with associated exact potential game gÌ‚ âˆˆ G then

Pr(s(T ; Ï„, Ï€, g) = ~1) â‰¥ Pr(s(T ; Ï„, Ï€, gÌ‚) = ~1) for any
temperature Ï„ > 0, Ï€ âˆˆ âˆ†(A).
The proof of Lemma 1 is technically involved and depends on our novel monotone coupling framework which
we present in Section V. Using this result we now present a
straightforward proof of Theorem 1.
Proof of Theorem 1: Let g âˆˆ G A be an aligned historydependent game and gÌ‚ be its associated exact potential game.
It is well-known [20] that in an exact potential game a âˆˆ A
is a stochastically stable state under log-linear learning if
a âˆˆ arg max Ï†(a0 ).
(5)
a0 âˆˆA

Therefore, because ~1 is the lone maximizer of Ï†Ì‚, it is strictly
stochastically stable. We apply Lemma 1 directly to the
definition of strict stochastic stability in (4). For any  > 0
there exists T > 0, T < âˆ such that such that
Pr(s(t; Ï„, Ï€, g)) â‰¥ Pr(s(t; Ï„, Ï€, gÌ‚)) > 1 âˆ’  âˆ€t > T, Ï„ < T
(6)
yielding stochastic stability of ~1 in game g.

IV. A PPLICATION TO S OCIAL D ISTANCING IN A
PANDEMIC
A. SIS Preliminaries
It is appropriate to model epidemics that do not grant
long term immunity with the SIS compartmentalized model.
Here, we assume that every member of the population is in
one of two states, susceptible (S) or infected (I). Individuals
transition from susceptible to infected with rate Î² > 0, and
infected to susceptible with rate Î³ > 0. The fraction s(t)
of the population that is susceptible at time t evolves as a
function of time according to the one-dimensional nonlinear
ODE
sÌ‡ = âˆ’Î²s(1 âˆ’ s) + Î³(1 âˆ’ s).
(7)
The solutions to (7) when s(0) âˆˆ [0, 1] are fully characterized; in particular, it is known that when Î²/Î³ > 1,
there exists an asymptotically stable equilibrium (called the
endemic equilibrium) sâˆ— = Î³/Î², and that every solution
to (7) with s(0) < 1 converges to sâˆ— . For more information,
the reader may reference the survey [23].
B. The Graphical Coordination Game
One model for the spread of conventions in society is the
Graphical Coordination Game (GCG) [4]. In this networked
game, each agent plays the following two-player coordination game with each of her neighbors:
1
0
(8)
u(ai , aj ) = 1 q + c, q + c 0, 0
0
0, 0
1, 1
where ai âˆˆ {0, 1} is taken to be the row playerâ€™s action
and aj is the column playerâ€™s action, constant q âˆˆ (0, 1]
is a base coordination incentive and c â‰¥ 0 is the payoff
gain, indicating the benefit of coordinating on 1 over 0. The
graphical coordination game is played between |N | players
on an undirected graph G = (N, E) where E is the edge set.
We define the neighbor set of any agent i as Ni = {j âˆˆ N |
(i, j) âˆˆ E, i 6= j}. In the GCG an agentâ€™s utility function for
action profile a âˆˆ A is given by

X

ui (a) =

u(ai , aj ).

(9)

jâˆˆNi

When c is a constant it is well understood that the GCG
in this formulation denotes an exact potential game. For
convience let E1 (a) âŠ† E denote the set of edges whose
agents are coordinated on action 1, and likewise for E0 (a).
The potential function is then given by
Ï†(a) = (1 + c)|E1 (a)| + |E0 (a)|.
(10)
C. A Socially Aware Epidemic Model
This model which we call SISGCG is defined by the
following nonlinear hybrid dynamical system:
sÌ‡ = (1 âˆ’ s)(Î³ âˆ’ Î²(t)s).
(11)
Here, Î²(t) is piecewise-constant and defined as follows: Let
{tk }kâˆˆN âˆˆ [0, âˆ) be an unbounded set of agent revision
opportunities. At each time tk , an agent is selected uniformly
at random to update her action, which she does according to
log-linear learning with probability (2) with utilities associated with the following graphical coordination game , where
we write I(t) := 1 âˆ’ s(t), and write player iâ€™s action at time
t as ati :
uSISGCG (ati , atj ) = u(ati , atj ) with c = I(t)

(12)

where q âˆˆ (0, 1] and represents how willing a population is
to practice safe conventions in the absence of an epidemic.
The action 1 represents a â€œsafe conventionâ€ action in which
a player is acting to reduce contagion; the action 0 represents
conventions ignoring the pandemic. These actions are associated with infection coefficients 0 < Î²1 < Î²0 , respectively.
Note in the SISGCG model we define the coordination game
with log-linear learning in the same way as previously stated,
but the 2-player coordination game is played with uSISGCG
in place of u. Finally, Î²(t) is simply the average infection
rate of all individuals, given their choices:
1 X t
ai Î²1 + (1 âˆ’ ati )Î²0 .
(13)
Î²(t) =
N
iâˆˆN

Proposition 1: If s(0) âˆˆ [0, 1), then if s(t) is a solution
of (11) with Î²(t) given by (13), there exists a tÌ„ such that
s(t) â‰¤ Î³/Î²1 for all t â‰¥ tÌ„ almost surely.
Proof: We write sâˆ—1 := Î³/Î²1 . Note that if s(t) â‰¥ sâˆ—1 ,
then because Î²(t) â‰¥ Î²1 , we have that sÌ‡ â‰¤ 0 by (11), and
that this inequality is strict whenever s(t) > sâˆ—1 . Thus, the
set [0, sâˆ—1 ] is positively invariant for the hybrid nonlinear
dynamics given in (11).
To see that s(t) eventually enters [0, sâˆ—1 ] almost surely,
consider the event that s(t) > sâˆ—1 for all t. Since sâˆ—1 is
asymptotically stable when Î²(t) â‰¡ Î²1 and for any action
profile a 6= ~1 that its associated Î²(t) > Î²1 , it follows that
the event that Î²(t) â‰¡ Î²1 for all t is the same event as
s(0) > sâˆ—1 and s(t) > sâˆ—1 for all t. However, it can be seen
that the log-linear learning (3) action update probabilities
define a stochastic process which visits every action profile
in A infinitely often. That is, the probability that Î²(t) â‰¡ Î²1
is 0, and thus there must exist a tÌ„ such that s(t) â‰¤ sâˆ—1 for all
t â‰¥ tÌ„ almost surely.
Immediately, it can be seen that this model exhibits
some desirable intuitions about how populations behave both

during and in the absence of a pandemic. Consider the case
with no pandemic present, that is I(0) = 0 which is a fixed
point of the SIS dynamical system. In this scenario it can
be seen that (12) is a constant payoff matrix, therefore this
instance of SISGCG denotes an exact potential game. It is
easy to see that ~0 uniquely maximizes the potential function
and therefore is strictly stochastically stable.
However, as a consequence of Proposition 1, if I(0) > 0
then I(t) > 0 for all t. It can be seen from (12) that SISGCG
can be represented by a history-dependent game, as the utility
function depends on the history of play. Therefore, agent
decision-making inherits this dependence on the history of
play, leading to non-Markovian behavior which complicates
traditional analysis techniques. However, our Theorem 1
allows us to reference SISGCG to a related exact potential
game and deduce conditions guaranteeing that ~1 is strictly
stochastically stable.
Proposition 2: Let g S be an instance of SISGCG. If
Î²1 /Î³ > 1, q + Î³/Î²1 > 1 and I(0) > 0 then we have ~1
is stochastically stable in g.
Proof: To show stochastic stability of ~1 in the SISGCG
model we must:
1) show SISGCG is a history-dependent game,
2) show a corresponding fixed game gÌ‚ exists, and
3) apply Theorem 1.
Let a SISGCG g S be played on graph G = (V, E) with q +
Î³/Î²1 > 1 and I(0) > 0, and we consider g S as played after
time tÌ„ that exists almost surely as shown in Proposition 1.
It is easy to show SISGCG is a history-dependent game,
g S = (N, A, UÌ„ ). Note that trivially both the player set N and
action set A match the history-dependent framework. To see
that the utility functions fit into the framework note that the
utility function, (12), depends on I(t) which represents the
proportion of agents who are infected. To find this constant
for some t âˆˆ N, it can be seen from (11) we require the
history of agent up until that point. Thus we have at time t
the utility functions UÌ„ Î± depend on Î± âˆˆ At , which admits
readily into the history-depend framework. Thus we have
g S = (N, A, UÌ„ ) âˆˆ G A
Now we let gÌ‚ S = (N, A, UÌ‚ S ) âˆˆ G be a GCG played on
graph G. We define the pairwise coordination game payoff
matrix by (8) with c = Î³/Î²1 inducing that gÌ‚ S denotes an
exact potential game.
We now use gÌ‚ S to show g S is an aligned history-dependent
game. Because q + Î³/Î²1 > 1 its easy to see that ~1 is the
lone maximizer of the the potential function. Now we verify
T
T
UiÎ± (1, Î±âˆ’i
) â‰¥ UÌ‚iS (1, aâˆ’1 ) anytime Î±âˆ’i
â‰¥Aâˆ’i aâˆ’i , t > tÌ„.
This can be rewritten
for
t
>
tÌ„
as
X
X
q + I(t) â‰¥
q + Î³/Î²1
(14)
jâˆˆNi1 (Î±T
âˆ’i )

jâˆˆNi1 (aâˆ’i )

where Ni1 (aâˆ’i ) denotes the neighbors of i who are playing
T
1 given profile a. This expressions holds because Î±âˆ’i
â‰¥Aâˆ’i
T
aâˆ’i â‡’ |Ni1 (Î±âˆ’i
)| â‰¥ |Ni1 (aâˆ’i )| and by Proposition 1 I(t) â‰¥
Î³/Î²1 for all t > tÌ„. An argument with the same structure
T
holds for UiÎ± (0, Î±âˆ’i
) â‰¤ UÌ‚iS (0, aâˆ’1 ). Thus g S is an aligned
history-dependent game and we apply Theorem 1 to discover
stochastic stability of ~1 as desired.

V. P ROOF VIA M ONOTONE C OUPLINGS
A. A Primer on Monotone Couplings
We begin with the definition of a monotone coupling, the
core analytical device for our paper. Using this we present
the definition of monotone coupling:
Definition 2: Let X be a countable set with partial ordering â‰¤X and p1 , p2 be probability measures on measure
space (X, F). Then a monotone coupling of p1 , p2 is a
probability measure p on (X 2 , F 2 ) satisfying the following
for all
x, y âˆˆ X
X
X
p(x0 , y) = p1 (x0 ).
p(x, y 0 ) = p2 (y 0 ) and
yâ‰¥X x0

xâ‰¤X y 0

(15)
A monotone coupling is a useful tool for analysis of the
component probability measures p1 and p2 . In particular the
following property holds in general for monotone couplings.
Proposition 3 (Paarporn et al., [24]): Let p1 , p2 be probability measures on (X, F). If p is a monotone coupling of
p1 , p2 then for any increasing random variable Z : X â†’ Z+
we have
âˆ
X
Ep1 (Z) âˆ’ Ep2 (Z) =
p(ZÎ·c , ZÎ· )
(16)
Î·=0

where ZÎ· = {a | Z(a) > Î·}.
Where we denote a complement set of Z âŠ‚ X as Z c . The
proof is given in [24, Proposition 1].
B. Notation Required for Proofs
Taking gÌ‚ âˆˆ G, we give equations analogous to (2), (3)
that give the transition probabilities for gÌ‚ under log-linear
learning. In particular, if agent i is selected to update her
action then she will do so according to:
1
e Ï„ Ui (ai ,aâˆ’i )
(17)
PÌ‚ai (ai ) = P
1
0
Ï„ Ui (ai ,aâˆ’i )
a0i âˆˆAi e
Building on (17), we define the probability that action profile
a transitions to a0 under log-linear learning in a single
transition asï£±
P
ï£´ |N1 | jâˆˆN PÌ‚aj (aj ) a = a0
ï£²
PÌ‚ a (a0 ) = |N1 | PÌ‚ai (a0i )
ai 6= a0i , aâˆ’i = a0âˆ’i (18)
ï£´
ï£³
0
else
0
for some i âˆˆ N and a, a âˆˆ A. Additionally, we define the
probability that path Î± âˆˆ AT occurs with initial distribution
Ï€ âˆˆ âˆ†(A) as
TY
âˆ’1
t
PÌ‚Ï€ (Î±) = Ï€(Î±1 )
PÌ‚ Î± (Î±t+1 )
(19)
t=1

noting that Ï€(Î±1 ) denotes the probability of Î±1 in initial
distribution Ï€.
Corrospondingly, the probability that path Î± âˆˆ AT occurs
with initial distribution Ï€ âˆˆ âˆ†(A) on g âˆˆ G A is
TY
âˆ’1
â‰¤t
PÏ€ (Î±) = Ï€(Î±1 )
P Î± (Î±t+1 )
(20)
t=1

where we use Î±â‰¤t âˆˆ At to mean history Î± until time t âˆˆ
{1, 2, 3, . . . , T }.
We now present a result connecting the utility conditions
of aligned history-varying potential games to (17) and (2).

Lemma 2: Let g = (N, A, U A ) âˆˆ G, gÌ‚ = (N, A, UÌ‚ ) âˆˆ G
T
and let i âˆˆ N, a âˆˆ A, Î± âˆˆ A such that Î±âˆ’i
â‰¥Aâˆ’i aâˆ’i . If
Î±
T
T
Ui (1, Î±âˆ’i ) â‰¥ UÌ‚i (1, aâˆ’i ) and UÌ‚i (0, aâˆ’i ) â‰¥ UiÎ± (0, Î±âˆ’i
) then
a
PÎ±
(1)
â‰¥
PÌ‚
(1).
i
i
Proof: Let g = (N, A, U A ) âˆˆ G, gÌ‚ = (N, A, UÌ‚ ) âˆˆ
T
G and let i âˆˆ N, a âˆˆ A, Î± âˆˆ A be such that Î±âˆ’i
â‰¥Aâˆ’i
T
Î±
aâˆ’i . Further let Ui (1, Î±âˆ’i ) â‰¥ UÌ‚i (1, aâˆ’i ) and UÌ‚i (0, aâˆ’i ) â‰¥
T
UiÎ± (0, Î±âˆ’i
). Recalling Ï„ > 0, we begin by considering PÎ±
i
e Ï„ Ui

(1,Î±T
âˆ’i )

Î± (1,Î±T )
âˆ’i

+ e Ï„ Ui

1

PÎ±
i (1)

=

1

e Ï„ Ui

Î±

1

Î± (0,Î±T )
âˆ’i

(21)

1

â‰¥

e Ï„ UÌ‚i (1,aâˆ’i )
1

1

e Ï„ UÌ‚i (1,aâˆ’i ) + e Ï„ UÌ‚i (0,aâˆ’i )

=

PÌ‚ai (1).

To see the inequality, it suffices to apply the hypothesis to
x
the fact that ex and l(x) = exe+c are both increasing in x for
a
c > 0. Thus PÎ±
i (1) â‰¥ PÌ‚i (1) holds as desired.
Our framework requires a careful partitioning of the action
space into several sets corresponding to different types of
agent action deviations. We use these formalizations for the
various cases of the monotone coupling given in Theorem 2
as seen in (23).
Let f : A â†’ 2A be defined as f (a) = {a0 âˆˆ A | ai 6=
0
ai , aâˆ’i = a0âˆ’i for i âˆˆ N } be the set of action profiles
reachable from a via exactly one unilateral deviation. For
a, a0 âˆˆ A let
ï£±
ï£´
ai 6= a0i
ï£²i
(22)
g(a, a0 ) = 0
a = a0
ï£´
ï£³
0
âˆ’1 ai 6= ai , aj 6= aj , i 6= j
indicate which agent unilaterally deviated their action between action profiles a, a0 , and be âˆ’1 if multiple agents have
deviated.
Now, let a, a0 âˆˆ A where a0 â‰¥A a. We denote several
disjoint subsets of f (a):
1) r(a) = {z âˆˆ f (a) | ag(a,z) = 1},
2) q(a, a0 ) = {z âˆˆ f (a) | z â‰¤A a0 } \ r(a), and
3) s(a, a0 ) = f (a) \ (q(a, a0 ) âˆª r(a)).
These sets can be interpreted in the following way. The set
r(a) is the set of action profiles which decreased with respect
to â‰¥A and q(Â·), s(Â·) both increased. Between q(Â·) and s(Â·),
q(Â·)â€™s action profiles remain less than a0 and s(Â·)â€™s profiles
are greater then or incomparable to a0 . We now present three
more analogous sets that are disjoint subsets of f (a0 ):
1) R(a0 ) = {z âˆˆ f (a0 ) | a0g(a0 ,z) = 0},
2) Q(a, a0 ) = {z âˆˆ f (a0 ) | z â‰¥A a} \ R(a0 ), and
3) S(a, a0 ) = f (a0 ) \ (Q(a, a0 ) âˆª R(a)).
The interpretation of these sets are flipped relative to r(Â·),
q(Â·) and s(Â·).
We now highlight some useful features of these sets. By
their definitions it is evident that q(Â·), r(Â·), s(Â·) are a disjoint
partition of f (a), and that Q(Â·), R(Â·), S(Â·) are a disjoint
partition of f (a0 ). For any a, a0 , a0 â‰¥A a we relate these
0
0
sets by a function ba,a : f (a) â†’ f (a0 ). To evaluate ba,a (aÌ„),
identify the agent who deviated their action between a, aÌ„ and
0
then deviate that agentâ€™s action in a0 . Formally, ba,a (aÌ„) =

(Â¬a0g(a,aÌ„) , a0âˆ’g(a,aÌ„) ) where for convenience we define Â¬ai âˆˆ
{0, 1}\{ai } for ai âˆˆ Ai = {0, 1}. In particular, this function
relates the disjoint subsets of f (a), f (a0 ) according to the
following lemma.
Lemma 3: If a, a0 âˆˆ A and a â‰¤A a0 , then the following
statements hold:
0
1) ba,a : r(a) â†’ S(a, a0 ) is a bijection,
0
2) ba,a : s(a, a0 ) â†’ R(a0 ) is a bijection, and
0
3) ba,a : q(a, a0 ) â†’ Q(a, a0 ) is a bijection.
Lemma 3 is proved in the Appendix.
C. The One-Step Couplings
To prove Lemma 1 and obtain Theorem 1, we construct
a monotone coupling Î½Ï€gÌ‚ between measures PÏ€ , PÌ‚Ï€ . Noting
that Î½Ï€gÌ‚ is a coupling of measures over histories, we first
construct a family of monotone couplings for each one-step
transition (Theorem 2), and subsequently use these couplings
to build the desired coupling over histories (Theorem 3).
Theorem 2: Let g âˆˆ G A denote an aligned historydependent game and gÌ‚ âˆˆ G be its associated exact potential
game. Then a monotone coupling exists between PÌ‚ a and P Î±
for any Î± âˆˆ A, a âˆˆ A whenever a â‰¤A Î±T . This monotone
coupling Î½ a,Î± : A2 â†’ [0, 1] is given in (23) in Figure 2.
Proof: Let a âˆˆ A, Î± âˆˆ A such that a â‰¤A Î±T and let
g âˆˆ G A be a aligned history-dependent game where gÌ‚ âˆˆ G
is its associated exact potential game. To verify Î½ a,Î± is a
monotone coupling we must show the following conditions
from Definition 2 for any aÌ„, aÌ„0 âˆˆ A:
1) Î½ a,Î±
a well-defined probability measure,
P is a,Î±
2)
Î½ (aÌ„, z 0 ) = PÌ‚ a (aÌ„), and
z 0P
â‰¥A aÌ„
3)
Î½ a,Î± (z, aÌ„0 ) = P Î± (aÌ„0 ).
zâ‰¤A aÌ„0

We begin by verifying Condition 2. We consider cases
aÌ„ âˆˆ
/ (f (a) âˆª {a}), aÌ„ âˆˆ q, aÌ„ âˆˆ r, aÌ„ âˆˆ s and aÌ„ = a separately.
We use the notational convention that q, s, Q, S are assumed
to take arguments (a, Î±T ) and r, R take the argument a, Î±T
respectively. The first case represents any aÌ„ that cannot be
achieved in a single unilateral deviation from a. Trivially, this
gives that PÌ‚ a (aÌ„) = 0, and thus all pairs of aÌ„, z 0 must satisfy
Î½ a,Î± (aÌ„, z 0 ) = 0. This holds as all parts of (23) require aÌ„ âˆˆ
(f (a) âˆª {a}) except (23h), which has the desired property.
We now consider the second case that aÌ„ âˆˆ q. Note that
only (23d) satisfies this condition, so
X
Î½ a,Î± (aÌ„, z 0 ) = Î½ a,Î± (aÌ„, Î±T )
z 0 â‰¥A aÌ„0
(24)
= PÌ‚ag(a,aÌ„) (aÌ„g(a,aÌ„) )/|N | = PÌ‚ a (aÌ„)
as desired.
Next we consider aÌ„ âˆˆ r which satisfies (23c), (23f)
T
uniquely since ba,Î± is a bijection by Lemma 3. Thus
X
1
PÌ‚ag(a,aÌ„) (0)
Î½ a,Î± (aÌ„, z 0 ) =
|N
|
z 0 â‰¥A aÌ„0

Î±
(25)
âˆ’ PÎ±
g(a,aÌ„) (0) + Pg(Î±T ,aÌ„0 ) (0)
1 a
=
PÌ‚
(0) = PÌ‚ a (aÌ„)
|N | g(a,aÌ„)

where the second equality follows as g(a, aÌ„) = g(Î±T , aÌ„0 ) by
T
definition of ba,Î± . The third equality follows as aÌ„ âˆˆ r =â‡’
aÌ„g(a,aÌ„) = 0.
Considering
aÌ„ âˆˆ s, we find only
applies,
thus for
 (23e)

X
a,Î±
0
a,Î±
a,Î±T
aÌ„, b
Î½ (aÌ„, z ) = Î½
(aÌ„)
z 0 â‰¥A aÌ„0

(26)
1 a
PÌ‚g(a,aÌ„) (1) = PÌ‚ a (aÌ„)
|N |
where aÌ„ âˆˆ s =â‡’ aÌ„g(a,aÌ„) = 1 or else aÌ„ would be in q.
The final case for Condition 2 is aÌ„ = a. we find cases
(23a), (23b), and (23g) apply
 yielding:
X
X
1
Î½ a,Î± (aÌ„, z 0 ) =
|N | âˆ’
PÌ‚ag(a,z) (zg(a,z) )
|N |
0
0
zâˆˆqâˆªr
z â‰¥A aÌ„

X
PÌ‚ag(Î±T ,z0 ) (1)
âˆ’
=

z 0 âˆˆR


1 X 
=
1 âˆ’ PÌ‚ag(a,z) (zg(a,z) )
|N |
zâˆˆf (a)

1 X a
=
PÌ‚i (ai ) = PÌ‚ a (aÌ„)
|N |
iâˆˆN

(27)
where the first equality follows as sums over Q âˆª R are
equivalent to the sums over Q and R as Q, R are disjoint,
0
and that z 0 âˆˆ R â‡” zg(Î±
T ,z 0 ) = 1 by definition of R. The
second equality follows as the R sum is equivalent to one
T
over s by bijection ba,Î± , and then we may combine it with
the sum over q âˆª r, to a sum over f (a) and |f (a)| = |N |.
This concludes all cases for Condition 2. We omit arguments
for Condition 3 as they run parallel to Condition 2.
To verify Condition 1, we consider each case of (23)
separately. Equations (23b), (23d), (23e), (23f), and (23h) are
trivial as these probabilities are well defined by definition.
Lemma 2 provides:
a
a
Î±
PÎ±
(28)
i (1) â‰¥ PÌ‚i (1) â‡” PÌ‚i (0) â‰¥ Pi (0)
where the right hand side follows from Pi (1, a0 , w) +
Pi (0, a0 , w) = 1 = Pi (1, a0 , w0 ) + Pi (0, a0 , w0 ). Equation (23a) follows directly from the hypothesis and (23c)
holds from the right side of the equivalence.
The lone remaining case is (23g), for which we define
sets Nq = {g(a, z) | z âˆˆ q}, NQ = {g(Î±T , z) | z âˆˆ Q}
and so on for r, s, R, S. For convenience we denote unions
of these sets as Nqr := Nq âˆª Nr , NQR := NQ âˆª NR
and so on for other combinations of q, r, s and Q, R, S.
Recalling q, r, Q, R are partitions over states that a, Î±T may
transition to, similarly, Nqr , NQR are partitions of agents
whose unilateral deviations result in such transitions. These
sets are subsets of N and enable us to expand (23g)
Î½ a,Î± (aÌ„, aÌ„0 ) =

1
|N |

X

(1 âˆ’ PÌ‚ai (Â¬ai )

iâˆˆNqr âˆ©NQR

âˆ’

T
PÎ±
i (Â¬Î±i ))

+

X

(1 âˆ’ PÌ‚ai (Â¬ai ))

iâˆˆNqr \NQR

+

X
iâˆˆNQR \Nqr

T
(1 âˆ’ PÎ±
i (Â¬Î±i ).

(29)

ï£±

1  Î±
a
ï£´
ï£´
(1)
P
(1)
âˆ’
PÌ‚
0
0
T
0
ï£´
g(a ,aÌ„ )
ï£´
|N | g(Î± ,aÌ„ )
ï£´
ï£´
ï£´
ï£´
1 Î±
ï£´
ï£´
Pg(Î±T ,aÌ„0 ) (aÌ„0g(Î±T ,aÌ„0 ) )
ï£´
ï£´
|N
|
ï£´
ï£´
ï£´

ï£´
1  a
ï£´
ï£´
ï£´
(0)
PÌ‚g(a,aÌ„) (0) âˆ’ PÎ±
ï£´
g(a,aÌ„)
ï£´
|N |
ï£´
ï£´
ï£´
ï£´
ï£² 1 a
(aÌ„
)
PÌ‚
a,Î±
0
Î½ (aÌ„, aÌ„ ) = |N | g(a,aÌ„) g(a,aÌ„)
ï£´
ï£´
1 a
ï£´
ï£´
ï£´
PÌ‚
(1)
ï£´
ï£´
|N
| g(a,aÌ„)
ï£´
ï£´
ï£´
ï£´
ï£´ 1 Î±
ï£´
P T 0 (0)
ï£´
ï£´
ï£´ |N | g(Î± ,aÌ„ )
ï£´


ï£´
ï£´
P a
P
ï£´
1
Î±
0
ï£´
|N
|
âˆ’
PÌ‚
(z
)
âˆ’
P
(z
)
ï£´
T
0
T
0
g(a,z)
g(a,z)
g(Î± ,z ) g(Î± ,z )
|N |
ï£´
ï£´
zâˆˆqâˆªr
z 0 âˆˆQâˆªR
ï£´
ï£³
0

aÌ„ = a, aÌ„0 âˆˆ R

(23a)

aÌ„ = a, aÌ„0 âˆˆ Q

(23b)

aÌ„ âˆˆ r, aÌ„0 = Î±T

(23c)

aÌ„ âˆˆ q, Î±T = aÌ„0

(23d)

T

aÌ„ = ba,Î± (aÌ„0 ), aÌ„0 âˆˆ R
T

(23e)

aÌ„ âˆˆ r, aÌ„0 = ba,Î± (aÌ„)

(23f)

a = aÌ„, Î±T = aÌ„0

(23g)

otherwise.

(23h)

Fig. 2. The full specification of the one-step monotone coupling for Theorem 2. We adopt the notational convention that q, s, Q, S are assumed to take
arguments a, a0 and r, R take the argument a, a0 respectively.

This expansion takes advantage of |N | = |f (a)| which
allows |N | to enter the sums as 1. It now suffices to show
that the summand of each sum is a well defined probability,
then the sum of |N | well defined probabilities divided by
|N | must also be a well defined probability. The last two
terms in (29) are clearly well-defined probabilities, leaving
only the first term.
We begin by investigating i âˆˆ Nqr âˆ© NQR . In particular,
T
we have Nq = NQ , Ns = NR , Nr = NS due to ba,Î± and
its bijectiveness due to Lemmas 3. By disjointness of q, r
we have Nqr = NQS which we apply to Nqr âˆ© NQR =
NQS âˆ© NQR = NQ = Nq . Applying definitions of q, Q we
find i âˆˆ Nq =â‡’ Â¬ai = 1, Â¬Î±iT = 0. Thus the summand of
the first sum for i âˆˆ Nq is given by
Î±
Î±
1 âˆ’ PÌ‚ai (1) âˆ’ PÎ±
(30)
i (0) â‰¥ 1 âˆ’ Pi (1) âˆ’ Pi (0) = 0
where in the inequality is by (28), giving that the summands
in the first term of (29) are themselves well defined probabilities. As all conditions have been met, Î½ a,Î± is a monotone
coupling as desired.

must exist some t âˆˆ {1, 2, 3, . . . , T âˆ’ 1} such that Î±t â‰¤A Î±Ì„t
but Î±t+1 A Î±Ì„t+1 , and let t be the minimal such value. In
t
â‰¤t
t
â‰¤t
this case we have Î½ Î± ,Î±Ì„ (Î±t+1 , Î±Ì„t+1 ) = 0 because Î½ Î± ,Î±Ì‚
is a well defined monotone coupling by Theorem 2, yielding
Î½Ï€gÌ‚ (Î±, Î±Ì„) = 0 as desired. It also follows that Î½Ï€gÌ‚ will always
yield a well defined probability as it is either 0 or a product
of well defined probabilities. Thus we only need to show
that the marginal probabilities are preserved given by (15).
We begin by showing the left equation of (15), that is:
X
Î½Ï€gÌ‚ (Î±, z) = PÌ‚Ï€ (Î±) for each z âˆˆ AT
(32)
Î±â‰¤AT z

and omit the proof for the right hand equation as it proceeds
identically. By inspecting (31), we only need to consider z
such that z 1 = Î±1 and z features at most a single unilateral
deviation between any t, t + 1. With these two conditions we
rewrite
TY
âˆ’1
X
X
t â‰¤t
Î½Ï€gÌ‚ (Î±, z) =
Ï€(Î±1 )
Î½ Î± ,z (Î±t+1 , z t+1 )
Î±â‰¤AT z

1

= Ï€(Î± )

D. A monotone coupling over histories
We now present coupling Î½Ï€gÌ‚ which is constructed using
the one-step coupling. Using this coupling we then go on to
prove Lemma 1. Note, we define an indicator functions as
1(a = a0 ) to be 1 if a = a0 and 0 else for a, a0 âˆˆ A.
Theorem 3: Let g âˆˆ G be an aligned history-dependent
game and gÌ‚ be its corresponding exact potential game. Then
Î½Ï€gÌ‚ : A2T â†’ [0, 1] is a monotone coupling between PÌ‚Ï€ , PÏ€ .
This coupling is given by
TY
âˆ’1
t
â‰¤t
Î½Ï€gÌ‚ (Î±, Î±Ì„) = Ï€(Î±1 )1(Î±1 = Î±Ì„1 )
Î½ Î± ,Î±Ì„ (Î±t+1 , Î±Ì„t+1 )
t=1

(31)
where Î±, Î±Ì„ âˆˆ AT , Ï€ âˆˆ âˆ†(A).
Proof: Let Î±, Î±Ì„ âˆˆ AT and let gÌ‚ be a fixed corresponding fixed game to g âˆˆ G. We begin by showing that
if Î± AT Î±Ì„, then Î½Ï€gÌ‚ (Î±, Î±Ì„) = 0. Immediately, we have
Î½Ï€gÌ‚ (Î±, Î±Ì„) = 0 if Î±1 6= Î±Ì„1 , so we need only consider cases
where Î±1 = Î±Ì„1 . Inductively we find that if Î± AT Î±Ì„ there

t=1

Î±â‰¤AT z

X

Î½Î±

1

,z â‰¤1

(Î±2 , z 2 ) . . .

Î± 2 â‰¤A z 2

X

Î½

Î±T âˆ’1 ,z â‰¤T âˆ’1

(Î±T , z T ).

Î±T â‰¤A z T

(33)
as the combinatorial form. Critically, this allows us to to apt â‰¤t
ply the marginal sum properties of Î½ Î± ,z from Theorem 2
for each t âˆˆ {1, 2, .., T }. First, considering the rightmost
sum in (33), it holds that
X
T âˆ’1 â‰¤T âˆ’1
T âˆ’1
Î½ Î± ,z
(Î±T , z T ) = PÌ‚ Î± (Î±T ).
(34)
Î± T â‰¤A z T

Because this has no dependence on z we may factor out
T âˆ’1
PÌ‚ Î± (Î±T ) and repeat the process on the new rightmost
sum. After performing this process recursively on all sums,
we have
TY
âˆ’1
X
t
Î½Ï€gÌ‚ (Î±, z) = Ï€(Î±1 )
PÌ‚ Î± (Î±t+1 ) = PÌ‚Ï€ (Î±) (35)
Î±â‰¤AT z

t=1

as desired, noting we accounted for the indicator functions
in Î½Ï€gÌ‚ . This concludes the proof of Theorem 3.
Now that the necessary results have been developed we
proceed with the proof of Lemma 1.
Proof of Lemma 1: Let g âˆˆ G A be an aligned historydependent game and I âŠ‚ AT be an upper set. Define
1I (a) := 1(a âˆˆ I) as an indicator function. Consider
probability measures PÏ€ , PÌ‚Ï€ coupled by Î½Ï€gÌ‚ in Theorem 3,
we have
PÏ€ (I) âˆ’ PÌ‚Ï€ (I) = EPÏ€ (1I ) âˆ’ EPÌ‚Ï€ (1I )
(36)
= Î½Ï€gÌ‚ (I C , I) â‰¥ 0.
where the second equality follows by Proposition 3 as 1I is
increasing in AT . Note (36) runs parallel to the proof of [24,
Corollary 3]. That is, for any upper set I âŠ‚ AT we have
PÏ€ (I) â‰¥ PÌ‚Ï€ (I)
(37)
âˆ’1 ~
meaning we have stochastic dominance. Let ((~0)Tt=1
, 1) âˆˆ I.
This induces I such that it includes every path such that
at time T the ~1 state is played. This yields the following
interpretation
PÏ€ (I) = Pr(s(T ; Ï„, Ï€, g) = ~1)
(38)
representing the probability that at time T game g is in
the ~1 action profile given initial distribution Ï€ âˆˆ âˆ†(A)
and learning temperature parameter Ï„ . Noting a parallel
interpretation to (38) holds for PÌ‚Ï€ , gÌ‚, we apply these to (37)
to obtain
Pr(s(T ; Ï„, Ï€, g) = ~1) â‰¥ Pr(s(T ; Ï„, Ï€, gÌ‚) = ~1)
(39)
as desired.

R EFERENCES
[1] J. Howard, A. Huang, Z. Li, Z. Tufekci, V. Zdimal, H.-M. van der
Westhuizen, A. von Delft, A. Price, L. Fridman, L.-H. Tang, V. Tang,
G. L. Watson, C. E. Bax, R. Shaikh, F. Questier, D. Hernandez, L. F.
Chu, C. M. Ramirez, and A. W. Rimoin, â€œAn evidence review of
face masks against covid-19,â€ Proceedings of the National Academy
of Sciences, vol. 118, no. 4, 2021.
[2] L. Matrajt and T. Leung, â€œEvaluating the effectiveness of social
distancing interventions to delay or flatten the epidemic curve of
coronavirus disease,â€ Emerging infectious diseases, vol. 26, no. 8,
p. 1740, 2020.
[3] H. P. Young, â€œThe evolution of conventions,â€ Econometrica: Journal
of the Econometric Society, pp. 57â€“84, 1993.
[4] M. Kearns, M. L. Littman, and S. Singh, â€œGraphical models for game
theory,â€ arXiv preprint arXiv:1301.2281, 2013.
[5] J. R. Marden and J. S. Shamma, â€œRevisiting log-linear learning:
Asynchrony, completeness and payoff-based implementation,â€ Games
and Economic Behavior, vol. 75, no. 2, pp. 788â€“808, 2012.
[6] R. Chandan, D. Paccagnan, and J. R. Marden, â€œWhen Smoothness
is Not Enough: Toward Exact Quantification and Optimization of the
Price-of-Anarchy,â€ in 58th IEEE Conference on Decision and Control,
pp. 4041â€“4046, 2019.
[7] K. Paarporn, B. Canty, P. N. Brown, M. Alizadeh, and J. R. Marden,
â€œThe Impact of Complex and Informed Adversarial Behavior in
Graphical Coordination Games,â€ IEEE Transactions on Control of
Network Systems, vol. 8, no. 1, pp. 200â€“211, 2020.
[8] J. R. Marden and A. Wierman, â€œDistributed welfare games,â€ Operations Research, vol. 61, no. 1, pp. 155â€“168, 2013.
[9] A. Kanakia, B. Touri, and N. Correll, â€œModeling multi-robot task allocation with limited information as global game,â€ Swarm Intelligence,
vol. 10, no. 2, pp. 147â€“160, 2016.
[10] C. Wang, C. Xu, X. Yao, and D. Tao, â€œEvolutionary generative adversarial networks,â€ IEEE Transactions on Evolutionary Computation,
vol. 23, no. 6, pp. 921â€“934, 2019.
[11] U. Garciarena, R. Santana, and A. Mendiburu, â€œEvolved gans for
generating pareto set approximations,â€ in Proceedings of the Genetic
and Evolutionary Computation Conference, pp. 434â€“441, 2018.

[12] V. Costa, N. LourencÌ§o, J. Correia, and P. Machado, â€œCoegan: evaluating the coevolution effect in generative adversarial networks,â€ in
Proceedings of the Genetic and Evolutionary Computation Conference, pp. 374â€“382, 2019.
[13] A. R. Tilman, J. R. Watson, and S. Levin, â€œMaintaining cooperation
in social-ecological systems,â€ Theoretical Ecology, vol. 10, no. 2,
pp. 155â€“165, 2017.
[14] A. R. Tilman, J. B. Plotkin, and E. AkcÌ§ay, â€œEvolutionary games with
environmental feedbacks,â€ Nature communications, vol. 11, no. 1,
pp. 1â€“11, 2020.
[15] S. Skoulakis, T. Fiez, R. Sim, G. Piliouras, and L. Ratliff, â€œEvolutionary game theory squared: Evolving agents in endogenously evolving
zero-sum games,â€ arXiv preprint arXiv:2012.08382, 2020.
[16] J. S. Weitz, C. Eksin, K. Paarporn, S. P. Brown, and W. C. Ratcliff,
â€œAn oscillating tragedy of the commons in replicator dynamics with
game-environment feedback,â€ Proceedings of the National Academy
of Sciences, vol. 113, no. 47, pp. E7518â€“E7525, 2016.
[17] J. A. Weill, M. Stigler, O. Deschenes, and M. R. Springborn, â€œSocial
distancing responses to covid-19 emergency declarations strongly
differentiated by income,â€ Proceedings of the National Academy of
Sciences, vol. 117, no. 33, pp. 19658â€“19660, 2020.
[18] B. Jeffrey, C. E. Walters, K. E. Ainslie, O. Eales, C. Ciavarella,
S. Bhatia, S. Hayes, M. Baguelin, A. Boonyasiri, N. F. Brazeau, et al.,
â€œAnonymised and aggregated crowd level mobility data from mobile
phones suggests that initial compliance with covid-19 social distancing
interventions was high and geographically consistent across the uk,â€
Wellcome Open Research, vol. 5, 2020.
[19] J. Jay, J. Bor, E. O. Nsoesie, S. K. Lipson, D. K. Jones, S. Galea, and
J. Raifman, â€œNeighbourhood income and physical distancing during
the covid-19 pandemic in the united states,â€ Nature human behaviour,
vol. 4, no. 12, pp. 1294â€“1302, 2020.
[20] C. AloÌs-Ferrer and N. Netzer, â€œThe logit-response dynamics,â€ Games
and Economic Behavior, vol. 68, no. 2, pp. 413â€“427, 2010.
[21] P. N. Brown, H. P. Borowski, and J. R. Marden, â€œSecurity against
impersonation attacks in distributed systems,â€ IEEE Transactions on
Control of Network Systems, vol. 6, no. 1, pp. 440â€“450, 2019.
[22] B. S. Pradelski and H. P. Young, â€œLearning efficient nash equilibria in
distributed systems,â€ Games and Economic behavior, vol. 75, no. 2,
pp. 882â€“897, 2012.
[23] C. Nowzari, V. M. Preciado, and G. J. Pappas, â€œAnalysis and Control of
Epidemics: A Survey of Spreading Processes on Complex Networks,â€
IEEE Control Systems, vol. 36, no. 1, pp. 26â€“46, 2016.
[24] K. Paarporn, C. Eksin, J. S. Weitz, and J. S. Shamma, â€œNetworked
SIS Epidemics with Awareness,â€ IEEE Transactions on Computational
Social Systems, vol. 4, no. 3, pp. 93â€“103, 2017.

A PPENDIX
Proof of Lemma 3: Let a, a0 âˆˆ A such that a0 â‰¥A a. We
0
proceed by proving ba,a : r(a) â†’ S(a0 ) is a bijection; the
other bijection statements are proved similarly.
0
We begin by proving injectiveness, that is ba,a (z) =
0
ba,a (z 0 ) =â‡’ z = z 0 for z, z 0 âˆˆ r(a). Observe g(a, z) =
0
0
g(a0 , ba,a (z)) = g(a0 , ba,a (z 0 )) = g(a, z 0 ) where the first
0
and third inequalities follow by definition of ba,a and the
middle by hypothesis. Injectiveness follows from g(a, z) =
g(a, z 0 ) meaning a, z and a, z 0 differ by the same agentâ€™s
unilateral deviation. In that context, the possible actions
agent g(a, z) is given by Ag(a,z) \ {ag(a,z) } which is a
singleton by the binary action property, leaving only one
possible state a could transition to in r(a) via a unilateral
deviation. Thus z = z 0 as desired.
Next we show surjection, that is for any z 0 âˆˆ S(a, a0 ) there
0
exists a z âˆˆ r(a) such that ba,a (z) = z 0 , for a, a0 âˆˆ A and
a â‰¤A a0 . By definition of S(a, a0 ), z 0  a0 , but as z 0 âˆˆ f (a0 )
z 0 , a0 differ by only a single unilateral deviation by some
agent i. By partial ordering â‰¤A we may infer a0i = 1, zi0 = 0
else z 0  a0 would be violated. Further, we may infer a = 1
as suppose a = 0, then z 0 âˆˆ Q(a, a0 ), giving a contradiction

to the definition of z 0 . It is easy to see by definition of r(a)
that ai = 1 =â‡’ z âˆˆ r(a) satisfying g(a, z) = g(a0 , z 0 ) as
zi 6= ai but zâˆ’i = aâˆ’i by z âˆˆ f (a). Note g(a, z) = g(a0 , z 0 )
0
is always satisfied when ba,a (z) = z 0 by definition of the
function.


